"""
Deduplication Tool - ì¤‘ë³µ ì²­í¬ ì œê±° ë„êµ¬
"""
import uuid
from typing import List
from datetime import datetime
from loguru import logger

try:
    from langchain_core.tools import BaseTool
except ImportError:
    from langchain.tools import BaseTool

from app.tools.contracts import SearchToolResult, SearchChunk, ToolMetrics


class DeduplicateTool(BaseTool):
    """
    ì¤‘ë³µ ì œê±° ë„êµ¬
    
    ì±…ì„:
    - ë™ì¼ íŒŒì¼ì˜ ë™ì¼/ìœ ì‚¬ ì²­í¬ ì œê±°
    - ë‚´ìš© í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ê°ì§€
    
    ì „ëµ:
    - ê°™ì€ file_id + chunk_id â†’ ì™„ì „ ì¤‘ë³µ
    - ê°™ì€ file_id + ë‚´ìš© ìœ ì‚¬ë„ > threshold â†’ ìœ ì‚¬ ì¤‘ë³µ
    """
    name: str = "deduplicate"
    description: str = """ì¤‘ë³µëœ ì²­í¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤. ê°™ì€ íŒŒì¼ì—ì„œ ì¤‘ë³µ ì²­í¬ê°€ ìˆê±°ë‚˜ 
ë‚´ìš©ì´ ê±°ì˜ ë™ì¼í•œ ì²­í¬ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤."""
    version: str = "1.0.0"
    
    async def _arun(
        self,
        chunks: List[SearchChunk],
        similarity_threshold: float = 0.95,
        keep_strategy: str = "highest_score",
        **kwargs
    ) -> SearchToolResult:
        """
        ì¤‘ë³µ ì œê±° ì‹¤í–‰
        
        Args:
            chunks: ì…ë ¥ ì²­í¬ ëª©ë¡
            similarity_threshold: ìœ ì‚¬ ì¤‘ë³µ íŒë‹¨ ì„ê³„ê°’
            keep_strategy: ìœ ì§€ ì „ëµ (highest_score/first/last)
        """
        start_time = datetime.utcnow()
        trace_id = str(uuid.uuid4())
        
        try:
            if not chunks:
                return SearchToolResult(
                    success=True, data=[], total_found=0, filtered_count=0,
                    search_params={}, metrics=ToolMetrics(latency_ms=0, provider="internal"),
                    errors=[], trace_id=trace_id, tool_name=self.name, tool_version=self.version
                )
            
            logger.info(f"ğŸ”§ [Dedupe] ì…ë ¥: {len(chunks)}ê°œ ì²­í¬")
            
            # 1) ì™„ì „ ì¤‘ë³µ ì œê±° (ê°™ì€ chunk_id)
            seen_ids = set()
            unique_chunks = []
            for chunk in chunks:
                key = f"{chunk.file_id}:{chunk.chunk_id}"
                if key not in seen_ids:
                    seen_ids.add(key)
                    unique_chunks.append(chunk)
            
            logger.info(f"   - ì™„ì „ ì¤‘ë³µ ì œê±° í›„: {len(unique_chunks)}ê°œ")
            
            # 2) ìœ ì‚¬ ì¤‘ë³µ ì œê±° (ê°„ë‹¨í•œ ë‚´ìš© ë¹„êµ)
            final_chunks = []
            for chunk in unique_chunks:
                is_duplicate = False
                content_lower = chunk.content.lower().strip()
                
                for existing in final_chunks:
                    if chunk.file_id != existing.file_id:
                        continue
                    
                    existing_lower = existing.content.lower().strip()
                    
                    # ë‹¨ìˆœ í¬í•¨ ê´€ê³„ ì²´í¬
                    if len(content_lower) < len(existing_lower):
                        shorter, longer = content_lower, existing_lower
                    else:
                        shorter, longer = existing_lower, content_lower
                    
                    # ì§§ì€ ê²ƒì´ ê¸´ ê²ƒì— ê±°ì˜ í¬í•¨ë˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                    if len(shorter) > 0:
                        overlap = sum(1 for c in shorter if c in longer) / len(shorter)
                        if overlap >= similarity_threshold:
                            is_duplicate = True
                            logger.debug(f"   - ìœ ì‚¬ ì¤‘ë³µ ê°ì§€: overlap={overlap:.2f}")
                            break
                
                if not is_duplicate:
                    final_chunks.append(chunk)
                elif keep_strategy == "highest_score" and chunk.similarity_score > existing.similarity_score:
                    # ì ìˆ˜ê°€ ë” ë†’ìœ¼ë©´ êµì²´
                    final_chunks.remove(existing)
                    final_chunks.append(chunk)
            
            logger.info(f"   - ìœ ì‚¬ ì¤‘ë³µ ì œê±° í›„: {len(final_chunks)}ê°œ")
            
            # 3) ì •ë ¬ (ì ìˆ˜ ê¸°ì¤€)
            final_chunks.sort(key=lambda x: x.similarity_score, reverse=True)
            
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(f"âœ… [Dedupe] ì™„ë£Œ: {len(chunks)} â†’ {len(final_chunks)}ê°œ, latency={latency_ms:.1f}ms")
            
            return SearchToolResult(
                success=True,
                data=final_chunks,
                total_found=len(chunks),
                filtered_count=len(final_chunks),
                search_params={
                    "input_count": len(chunks),
                    "similarity_threshold": similarity_threshold,
                    "keep_strategy": keep_strategy
                },
                metrics=ToolMetrics(latency_ms=latency_ms, provider="internal"),
                errors=[],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        except Exception as e:
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            logger.error(f"âŒ [Dedupe] ì‹¤íŒ¨: {e}", exc_info=True)
            return SearchToolResult(
                success=False, data=chunks, total_found=len(chunks), filtered_count=len(chunks),
                search_params={}, metrics=ToolMetrics(latency_ms=latency_ms, provider="internal"),
                errors=[str(e)], trace_id=trace_id, tool_name=self.name, tool_version=self.version
            )
    
    def _run(self, **kwargs) -> SearchToolResult:
        import asyncio
        try:
            return asyncio.run(self._arun(**kwargs))
        except RuntimeError:
            chunks = kwargs.get("chunks", [])
            return SearchToolResult(
                success=False, data=chunks, total_found=len(chunks), filtered_count=len(chunks),
                search_params={}, metrics=ToolMetrics(latency_ms=0, provider="internal"),
                errors=["use _arun"], trace_id=str(uuid.uuid4()),
                tool_name=self.name, tool_version=self.version
            )
    
    def validate_input(self, **kwargs) -> bool:
        return "chunks" in kwargs and isinstance(kwargs["chunks"], list)


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
deduplicate_tool = DeduplicateTool()

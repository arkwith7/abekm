"""
Reranking Tool - ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”
Cross-encoder ëª¨ë¸ ê¸°ë°˜ ì •í™•ë„ ê°œì„ 
"""
import os
import uuid
from typing import List, Optional, Dict, Any
from datetime import datetime
from loguru import logger

from app.tools.contracts import SearchChunk, ToolResult, ToolMetrics
from langchain_core.tools import BaseTool


class RerankTool(BaseTool):
    """
    ì¬ìˆœìœ„í™” ë„êµ¬
    
    ê¸°ëŠ¥:
    - ê²€ìƒ‰ ê²°ê³¼ë¥¼ cross-encoder ëª¨ë¸ë¡œ ì¬ì ìˆ˜í™”
    - ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ì‹¤ì œ ê´€ë ¨ë„ë¥¼ ì •ë°€í•˜ê²Œ í‰ê°€
    - ì´ˆê¸° ê²€ìƒ‰(ë²¡í„°/í‚¤ì›Œë“œ)ë³´ë‹¤ ì •í™•í•œ ìˆœìœ„
    
    ì…ë ¥: ê²€ìƒ‰ ì²­í¬ ë¦¬ìŠ¤íŠ¸, ì¿¼ë¦¬
    ì¶œë ¥: ì¬ìˆœìœ„í™”ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    
    name: str = "rerank_tool"
    description: str = "Cross-encoderë¡œ ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”"
    
    def _run(self, *args, **kwargs):
        """ë™ê¸° ì‹¤í–‰ (ì§€ì›í•˜ì§€ ì•ŠìŒ)"""
        raise NotImplementedError("Use async _arun instead")
    
    async def _arun(
        self,
        chunks: List[SearchChunk],
        query: str,
        top_k: Optional[int] = None,
        model_name: str = "bge-reranker-base"
    ) -> ToolResult:
        """
        ì¬ìˆœìœ„í™” ì‹¤í–‰
        
        Args:
            chunks: ì…ë ¥ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            query: ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ Kê°œ (Noneì´ë©´ ì „ì²´)
            model_name: ì¬ìˆœìœ„í™” ëª¨ë¸ ì´ë¦„
        """
        start_time = datetime.utcnow()
        trace_id = f"rerank_{uuid.uuid4().hex[:8]}"
        
        try:
            if not chunks:
                logger.warning(f"[{trace_id}] ì…ë ¥ ì²­í¬ ì—†ìŒ")
                return ToolResult(
                    success=True,
                    data=[],
                    metrics=ToolMetrics(
                        latency_ms=0,
                        cost_estimate=0.0,
                        items_returned=0,
                        trace_id=trace_id
                    ),
                    errors=[],
                    trace_id=trace_id,
                    tool_name="rerank_tool",
                    tool_version="1.0.0"
                )
            
            logger.info(f"[{trace_id}] ì¬ìˆœìœ„í™” ì‹œì‘: {len(chunks)}ê°œ ì²­í¬")
            
            # Cross-encoder ì ìˆ˜ ê³„ì‚°
            reranked_chunks = await self._compute_cross_encoder_scores(
                chunks=chunks,
                query=query,
                model_name=model_name
            )
            
            # Top-K ì„ íƒ
            if top_k:
                reranked_chunks = reranked_chunks[:top_k]
            
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(f"[{trace_id}] ì¬ìˆœìœ„í™” ì™„ë£Œ: {len(reranked_chunks)}ê°œ ë°˜í™˜, {latency_ms:.1f}ms")
            
            return ToolResult(
                success=True,
                data=reranked_chunks,
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    cost_estimate=0.0,
                    items_returned=len(reranked_chunks),
                    trace_id=trace_id
                ),
                errors=[],
                trace_id=trace_id,
                tool_name="rerank_tool",
                tool_version="1.0.0"
            )
            
        except Exception as e:
            logger.error(f"[{trace_id}] ì¬ìˆœìœ„í™” ì‹¤íŒ¨: {e}", exc_info=True)
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
            return ToolResult(
                success=False,
                data=chunks,  # fallback
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    cost_estimate=0.0,
                    items_returned=len(chunks) if chunks else 0,
                    trace_id=trace_id
                ),
                errors=[str(e)],
                trace_id=trace_id,
                tool_name="rerank_tool",
                tool_version="1.0.0"
            )
    
    async def _compute_cross_encoder_scores(
        self,
        chunks: List[SearchChunk],
        query: str,
        model_name: str
    ) -> List[SearchChunk]:
        """
        LLM ê¸°ë°˜ ë¦¬ë­í‚¹ - Providerë³„ ë™ì  ì²˜ë¦¬
        
        RAG_RERANKING_PROVIDER ì„¤ì •ì— ë”°ë¼:
        - azure_openai: Azure OpenAI ëª¨ë¸ ì‚¬ìš©
        - bedrock: AWS Bedrock ëª¨ë¸ ì‚¬ìš©
        """
        from app.core.config import settings
        from langchain.schema import HumanMessage
        
        try:
            provider = settings.rag_reranking_provider
            logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ì œê³µì: {provider}")
            
            # Providerë³„ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            if provider == "azure_openai":
                from langchain_openai import AzureChatOpenAI
                
                rerank_endpoint = settings.rag_reranking_endpoint or settings.azure_openai_endpoint
                rerank_deployment = settings.rag_reranking_deployment
                rerank_api_key = settings.rag_reranking_api_key or settings.azure_openai_api_key
                rerank_api_version = settings.rag_reranking_api_version or settings.azure_openai_api_version
                
                if not rerank_deployment:
                    raise ValueError("RAG_RERANKING_DEPLOYMENT í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ëª¨ë¸: {rerank_deployment}")
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ì—”ë“œí¬ì¸íŠ¸: {rerank_endpoint}")
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ API ë²„ì „: {rerank_api_version}")
                
                # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
                deployment_lower = rerank_deployment.lower()
                is_reasoning_model = (
                    "gpt-5" in deployment_lower
                    or "nano" in deployment_lower
                    or "o1" in deployment_lower
                    or "o3" in deployment_lower
                )
                
                if is_reasoning_model:
                    model_kwargs: Dict[str, Any] = {
                        "max_completion_tokens": settings.rag_reranking_max_completion_tokens,
                    }
                    if settings.rag_reranking_reasoning_effort:
                        model_kwargs["reasoning_effort"] = settings.rag_reranking_reasoning_effort
                    rerank_llm = AzureChatOpenAI(
                        azure_endpoint=rerank_endpoint,
                        api_key=rerank_api_key,
                        api_version=rerank_api_version,
                        azure_deployment=rerank_deployment,
                        model_kwargs=model_kwargs,
                    )
                else:
                    rerank_llm = AzureChatOpenAI(
                        azure_endpoint=rerank_endpoint,
                        api_key=rerank_api_key,
                        api_version=rerank_api_version,
                        azure_deployment=rerank_deployment,
                        temperature=settings.rag_reranking_temperature,
                        max_tokens=settings.rag_reranking_max_tokens,
                    )
                    
            elif provider == "bedrock":
                from langchain_aws import ChatBedrock
                
                rerank_model_id = settings.rag_reranking_bedrock_model_id or settings.bedrock_llm_model_id
                rerank_region = settings.rag_reranking_bedrock_region or settings.aws_region
                
                if not rerank_model_id:
                    raise ValueError("RAG_RERANKING_BEDROCK_MODEL_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ëª¨ë¸: {rerank_model_id}")
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ë¦¬ì „: {rerank_region}")
                
                rerank_llm = ChatBedrock(
                    model=rerank_model_id,
                    region_name=rerank_region,
                    model_kwargs={
                        "temperature": settings.rag_reranking_temperature,
                        "max_tokens": settings.rag_reranking_max_tokens,
                    }
                )
                
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ë­í‚¹ ì œê³µì: {provider}")
            
            # ë¦¬ë­í‚¹ í”„ë¡¬í”„íŠ¸ ìƒì„±
            chunks_text = "\n\n".join([
                f"ë¬¸ì„œ {i+1}:\n{chunk.content[:300]}"
                for i, chunk in enumerate(chunks)
            ])
            
            rerank_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬í•˜ì„¸ìš”.

ì§ˆë¬¸: "{query}"

ë¬¸ì„œë“¤:
{chunks_text}

ì§€ì‹œì‚¬í•­:
1. ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë¶€í„° ë‚®ì€ ìˆœì„œë¡œ ë²ˆí˜¸ë¥¼ ë‚˜ì—´í•˜ì„¸ìš”.
2. ë‹µë³€ í˜•ì‹: ìˆ«ìë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„ (ì˜ˆ: 3,1,5,2,4,6,7)
3. ëª¨ë“  ë¬¸ì„œ ë²ˆí˜¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ê´€ë ¨ë„ê°€ ë†’ì€ ìˆœì„œ:"""
            
            # ë¦¬ë­í‚¹ ì‹¤í–‰
            response = await rerank_llm.ainvoke([HumanMessage(content=rerank_prompt)])
            rerank_response = response.content if hasattr(response, 'content') else str(response)
            
            # ë””ë²„ê¹…: ì›ë³¸ ì‘ë‹µ ë¡œê·¸
            logger.debug(f"ğŸ” LLM ë¦¬ë­í‚¹ ì›ë³¸ ì‘ë‹µ (ì²˜ìŒ 200ì): {str(rerank_response)[:200]}")
            
            # ì‘ë‹µ íŒŒì‹± (ë” ê²¬ê³ í•œ ë¡œì§)
            import re
            
            # ì‘ë‹µì—ì„œ ìˆ«ì ì¶”ì¶œ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
            # ì˜ˆ: "3,1,5,2,4" or "3, 1, 5" or "ìˆœì„œ: 3 1 5" or "[3,1,5]"
            rerank_response_str = str(rerank_response)
            
            # 1. ì‰¼í‘œ/ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ìˆ«ì ì°¾ê¸°
            numbers = re.findall(r'\d+', rerank_response_str)
            
            logger.debug(f"ğŸ” ì¶”ì¶œëœ ìˆ«ì: {numbers}")
            
            reranked_order = []
            for num_str in numbers:
                num = int(num_str) - 1  # 0-based index
                if 0 <= num < len(chunks) and num not in reranked_order:
                    reranked_order.append(num)
            
            logger.debug(f"ğŸ” ìœ íš¨í•œ ì¸ë±ìŠ¤: {reranked_order}")
            
            # ì¬ì •ë ¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìˆœì„œ ìœ ì§€
            if len(reranked_order) == 0:
                logger.warning(f"âš ï¸ ë¦¬ë­í‚¹ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ìœ ì§€")
                return chunks
            
            # ìƒˆë¡œìš´ ìˆœì„œë¡œ ì¬ì •ë ¬
            sorted_chunks = [chunks[i] for i in reranked_order if i < len(chunks)]
            
            # ë¦¬ë­í‚¹ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ì¶”ê°€
            remaining = [c for i, c in enumerate(chunks) if i not in reranked_order]
            sorted_chunks.extend(remaining)
            
            logger.info(f"âœ… LLM ë¦¬ë­í‚¹ ì™„ë£Œ: {len(reranked_order)}ê°œ ì¬ì •ë ¬")
            return sorted_chunks
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¦¬ë­í‚¹ ì‹¤íŒ¨, ì›ë³¸ ì ìˆ˜ ì‚¬ìš©: {e}")
            # í´ë°±: ê¸°ì¡´ ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            sorted_chunks = sorted(
                chunks,
                key=lambda c: c.score,
                reverse=True
        )
        
        return sorted_chunks


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rerank_tool = RerankTool()

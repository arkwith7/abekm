"""
Reranking Tool - ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”
Cross-encoder ëª¨ë¸ ê¸°ë°˜ ì •í™•ë„ ê°œì„ 
"""
import os
import uuid
from typing import List, Optional, Dict, Any
from datetime import datetime
from loguru import logger

from app.tools.contracts import SearchChunk, ToolResult, ToolMetrics
from langchain_core.tools import BaseTool


class RerankTool(BaseTool):
    """
    ì¬ìˆœìœ„í™” ë„êµ¬
    
    ê¸°ëŠ¥:
    - ê²€ìƒ‰ ê²°ê³¼ë¥¼ cross-encoder ëª¨ë¸ë¡œ ì¬ì ìˆ˜í™”
    - ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ì‹¤ì œ ê´€ë ¨ë„ë¥¼ ì •ë°€í•˜ê²Œ í‰ê°€
    - ì´ˆê¸° ê²€ìƒ‰(ë²¡í„°/í‚¤ì›Œë“œ)ë³´ë‹¤ ì •í™•í•œ ìˆœìœ„
    
    ì…ë ¥: ê²€ìƒ‰ ì²­í¬ ë¦¬ìŠ¤íŠ¸, ì¿¼ë¦¬
    ì¶œë ¥: ì¬ìˆœìœ„í™”ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    
    name: str = "rerank_tool"
    description: str = "Cross-encoderë¡œ ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”"
    
    def _run(self, *args, **kwargs):
        """ë™ê¸° ì‹¤í–‰ (ì§€ì›í•˜ì§€ ì•ŠìŒ)"""
        raise NotImplementedError("Use async _arun instead")
    
    async def _arun(
        self,
        chunks: List[SearchChunk],
        query: str,
        top_k: Optional[int] = None,
        model_name: str = "bge-reranker-base",
        threshold: float = 0.3  # ê´€ë ¨ì„± ì„ê³„ê°’ ì¶”ê°€
    ) -> ToolResult:
        """
        ì¬ìˆœìœ„í™” ì‹¤í–‰
        
        Args:
            chunks: ì…ë ¥ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            query: ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ Kê°œ (Noneì´ë©´ ì „ì²´)
            model_name: ì¬ìˆœìœ„í™” ëª¨ë¸ ì´ë¦„
            threshold: ê´€ë ¨ì„± ì ìˆ˜ ì„ê³„ê°’ (0.0~1.0)
        """
        start_time = datetime.utcnow()
        trace_id = f"rerank_{uuid.uuid4().hex[:8]}"
        
        try:
            if not chunks:
                logger.warning(f"[{trace_id}] ì…ë ¥ ì²­í¬ ì—†ìŒ")
                return ToolResult(
                    success=True,
                    data=[],
                    metrics=ToolMetrics(
                        latency_ms=0,
                        cost_estimate=0.0,
                        items_returned=0,
                        trace_id=trace_id
                    ),
                    errors=[],
                    trace_id=trace_id,
                    tool_name="rerank_tool",
                    tool_version="1.0.0"
                )
            
            logger.info(f"[{trace_id}] ì¬ìˆœìœ„í™” ì‹œì‘: {len(chunks)}ê°œ ì²­í¬, threshold={threshold}")
            
            # Cross-encoder ì ìˆ˜ ê³„ì‚°
            reranked_chunks = await self._compute_cross_encoder_scores(
                chunks=chunks,
                query=query,
                model_name=model_name,
                threshold=threshold
            )
            
            # Top-K ì„ íƒ
            if top_k:
                reranked_chunks = reranked_chunks[:top_k]
            
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(f"[{trace_id}] ì¬ìˆœìœ„í™” ì™„ë£Œ: {len(reranked_chunks)}ê°œ ë°˜í™˜ (í•„í„°ë§ë¨), {latency_ms:.1f}ms")
            
            return ToolResult(
                success=True,
                data=reranked_chunks,
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    cost_estimate=0.0,
                    items_returned=len(reranked_chunks),
                    trace_id=trace_id
                ),
                errors=[],
                trace_id=trace_id,
                tool_name="rerank_tool",
                tool_version="1.0.0"
            )
            
        except Exception as e:
            logger.error(f"[{trace_id}] ì¬ìˆœìœ„í™” ì‹¤íŒ¨: {e}", exc_info=True)
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
            return ToolResult(
                success=False,
                data=chunks,  # fallback
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    cost_estimate=0.0,
                    items_returned=len(chunks) if chunks else 0,
                    trace_id=trace_id
                ),
                errors=[str(e)],
                trace_id=trace_id,
                tool_name="rerank_tool",
                tool_version="1.0.0"
            )
    
    async def _compute_cross_encoder_scores(
        self,
        chunks: List[SearchChunk],
        query: str,
        model_name: str,
        threshold: float = 0.3
    ) -> List[SearchChunk]:
        """
        LLM ê¸°ë°˜ ë¦¬ë­í‚¹ - Providerë³„ ë™ì  ì²˜ë¦¬
        
        RAG_RERANKING_PROVIDER ì„¤ì •ì— ë”°ë¼:
        - azure_openai: Azure OpenAI ëª¨ë¸ ì‚¬ìš©
        - bedrock: AWS Bedrock ëª¨ë¸ ì‚¬ìš©
        """
        from app.core.config import settings
        from langchain_core.messages import HumanMessage
        
        try:
            provider = settings.rag_reranking_provider
            logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ì œê³µì: {provider}")
            
            # Providerë³„ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            if provider == "azure_openai":
                from langchain_openai import AzureChatOpenAI
                
                rerank_endpoint = settings.rag_reranking_endpoint or settings.azure_openai_endpoint
                rerank_deployment = settings.rag_reranking_deployment
                rerank_api_key = settings.rag_reranking_api_key or settings.azure_openai_api_key
                rerank_api_version = settings.rag_reranking_api_version or settings.azure_openai_api_version
                
                if not rerank_deployment:
                    raise ValueError("RAG_RERANKING_DEPLOYMENT í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ëª¨ë¸: {rerank_deployment}")
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ì—”ë“œí¬ì¸íŠ¸: {rerank_endpoint}")
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ API ë²„ì „: {rerank_api_version}")
                
                # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ì„¤ì •
                deployment_lower = rerank_deployment.lower()
                is_reasoning_model = (
                    "gpt-5" in deployment_lower
                    or "nano" in deployment_lower
                    or "o1" in deployment_lower
                    or "o3" in deployment_lower
                )
                
                if is_reasoning_model:
                    model_kwargs: Dict[str, Any] = {
                        "max_completion_tokens": settings.rag_reranking_max_completion_tokens,
                    }
                    if settings.rag_reranking_reasoning_effort:
                        model_kwargs["reasoning_effort"] = settings.rag_reranking_reasoning_effort
                    rerank_llm = AzureChatOpenAI(
                        azure_endpoint=rerank_endpoint,
                        api_key=rerank_api_key,
                        api_version=rerank_api_version,
                        azure_deployment=rerank_deployment,
                        model_kwargs=model_kwargs,
                    )
                else:
                    rerank_llm = AzureChatOpenAI(
                        azure_endpoint=rerank_endpoint,
                        api_key=rerank_api_key,
                        api_version=rerank_api_version,
                        azure_deployment=rerank_deployment,
                        temperature=settings.rag_reranking_temperature,
                        max_tokens=settings.rag_reranking_max_tokens,
                    )
                    
            elif provider == "bedrock":
                from langchain_aws import ChatBedrock, ChatBedrockConverse
                
                rerank_model_id = settings.rag_reranking_bedrock_model_id or settings.bedrock_llm_model_id
                rerank_region = settings.rag_reranking_bedrock_region or settings.aws_region
                
                if not rerank_model_id:
                    raise ValueError("RAG_RERANKING_BEDROCK_MODEL_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ëª¨ë¸: {rerank_model_id}")
                logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ë¦¬ì „: {rerank_region}")
                
                # êµì°¨ ë¦¬ì „ ì¶”ë¡  ëª¨ë¸ ê°ì§€ (us., eu., apac. ë“± í”„ë¦¬í”½ìŠ¤)
                is_cross_region = any(rerank_model_id.startswith(prefix) for prefix in ["us.", "eu.", "apac.", "global."])
                
                if is_cross_region:
                    # êµì°¨ ë¦¬ì „ ì¶”ë¡ : ChatBedrockConverse ì‚¬ìš©
                    logger.info(f"ğŸŒ êµì°¨ ë¦¬ì „ ë¦¬ë­í‚¹ ëª¨ë¸: {rerank_model_id}")
                    rerank_llm = ChatBedrockConverse(
                        model=rerank_model_id,
                        region_name=rerank_region,
                        max_tokens=settings.rag_reranking_max_tokens,
                        temperature=settings.rag_reranking_temperature,
                    )
                else:
                    # ë‹¨ì¼ ë¦¬ì „: ChatBedrock ì‚¬ìš©
                    rerank_llm = ChatBedrock(
                        model=rerank_model_id,
                        region_name=rerank_region,
                        model_kwargs={
                            "temperature": settings.rag_reranking_temperature,
                            "max_tokens": settings.rag_reranking_max_tokens,
                        }
                    )
                
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¦¬ë­í‚¹ ì œê³µì: {provider}")
            
            # ë¦¬ë­í‚¹ í”„ë¡¬í”„íŠ¸ ìƒì„±
            chunks_text = "\n\n".join([
                f"ë¬¸ì„œ {i+1}:\n{chunk.content[:300]}"
                for i, chunk in enumerate(chunks)
            ])
            
            rerank_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬í•˜ê³ , ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.

ì§ˆë¬¸: "{query}"

ë¬¸ì„œë“¤:
{chunks_text}

ì§€ì‹œì‚¬í•­:
1. ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë¶€í„° ë‚®ì€ ìˆœì„œë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
2. ê° ë¬¸ì„œì— ëŒ€í•´ 0.0~1.0 ì‚¬ì´ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš” (1.0: ë§¤ìš° ê´€ë ¨ë¨, 0.0: ì „í˜€ ê´€ë ¨ ì—†ìŒ).
3. ë‹µë³€ í˜•ì‹: ë¬¸ì„œë²ˆí˜¸:ì ìˆ˜ (ì˜ˆ: 3:0.95, 1:0.80, 5:0.30, 2:0.10)
4. ëª¨ë“  ë¬¸ì„œ ë²ˆí˜¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ê´€ë ¨ë„ê°€ ë†’ì€ ìˆœì„œ:"""
            
            # ë¦¬ë­í‚¹ ì‹¤í–‰
            response = await rerank_llm.ainvoke([HumanMessage(content=rerank_prompt)])
            rerank_response = response.content if hasattr(response, 'content') else str(response)
            
            # ë””ë²„ê¹…: ì›ë³¸ ì‘ë‹µ ë¡œê·¸
            logger.debug(f"ğŸ” LLM ë¦¬ë­í‚¹ ì›ë³¸ ì‘ë‹µ (ì²˜ìŒ 200ì): {str(rerank_response)[:200]}")
            
            # ì‘ë‹µ íŒŒì‹± (ë” ê²¬ê³ í•œ ë¡œì§)
            import re
            
            # ì‘ë‹µì—ì„œ "ìˆ«ì:ì ìˆ˜" íŒ¨í„´ ì¶”ì¶œ
            # ì˜ˆ: "3:0.95", "1: 0.8", "5 : 0.3"
            matches = re.findall(r'(\d+)\s*:\s*([0-9.]+)', str(rerank_response))
            
            logger.debug(f"ğŸ” ì¶”ì¶œëœ íŒ¨í„´: {matches}")
            
            reranked_chunks = []
            seen_indices = set()
            
            for idx_str, score_str in matches:
                try:
                    idx = int(idx_str) - 1  # 0-based index
                    score = float(score_str)
                    
                    if 0 <= idx < len(chunks) and idx not in seen_indices:
                        # ì ìˆ˜ ì„ê³„ê°’ í•„í„°ë§
                        if score >= threshold:
                            chunk = chunks[idx]
                            # ì ìˆ˜ ì—…ë°ì´íŠ¸ (ì„ íƒì‚¬í•­)
                            chunk.score = score
                            chunk.metadata["rerank_score"] = score
                            reranked_chunks.append(chunk)
                            seen_indices.add(idx)
                        else:
                            logger.debug(f"   - ë¬¸ì„œ {idx+1} ì œì™¸ (ì ìˆ˜ {score} < {threshold})")
                except ValueError:
                    continue
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜ (ì•ˆì „ì¥ì¹˜)
            if not matches and not reranked_chunks:
                logger.warning(f"âš ï¸ ë¦¬ë­í‚¹ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ìœ ì§€")
                return chunks
            
            logger.info(f"âœ… LLM ë¦¬ë­í‚¹ ì™„ë£Œ: {len(reranked_chunks)}/{len(chunks)}ê°œ ì„ íƒ (threshold={threshold})")
            return reranked_chunks
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¦¬ë­í‚¹ ì‹¤íŒ¨, ì›ë³¸ ì ìˆ˜ ì‚¬ìš©: {e}")
            # í´ë°±: ê¸°ì¡´ ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            return sorted(chunks, key=lambda x: x.score or 0, reverse=True)


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rerank_tool = RerankTool()

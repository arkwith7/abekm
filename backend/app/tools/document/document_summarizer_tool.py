"""
Document Summarizer Tool - ë¬¸ì„œ ìš”ì•½ í†µí•© ë„êµ¬
ë‘ ê°€ì§€ ì…ë ¥ ê²½ë¡œë¥¼ ëª¨ë‘ ì§€ì›:
1. DB ì €ì¥ ë¬¸ì„œ (Vector Store) - ì²­í¬ ì¡°íšŒ í›„ ìš”ì•½
2. ì²¨ë¶€ íŒŒì¼ (Upload) - í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ ìš”ì•½
"""
import uuid
from typing import List, Optional, Dict, Any
from datetime import datetime
from loguru import logger
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
import os

try:
    from langchain_core.tools import BaseTool
except ImportError:
    from langchain.tools import BaseTool

from app.tools.contracts import ToolResult, ToolMetrics, SearchChunk
from app.tools.document.document_loader_tool import document_loader_tool
from app.tools.retrieval.vector_search_tool import vector_search_tool
from app.tools.retrieval.keyword_search_tool import keyword_search_tool
from app.tools.processing.deduplicate_tool import deduplicate_tool
from app.tools.processing.rerank_tool import rerank_tool
from app.tools.context.context_builder_tool import context_builder_tool


class DocumentSummarizerTool(BaseTool):
    """
    ë¬¸ì„œ ìš”ì•½ í†µí•© ë„êµ¬
    
    ì±…ì„:
    - ë‘ ê°€ì§€ ì…ë ¥ ê²½ë¡œ ì²˜ë¦¬:
      1) DB ë¬¸ì„œ: file_id/document_id â†’ ì²­í¬ ë¡œë“œ â†’ ìš”ì•½
      2) ì²¨ë¶€ íŒŒì¼: file_path â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ìš”ì•½
    - í†µì¼ëœ ìš”ì•½ ê²°ê³¼ ë°˜í™˜
    
    ì‚¬ìš© ì¼€ì´ìŠ¤:
    - "ì„ íƒí•œ ë…¼ë¬¸ ìš”ì•½í•´ì¤˜" (DB ë¬¸ì„œ)
    - "ì²¨ë¶€ íŒŒì¼ ìš”ì•½í•´ì¤˜" (ì—…ë¡œë“œ íŒŒì¼)
    - "ì´ ë¬¸ì„œë“¤ì˜ ì£¼ìš” ë‚´ìš© ì •ë¦¬í•´ì¤˜"
    
    ì˜ì¡´ì„±:
    - DocumentLoaderTool: DB ë¬¸ì„œ ë¡œë“œ
    - Azure Document Intelligence: ìƒˆ íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - LLM Service: ìš”ì•½ ìƒì„±
    """
    name: str = "document_summarizer"
    description: str = """ë¬¸ì„œë¥¼ ìš”ì•½í•©ë‹ˆë‹¤. 
DBì— ì €ì¥ëœ ë¬¸ì„œì™€ ìƒˆë¡œ ì²¨ë¶€ëœ íŒŒì¼ ëª¨ë‘ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤."""
    version: str = "2.0.0"
    
    async def _arun(
        self,
        # Input 1: DB ë¬¸ì„œ
        document_ids: Optional[List[int]] = None,
        # Input 2: ì²¨ë¶€ íŒŒì¼
        attachment_paths: Optional[List[str]] = None,
        attachment_metadata: Optional[List[Dict[str, Any]]] = None,
        # ê³µí†µ íŒŒë¼ë¯¸í„°
        db_session: Optional[AsyncSession] = None,
        max_chunks: int = 50,
        summarization_type: str = "comprehensive",  # comprehensive | brief | bullet_points
        user_emp_no: Optional[str] = None,
        request_type: Optional[str] = None,
        query_text: Optional[str] = None,
        container_ids: Optional[List[str]] = None,
        search_document_ids: Optional[List[int]] = None,
        context_max_tokens: int = 4000,
        **kwargs
    ) -> ToolResult:
        """
        ë¬¸ì„œ ìš”ì•½ ì‹¤í–‰
        
        Args:
            document_ids: DB ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ (Input 1)
            attachment_paths: ì²¨ë¶€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (Input 2)
            attachment_metadata: ì²¨ë¶€ íŒŒì¼ ë©”íƒ€ë°ì´í„°
            db_session: DB ì„¸ì…˜ (Input 1 í•„ìˆ˜)
            max_chunks: ìµœëŒ€ ì²­í¬ ìˆ˜
            summarization_type: ìš”ì•½ ìœ í˜•
            user_emp_no: ì‚¬ìš©ì ì‚¬ë²ˆ
            request_type: ìš”ì²­ ìœ í˜• íŒíŠ¸ (chat_prompt | selected_documents | uploaded_files)
            query_text: ì±„íŒ… ì…ë ¥ ê¸°ë°˜ ìš”ì•½ ì‹œ ì‚¬ìš©í•  í…ìŠ¤íŠ¸
            container_ids: ê²€ìƒ‰ ë²”ìœ„ë¥¼ ì œí•œí•  ì»¨í…Œì´ë„ˆ ID ëª©ë¡
            search_document_ids: ê²€ìƒ‰ ì‹œ ìš°ì„  ê³ ë ¤í•  ë¬¸ì„œ ID ëª©ë¡
            context_max_tokens: ì»¨í…ìŠ¤íŠ¸ ë¹Œë” í† í° ìƒí•œ
        
        Returns:
            ToolResult: í†µí•© ìš”ì•½ ê²°ê³¼
        """
        start_time = datetime.utcnow()
        trace_id = str(uuid.uuid4())

        try:
            normalized_query_text = query_text.strip() if isinstance(query_text, str) else None
            query_text = normalized_query_text if normalized_query_text else None

            # ì…ë ¥ ê²€ì¦
            if not document_ids and not attachment_paths and not query_text:
                return ToolResult(
                    success=False,
                    data={"summary": "", "source_count": 0},
                    metrics=ToolMetrics(latency_ms=0, provider="internal", trace_id=trace_id),
                    errors=["ë¬¸ì„œ ID, ì²¨ë¶€ íŒŒì¼, ë˜ëŠ” ìš”ì•½ ëŒ€ìƒ í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ì§€ ì•ŠìŒ"],
                    trace_id=trace_id,
                    tool_name=self.name,
                    tool_version=self.version
                )
            
            resolved_request_type = self._resolve_request_type(
                explicit=request_type,
                document_ids=document_ids,
                attachment_paths=attachment_paths,
                query_text=query_text
            )

            logger.info(
                f"ğŸ§­ [Summarizer] ìš”ì²­ ìœ í˜•={resolved_request_type}, "
                f"documents={len(document_ids or [])}, attachments={len(attachment_paths or [])}, "
                f"query_present={bool(query_text)}"
            )

            all_chunks = []
            source_info = {
                "db_documents": 0,
                "uploaded_files": 0,
                "total_chunks": 0,
                "extraction_errors": [],
                "request_type": resolved_request_type
            }
            
            # ===== Input 1: DB ë¬¸ì„œ ì²˜ë¦¬ =====
            if resolved_request_type in {"selected_documents", "auto"} and document_ids and db_session:
                logger.info(f"ğŸ“š [Summarizer] DB ë¬¸ì„œ ë¡œë“œ: {len(document_ids)}ê°œ")
                
                try:
                    # DocumentLoaderTool ì‚¬ìš©
                    loader_result = await document_loader_tool._arun(
                        document_ids=document_ids,
                        db_session=db_session,
                        max_chunks=max_chunks,
                        user_emp_no=user_emp_no
                    )
                    
                    if loader_result.success and loader_result.data:
                        all_chunks.extend(loader_result.data)
                        source_info["db_documents"] = len(set(c.file_id for c in loader_result.data))
                        logger.info(f"âœ… [Summarizer] DB ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(loader_result.data)}ê°œ ì²­í¬")
                    else:
                        error_msg = f"DB ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {loader_result.errors}"
                        logger.warning(f"âš ï¸ [Summarizer] {error_msg}")
                        source_info["extraction_errors"].append(error_msg)
                        
                except Exception as e:
                    error_msg = f"DB ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                    logger.error(f"âŒ [Summarizer] {error_msg}")
                    source_info["extraction_errors"].append(error_msg)
            
            # ===== Input 2: ì²¨ë¶€ íŒŒì¼ ì²˜ë¦¬ =====
            if attachment_paths and resolved_request_type in {"uploaded_files", "auto", "selected_documents"}:
                logger.info(f"ğŸ“ [Summarizer] ì²¨ë¶€ íŒŒì¼ ì²˜ë¦¬: {len(attachment_paths)}ê°œ")
                
                for idx, file_path in enumerate(attachment_paths):
                    try:
                        metadata = attachment_metadata[idx] if attachment_metadata and idx < len(attachment_metadata) else {}
                        
                        # íŒŒì¼ ì¡´ì¬ í™•ì¸
                        if not os.path.exists(file_path):
                            error_msg = f"íŒŒì¼ ì—†ìŒ: {file_path}"
                            logger.warning(f"âš ï¸ [Summarizer] {error_msg}")
                            source_info["extraction_errors"].append(error_msg)
                            continue
                        
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        extracted_text = await self._extract_text_from_file(
                            file_path=file_path,
                            mime_type=metadata.get("mime_type", "application/pdf")
                        )
                        
                        if extracted_text:
                            # SearchChunk í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                            from app.tools.contracts import SearchChunk
                            
                            chunk = SearchChunk(
                                chunk_id=f"upload_{idx}_{uuid.uuid4().hex[:8]}",
                                file_id=f"upload_{idx}",
                                content=extracted_text,
                                score=1.0,
                                match_type="file_upload",
                                metadata={
                                    "file_name": metadata.get("file_name", os.path.basename(file_path)),
                                    "file_type": metadata.get("mime_type", "unknown"),
                                    "source": "upload",
                                    "extraction_method": "azure_di"
                                }
                            )
                            all_chunks.append(chunk)
                            source_info["uploaded_files"] += 1
                            logger.info(f"âœ… [Summarizer] íŒŒì¼ ì¶”ì¶œ ì™„ë£Œ: {metadata.get('file_name', file_path)}")
                        else:
                            error_msg = f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {file_path}"
                            logger.warning(f"âš ï¸ [Summarizer] {error_msg}")
                            source_info["extraction_errors"].append(error_msg)
                            
                    except Exception as e:
                        error_msg = f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({file_path}): {str(e)}"
                        logger.error(f"âŒ [Summarizer] {error_msg}")
                        source_info["extraction_errors"].append(error_msg)
            
            # ===== Input 3: ì±„íŒ… ì§ˆì˜ ê¸°ë°˜ ìš”ì•½ =====
            if resolved_request_type in {"chat_prompt", "auto"} and query_text:
                retrieval_chunks, retrieval_info = await self._build_chunks_from_query(
                    query_text=query_text,
                    db_session=db_session,
                    max_candidates=max_chunks * 2,
                    container_ids=container_ids,
                    document_filter=search_document_ids,
                    user_emp_no=user_emp_no,
                    context_max_tokens=context_max_tokens
                )

                if retrieval_chunks:
                    all_chunks.extend(retrieval_chunks)
                    source_info.setdefault("retrieval_pipeline", retrieval_info)
                    logger.info(
                        f"âœ… [Summarizer] ì§ˆì˜ ê¸°ë°˜ ì²­í¬ í™•ë³´: {len(retrieval_chunks)}ê°œ (context_included={retrieval_info.get('context', {}).get('included', 0)})"
                    )
                else:
                    source_info["extraction_errors"].append("ì§ˆì˜ ê¸°ë°˜ ì²­í¬ í™•ë³´ ì‹¤íŒ¨")
                    logger.warning("âš ï¸ [Summarizer] ì§ˆì˜ ê¸°ë°˜ ì²­í¬ë¥¼ í™•ë³´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")

            # ===== ì²­í¬ ìˆ˜ì§‘ ì™„ë£Œ í™•ì¸ =====
            if not all_chunks:
                error_messages = source_info["extraction_errors"] or ["ë¬¸ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ"]
                return ToolResult(
                    success=False,
                    data={
                        "summary": "",
                        "source_info": source_info
                    },
                    metrics=ToolMetrics(
                        latency_ms=(datetime.utcnow() - start_time).total_seconds() * 1000,
                        provider="internal",
                        trace_id=trace_id
                    ),
                    errors=error_messages,
                    trace_id=trace_id,
                    tool_name=self.name,
                    tool_version=self.version
                )
            
            source_info["total_chunks"] = len(all_chunks)
            
            # ===== ìš”ì•½ ìƒì„± =====
            logger.info(f"ğŸ“ [Summarizer] ìš”ì•½ ìƒì„± ì‹œì‘: {len(all_chunks)}ê°œ ì²­í¬, type={summarization_type}")
            
            summary = await self._generate_summary(
                chunks=all_chunks,
                summarization_type=summarization_type
            )
            
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(
                f"âœ… [Summarizer] ìš”ì•½ ì™„ë£Œ: DB={source_info['db_documents']}, "
                f"Upload={source_info['uploaded_files']}, "
                f"latency={latency_ms:.1f}ms"
            )
            
            return ToolResult(
                success=True,
                data={
                    "summary": summary,
                    "source_info": source_info,
                    "chunks": [
                        {
                            "file_name": c.metadata.get("file_name", "Unknown"),
                            "source": c.metadata.get("source", "db"),
                            "content_preview": c.content[:200]
                        }
                        for c in all_chunks[:10]  # ìµœëŒ€ 10ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
                    ]
                },
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="internal",
                    items_returned=len(all_chunks),
                    trace_id=trace_id
                ),
                errors=source_info["extraction_errors"],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        except Exception as e:
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            error_msg = f"ë¬¸ì„œ ìš”ì•½ ì‹¤íŒ¨: {str(e)}"
            logger.error(f"âŒ [Summarizer] {error_msg}")
            
            return ToolResult(
                success=False,
                data={"summary": "", "source_info": {}},
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="internal",
                    trace_id=trace_id
                ),
                errors=[error_msg],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
    
    async def _extract_text_from_file(
        self,
        file_path: str,
        mime_type: str
    ) -> Optional[str]:
        """
        ì²¨ë¶€ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Azure Document Intelligence ì‚¬ìš©
        """
        try:
            from app.services.document.azure_document_intelligence_service import azure_di_service  # type: ignore[import-error]
            
            logger.info(f"ğŸ” [Summarizer] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {os.path.basename(file_path)}")
            
            # Azure DIë¡œ ë¬¸ì„œ ë¶„ì„
            result = await azure_di_service.analyze_document(
                file_path=file_path,
                analysis_type="layout"  # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            )
            
            if not result or "error" in result:
                logger.error(f"âŒ [Summarizer] Azure DI ë¶„ì„ ì‹¤íŒ¨: {result.get('error') if result else 'No result'}")
                return None
            
            # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ê²°í•©
            pages = result.get("pages", [])
            if not pages:
                logger.warning(f"âš ï¸ [Summarizer] í˜ì´ì§€ ì—†ìŒ: {file_path}")
                return None
            
            all_text = []
            for page in pages:
                page_text = page.get("text", "")
                if page_text:
                    all_text.append(f"[Page {page.get('page_number', '?')}]\n{page_text}")
            
            extracted = "\n\n".join(all_text)
            logger.info(f"âœ… [Summarizer] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(extracted)}ì")
            
            return extracted
            
        except Exception as e:
            logger.error(f"âŒ [Summarizer] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    async def _generate_summary(
        self,
        chunks: List,
        summarization_type: str
    ) -> str:
        """
        ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ ìƒì„±
        
        LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ (í˜„ì¬ëŠ” ë‹¨ìˆœ ê²°í•©, í–¥í›„ LLM í†µí•©)
        """
        try:
            from app.services.ai_service import ai_service  # type: ignore[import-error]
            
            # ì²­í¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            context_parts = []
            for chunk in chunks:
                file_name = chunk.metadata.get("file_name", "Unknown")
                page_num = chunk.metadata.get("page_number", "?")
                context_parts.append(f"[{file_name} - p.{page_num}]\n{chunk.content}")
            
            context_text = "\n\n---\n\n".join(context_parts)
            
            # ìš”ì•½ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸
            prompts = {
                "comprehensive": "ìœ„ ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ì„¹ì…˜ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                "brief": "ìœ„ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ëµíˆ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                "bullet_points": "ìœ„ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(â€¢)ë¥¼ ì‚¬ìš©í•˜ì—¬ 5-7ê°œ í•­ëª©ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
            }
            
            prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

{context_text}

{prompts.get(summarization_type, prompts['comprehensive'])}"""
            
            summary = await ai_service.generate_completion(
                prompt=prompt,
                max_tokens=2000,
                temperature=0.3
            )
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ [Summarizer] ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²°í•©
            return "\n\n".join([c.content[:500] for c in chunks[:5]])
    
    def _run(self, *args, **kwargs):
        """ë™ê¸° ì‹¤í–‰ì€ ì§€ì›í•˜ì§€ ì•ŠìŒ"""
        raise NotImplementedError("DocumentSummarizerToolì€ ë¹„ë™ê¸° ì‹¤í–‰ë§Œ ì§€ì›í•©ë‹ˆë‹¤. _arun()ì„ ì‚¬ìš©í•˜ì„¸ìš”.")

    def _resolve_request_type(
        self,
        explicit: Optional[str],
        document_ids: Optional[List[int]],
        attachment_paths: Optional[List[str]],
        query_text: Optional[str]
    ) -> str:
        if explicit:
            return explicit
        if attachment_paths:
            return "uploaded_files"
        if document_ids:
            return "selected_documents"
        if query_text:
            return "chat_prompt"
        return "unknown"

    async def _build_chunks_from_query(
        self,
        query_text: str,
        db_session: Optional[AsyncSession],
        max_candidates: int,
        container_ids: Optional[List[str]],
        document_filter: Optional[List[int]],
        user_emp_no: Optional[str],
        context_max_tokens: int
    ) -> tuple[List[SearchChunk], Dict[str, Any]]:
        if not db_session:
            logger.warning("âš ï¸ [Summarizer] DB ì„¸ì…˜ ì—†ì´ ì§ˆì˜ ê¸°ë°˜ ìš”ì•½ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return [], {"error": "db_session_required"}

        normalized_filter = [str(doc_id) for doc_id in document_filter] if document_filter else None

        retrieval_info: Dict[str, Any] = {
            "vector": {},
            "keyword": {},
            "dedupe": {},
            "rerank": {},
            "context": {}
        }

        combined_chunks: List[SearchChunk] = []

        # Vector Search
        vector_result = await vector_search_tool._arun(
            query=query_text,
            db_session=db_session,
            top_k=max_candidates,
            container_ids=container_ids,
            document_ids=normalized_filter,
            user_emp_no=user_emp_no
        )
        if vector_result.success:
            combined_chunks.extend(vector_result.data)
        else:
            retrieval_info["vector"]["errors"] = vector_result.errors
        retrieval_info["vector"].update({
            "count": len(vector_result.data),
            "latency_ms": vector_result.metrics.latency_ms
        })

        # Keyword Search (fallback/augmentation)
        keyword_result = await keyword_search_tool._arun(
            query=query_text,
            db_session=db_session,
            top_k=max_candidates,
            container_ids=container_ids,
            document_ids=normalized_filter,
            user_emp_no=user_emp_no
        )
        if keyword_result.success:
            combined_chunks.extend(keyword_result.data)
        else:
            retrieval_info["keyword"]["errors"] = keyword_result.errors
        retrieval_info["keyword"].update({
            "count": len(keyword_result.data),
            "latency_ms": keyword_result.metrics.latency_ms
        })

        if not combined_chunks:
            return [], retrieval_info

        # Deduplicate
        dedupe_result = await deduplicate_tool._arun(chunks=combined_chunks)
        deduped_chunks = dedupe_result.data if dedupe_result.success else combined_chunks
        retrieval_info["dedupe"].update({
            "input": len(combined_chunks),
            "output": len(deduped_chunks),
            "latency_ms": dedupe_result.metrics.latency_ms,
            "errors": dedupe_result.errors if not dedupe_result.success else []
        })

        # Rerank
        rerank_result = await rerank_tool._arun(
            chunks=deduped_chunks,
            query=query_text,
            top_k=min(max_candidates, max(len(deduped_chunks), 1))
        )
        reranked_chunks = rerank_result.data if rerank_result.success else deduped_chunks
        retrieval_info["rerank"].update({
            "input": len(deduped_chunks),
            "output": len(reranked_chunks),
            "latency_ms": rerank_result.metrics.latency_ms,
            "errors": rerank_result.errors if not rerank_result.success else []
        })

        # Context Builder (token pack)
        context_result = await context_builder_tool._arun(
            chunks=reranked_chunks,
            max_tokens=context_max_tokens,
            include_metadata=True,
            priority_by="hybrid"
        )
        used_chunks = context_result.used_chunks if context_result.success and context_result.used_chunks else reranked_chunks[:max_candidates]
        retrieval_info["context"].update({
            "included": len(used_chunks),
            "tokens": context_result.total_tokens if context_result.success else None,
            "latency_ms": context_result.metrics.latency_ms,
            "errors": context_result.errors if not context_result.success else []
        })

        return used_chunks, retrieval_info


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
document_summarizer_tool = DocumentSummarizerTool()

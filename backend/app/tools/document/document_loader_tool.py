"""
Document Loader Tool - ë¬¸ì„œ ì „ì²´ ë¡œë“œ ë„êµ¬
ìš”ì•½, ë¶„ì„ ë“±ì„ ìœ„í•´ íŠ¹ì • ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ë¥¼ ë¡œë“œ
"""
import uuid
from typing import List, Optional, Dict, Any
from datetime import datetime
from loguru import logger
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

try:
    from langchain_core.tools import BaseTool
except ImportError:
    from langchain.tools import BaseTool

from app.tools.contracts import (
    SearchToolResult, SearchChunk, ToolMetrics
)


class DocumentLoaderTool(BaseTool):
    """
    ë¬¸ì„œ ë¡œë” ë„êµ¬
    
    ì±…ì„:
    - íŠ¹ì • ë¬¸ì„œ(ë“¤)ì˜ ëª¨ë“  ì²­í¬ë¥¼ í˜ì´ì§€/ì²­í¬ ìˆœì„œëŒ€ë¡œ ë¡œë“œ
    - ìš”ì•½, ì „ì²´ ë‚´ìš© í™•ì¸ ë“±ì— ì‚¬ìš©
    - ê²€ìƒ‰ ì—†ì´ ì§ì ‘ ë¡œë“œ (ë¹ ë¥¸ ì‘ë‹µ)
    
    ì‚¬ìš© ì¼€ì´ìŠ¤:
    - ì‚¬ìš©ìê°€ ë¬¸ì„œë¥¼ ì„ íƒí•˜ê³  "ìš”ì•½í•´ì¤˜" ìš”ì²­
    - ë¬¸ì„œ ì „ì²´ ë‚´ìš© í™•ì¸
    - íŠ¹ì • ë¬¸ì„œ ê¸°ë°˜ ë¶„ì„
    
    ì±…ì„ ì—†ìŒ:
    - ê²€ìƒ‰ (VectorSearchTool, KeywordSearchTool)
    - ì¬ë­í‚¹ (RerankTool)
    - ì¤‘ë³µ ì œê±° (DeduplicateTool)
    """
    name: str = "document_loader"
    description: str = """ì„ íƒëœ ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ ë¡œë“œí•©ë‹ˆë‹¤. 
ìš”ì•½, ë¶„ì„, ë‚´ìš© í™•ì¸ ë“± ë¬¸ì„œ ì „ì²´ê°€ í•„ìš”í•œ ì‘ì—…ì— ì‚¬ìš©ë©ë‹ˆë‹¤."""
    version: str = "1.0.0"
    
    async def _arun(
        self,
        document_ids: List[int],
        db_session: AsyncSession,
        max_chunks: int = 50,
        user_emp_no: Optional[str] = None,
        **kwargs
    ) -> SearchToolResult:
        """
        ë¬¸ì„œ ë¡œë“œ ì‹¤í–‰
        
        Args:
            document_ids: ë¡œë“œí•  ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            db_session: DB ì„¸ì…˜
            max_chunks: ë°˜í™˜í•  ìµœëŒ€ ì²­í¬ ìˆ˜ (í† í° ì œí•œ ë°©ì§€)
            user_emp_no: ì‚¬ìš©ì ì‚¬ë²ˆ (ê¶Œí•œ í™•ì¸ìš©)
        
        Returns:
            SearchToolResult: ë¡œë“œëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        start_time = datetime.utcnow()
        trace_id = str(uuid.uuid4())
        
        try:
            if not document_ids:
                return SearchToolResult(
                    success=False,
                    data=[],
                    total_found=0,
                    filtered_count=0,
                    search_params={"document_ids": []},
                    metrics=ToolMetrics(
                        latency_ms=0,
                        provider="internal",
                        trace_id=trace_id
                    ),
                    errors=["ë¬¸ì„œ IDê°€ ì œê³µë˜ì§€ ì•ŠìŒ"],
                    trace_id=trace_id,
                    tool_name=self.name,
                    tool_version=self.version
                )
            
            logger.info(f"ğŸ“š [DocumentLoader] ë¬¸ì„œ ë¡œë“œ ì‹œì‘: {len(document_ids)}ê°œ ë¬¸ì„œ")
            
            # SQL ì¿¼ë¦¬: í˜ì´ì§€ ë²ˆí˜¸ì™€ ì²­í¬ ì¸ë±ìŠ¤ ìˆœì„œë¡œ ì •ë ¬
            sql = text("""
                SELECT 
                    c.id,
                    c.file_id,
                    c.chunk_text,
                    c.page_number,
                    c.chunk_index,
                    c.metadata,
                    c.token_count,
                    c.created_at,
                    f.file_name,
                    f.file_type,
                    f.container_id,
                    con.name as container_name
                FROM tb_document_chunks c
                JOIN tb_files f ON c.file_id = f.id
                LEFT JOIN tb_containers con ON f.container_id = con.id
                WHERE c.file_id = ANY(:document_ids)
                ORDER BY c.file_id, c.page_number, c.chunk_index
                LIMIT :max_chunks
            """)
            
            result = await db_session.execute(
                sql,
                {
                    "document_ids": document_ids,
                    "max_chunks": max_chunks
                }
            )
            rows = result.fetchall()
            
            if not rows:
                logger.warning(f"âš ï¸ [DocumentLoader] ì²­í¬ ì—†ìŒ: document_ids={document_ids}")
                return SearchToolResult(
                    success=True,
                    data=[],
                    total_found=0,
                    filtered_count=0,
                    search_params={"document_ids": document_ids, "max_chunks": max_chunks},
                    metrics=ToolMetrics(
                        latency_ms=(datetime.utcnow() - start_time).total_seconds() * 1000,
                        provider="internal",
                        items_returned=0,
                        trace_id=trace_id
                    ),
                    errors=[],
                    trace_id=trace_id,
                    tool_name=self.name,
                    tool_version=self.version
                )
            
            # ì²­í¬ ë³€í™˜
            chunks = []
            for row in rows:
                chunk = SearchChunk(
                    chunk_id=str(row.id),
                    file_id=str(row.file_id),
                    content=row.chunk_text or "",
                    score=1.0,  # ë¡œë“œëŠ” ê²€ìƒ‰ì´ ì•„ë‹ˆë¯€ë¡œ 1.0
                    match_type="document_load",
                    container_id=str(row.container_id) if row.container_id else None,
                    metadata={
                        "file_name": row.file_name,
                        "file_type": row.file_type,
                        "container_name": row.container_name,
                        "page_number": row.page_number,
                        "chunk_index": row.chunk_index,
                        "token_count": row.token_count,
                        "created_at": row.created_at.isoformat() if row.created_at else None,
                        "match_reason": "ì§ì ‘ ë¡œë“œ",
                        **(row.metadata or {})
                    }
                )
                chunks.append(chunk)
            
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(
                f"âœ… [DocumentLoader] ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬, "
                f"documents={len(set(c.file_id for c in chunks))}, "
                f"latency={latency_ms:.1f}ms"
            )
            
            return SearchToolResult(
                success=True,
                data=chunks,
                total_found=len(chunks),
                filtered_count=0,
                search_params={
                    "document_ids": document_ids,
                    "max_chunks": max_chunks,
                    "user_emp_no": user_emp_no
                },
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="internal",
                    items_returned=len(chunks),
                    trace_id=trace_id
                ),
                errors=[],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        except Exception as e:
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            error_msg = f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            logger.error(f"âŒ [DocumentLoader] {error_msg}")
            
            return SearchToolResult(
                success=False,
                data=[],
                total_found=0,
                filtered_count=0,
                search_params={"document_ids": document_ids},
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="internal",
                    trace_id=trace_id
                ),
                errors=[error_msg],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
    
    def _run(self, *args, **kwargs):
        """ë™ê¸° ì‹¤í–‰ì€ ì§€ì›í•˜ì§€ ì•ŠìŒ"""
        raise NotImplementedError("DocumentLoaderToolì€ ë¹„ë™ê¸° ì‹¤í–‰ë§Œ ì§€ì›í•©ë‹ˆë‹¤. _arun()ì„ ì‚¬ìš©í•˜ì„¸ìš”.")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
document_loader_tool = DocumentLoaderTool()

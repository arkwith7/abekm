"""
Tool and Agent Standard Contracts
í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ ì •ì˜ - ëª¨ë“  ë„êµ¬ì™€ ì—ì´ì „íŠ¸ê°€ ë”°ë¼ì•¼ í•  ê³„ì•½
"""
from typing import Protocol, TypedDict, Any, List, Optional, Dict, runtime_checkable
from pydantic import BaseModel, Field
from datetime import datetime
from enum import Enum


# =============================================================================
# Tool Metrics & Result Types
# =============================================================================

class ToolMetrics(BaseModel):
    """ë„êµ¬ ì‹¤í–‰ ë©”íŠ¸ë¦­"""
    latency_ms: float = Field(description="ì‹¤í–‰ ì†Œìš” ì‹œê°„ (ë°€ë¦¬ì´ˆ)")
    provider: str = Field(default="internal", description="ì œê³µì (internal/bedrock/azure/openai)")
    cache_hit: bool = Field(default=False, description="ìºì‹œ íˆíŠ¸ ì—¬ë¶€")
    retries: int = Field(default=0, description="ì¬ì‹œë„ íšŸìˆ˜")
    cost_estimate: Optional[float] = Field(default=None, description="ì˜ˆìƒ ë¹„ìš© ($)")
    tokens_used: Optional[int] = Field(default=None, description="ì‚¬ìš©ëœ í† í° ìˆ˜")
    items_returned: Optional[int] = Field(default=None, description="ë°˜í™˜ëœ í•­ëª© ìˆ˜")
    trace_id: Optional[str] = Field(default=None, description="ì¶”ì  ID")
    
    class Config:
        json_schema_extra = {
            "example": {
                "latency_ms": 234.5,
                "provider": "bedrock",
                "cache_hit": False,
                "retries": 0,
                "cost_estimate": 0.002,
                "tokens_used": 1500
            }
        }


class ToolResult(BaseModel):
    """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ê¸°ë³¸ íƒ€ì…"""
    success: bool = Field(description="ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€")
    data: Any = Field(description="ì‹¤í–‰ ê²°ê³¼ ë°ì´í„°")
    metrics: ToolMetrics = Field(description="ì‹¤í–‰ ë©”íŠ¸ë¦­")
    errors: List[str] = Field(default_factory=list, description="ì˜¤ë¥˜ ë©”ì‹œì§€ ëª©ë¡")
    trace_id: str = Field(description="ì¶”ì  ID (OpenTelemetry)")
    tool_name: str = Field(description="ë„êµ¬ ì´ë¦„")
    tool_version: str = Field(default="1.0.0", description="ë„êµ¬ ë²„ì „")
    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat(), description="ì‹¤í–‰ ì‹œê°")
    
    class Config:
        json_schema_extra = {
            "example": {
                "success": True,
                "data": {"chunks": [{"id": "1", "content": "..."}]},
                "metrics": {"latency_ms": 234.5, "provider": "internal"},
                "errors": [],
                "trace_id": "abc-123-def",
                "tool_name": "vector_search",
                "tool_version": "1.0.0",
                "timestamp": "2025-11-11T10:30:00Z"
            }
        }


# =============================================================================
# Search Tool Specific Types
# =============================================================================

class SearchChunk(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ ì²­í¬"""
    chunk_id: str = Field(description="ì²­í¬ ê³ ìœ  ID")
    content: str = Field(description="ì²­í¬ ë‚´ìš©")
    score: float = Field(description="ì ìˆ˜ (ìœ ì‚¬ë„/ê´€ë ¨ë„, 0.0~1.0)")
    file_id: Optional[str] = Field(default=None, description="íŒŒì¼ ID")
    match_type: Optional[str] = Field(default=None, description="ë§¤ì¹­ íƒ€ì… (vector/keyword/fulltext/hybrid)")
    container_id: Optional[str] = Field(default=None, description="ì»¨í…Œì´ë„ˆ ID")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="ì¶”ê°€ ë©”íƒ€ë°ì´í„°")
    
    # í˜¸í™˜ì„±ì„ ìœ„í•œ alias
    @property
    def similarity_score(self) -> float:
        """Backward compatibility"""
        return self.score
    
    class Config:
        json_schema_extra = {
            "example": {
                "chunk_id": "chunk_123",
                "file_id": "file_456",
                "content": "ì¸ìŠë¦° íŒí”„ëŠ” ì§€ì†ì ìœ¼ë¡œ ì¸ìŠë¦°ì„ ê³µê¸‰í•˜ëŠ” ì¥ì¹˜ì…ë‹ˆë‹¤.",
                "similarity_score": 0.87,
                "match_type": "vector",
                "container_id": "container_789",
                "metadata": {"page": 5, "section": "ë³¸ë¡ "}
            }
        }


class SearchToolResult(ToolResult):
    """ê²€ìƒ‰ ë„êµ¬ ì „ìš© ê²°ê³¼"""
    data: List[SearchChunk] = Field(description="ê²€ìƒ‰ëœ ì²­í¬ ëª©ë¡")
    total_found: int = Field(description="ì´ ë°œê²¬ëœ ê°œìˆ˜ (í•„í„° ì „)")
    filtered_count: int = Field(description="í•„í„°ë§ í›„ ê°œìˆ˜")
    search_params: Dict[str, Any] = Field(default_factory=dict, description="ê²€ìƒ‰ íŒŒë¼ë¯¸í„°")


class RankedResult(ToolResult):
    """ì¬ë­í‚¹ ê²°ê³¼"""
    data: List[SearchChunk] = Field(description="ì¬ë­í‚¹ëœ ì²­í¬ ëª©ë¡")
    original_scores: List[float] = Field(description="ì›ë³¸ ì ìˆ˜ ëª©ë¡")
    rerank_scores: List[float] = Field(description="ì¬ë­í‚¹ ì ìˆ˜ ëª©ë¡")
    score_improvement: float = Field(description="í‰ê·  ì ìˆ˜ í–¥ìƒë„")


class ContextResult(ToolResult):
    """ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ ê²°ê³¼"""
    data: str = Field(description="êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸")
    used_chunks: List[SearchChunk] = Field(description="ì‚¬ìš©ëœ ì²­í¬ ëª©ë¡")
    total_tokens: int = Field(description="ì´ í† í° ìˆ˜")
    chunks_included: int = Field(description="í¬í•¨ëœ ì²­í¬ ìˆ˜")
    chunks_truncated: int = Field(description="ì˜ë¦° ì²­í¬ ìˆ˜")


# =============================================================================
# Tool Protocol
# =============================================================================

@runtime_checkable
class ToolProtocol(Protocol):
    """ëª¨ë“  ë„êµ¬ê°€ êµ¬í˜„í•´ì•¼ í•  í”„ë¡œí† ì½œ"""
    name: str
    description: str
    version: str
    
    async def _arun(self, **kwargs) -> ToolResult:
        """ë¹„ë™ê¸° ì‹¤í–‰ (ê¶Œì¥)"""
        ...
    
    def _run(self, **kwargs) -> ToolResult:
        """ë™ê¸° ì‹¤í–‰ (í´ë°±)"""
        ...
    
    def validate_input(self, **kwargs) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        ...


# =============================================================================
# Agent Types
# =============================================================================

class AgentIntent(str, Enum):
    """ì—ì´ì „íŠ¸ ì˜ë„ ë¶„ë¥˜"""
    FACTUAL_QA = "factual_qa"              # ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸
    KEYWORD_SEARCH = "keyword_search"      # í‚¤ì›Œë“œ ê²€ìƒ‰
    EXPLORATORY = "exploratory"            # íƒìƒ‰ì  ì§ˆë¬¸
    DOCUMENT_ANALYSIS = "document_analysis"  # ë¬¸ì„œ ë¶„ì„
    PPT_GENERATION = "ppt_generation"      # PPT ìƒì„±
    COMPARISON = "comparison"              # ë¹„êµ ë¶„ì„
    SUMMARIZATION = "summarization"        # ìš”ì•½
    WEB_SEARCH = "web_search"              # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰


class AgentConstraints(BaseModel):
    """ì—ì´ì „íŠ¸ ì œì•½ ì¡°ê±´"""
    max_tokens: int = Field(default=4000, description="ìµœëŒ€ í† í° ìˆ˜")
    max_chunks: int = Field(default=10, description="ìµœëŒ€ ì²­í¬ ìˆ˜")
    similarity_threshold: float = Field(default=0.25, description="ìœ ì‚¬ë„ ì„ê³„ê°’")
    container_ids: Optional[List[str]] = Field(default=None, description="ê²€ìƒ‰ ëŒ€ìƒ ì»¨í…Œì´ë„ˆ")
    document_ids: Optional[List[str]] = Field(default=None, description="ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ")
    allow_web_fallback: bool = Field(default=False, description="ì›¹ ê²€ìƒ‰ í´ë°± í—ˆìš©")
    time_limit_ms: Optional[int] = Field(default=None, description="ì‹œê°„ ì œí•œ (ë°€ë¦¬ì´ˆ)")
    cost_limit: Optional[float] = Field(default=None, description="ë¹„ìš© ì œí•œ ($)")


class AgentStep(BaseModel):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰ ë‹¨ê³„"""
    step_number: int = Field(description="ë‹¨ê³„ ë²ˆí˜¸")
    tool_name: str = Field(description="ì‚¬ìš©ëœ ë„êµ¬ ì´ë¦„")
    tool_input: Dict[str, Any] = Field(description="ë„êµ¬ ì…ë ¥")
    tool_output: ToolResult = Field(description="ë„êµ¬ ì¶œë ¥")
    reasoning: str = Field(description="ì´ ë„êµ¬ë¥¼ ì„ íƒí•œ ì´ìœ ")
    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())


class AgentResult(BaseModel):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²°ê³¼"""
    answer: str = Field(description="ìµœì¢… ë‹µë³€")
    references: List[SearchChunk] = Field(description="ì°¸ê³  ë¬¸í—Œ")
    steps: List[AgentStep] = Field(description="ì‹¤í–‰ ë‹¨ê³„ ë¡œê·¸")
    metrics: Dict[str, Any] = Field(description="ì¢…í•© ë©”íŠ¸ë¦­")
    intent: AgentIntent = Field(description="ê°ì§€ëœ ì˜ë„")
    strategy_used: List[str] = Field(description="ì‚¬ìš©ëœ ì „ëµ (ë„êµ¬ ëª©ë¡)")
    success: bool = Field(description="ì„±ê³µ ì—¬ë¶€")
    errors: List[str] = Field(default_factory=list)
    
    class Config:
        json_schema_extra = {
            "example": {
                "answer": "ì¸ìŠë¦° íŒí”„ëŠ” ì§€ì†ì ìœ¼ë¡œ ì¸ìŠë¦°ì„ ê³µê¸‰í•˜ëŠ” ì¥ì¹˜ì…ë‹ˆë‹¤...",
                "references": [{"chunk_id": "1", "content": "..."}],
                "steps": [{"step_number": 1, "tool_name": "vector_search", "reasoning": "..."}],
                "metrics": {"total_latency_ms": 1234, "total_cost": 0.05},
                "intent": "factual_qa",
                "strategy_used": ["vector_search", "rerank", "context_builder"],
                "success": True,
                "errors": []
            }
        }


# =============================================================================
# Agent Protocol
# =============================================================================

@runtime_checkable
class AgentProtocol(Protocol):
    """ëª¨ë“  ì—ì´ì „íŠ¸ê°€ êµ¬í˜„í•´ì•¼ í•  í”„ë¡œí† ì½œ"""
    name: str
    description: str
    version: str
    tools: Dict[str, Any]  # tool_name -> tool_instance
    
    async def execute(
        self, 
        query: str, 
        constraints: AgentConstraints,
        context: Optional[Dict[str, Any]] = None
    ) -> AgentResult:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰"""
        ...
    
    def classify_intent(self, query: str) -> AgentIntent:
        """ì˜ë„ ë¶„ë¥˜"""
        ...
    
    def select_strategy(
        self, 
        intent: AgentIntent, 
        constraints: AgentConstraints
    ) -> List[str]:
        """ì „ëµ ì„ íƒ (ë„êµ¬ ëª©ë¡ ë°˜í™˜)"""
        ...


# =============================================================================
# Evaluation Types
# =============================================================================

class EvaluationMetric(str, Enum):
    """í‰ê°€ ë©”íŠ¸ë¦­ íƒ€ì…"""
    RECALL_AT_K = "recall@k"
    NDCG_AT_K = "ndcg@k"
    MRR = "mrr"
    PRECISION_AT_K = "precision@k"
    CONTEXT_UTILIZATION = "context_utilization"
    ANSWER_FAITHFULNESS = "answer_faithfulness"


class GoldenSample(BaseModel):
    """í‰ê°€ìš© ê³¨ë“  ìƒ˜í”Œ"""
    query: str = Field(description="ì§ˆì˜")
    intent: AgentIntent = Field(description="ì˜ë„")
    expected_docs: List[str] = Field(description="ì •ë‹µ ë¬¸ì„œ ID ëª©ë¡")
    expected_chunks: Optional[List[str]] = Field(default=None, description="ì •ë‹µ ì²­í¬ ID ëª©ë¡")
    min_similarity: float = Field(default=0.5, description="ìµœì†Œ ê¸°ëŒ€ ìœ ì‚¬ë„")
    relevance_judgments: Dict[str, int] = Field(
        default_factory=dict, 
        description="ë¬¸ì„œ ID -> ê´€ë ¨ë„ (0~3)"
    )
    metadata: Dict[str, Any] = Field(default_factory=dict)


class EvaluationResult(BaseModel):
    """í‰ê°€ ê²°ê³¼"""
    metric: EvaluationMetric
    score: float
    k: Optional[int] = None
    details: Dict[str, Any] = Field(default_factory=dict)
    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())

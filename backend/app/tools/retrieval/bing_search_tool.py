"""
Bing Search Tool - Azure Bing Search API ì›¹ ê²€ìƒ‰ ë„êµ¬
Microsoft Bing Search API v7ì„ ì‚¬ìš©í•˜ì—¬ ì›¹/ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰
"""
import asyncio
import uuid
import aiohttp
import hashlib
from typing import List, Optional, Dict, Any
from datetime import datetime
from loguru import logger

try:
    from langchain_core.tools import BaseTool
except ImportError:
    from langchain_core.tools import BaseTool

from app.tools.contracts import (
    SearchToolResult, SearchChunk, ToolMetrics
)
from app.core.config import settings


class BingSearchTool(BaseTool):
    """
    Bing Search ë„êµ¬ (Azure)
    
    íŠ¹ì§•:
    - Microsoft Azure Bing Search API v7
    - ì›¹, ë‰´ìŠ¤, ì´ë¯¸ì§€ ê²€ìƒ‰ ì§€ì›
    - ì—”í„°í”„ë¼ì´ì¦ˆ ì•ˆì •ì„±
    - í•œêµ­ì–´ ê²€ìƒ‰ ìš°ìˆ˜
    
    ì±…ì„:
    - Bing Search APIë¥¼ í†µí•œ ì›¹/ë‰´ìŠ¤ ê²€ìƒ‰
    - ê²€ìƒ‰ ê²°ê³¼ë¥¼ SearchChunk í˜•íƒœë¡œ ë³€í™˜
    """
    name: str = "bing_search"
    description: str = "Microsoft Bingì„ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰. ë‰´ìŠ¤, ìµœì‹  ì •ë³´, ê¸°ì—… ì •ë³´ ê²€ìƒ‰ì— ì í•©í•©ë‹ˆë‹¤."
    version: str = "1.0.0"
    
    # Bing Search API ì—”ë“œí¬ì¸íŠ¸
    WEB_SEARCH_ENDPOINT: str = "https://api.bing.microsoft.com/v7.0/search"
    NEWS_SEARCH_ENDPOINT: str = "https://api.bing.microsoft.com/v7.0/news/search"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def _format_query_for_log(self, query: str) -> str:
        """Avoid logging raw queries unless explicitly allowed."""
        q = (query or "").strip()
        if settings.web_search_log_queries:
            return q[:200]
        digest = hashlib.sha256(q.encode("utf-8")).hexdigest()[:12] if q else "empty"
        return f"len={len(q)} sha256={digest}"
        
    async def _arun(
        self,
        query: str,
        top_k: int = 5,
        search_type: str = "web",  # web | news | both
        market: str = "ko-KR",
        freshness: Optional[str] = None,  # Day | Week | Month
        **kwargs
    ) -> SearchToolResult:
        """
        Bing ê²€ìƒ‰ ì‹¤í–‰ (ë¹„ë™ê¸°)
        
        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            search_type: ê²€ìƒ‰ ìœ í˜• (web, news, both)
            market: ê²€ìƒ‰ ì‹œì¥/ì–¸ì–´ (ko-KR, en-US ë“±)
            freshness: ê²°ê³¼ ì‹ ì„ ë„ í•„í„° (Day, Week, Month)
        """
        start_time = datetime.utcnow()
        trace_id = str(uuid.uuid4())
        
        # API í‚¤ í™•ì¸
        api_key = settings.bing_search_api_key
        if not api_key:
            return SearchToolResult(
                success=False,
                data=[],
                total_found=0,
                filtered_count=0,
                search_params={"query": query},
                metrics=ToolMetrics(latency_ms=0, provider="bing", trace_id=trace_id),
                errors=["BING_SEARCH_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— BING_SEARCH_API_KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”."],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        try:
            logger.info(
                f"ğŸ” [BingSearch] ê²€ìƒ‰ ì‹œì‘: query=({self._format_query_for_log(query)}) (type={search_type}, market={market})"
            )
            
            chunks = []
            
            async with aiohttp.ClientSession() as session:
                # ì›¹ ê²€ìƒ‰
                if search_type in ["web", "both"]:
                    web_results = await self._search_web(
                        session, api_key, query, top_k, market, freshness
                    )
                    chunks.extend(web_results)
                
                # ë‰´ìŠ¤ ê²€ìƒ‰
                if search_type in ["news", "both"]:
                    news_count = top_k if search_type == "news" else min(3, top_k)
                    news_results = await self._search_news(
                        session, api_key, query, news_count, market, freshness
                    )
                    # ë‰´ìŠ¤ ê²°ê³¼ì— íƒœê·¸ ì¶”ê°€
                    for chunk in news_results:
                        chunk.metadata["result_type"] = "news"
                    chunks.extend(news_results)
            
            # trace_idë¡œ chunk_id ì—…ë°ì´íŠ¸
            for idx, chunk in enumerate(chunks):
                chunk.chunk_id = f"bing_{trace_id}_{idx}"
            
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(f"âœ… [BingSearch] ê²€ìƒ‰ ì™„ë£Œ: {len(chunks)}ê°œ ê²°ê³¼, {latency_ms:.0f}ms")
            
            return SearchToolResult(
                success=True,
                data=chunks[:top_k],  # top_k ì œí•œ
                total_found=len(chunks),
                filtered_count=max(0, len(chunks) - top_k),
                search_params={
                    "query": query,
                    "top_k": top_k,
                    "search_type": search_type,
                    "market": market
                },
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="bing",
                    items_returned=min(len(chunks), top_k),
                    trace_id=trace_id
                ),
                errors=[],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        except Exception as e:
            logger.error(f"âŒ [BingSearch] ì‹¤íŒ¨: {e}")
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            return SearchToolResult(
                success=False,
                data=[],
                total_found=0,
                filtered_count=0,
                search_params={"query": query},
                metrics=ToolMetrics(latency_ms=latency_ms, provider="bing", trace_id=trace_id),
                errors=[str(e)],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )

    async def _search_web(
        self,
        session: aiohttp.ClientSession,
        api_key: str,
        query: str,
        count: int,
        market: str,
        freshness: Optional[str]
    ) -> List[SearchChunk]:
        """ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        headers = {
            "Ocp-Apim-Subscription-Key": api_key
        }
        
        params = {
            "q": query,
            "count": str(count),
            "mkt": market,
            "responseFilter": "Webpages",
            "textDecorations": "false",
            "textFormat": "Raw"
        }
        
        if freshness:
            params["freshness"] = freshness
        
        async with session.get(
            self.WEB_SEARCH_ENDPOINT,
            headers=headers,
            params=params,
            timeout=aiohttp.ClientTimeout(total=settings.web_search_timeout_seconds)
        ) as response:
            if response.status != 200:
                error_text = await response.text()
                logger.error(f"âŒ [BingSearch] Web API ì˜¤ë¥˜: {response.status} - {error_text}")
                return []
            
            data = await response.json()
            
        chunks = []
        web_pages = data.get("webPages", {}).get("value", [])
        
        for idx, page in enumerate(web_pages):
            title = page.get("name", "")
            snippet = page.get("snippet", "")
            url = page.get("url", "")
            date_published = page.get("dateLastCrawled", "")
            
            content = f"ì œëª©: {title}\në‚´ìš©: {snippet}\nì¶œì²˜: {url}"
            if date_published:
                content += f"\në‚ ì§œ: {date_published[:10]}"
            
            chunk = SearchChunk(
                chunk_id=f"bing_web_{idx}",
                content=content,
                score=0.9 - (idx * 0.05),
                file_id=None,
                match_type="internet",
                container_id="bing",
                metadata={
                    "source": "bing",
                    "result_type": "web",
                    "url": url,
                    "title": title,
                    "snippet": snippet,
                    "date_crawled": date_published
                }
            )
            chunks.append(chunk)
        
        return chunks

    async def _search_news(
        self,
        session: aiohttp.ClientSession,
        api_key: str,
        query: str,
        count: int,
        market: str,
        freshness: Optional[str]
    ) -> List[SearchChunk]:
        """ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰"""
        headers = {
            "Ocp-Apim-Subscription-Key": api_key
        }
        
        params = {
            "q": query,
            "count": str(count),
            "mkt": market,
            "textDecorations": "false",
            "textFormat": "Raw"
        }
        
        if freshness:
            params["freshness"] = freshness
        
        async with session.get(
            self.NEWS_SEARCH_ENDPOINT,
            headers=headers,
            params=params,
            timeout=aiohttp.ClientTimeout(total=settings.web_search_timeout_seconds)
        ) as response:
            if response.status != 200:
                error_text = await response.text()
                logger.error(f"âŒ [BingSearch] News API ì˜¤ë¥˜: {response.status} - {error_text}")
                return []
            
            data = await response.json()
            
        chunks = []
        news_items = data.get("value", [])
        
        for idx, news in enumerate(news_items):
            title = news.get("name", "")
            description = news.get("description", "")
            url = news.get("url", "")
            date_published = news.get("datePublished", "")
            provider = news.get("provider", [{}])[0].get("name", "") if news.get("provider") else ""
            
            content = f"[ë‰´ìŠ¤] ì œëª©: {title}\në‚´ìš©: {description}\nì¶œì²˜: {url}"
            if provider:
                content += f"\nì œê³µ: {provider}"
            if date_published:
                content += f"\në°œí–‰ì¼: {date_published[:10]}"
            
            chunk = SearchChunk(
                chunk_id=f"bing_news_{idx}",
                content=content,
                score=0.95 - (idx * 0.05),  # ë‰´ìŠ¤ëŠ” ì¡°ê¸ˆ ë” ë†’ì€ ì ìˆ˜
                file_id=None,
                match_type="internet",
                container_id="bing",
                metadata={
                    "source": "bing",
                    "result_type": "news",
                    "url": url,
                    "title": title,
                    "snippet": description,
                    "provider": provider,
                    "date_published": date_published
                }
            )
            chunks.append(chunk)
        
        return chunks

    def _run(
        self,
        query: str,
        top_k: int = 5,
        search_type: str = "web",
        market: str = "ko-KR",
        freshness: Optional[str] = None,
        **kwargs
    ) -> SearchToolResult:
        """
        Bing ê²€ìƒ‰ ì‹¤í–‰ (ë™ê¸°)
        """
        import requests
        
        start_time = datetime.utcnow()
        trace_id = str(uuid.uuid4())
        
        api_key = settings.bing_search_api_key
        if not api_key:
            return SearchToolResult(
                success=False,
                data=[],
                total_found=0,
                filtered_count=0,
                search_params={"query": query},
                metrics=ToolMetrics(latency_ms=0, provider="bing", trace_id=trace_id),
                errors=["BING_SEARCH_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        try:
            logger.info(f"ğŸ” [BingSearch] ê²€ìƒ‰ ì‹œì‘: query=({self._format_query_for_log(query)})")
            
            headers = {"Ocp-Apim-Subscription-Key": api_key}
            params = {
                "q": query,
                "count": str(top_k),
                "mkt": market,
                "responseFilter": "Webpages",
                "textDecorations": "false",
                "textFormat": "Raw"
            }
            
            if freshness:
                params["freshness"] = freshness
            
            response = requests.get(
                self.WEB_SEARCH_ENDPOINT,
                headers=headers,
                params=params,
                timeout=settings.web_search_timeout_seconds
            )
            
            if response.status_code != 200:
                raise Exception(f"Bing API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            
            data = response.json()
            
            chunks = []
            web_pages = data.get("webPages", {}).get("value", [])
            
            for idx, page in enumerate(web_pages):
                title = page.get("name", "")
                snippet = page.get("snippet", "")
                url = page.get("url", "")
                
                content = f"ì œëª©: {title}\në‚´ìš©: {snippet}\nì¶œì²˜: {url}"
                
                chunk = SearchChunk(
                    chunk_id=f"bing_{trace_id}_{idx}",
                    content=content,
                    score=0.9 - (idx * 0.05),
                    file_id=None,
                    match_type="internet",
                    container_id="bing",
                    metadata={
                        "source": "bing",
                        "result_type": "web",
                        "url": url,
                        "title": title,
                        "snippet": snippet
                    }
                )
                chunks.append(chunk)
            
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(f"âœ… [BingSearch] ê²€ìƒ‰ ì™„ë£Œ: {len(chunks)}ê°œ ê²°ê³¼")
            
            return SearchToolResult(
                success=True,
                data=chunks,
                total_found=len(chunks),
                filtered_count=0,
                search_params={"query": query, "top_k": top_k},
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="bing",
                    items_returned=len(chunks),
                    trace_id=trace_id
                ),
                errors=[],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        except Exception as e:
            logger.error(f"âŒ [BingSearch] ì‹¤íŒ¨: {e}")
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            return SearchToolResult(
                success=False,
                data=[],
                total_found=0,
                filtered_count=0,
                search_params={"query": query},
                metrics=ToolMetrics(latency_ms=latency_ms, provider="bing", trace_id=trace_id),
                errors=[str(e)],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
bing_search_tool = BingSearchTool()

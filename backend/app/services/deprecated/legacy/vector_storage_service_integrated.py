"""
ğŸ”® ë²¡í„° ì €ì¥ ì„œë¹„ìŠ¤ - vs_ ì¤‘ì‹¬ í†µí•© ì•„í‚¤í…ì²˜
================================================

ìµœì  í†µí•© ë°©ì•ˆ 1: vs_ ì¤‘ì‹¬ í™œìš© â­ [ê¶Œì¥]

ğŸ¯ ì—­í•  ë¶„ë‹´:
  Primary Vector Store: vs_doc_contents_index
    - ëª¨ë“  ë¬¸ì„œ ì²­í¬ ë²¡í„° ì €ì¥
    - ë¹ ë¥¸ ë²¡í„° ê²€ìƒ‰ ìµœì í™”
    - ê¶Œí•œ ì •ë³´ëŠ” file_bss_info_snoë¡œ ì—°ê²°
  
  Metadata & Reference: tb_document_chunks  
    - ìƒì„¸ ë©”íƒ€ë°ì´í„° (í˜ì´ì§€, ì„¹ì…˜)
    - ì°¸ì¡° ì¶”ì  ("3í˜ì´ì§€ 2ë²ˆì§¸ ë‹¨ë½")
    - ê¶Œí•œ ë° ì»¨í…Œì´ë„ˆ ì •ë³´
  
  Hybrid Search: tb_search_documents
    - í‚¤ì›Œë“œ + ë²¡í„° ì¡°í•© ê²€ìƒ‰
    - tsvector ìµœì í™”
    - ì‹¤ì‹œê°„ ê²€ìƒ‰ ì„±ëŠ¥

PostgreSQL + pgvector í™œìš©
"""

import json
import logging
from typing import Dict, Any, List, Optional
import numpy as np
from datetime import datetime

# Database imports
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession
from app.core.database import get_async_session_local
from app.core.config import settings
from app.services.korean_nlp_service import korean_nlp_service

logger = logging.getLogger(__name__)

class VectorStorageServiceIntegrated:
    """
    ğŸ”® vs_ ì¤‘ì‹¬ í†µí•© ë²¡í„° ì €ì¥ ì„œë¹„ìŠ¤
    ===============================
    
    Primary: vs_doc_contents_index (ë²¡í„° ê²€ìƒ‰)
    Metadata: tb_document_chunks (ì°¸ì¡° ì¶”ì )  
    Hybrid: tb_search_documents (í‚¤ì›Œë“œ+ë²¡í„°)
    """
    
    def __init__(self):
        """ë²¡í„° ì €ì¥ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.embedding_dimension = settings.bedrock_embedding_dimension
        logger.info(f"ğŸ”® vs_ ì¤‘ì‹¬ ë²¡í„° ì €ì¥ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ - ì„ë² ë”© ì°¨ì›: {self.embedding_dimension}")
        print(f"âœ… VectorStorageServiceIntegrated ì´ˆê¸°í™” ì„±ê³µ - vs_ ì¤‘ì‹¬ ì•„í‚¤í…ì²˜, ì°¨ì›: {self.embedding_dimension}")
    
    async def store_processed_document(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        preprocessed_data: Dict[str, Any],
        nlp_results: List[Dict[str, Any]],
        user_info: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ğŸ”® vs_ ì¤‘ì‹¬ ë¬¸ì„œ ë²¡í„° ì €ì¥ í†µí•© í”„ë¡œì„¸ìŠ¤
        
        1. vs_doc_contents_index: ì£¼ìš” ë²¡í„° ì €ì¥
        2. tb_document_chunks: ë©”íƒ€ë°ì´í„° ë° ì°¸ì¡° ì •ë³´
        3. tb_search_documents: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš©
        """
        try:
            # ê¶Œí•œ ì •ë³´ í™•ì¸
            access_permissions = await self._get_container_permissions(
                session, container_id, user_info
            )
            
            result = {
                "success": False,
                "primary_vectors": 0,     # vs_doc_contents_index ì €ì¥ ìˆ˜
                "metadata_chunks": 0,     # tb_document_chunks ì €ì¥ ìˆ˜
                "hybrid_records": 0,      # tb_search_documents ì €ì¥ ìˆ˜
                "errors": [],
                "container_id": container_id,
                "permissions": access_permissions,
                "architecture": "vs_primary"
            }
            
            chunks = preprocessed_data.get('chunks', [])
            
            for i, (chunk, nlp_result) in enumerate(zip(chunks, nlp_results)):
                try:
                    # 1ë‹¨ê³„: Primary Vector Store (vs_doc_contents_index)
                    primary_result = await self._store_primary_vector(
                        session, file_bss_info_sno, container_id, chunk, nlp_result, i, access_permissions
                    )
                    
                    if primary_result["success"]:
                        result["primary_vectors"] += 1
                    else:
                        result["errors"].append(f"ì²­í¬ {i} ì£¼ìš” ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {primary_result.get('error')}")
                    
                    # 2ë‹¨ê³„: Metadata & Reference (tb_document_chunks)
                    metadata_result = await self._store_metadata_chunk(
                        session, file_bss_info_sno, container_id, chunk, nlp_result, i
                    )
                    
                    if metadata_result["success"]:
                        result["metadata_chunks"] += 1
                    else:
                        result["errors"].append(f"ì²­í¬ {i} ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {metadata_result.get('error')}")
                    
                    # 3ë‹¨ê³„: Hybrid Search (tb_search_documents) - ì„ íƒì 
                    hybrid_result = await self._store_hybrid_search(
                        session, file_bss_info_sno, container_id, chunk, nlp_result, i
                    )
                    
                    if hybrid_result["success"]:
                        result["hybrid_records"] += 1
                    else:
                        result["errors"].append(f"ì²­í¬ {i} í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ì‹¤íŒ¨: {hybrid_result.get('error')}")
                    
                except Exception as e:
                    error_msg = f"ì²­í¬ {i} í†µí•© ì €ì¥ ì‹¤íŒ¨: {str(e)}"
                    result["errors"].append(error_msg)
                    logger.error(error_msg)
            
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            await self._update_file_metadata_integrated(
                session, file_bss_info_sno, len(chunks), preprocessed_data, access_permissions, result
            )
            
            # ì„±ê³µ ì—¬ë¶€ íŒì •: ì£¼ìš” ë²¡í„° ì €ì¥ ì„±ê³µ ê¸°ì¤€
            result["success"] = result["primary_vectors"] > 0
            
            logger.info(f"ğŸ”® vs_ ì¤‘ì‹¬ ë²¡í„° ì €ì¥ ì™„ë£Œ - ì»¨í…Œì´ë„ˆ: {container_id}, "
                       f"ì£¼ìš”ë²¡í„°: {result['primary_vectors']}, ë©”íƒ€ë°ì´í„°: {result['metadata_chunks']}, "
                       f"í•˜ì´ë¸Œë¦¬ë“œ: {result['hybrid_records']}")
            
            return result
            
        except Exception as e:
            logger.error(f"vs_ ì¤‘ì‹¬ ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "primary_vectors": 0,
                "metadata_chunks": 0,
                "hybrid_records": 0,
                "container_id": container_id,
                "architecture": "vs_primary"
            }
    
    # ==============================================
    # ğŸ”® Primary Vector Store: vs_doc_contents_index
    # ==============================================
    
    async def _store_primary_vector(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        chunk: Dict[str, Any],
        nlp_result: Dict[str, Any],
        chunk_index: int,
        permissions: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ğŸ”® Primary Vector Store: vs_doc_contents_indexì— ì£¼ìš” ë²¡í„° ì €ì¥"""
        try:
            # ê¶Œí•œ í™•ì¸
            if not permissions.get("write_permission", False):
                return {
                    "success": False,
                    "error": f"ì»¨í…Œì´ë„ˆ {container_id}ì— ëŒ€í•œ ì“°ê¸° ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ì„ë² ë”© ë²¡í„° ì¤€ë¹„
            embedding = nlp_result.get('embedding')
            vector_dimension = self.embedding_dimension
            
            if not embedding or len(embedding) != vector_dimension:
                return {
                    "success": False,
                    "error": f"ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {vector_dimension}, ì‹¤ì œ {len(embedding) if embedding else 0}"
                }
            
            embedding_str = f"[{','.join(map(str, embedding))}]"
            content = chunk.get('content', '')
            
            # vs_doc_contents_indexì— ì €ì¥
            query = text("""
                INSERT INTO vs_doc_contents_index (
                    file_bss_info_sno, knowledge_container_id, chunk_index,
                    content, content_vector, created_at, updated_at
                ) VALUES (
                    :file_sno, :container_id, :chunk_index,
                    :content, CAST(:embedding AS vector), NOW(), NOW()
                )
            """)
            
            await session.execute(query, {
                "file_sno": file_bss_info_sno,
                "container_id": container_id,
                "chunk_index": chunk_index,
                "content": content,
                "embedding": embedding_str
            })
            
            logger.debug(f"ğŸ”® ì£¼ìš” ë²¡í„° ì €ì¥ ì„±ê³µ - íŒŒì¼: {file_bss_info_sno}, ì²­í¬: {chunk_index}")
            return {"success": True}
            
        except Exception as e:
            logger.error(f"ì£¼ìš” ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

    # ==============================================
    # ğŸ”® Metadata & Reference: tb_document_chunks
    # ==============================================

    async def _store_metadata_chunk(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        chunk: Dict[str, Any],
        nlp_result: Dict[str, Any],
        chunk_index: int
    ) -> Dict[str, Any]:
        """ğŸ”® Metadata & Reference: tb_document_chunksì— ìƒì„¸ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            # ì°¸ì¡° ì •ë³´ ì¶”ì¶œ
            page_info = chunk.get('page_number', 0)
            section_info = chunk.get('section_title', 'content')
            paragraph_info = chunk.get('paragraph_index', chunk_index)
            
            # ì°¸ì¡° ë¬¸ìì—´ ìƒì„± ("3í˜ì´ì§€ 2ë²ˆì§¸ ë‹¨ë½")
            reference_info = f"{page_info}í˜ì´ì§€ {paragraph_info}ë²ˆì§¸ ë‹¨ë½" if page_info > 0 else f"{paragraph_info}ë²ˆì§¸ ë‹¨ë½"
            
            # í‚¤ì›Œë“œ ì •ë³´
            keywords = nlp_result.get('korean_keywords', [])
            entities = nlp_result.get('named_entities', [])
            
            # tb_document_chunksì— ì €ì¥ (ë²¡í„° ì—†ì´ ë©”íƒ€ë°ì´í„°ë§Œ)
            query = text("""
                INSERT INTO tb_document_chunks (
                    "FILE_BSS_INFO_SNO", "CHUNK_INDEX", "CHUNK_TEXT", 
                    "CHUNK_SIZE", "SECTION_TITLE", "PAGE_NUMBER",
                    "PARAGRAPH_INDEX", "REFERENCE_INFO", "KEYWORDS",
                    "NAMED_ENTITIES", "KNOWLEDGE_CONTAINER_ID",
                    "CREATED_BY", "LAST_MODIFIED_BY"
                ) VALUES (
                    :file_sno, :chunk_index, :chunk_text,
                    :chunk_size, :section_title, :page_number,
                    :paragraph_index, :reference_info, :keywords,
                    :entities, :container_id,
                    :created_by, :modified_by
                )
            """)
            
            await session.execute(query, {
                "file_sno": file_bss_info_sno,
                "chunk_index": chunk_index,
                "chunk_text": chunk.get('content', ''),
                "chunk_size": chunk.get('size', len(chunk.get('content', ''))),
                "section_title": section_info,
                "page_number": page_info,
                "paragraph_index": paragraph_info,
                "reference_info": reference_info,
                "keywords": json.dumps(keywords),
                "entities": json.dumps(entities),
                "container_id": container_id,
                "created_by": "system",
                "modified_by": "system"
            })
            
            logger.debug(f"ğŸ”® ë©”íƒ€ë°ì´í„° ì €ì¥ ì„±ê³µ - íŒŒì¼: {file_bss_info_sno}, ì°¸ì¡°: {reference_info}")
            return {"success": True}
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

    # ==============================================
    # ğŸ”® Hybrid Search: tb_search_documents
    # ==============================================

    async def _store_hybrid_search(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        chunk: Dict[str, Any],
        nlp_result: Dict[str, Any],
        chunk_index: int
    ) -> Dict[str, Any]:
        """ğŸ”® Hybrid Search: tb_search_documentsì— í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© ì €ì¥"""
        try:
            # ì„ë² ë”© ë²¡í„° ì¤€ë¹„ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš©)
            embedding = nlp_result.get('embedding')
            vector_dimension = self.embedding_dimension
            
            if embedding and len(embedding) == vector_dimension:
                embedding_str = f"[{','.join(map(str, embedding))}]"
            else:
                embedding_str = None
                if embedding:
                    logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {vector_dimension}, ì‹¤ì œ {len(embedding)}")
            
            # í‚¤ì›Œë“œ, ê³ ìœ ëª…ì‚¬, íšŒì‚¬ëª… ë°°ì—´ ì¤€ë¹„
            keywords = nlp_result.get('korean_keywords', [])
            proper_nouns = nlp_result.get('named_entities', [])
            corp_names = []  # ì¶”í›„ íšŒì‚¬ëª… ì¶”ì¶œ ë¡œì§ ì¶”ê°€
            
            # PostgreSQL ë°°ì—´ í˜•íƒœë¡œ ë³€í™˜
            keywords_array = '{' + ','.join([f'"{k}"' for k in keywords if k]) + '}'
            proper_nouns_array = '{' + ','.join([f'"{p}"' for p in proper_nouns if p]) + '}'
            corp_names_array = '{}'
            
            content = chunk.get('content', '')
            
            # tb_search_documentsì— ì €ì¥
            if embedding_str:
                query = text("""
                    INSERT INTO tb_search_documents (
                        file_bss_info_sno, knowledge_container_id, chunk_index,
                        content, keywords, proper_nouns, corp_names,
                        content_vector, keyword_tsvector, content_tsvector
                    ) VALUES (
                        :file_sno, :container_id, :chunk_index,
                        :content, CAST(:keywords AS text[]), CAST(:proper_nouns AS text[]), CAST(:corp_names AS text[]),
                        CAST(:embedding AS vector), 
                        to_tsvector('korean', :content),
                        to_tsvector('korean', :content)
                    )
                """)
                
                await session.execute(query, {
                    "file_sno": file_bss_info_sno,
                    "container_id": container_id,
                    "chunk_index": chunk_index,
                    "content": content,
                    "keywords": keywords_array,
                    "proper_nouns": proper_nouns_array,
                    "corp_names": corp_names_array,
                    "embedding": embedding_str
                })
            else:
                query = text("""
                    INSERT INTO tb_search_documents (
                        file_bss_info_sno, knowledge_container_id, chunk_index,
                        content, keywords, proper_nouns, corp_names,
                        keyword_tsvector, content_tsvector
                    ) VALUES (
                        :file_sno, :container_id, :chunk_index,
                        :content, CAST(:keywords AS text[]), CAST(:proper_nouns AS text[]), CAST(:corp_names AS text[]),
                        to_tsvector('korean', :content),
                        to_tsvector('korean', :content)
                    )
                """)
                
                await session.execute(query, {
                    "file_sno": file_bss_info_sno,
                    "container_id": container_id,
                    "chunk_index": chunk_index,
                    "content": content,
                    "keywords": keywords_array,
                    "proper_nouns": proper_nouns_array,
                    "corp_names": corp_names_array
                })
            
            logger.debug(f"ğŸ”® í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì €ì¥ ì„±ê³µ - íŒŒì¼: {file_bss_info_sno}, ì²­í¬: {chunk_index}")
            return {"success": True}
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

    # ==============================================
    # ğŸ”® ê²€ìƒ‰ ë©”ì„œë“œë“¤
    # ==============================================

    async def search_vector_primary(
        self,
        session: AsyncSession,
        query_embedding: List[float],
        container_ids: Optional[List[str]] = None,
        limit: int = 10,
        similarity_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """ğŸ”® Primary Vector Search: vs_doc_contents_index ê¸°ë°˜ ê³ ì† ë²¡í„° ê²€ìƒ‰"""
        try:
            conditions = []
            params = {"query_embedding": f"[{','.join(map(str, query_embedding))}]", 
                     "threshold": similarity_threshold, "limit": limit}
            
            # ì»¨í…Œì´ë„ˆ í•„í„°
            if container_ids:
                conditions.append("knowledge_container_id = ANY(:container_ids)")
                params["container_ids"] = container_ids
            
            # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°
            conditions.append("content_vector <-> :query_embedding::vector < :threshold")
            
            where_clause = " AND ".join(conditions) if conditions else "1=1"
            
            query = text(f"""
                SELECT 
                    vs_doc_id,
                    file_bss_info_sno,
                    knowledge_container_id,
                    chunk_index,
                    content,
                    1 - (content_vector <-> :query_embedding::vector) as similarity,
                    created_at
                FROM vs_doc_contents_index
                WHERE {where_clause}
                ORDER BY content_vector <-> :query_embedding::vector
                LIMIT :limit
            """)
            
            result = await session.execute(query, params)
            rows = result.fetchall()
            
            search_results = []
            for row in rows:
                search_results.append({
                    "vs_doc_id": row[0],
                    "file_bss_info_sno": row[1],
                    "container_id": row[2],
                    "chunk_index": row[3],
                    "content": row[4],
                    "similarity": float(row[5]),
                    "created_at": row[6],
                    "search_type": "vector_primary"
                })
            
            logger.info(f"ğŸ”® ì£¼ìš” ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            logger.error(f"ì£¼ìš” ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    async def search_with_reference(
        self,
        session: AsyncSession,
        query_text: str,
        container_ids: Optional[List[str]] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """ğŸ”® Reference Search: tb_document_chunks ê¸°ë°˜ ì°¸ì¡° ì •ë³´ í¬í•¨ ê²€ìƒ‰"""
        try:
            conditions = []
            params = {"query_text": f"%{query_text}%", "limit": limit}
            
            # ì»¨í…Œì´ë„ˆ í•„í„°
            if container_ids:
                conditions.append("\"KNOWLEDGE_CONTAINER_ID\" = ANY(:container_ids)")
                params["container_ids"] = container_ids
            
            # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¡°ê±´
            conditions.append("\"CHUNK_TEXT\" ILIKE :query_text")
            
            where_clause = " AND ".join(conditions) if conditions else "1=1"
            
            query = text(f"""
                SELECT 
                    "CHUNK_SNO",
                    "FILE_BSS_INFO_SNO",
                    "KNOWLEDGE_CONTAINER_ID",
                    "CHUNK_INDEX",
                    "CHUNK_TEXT",
                    "SECTION_TITLE",
                    "PAGE_NUMBER",
                    "PARAGRAPH_INDEX",
                    "REFERENCE_INFO",
                    "KEYWORDS",
                    "NAMED_ENTITIES"
                FROM tb_document_chunks
                WHERE {where_clause}
                ORDER BY "CHUNK_INDEX"
                LIMIT :limit
            """)
            
            result = await session.execute(query, params)
            rows = result.fetchall()
            
            search_results = []
            for row in rows:
                keywords = json.loads(row[9]) if row[9] else []
                entities = json.loads(row[10]) if row[10] else []
                
                search_results.append({
                    "chunk_sno": row[0],
                    "file_bss_info_sno": row[1],
                    "container_id": row[2],
                    "chunk_index": row[3],
                    "content": row[4],
                    "section_title": row[5],
                    "page_number": row[6],
                    "paragraph_index": row[7],
                    "reference_info": row[8],  # "3í˜ì´ì§€ 2ë²ˆì§¸ ë‹¨ë½"
                    "keywords": keywords,
                    "named_entities": entities,
                    "search_type": "reference"
                })
            
            logger.info(f"ğŸ”® ì°¸ì¡° ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            logger.error(f"ì°¸ì¡° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    async def search_hybrid(
        self,
        session: AsyncSession,
        query_text: str,
        query_embedding: Optional[List[float]] = None,
        container_ids: Optional[List[str]] = None,
        limit: int = 10,
        similarity_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """ğŸ”® í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ë²¡í„° ìœ ì‚¬ë„) - tb_search_documents í™œìš©"""
        try:
            # í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ë²¡í„° ê²€ìƒ‰ì„ ê²°í•©í•œ ì¿¼ë¦¬
            conditions = []
            params = {"query_text": query_text, "limit": limit}
            
            # ì»¨í…Œì´ë„ˆ í•„í„°
            if container_ids:
                conditions.append("knowledge_container_id = ANY(:container_ids)")
                params["container_ids"] = container_ids
            
            # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì¶”ê°€
            vector_similarity_clause = "0"
            if query_embedding:
                embedding_str = f"[{','.join(map(str, query_embedding))}]"
                params["query_embedding"] = embedding_str
                params["threshold"] = similarity_threshold
                
                vector_similarity_clause = """
                    CASE 
                        WHEN content_vector IS NOT NULL
                        THEN 1 - (content_vector <-> :query_embedding::vector)
                        ELSE 0
                    END
                """
                
                conditions.append("content_vector <-> :query_embedding::vector < :threshold")
            
            where_clause = " AND ".join(conditions) if conditions else "1=1"
            
            # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ ì¿¼ë¦¬
            query = text(f"""
                SELECT 
                    search_doc_id,
                    file_bss_info_sno,
                    knowledge_container_id,
                    chunk_index,
                    content,
                    keywords,
                    proper_nouns,
                    -- í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                    ts_rank(content_tsvector, plainto_tsquery('korean', :query_text)) as keyword_score,
                    -- ë²¡í„° ìœ ì‚¬ë„ ì ìˆ˜
                    {vector_similarity_clause} as vector_similarity,
                    -- í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (í‚¤ì›Œë“œ 30% + ë²¡í„° 70%)
                    (ts_rank(content_tsvector, plainto_tsquery('korean', :query_text)) * 0.3 +
                     {vector_similarity_clause} * 0.7) as hybrid_score
                FROM tb_search_documents
                WHERE {where_clause}
                ORDER BY hybrid_score DESC, keyword_score DESC
                LIMIT :limit
            """)
            
            result = await session.execute(query, params)
            rows = result.fetchall()
            
            # ê²°ê³¼ í¬ë§·íŒ…
            search_results = []
            for row in rows:
                search_results.append({
                    "search_doc_id": row[0],
                    "file_bss_info_sno": row[1],
                    "container_id": row[2],
                    "chunk_index": row[3],
                    "content": row[4],
                    "keywords": row[5] if row[5] else [],
                    "proper_nouns": row[6] if row[6] else [],
                    "keyword_score": float(row[7]) if row[7] else 0.0,
                    "vector_similarity": float(row[8]) if row[8] else 0.0,
                    "hybrid_score": float(row[9]) if row[9] else 0.0,
                    "search_type": "hybrid"
                })
            
            logger.info(f"ğŸ”® í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    # ==============================================
    # ğŸ”® ê¶Œí•œ ê´€ë¦¬ ë° ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    # ==============================================
    
    async def _get_container_permissions(
        self,
        session: AsyncSession,
        container_id: str,
        user_info: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ì§€ì‹ ì»¨í…Œì´ë„ˆ ê¶Œí•œ ì •ë³´ ì¡°íšŒ"""
        try:
            # ê¸°ë³¸ ê¶Œí•œ ì •ë³´
            permissions = {
                "container_id": container_id,
                "read_permission": True,
                "write_permission": True,
                "access_level": "full",
                "user_role": "system"
            }
            
            # ì‚¬ìš©ì ì •ë³´ê°€ ìˆìœ¼ë©´ ì‹¤ì œ ê¶Œí•œ ì¡°íšŒ
            if user_info and user_info.get("user_id"):
                # TODO: ì‹¤ì œ ê¶Œí•œ í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
                # í˜„ì¬ëŠ” ê¸°ë³¸ ê¶Œí•œ ë°˜í™˜
                permissions.update({
                    "user_id": user_info.get("user_id"),
                    "user_role": user_info.get("role", "user")
                })
            
            return permissions
            
        except Exception as e:
            logger.error(f"ì»¨í…Œì´ë„ˆ ê¶Œí•œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {
                "container_id": container_id,
                "read_permission": False,
                "write_permission": False,
                "access_level": "none",
                "error": str(e)
            }

    async def _update_file_metadata_integrated(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        chunk_count: int,
        preprocessed_data: Dict[str, Any],
        permissions: Dict[str, Any],
        storage_result: Dict[str, Any]
    ):
        """ğŸ”® í†µí•© ì•„í‚¤í…ì²˜ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        try:
            # ê¶Œí•œ í™•ì¸
            if not permissions.get("write_permission", False):
                logger.warning(f"íŒŒì¼ {file_bss_info_sno} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ê¶Œí•œ ì—†ìŒ")
                return
            
            # í†µí•© ë©”íƒ€ë°ì´í„° ìƒì„±
            integrated_metadata = {
                "vectorized": True,
                "architecture": "vs_primary",
                "chunk_count": chunk_count,
                "vector_dimension": self.embedding_dimension,
                "storage_summary": {
                    "primary_vectors": storage_result.get("primary_vectors", 0),
                    "metadata_chunks": storage_result.get("metadata_chunks", 0),
                    "hybrid_records": storage_result.get("hybrid_records", 0)
                },
                "permissions": permissions,
                "container_id": permissions.get("container_id"),
                "processing_status": "completed",
                "last_vectorized": datetime.now().isoformat()
            }
            
            query = text("""
                UPDATE tb_file_bss_info 
                SET chunk_count = :chunk_count,
                    korean_metadata = COALESCE(korean_metadata, '{}') || CAST(:metadata AS json)
                WHERE file_bss_info_sno = :file_sno
            """)
            
            await session.execute(query, {
                "chunk_count": chunk_count,
                "metadata": json.dumps(integrated_metadata),
                "file_sno": file_bss_info_sno
            })
            
            logger.info(f"ğŸ”® í†µí•© ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ - íŒŒì¼: {file_bss_info_sno}")
            
        except Exception as e:
            logger.error(f"í†µí•© ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")


# í†µí•© ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
vector_storage_service_integrated = VectorStorageServiceIntegrated()

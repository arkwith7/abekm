"""
ì˜ì–´ NLP ì²˜ë¦¬ ì„œë¹„ìŠ¤

ì—­í• :
1. ì˜ì–´ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• (NLTK)
2. ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬)
3. ë¶ˆìš©ì–´ ì œê±°
4. ì–¸ì–´ ê°ì§€ (ì˜ì–´ íŒë³„)

Dependencies:
- nltk: ê²½ëŸ‰ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬
"""
import logging
import re
from typing import List, Dict, Optional
import asyncio

logger = logging.getLogger(__name__)

# NLTK import (ê²½ëŸ‰, ë¹ ë¦„)
try:
    import nltk
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    from nltk.tag import pos_tag
    
    # í•„ìš”í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì´ˆê¸° 1íšŒë§Œ)
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        logger.info("ğŸ“¥ NLTK punkt tokenizer ë‹¤ìš´ë¡œë“œ ì¤‘...")
        nltk.download('punkt', quiet=True)
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        logger.info("ğŸ“¥ NLTK stopwords ë‹¤ìš´ë¡œë“œ ì¤‘...")
        nltk.download('stopwords', quiet=True)
    
    try:
        nltk.data.find('taggers/averaged_perceptron_tagger')
    except LookupError:
        logger.info("ğŸ“¥ NLTK POS tagger ë‹¤ìš´ë¡œë“œ ì¤‘...")
        nltk.download('averaged_perceptron_tagger', quiet=True)
    
    NLTK_AVAILABLE = True
    logger.info("âœ… NLTK ë¡œë“œ ì™„ë£Œ")
except ImportError:
    NLTK_AVAILABLE = False
    logger.warning("âš ï¸ NLTK ë¡œë“œ ì‹¤íŒ¨ - pip install nltk í•„ìš”")


class EnglishNLPService:
    """
    ì˜ì–´ NLP ì„œë¹„ìŠ¤
    
    íŠ¹ì§•:
    - NLTK ê¸°ë°˜ í† í¬ë‚˜ì´ì§• (ë¹ ë¥´ê³  ê°€ë²¼ì›€)
    - í’ˆì‚¬ íƒœê¹… (POS Tagging)
    - ë¶ˆìš©ì–´ ì œê±°
    - ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    ì„±ëŠ¥:
    - í† í¬ë‚˜ì´ì§•: ~10ms (100ë‹¨ì–´ ê¸°ì¤€)
    - í’ˆì‚¬ íƒœê¹…: ~50ms (100ë‹¨ì–´ ê¸°ì¤€)
    """
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(EnglishNLPService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if EnglishNLPService._initialized:
            return
        
        self.available = NLTK_AVAILABLE
        
        if self.available:
            # ì˜ì–´ ë¶ˆìš©ì–´ ì„¸íŠ¸
            self.stopwords = set(stopwords.words('english'))
            
            # ì¶”ê°€ ë¶ˆìš©ì–´ (ê²€ìƒ‰ì—ì„œ ì œì™¸í•  ë‹¨ì–´)
            self.stopwords.update([
                'will', 'can', 'may', 'must', 'would', 'could', 'should',
                'also', 'however', 'therefore', 'thus', 'furthermore',
                'said', 'says', 'like', 'get', 'got', 'getting', 'using',
                'used', 'use', 'make', 'made', 'makes', 'include', 'includes'
            ])
            
            logger.info("âœ… EnglishNLPService ì´ˆê¸°í™” ì™„ë£Œ")
            print("âœ… EnglishNLPService ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ EnglishNLPService ì‚¬ìš© ë¶ˆê°€ - NLTK ë¯¸ì„¤ì¹˜")
            print("âš ï¸ EnglishNLPService ì‚¬ìš© ë¶ˆê°€ - pip install nltk ì‹¤í–‰ í•„ìš”")
        
        EnglishNLPService._initialized = True
    
    def is_english(self, text: str) -> bool:
        """
        í…ìŠ¤íŠ¸ê°€ ì˜ì–´ì¸ì§€ íŒë‹¨
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            bool: ì˜ì–´ í…ìŠ¤íŠ¸ ì—¬ë¶€
        """
        if not text:
            return False
        
        # ì˜ì–´ ì•ŒíŒŒë²³ ë¹„ìœ¨ ê³„ì‚°
        english_chars = sum(1 for c in text if c.isascii() and c.isalpha())
        total_chars = sum(1 for c in text if c.isalpha())
        
        if total_chars == 0:
            return False
        
        # ì˜ì–´ ë¹„ìœ¨ 70% ì´ìƒì´ë©´ ì˜ì–´ë¡œ íŒë‹¨
        return (english_chars / total_chars) >= 0.7
    
    async def analyze_english_text(self, text: str) -> Dict:
        """
        ì˜ì–´ í…ìŠ¤íŠ¸ ë¶„ì„
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            dict: {
                'tokens': List[str],  # í† í° ëª©ë¡
                'keywords': List[str],  # í‚¤ì›Œë“œ ëª©ë¡
                'pos_tags': List[Tuple[str, str]]  # (í† í°, í’ˆì‚¬) ìŒ
            }
        """
        if not self.available:
            return self._fallback_analysis(text)
        
        try:
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self._analyze_sync, text)
            return result
        except Exception as e:
            logger.error(f"ì˜ì–´ ë¶„ì„ ì‹¤íŒ¨: {e}, í´ë°± ì‚¬ìš©")
            return self._fallback_analysis(text)
    
    def _analyze_sync(self, text: str) -> Dict:
        """
        ë™ê¸° ì˜ì–´ ë¶„ì„ (NLTK)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼
        """
        # í† í°í™” (ì†Œë¬¸ì ë³€í™˜)
        tokens = word_tokenize(text.lower())
        
        # í’ˆì‚¬ íƒœê¹…
        pos_tags = pos_tag(tokens)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ì „ëµ:
        # 1. ëª…ì‚¬ (NN, NNS, NNP, NNPS)
        # 2. ë™ì‚¬ (VB, VBD, VBG, VBN, VBP, VBZ)
        # 3. í˜•ìš©ì‚¬ (JJ, JJR, JJS)
        # 4. ìµœì†Œ 2ê¸€ì ì´ìƒ
        # 5. ë¶ˆìš©ì–´ ì œì™¸
        
        keyword_pos = {
            'NN', 'NNS', 'NNP', 'NNPS',  # ëª…ì‚¬
            'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # ë™ì‚¬
            'JJ', 'JJR', 'JJS'  # í˜•ìš©ì‚¬
        }
        
        keywords = []
        for token, pos in pos_tags:
            # ì•ŒíŒŒë²³ë§Œ í¬í•¨ëœ ë‹¨ì–´
            if not token.isalpha():
                continue
            
            # ìµœì†Œ ê¸¸ì´ ì²´í¬
            if len(token) < 2:
                continue
            
            # í’ˆì‚¬ ì²´í¬
            if pos not in keyword_pos:
                continue
            
            # ë¶ˆìš©ì–´ ì²´í¬
            if token in self.stopwords:
                continue
            
            keywords.append(token)
        
        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        unique_keywords = []
        seen = set()
        for kw in keywords:
            if kw not in seen:
                seen.add(kw)
                unique_keywords.append(kw)
        
        logger.info(f"âœ… ì˜ì–´ ë¶„ì„: {len(tokens)}ê°œ í† í° â†’ {len(unique_keywords)}ê°œ í‚¤ì›Œë“œ")
        
        return {
            'tokens': tokens,
            'keywords': unique_keywords[:30],  # ìµœëŒ€ 30ê°œ
            'pos_tags': pos_tags
        }
    
    def _fallback_analysis(self, text: str) -> Dict:
        """
        í´ë°±: ì •ê·œì‹ ê¸°ë°˜ ê°„ë‹¨ ë¶„ì„
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼
        """
        # ë‹¨ìˆœ ë‹¨ì–´ ë¶„ë¦¬
        tokens = re.findall(r'\b[a-zA-Z]{2,}\b', text.lower())
        
        # ê¸°ë³¸ ë¶ˆìš©ì–´ ì œê±°
        basic_stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',
            'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was',
            'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
            'do', 'does', 'did', 'will', 'would', 'could', 'should',
            'that', 'this', 'these', 'those', 'it', 'its', 'they', 'their'
        }
        
        keywords = [t for t in tokens if t not in basic_stopwords]
        
        # ì¤‘ë³µ ì œê±°
        unique_keywords = list(dict.fromkeys(keywords))
        
        logger.info(f"âœ… ì˜ì–´ ë¶„ì„(í´ë°±): {len(tokens)}ê°œ í† í° â†’ {len(unique_keywords)}ê°œ í‚¤ì›Œë“œ")
        
        return {
            'tokens': tokens,
            'keywords': unique_keywords[:30],
            'pos_tags': []
        }

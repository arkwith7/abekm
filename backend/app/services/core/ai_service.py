from typing import List, Optional, Dict, Any
import time
from langchain.llms.base import BaseLLM
from langchain.embeddings.base import Embeddings
from langchain_aws import ChatBedrock, ChatBedrockConverse, BedrockEmbeddings
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings, ChatOpenAI, OpenAIEmbeddings
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from loguru import logger
import os
from app.core.config import settings


class MultiVendorAIService:
    """ë©€í‹° ë²¤ë” AI ì„œë¹„ìŠ¤ - AWS Bedrock, Azure OpenAI, OpenAI ì§€ì›"""
    
    # ì¶”ë¡  ëª¨ë¸ ëª©ë¡ (temperature ë¯¸ì§€ì›)
    REASONING_MODELS = ["o1", "o3", "gpt-5"]
    
    def __init__(self):
        # Provider ë³„ LLM / Embedding ì¸ìŠ¤í„´ìŠ¤
        self.llm_providers: Dict[str, BaseLLM] = {}
        self.embedding_providers: Dict[str, Embeddings] = {}

        # ê¸°ë³¸ ì œê³µì ë° ê°„ë‹¨ ë©”íŠ¸ë¦­ êµ¬ì¡°
        self.default_provider: str = settings.get_current_llm_provider()
        self._stats: Dict[str, Dict[str, Any]] = {}
        self._last_switch_time: Optional[str] = None
        self._init_errors: Dict[str, str] = {}

        # ê° AI ì œê³µì ì´ˆê¸°í™” (ì‹¤íŒ¨ëŠ” ê¸°ë¡)
        self._init_azure_openai()
        self._init_bedrock()
        self._init_openai()
    
    def _is_reasoning_model(self, model_name: str) -> bool:
        """ì¶”ë¡  ëª¨ë¸ ì—¬ë¶€ í™•ì¸ (temperature ë¯¸ì§€ì› ëª¨ë¸)"""
        model_lower = model_name.lower()
        return any(reasoning_model in model_lower for reasoning_model in self.REASONING_MODELS)
        
    def _init_azure_openai(self):
        """Azure OpenAI ì´ˆê¸°í™”"""
        try:
            if settings.azure_openai_api_key and settings.azure_openai_endpoint:
                # LLM ì´ˆê¸°í™” - ë™ì  ëª¨ë¸ ì‚¬ìš©
                deployment = settings.get_current_llm_model() if settings.get_current_llm_provider() == "azure_openai" else settings.azure_openai_llm_deployment

                llm_kwargs: Dict[str, Any] = {
                    "azure_endpoint": settings.azure_openai_endpoint,
                    "api_key": settings.azure_openai_api_key,
                    "api_version": settings.azure_openai_api_version,
                    "deployment_name": deployment,
                }
                
                # ì¶”ë¡  ëª¨ë¸ ì—¬ë¶€ í™•ì¸
                is_reasoning = self._is_reasoning_model(deployment)
                
                if is_reasoning:
                    # ì¶”ë¡  ëª¨ë¸: temperature ì œê±°, max_completion_tokens ì‚¬ìš©
                    llm_kwargs["model_kwargs"] = {"max_completion_tokens": settings.max_tokens}
                    logger.info(f"ğŸ§  ì¶”ë¡  ëª¨ë¸ ê°ì§€: {deployment} (temperature ë¯¸ì§€ì›)")
                else:
                    # ì¼ë°˜ ëª¨ë¸: temperature ì‚¬ìš©
                    llm_kwargs["temperature"] = settings.temperature
                    llm_kwargs["max_tokens"] = settings.max_tokens
                    logger.info(f"ğŸ’¬ ì¼ë°˜ ëª¨ë¸: {deployment} (temperature={settings.temperature})")
                
                self.llm_providers["azure_openai"] = AzureChatOpenAI(**llm_kwargs)
                
                # ì„ë² ë”© ì´ˆê¸°í™” - ë™ì  ëª¨ë¸ ì‚¬ìš©
                self.embedding_providers["azure_openai"] = AzureOpenAIEmbeddings(
                    azure_endpoint=settings.azure_openai_endpoint,
                    api_key=settings.azure_openai_api_key,
                    api_version=settings.azure_openai_api_version,
                    deployment=settings.get_current_embedding_model() if settings.get_current_embedding_provider() == "azure_openai" else settings.azure_openai_embedding_deployment,
                )
                
                logger.info("Azure OpenAI ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Azure OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _init_bedrock(self):
        """AWS Bedrock ì´ˆê¸°í™”"""
        try:
            if settings.aws_access_key_id and settings.aws_secret_access_key:
                # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                os.environ["AWS_ACCESS_KEY_ID"] = settings.aws_access_key_id
                os.environ["AWS_SECRET_ACCESS_KEY"] = settings.aws_secret_access_key
                os.environ["AWS_DEFAULT_REGION"] = settings.aws_region
                
                # LLM ì´ˆê¸°í™” - ë™ì  ëª¨ë¸ ì‚¬ìš©
                model_id = settings.get_current_llm_model() if settings.get_current_llm_provider() == "bedrock" else settings.bedrock_llm_model_id
                
                # êµì°¨ ë¦¬ì „ ì¶”ë¡  ëª¨ë¸ ê°ì§€ (us., eu., apac. ë“± í”„ë¦¬í”½ìŠ¤)
                is_cross_region = any(model_id.startswith(prefix) for prefix in ["us.", "eu.", "apac.", "global."])
                
                if is_cross_region:
                    # êµì°¨ ë¦¬ì „ ì¶”ë¡ : ChatBedrockConverse ì‚¬ìš© (Converse API)
                    logger.info(f"ğŸŒ êµì°¨ ë¦¬ì „ ì¶”ë¡  ëª¨ë¸ ê°ì§€: {model_id} â†’ ChatBedrockConverse ì‚¬ìš©")
                    self.llm_providers["bedrock"] = ChatBedrockConverse(
                        model=model_id,
                        region_name=settings.aws_region,
                        max_tokens=settings.max_tokens,
                        temperature=settings.temperature,
                    )
                else:
                    # ë‹¨ì¼ ë¦¬ì „: ChatBedrock ì‚¬ìš© (InvokeModel API)
                    logger.info(f"ğŸ“ ë‹¨ì¼ ë¦¬ì „ ëª¨ë¸: {model_id} â†’ ChatBedrock ì‚¬ìš©")
                    self.llm_providers["bedrock"] = ChatBedrock(
                        model_id=model_id,
                        region_name=settings.aws_region,
                        model_kwargs={
                            "max_tokens": settings.max_tokens,
                            "temperature": settings.temperature,
                            "top_p": settings.top_p,
                        }
                    )
                
                # ì„ë² ë”© ì´ˆê¸°í™” - ë™ì  ëª¨ë¸ ì‚¬ìš©
                self.embedding_providers["bedrock"] = BedrockEmbeddings(
                    model_id=settings.get_current_embedding_model() if settings.get_current_embedding_provider() == "bedrock" else settings.bedrock_embedding_model_id,
                    region_name=settings.aws_region,
                )
                
                logger.info("AWS Bedrock ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"AWS Bedrock ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _init_openai(self):
        """OpenAI ì´ˆê¸°í™”"""
        try:
            if settings.openai_api_key:
                # LLM ì´ˆê¸°í™” - ë™ì  ëª¨ë¸ ì‚¬ìš©
                model_name = settings.get_current_llm_model() if settings.get_current_llm_provider() == "openai" else settings.openai_llm_model
                
                llm_kwargs: Dict[str, Any] = {
                    "api_key": settings.openai_api_key,
                    "model": model_name,
                }
                
                # ì¶”ë¡  ëª¨ë¸ ì—¬ë¶€ í™•ì¸
                is_reasoning = self._is_reasoning_model(model_name)
                
                if is_reasoning:
                    # ì¶”ë¡  ëª¨ë¸: temperature ì œê±°, max_completion_tokens ì‚¬ìš©
                    llm_kwargs["model_kwargs"] = {"max_completion_tokens": settings.max_tokens}
                    logger.info(f"ğŸ§  ì¶”ë¡  ëª¨ë¸ ê°ì§€: {model_name} (temperature ë¯¸ì§€ì›)")
                else:
                    # ì¼ë°˜ ëª¨ë¸: temperature ì‚¬ìš©
                    llm_kwargs["temperature"] = settings.temperature
                    llm_kwargs["max_tokens"] = settings.max_tokens
                    logger.info(f"ğŸ’¬ ì¼ë°˜ ëª¨ë¸: {model_name} (temperature={settings.temperature})")
                
                self.llm_providers["openai"] = ChatOpenAI(**llm_kwargs)
                
                # ì„ë² ë”© ì´ˆê¸°í™” - ë™ì  ëª¨ë¸ ì‚¬ìš©
                self.embedding_providers["openai"] = OpenAIEmbeddings(
                    api_key=settings.openai_api_key,
                    model=settings.get_current_embedding_model() if settings.get_current_embedding_provider() == "openai" else settings.openai_embedding_model,
                )
                
                logger.info("OpenAI ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def get_llm(self, provider: Optional[str] = None) -> Optional[BaseLLM]:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        provider = provider or self.default_provider
        
        if provider in self.llm_providers:
            return self.llm_providers[provider]
        
        # ê¸°ë³¸ ì œê³µìê°€ ì‹¤íŒ¨í•˜ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ì œê³µì ì‹œë„
        for fallback_provider in settings.llm_providers:
            if fallback_provider in self.llm_providers:
                logger.warning(f"ê¸°ë³¸ ì œê³µì {provider} ì‹¤íŒ¨, {fallback_provider}ë¡œ í´ë°±")
                return self.llm_providers[fallback_provider]
        
        logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    def get_embeddings(self, provider: Optional[str] = None) -> Optional[Embeddings]:
        """ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        provider = provider or self.default_provider
        
        if provider in self.embedding_providers:
            return self.embedding_providers[provider]
        
        # ê¸°ë³¸ ì œê³µìê°€ ì‹¤íŒ¨í•˜ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ì œê³µì ì‹œë„
        for fallback_provider in settings.llm_providers:
            if fallback_provider in self.embedding_providers:
                logger.warning(f"ê¸°ë³¸ ì„ë² ë”© ì œê³µì {provider} ì‹¤íŒ¨, {fallback_provider}ë¡œ í´ë°±")
                return self.embedding_providers[fallback_provider]
        
        logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    async def chat(self, message: str, provider: Optional[str] = None) -> str:
        """ë‹¨ì¼ ì‚¬ìš©ì ì…ë ¥ ë¬¸ìì—´ì— ëŒ€í•œ LLM ì‘ë‹µ (ë‹¨ìˆœ ë¬¸ìì—´ ë°˜í™˜).
        NOTE: ë‹¤ì¤‘ turn ë° ê¸°ë¡ í¬í•¨ ì‘ë‹µì€ chat_completion() ì‚¬ìš© ê¶Œì¥.
        """
        try:
            llm = self.get_llm(provider)
            if not llm:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ LLMì´ ì—†ìŠµë‹ˆë‹¤")

            messages = [HumanMessage(content=message)]
            start = time.time()
            response = await llm.ainvoke(messages)
            elapsed = int((time.time() - start) * 1000)
            used_provider = provider or self.default_provider
            if used_provider not in self._stats:
                self._stats[used_provider] = {"requests": 0, "errors": 0, "last_error": None, "latencies_ms": []}
            self._stats[used_provider]["requests"] += 1
            self._stats[used_provider]["latencies_ms"].append(elapsed)
            # ì¼ë¶€ LLM êµ¬í˜„ì€ ê°ì²´/ë¬¸ìì—´ ëª¨ë‘ ê°€ëŠ¥
            if hasattr(response, 'content'):
                return getattr(response, 'content')  # type: ignore[attr-defined]
            return str(response)
        except Exception as e:
            used_provider = provider or self.default_provider
            if used_provider not in self._stats:
                self._stats[used_provider] = {"requests": 0, "errors": 0, "last_error": None, "latencies_ms": []}
            self._stats[used_provider]["errors"] += 1
            self._stats[used_provider]["last_error"] = str(e)
            logger.error(f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        provider: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> Dict[str, Any]:
        """ë‹¤ì¤‘ ë©”ì‹œì§€(chat history) ì…ë ¥ì„ ë°›ì•„ í‘œì¤€ dict ë°˜í™˜.
        messages ì˜ˆì‹œ: [{"role": "user"|"assistant"|"system", "content": "..."}, ...]
        ë°˜í™˜: {"response": str, "provider": str, "raw": Any}
        """
        if not messages:
            raise ValueError("messages ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        llm = self.get_llm(provider)
        if not llm:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ LLMì´ ì—†ìŠµë‹ˆë‹¤")
        
        # íŒŒë¼ë¯¸í„° ì ìš© (reasoning ëª¨ë¸ì€ temperature ë¯¸ì§€ì›)
        # gpt-5-nanoëŠ” reasoning ëª¨ë¸ì´ë¯€ë¡œ temperature ì œê±°
        used_provider = provider or self.default_provider
        model_name = getattr(llm, "deployment_name", "") or getattr(llm, "model_id", "")
        is_reasoning = self._is_reasoning_model(model_name)
        
        if not is_reasoning:
            if max_tokens:
                llm = llm.bind(max_tokens=max_tokens)
            if temperature is not None:
                llm = llm.bind(temperature=temperature)

        lc_messages: List[HumanMessage | AIMessage] = []
        for m in messages:
            role = (m.get("role") or "user").lower()
            content = m.get("content") or ""
            if role == "assistant":
                lc_messages.append(AIMessage(content=content))
            else:
                # system / user ë‘˜ ë‹¤ HumanMessage ë¡œ ì²˜ë¦¬ (ê°„ë‹¨í™”)
                lc_messages.append(HumanMessage(content=content))

        start = time.time()
        used_provider = provider or self.default_provider
        try:
            result = await llm.ainvoke(lc_messages)
            elapsed = int((time.time() - start) * 1000)
            if used_provider not in self._stats:
                self._stats[used_provider] = {"requests": 0, "errors": 0, "last_error": None, "latencies_ms": []}
            self._stats[used_provider]["requests"] += 1
            self._stats[used_provider]["latencies_ms"].append(elapsed)
            if hasattr(result, "content"):
                text = getattr(result, "content")  # type: ignore[attr-defined]
            else:
                text = str(result)
            return {"response": text, "provider": used_provider, "raw": result}
        except Exception as e:
            if used_provider not in self._stats:
                self._stats[used_provider] = {"requests": 0, "errors": 0, "last_error": None, "latencies_ms": []}
            self._stats[used_provider]["errors"] += 1
            self._stats[used_provider]["last_error"] = str(e)
            logger.error(f"chat_completion ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    async def chat_stream(
        self,
        messages: List[Dict[str, str]],
        provider: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ):
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬. ë©”ì‹œì§€ ëª©ë¡ì„ ì§ì ‘ ë°›ìŠµë‹ˆë‹¤."""
        try:
            current_provider = provider or self.default_provider
            llm = self.get_llm(current_provider)
            if not llm:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ LLMì´ ì—†ìŠµë‹ˆë‹¤")

            # í˜„ì¬ ëª¨ë¸ëª… ì¶”ì  (reasoning ëª¨ë¸ ì—¬ë¶€ íŒë‹¨ìš©)
            current_model = None
            if current_provider == "azure_openai":
                current_model = settings.azure_openai_llm_deployment
            elif current_provider == "bedrock":
                current_model = settings.bedrock_llm_model_id
            elif current_provider == "openai":
                current_model = settings.openai_llm_model

            is_reasoning = self._is_reasoning_model(current_model or "")

            # ìš”ì²­ ë‹¨ìœ„ LLM íŒŒë¼ë¯¸í„° ì ìš© (reasoning ëª¨ë¸ì€ temperature ë¯¸ì§€ì›)
            bind_kwargs: Dict[str, Any] = {}
            if max_tokens:
                if is_reasoning:
                    bind_kwargs["max_completion_tokens"] = max_tokens
                else:
                    bind_kwargs["max_tokens"] = max_tokens
            if temperature is not None:
                if is_reasoning:
                    logger.info(
                        f"âš ï¸ reasoning ëª¨ë¸({current_model})ì€ temperatureë¥¼ ì§€ì›í•˜ì§€ ì•Šì•„ ë¬´ì‹œí•©ë‹ˆë‹¤"
                    )
                else:
                    bind_kwargs["temperature"] = temperature
            if bind_kwargs:
                llm = llm.bind(**bind_kwargs)
                logger.info(f"ğŸ”§ LLM íŒŒë¼ë¯¸í„° ë°”ì¸ë”©: provider={current_provider}, params={bind_kwargs}")

            # LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            lc_messages: List[HumanMessage | AIMessage | SystemMessage] = []
            for m in messages:
                role = (m.get("role") or "user").lower()
                content = m.get("content") or ""
                if not content:
                    continue
                if role == "assistant":
                    lc_messages.append(AIMessage(content=content))
                elif role == "system":
                    lc_messages.append(SystemMessage(content=content))
                else:  # "user"
                    lc_messages.append(HumanMessage(content=content))

            logger.info(f"ğŸ” AI ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ - ì œê³µì: {current_provider}, ë©”ì‹œì§€ ìˆ˜: {len(lc_messages)}")

            # ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
            chunk_count = 0
            async for chunk in llm.astream(lc_messages):
                chunk_count += 1
                # ì´ˆê¸° ëª‡ ê°œ ì²­í¬ëŠ” ë””ë²„ê·¸ë¡œ ë‚¨ê²¨ ìŠ¤íŠ¸ë¦¬ë° ê±´ê°•ìƒíƒœ í™•ì¸
                if chunk_count <= 3:
                    try:
                        logger.debug(f"ğŸ”¹ ìŠ¤íŠ¸ë¦¼ ì²­í¬#{chunk_count} ìˆ˜ì‹ : {str(chunk)[:120]}...")
                    except Exception:
                        pass
                content = None

                # content ì†ì„±ì´ ìˆëŠ” ê²½ìš°
                if hasattr(chunk, 'content') and getattr(chunk, 'content'):
                    content = getattr(chunk, 'content')
                # text ì†ì„±ì´ ìˆëŠ” ê²½ìš° (ë©”ì„œë“œê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸)
                elif hasattr(chunk, 'text'):
                    chunk_text_attr = getattr(chunk, 'text')
                    if callable(chunk_text_attr):
                        try:
                            content = chunk_text_attr()
                        except Exception as e:
                            logger.error(f"ğŸ” ì²­í¬ #{chunk_count} - text() í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                            content = str(chunk)
                    else:
                        content = chunk_text_attr
                # ê¸°íƒ€ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
                elif str(chunk).strip():
                    content = str(chunk)

                # ìœ íš¨í•œ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ yield
                if content:
                    content_str = str(content)
                    if content_str.strip():
                        yield content_str

        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    async def get_text_embeddings(self, texts: List[str], provider: Optional[str] = None) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            embeddings = self.get_embeddings(provider)
            if not embeddings:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # ì„ë² ë”© ìƒì„±
            embedding_vectors = await embeddings.aembed_documents(texts)
            return embedding_vectors
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    async def search_documents(self, query: str, provider: Optional[str] = None) -> List[float]:
        """ë¬¸ì„œ ê²€ìƒ‰ìš© ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„° ìƒì„± (ë‹¨ì¼ ë²¡í„° List[float] ë°˜í™˜)"""
        try:
            embeddings = self.get_embeddings(provider)
            if not embeddings:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = await embeddings.aembed_query(query)
            return query_embedding
            
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def get_available_providers(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì ëª©ë¡ ë° ê¸°ë³¸ ì œê³µì ë°˜í™˜"""
        return {
            "llm_providers": list(self.llm_providers.keys()),
            "embedding_providers": list(self.embedding_providers.keys()),
            "default_provider": self.default_provider,
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ai_service = MultiVendorAIService()

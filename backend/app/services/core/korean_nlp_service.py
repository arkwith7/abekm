"""
í•œêµ­ì–´ NLP ì²˜ë¦¬ ì„œë¹„ìŠ¤ (with kiwipiepy)

ë³€ê²½ ì‚¬í•­ (2025-10-17):
- kiwipiepy ì¬ë„ì… (í˜•íƒœì†Œ ë¶„ì„ ì •í™•ë„ í–¥ìƒ)
- ê·œì¹™ ê¸°ë°˜ â†’ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜
- ê²€ìƒ‰ í’ˆì§ˆ ê°œì„  ëª©í‘œ

ì—­í• :
1. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ (kiwipiepy)
2. í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬ ê¸°ë°˜)
3. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (Azure OpenAI text-embedding-3-small)
4. ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
"""
import logging
from typing import List, Optional, Tuple
import hashlib
import struct
import random

# kiwipiepy import
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False
    print("âš ï¸ kiwipiepy ë¡œë“œ ì‹¤íŒ¨")

# ì„ë² ë”© ì„œë¹„ìŠ¤ import
try:
    from app.services.core.embedding_service import EmbeddingService
    EMBEDDING_SERVICE_AVAILABLE = True
except ImportError:
    EMBEDDING_SERVICE_AVAILABLE = False
    print("âš ï¸ EmbeddingService ë¡œë“œ ì‹¤íŒ¨")

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


class KoreanNLPService:
    """
    í•œêµ­ì–´ NLP ì„œë¹„ìŠ¤ (with kiwipiepy)
    
    ì—­í• :
    1. í˜•íƒœì†Œ ë¶„ì„ (kiwipiepy)
    2. í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬)
    3. ì„ë² ë”© ìƒì„± (Azure OpenAI text-embedding-3-small)
    
    ì„±ëŠ¥ ì¸¡ì •:
    - í˜•íƒœì†Œ ë¶„ì„: ~50ms
    - ì„ë² ë”© ìƒì„±: ~200ms
    """
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(KoreanNLPService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ
        if KoreanNLPService._initialized:
            return
            
        self.kiwi = None
        self.embedding_service = None
        self.english_nlp = None
        
        # kiwipiepy ì´ˆê¸°í™”
        if KIWI_AVAILABLE:
            try:
                self.kiwi = Kiwi()
                logger.info("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                print("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print(f"âŒ Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        else:
            logger.warning("kiwipiepy ì‚¬ìš© ë¶ˆê°€, ê·œì¹™ ê¸°ë°˜ í´ë°± ì‚¬ìš©")
        
        # ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        if EMBEDDING_SERVICE_AVAILABLE:
            try:
                self.embedding_service = EmbeddingService()
                logger.info("âœ… KoreanNLPService ì´ˆê¸°í™” ì™„ë£Œ (Kiwi + ì„ë² ë”©)")
                print("âœ… KoreanNLPService ì´ˆê¸°í™” ì™„ë£Œ (Kiwi + ì„ë² ë”©)")
            except Exception as e:
                logger.error(f"EmbeddingService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print(f"âŒ EmbeddingService ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        else:
            logger.warning("EmbeddingService ì‚¬ìš© ë¶ˆê°€, ë”ë¯¸ ì„ë² ë”© ì‚¬ìš©")
        
        # ì˜ì–´ NLP ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        try:
            from app.services.core.english_nlp_service import EnglishNLPService
            self.english_nlp = EnglishNLPService()
            logger.info("âœ… ì˜ì–´ NLP ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            print("âœ… ì˜ì–´ NLP ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì˜ì–´ NLP ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print(f"âš ï¸ ì˜ì–´ NLP ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ì´ˆê¸°í™” ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
        KoreanNLPService._initialized = True
    
    async def generate_korean_embedding(self, text: str) -> Optional[List[float]]:
        """
        í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (Azure OpenAI)
        
        Args:
            text: ì„ë² ë”©ì„ ìƒì„±í•  í…ìŠ¤íŠ¸
            
        Returns:
            1536ì°¨ì› ì„ë² ë”© ë²¡í„° ë˜ëŠ” None
        """
        if not text or not text.strip():
            return None
            
        try:
            if self.embedding_service:
                # Azure OpenAI ì„ë² ë”© ì„œë¹„ìŠ¤ ì‚¬ìš©
                embedding = await self.embedding_service.get_embedding(text)
                logger.info(f"ì„ë² ë”© ìƒì„± ì„±ê³µ: {len(embedding)}ì°¨ì›")
                return embedding
            else:
                # ì„ë² ë”© ì„œë¹„ìŠ¤ê°€ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ë²¡í„° ë°˜í™˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
                logger.warning("ì„ë² ë”© ì„œë¹„ìŠ¤ ì—†ìŒ, ë”ë¯¸ ë²¡í„° ë°˜í™˜")
                return self._create_dummy_embedding(text)
                
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_dummy_embedding(text)
    
    async def generate_embeddings_batch(
        self, 
        texts: List[str], 
        batch_size: int = 16
    ) -> List[Optional[List[float]]]:
        """
        ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (Azure OpenAIëŠ” ìµœëŒ€ 16ê°œ)
            
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            
            try:
                if self.embedding_service and hasattr(self.embedding_service, 'get_embeddings_batch'):
                    # Azure OpenAI ë°°ì¹˜ ì²˜ë¦¬
                    batch_embeddings = await self.embedding_service.get_embeddings_batch(batch)
                    embeddings.extend(batch_embeddings)
                else:
                    # ê°œë³„ ì²˜ë¦¬ í´ë°±
                    for text in batch:
                        emb = await self.generate_korean_embedding(text)
                        embeddings.append(emb)
                        
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì±„ìš°ê¸°
                embeddings.extend([None] * len(batch))
        
        logger.info(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
        return embeddings
    
    async def analyze_korean_text(self, text: str) -> dict:
        """
        í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ (kiwipiepy í˜•íƒœì†Œ ë¶„ì„)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            dict: {
                'tokens': List[str],  # í˜•íƒœì†Œ ë¶„ì„ëœ í† í°
                'keywords': List[str],  # ì¶”ì¶œëœ í‚¤ì›Œë“œ (ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬)
                'pos_tags': List[Tuple[str, str]]  # (í† í°, í’ˆì‚¬) ìŒ
            }
        """
        logger.info(f"âœ… analyze_korean_text í˜¸ì¶œ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
        
        # kiwipiepy ì‚¬ìš© (ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰)
        if self.kiwi:
            try:
                import asyncio
                
                # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, self.kiwi.analyze, text)
                
                # ì²« ë²ˆì§¸ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
                if result and len(result) > 0:
                    tokens = result[0][0]  # List[Token]
                    
                    # í’ˆì‚¬ íƒœê¹…
                    pos_tags = [(token.form, token.tag) for token in tokens]
                    
                    # í‚¤ì›Œë“œ ì¶”ì¶œ ì „ëµ:
                    # 1. ëª…ì‚¬ëŠ” ëª¨ë‘ í¬í•¨ (NNG, NNP, NNB)
                    # 2. ë™ì‚¬/í˜•ìš©ì‚¬ëŠ” ì–´ê°„ë§Œ (VV, VA) - ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
                    # 3. ë³µí•©ëª…ì‚¬ ì¬êµ¬ì„± (ì—°ì†ëœ ëª…ì‚¬ ê²°í•©)
                    
                    # ë‹¨ê³„ 1: ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                    noun_pos = ['NNG', 'NNP', 'NNB']  # ëª…ì‚¬ë¥˜
                    verb_adj_pos = ['VV', 'VA']  # ë™ì‚¬, í˜•ìš©ì‚¬
                    
                    keywords = []
                    compound_noun = []  # ë³µí•©ëª…ì‚¬ ë²„í¼
                    
                    for i, token in enumerate(tokens):
                        # ëª…ì‚¬ì¸ ê²½ìš°
                        if token.tag in noun_pos and len(token.form) >= 2:
                            compound_noun.append(token.form)
                        else:
                            # ë³µí•©ëª…ì‚¬ ì™„ì„±
                            if compound_noun:
                                if len(compound_noun) == 1:
                                    keywords.append(compound_noun[0])
                                else:
                                    # ë³µí•©ëª…ì‚¬ ê²°í•© (ì˜ˆ: ['í˜ì‹ ', 'ê°€'] â†’ 'í˜ì‹ ê°€')
                                    combined = ''.join(compound_noun)
                                    keywords.append(combined)
                                    # ê°œë³„ ëª…ì‚¬ë„ ì¶”ê°€ (ë¶€ë¶„ ë§¤ì¹­ìš©)
                                    for noun in compound_noun:
                                        if len(noun) >= 2:
                                            keywords.append(noun)
                                compound_noun = []
                            
                            # ë™ì‚¬/í˜•ìš©ì‚¬ ì–´ê°„ ì¶”ê°€
                            if token.tag in verb_adj_pos and len(token.form) >= 2:
                                keywords.append(token.form)
                    
                    # ë§ˆì§€ë§‰ ë³µí•©ëª…ì‚¬ ì²˜ë¦¬
                    if compound_noun:
                        if len(compound_noun) == 1:
                            keywords.append(compound_noun[0])
                        else:
                            combined = ''.join(compound_noun)
                            keywords.append(combined)
                            for noun in compound_noun:
                                if len(noun) >= 2:
                                    keywords.append(noun)
                    
                    # ë¶ˆìš©ì–´ ì œê±°
                    stopwords = {
                        'ê²ƒ', 'ê±°', 'ìˆ˜', 'ë“±', 'ë€', 'ëŒ€í•´', 'ê´€í•´', 'ìœ„í•´', 'ë•Œë¬¸',
                        'ê·¸', 'ì´', 'ì €', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ëŒ€í•˜', 'ì•Œë¦¬', 'ì£¼'
                    }
                    keywords = [kw for kw in keywords if kw not in stopwords]
                    
                    # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
                    seen = set()
                    unique_keywords = []
                    for kw in keywords:
                        if kw not in seen:
                            seen.add(kw)
                            unique_keywords.append(kw)
                    
                    logger.info(f"âœ… Kiwi ë¶„ì„: {len(tokens)}ê°œ í† í° â†’ {len(unique_keywords)}ê°œ í‚¤ì›Œë“œ")
                    
                    return {
                        'tokens': [token.form for token in tokens],
                        'keywords': unique_keywords[:30],
                        'pos_tags': pos_tags
                    }
                    
            except Exception as e:
                logger.error(f"Kiwi ë¶„ì„ ì‹¤íŒ¨: {e}, ê·œì¹™ ê¸°ë°˜ í´ë°± ì‚¬ìš©")
        
        # í´ë°±: ê·œì¹™ ê¸°ë°˜ ë¶„ì„
        return self._analyze_korean_text_fallback(text)
    
    def _analyze_korean_text_fallback(self, text: str) -> dict:
        """
        ê·œì¹™ ê¸°ë°˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ (í´ë°±)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            dict: analyze_korean_textì™€ ë™ì¼í•œ í˜•ì‹
        """
        logger.info(f"âš ï¸ ê·œì¹™ ê¸°ë°˜ í´ë°± ì‚¬ìš© - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
        
        # ê°„ë‹¨í•œ í† í° ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
        tokens = text.strip().split()
        
        # í•œêµ­ì–´ ì¡°ì‚¬ ë° ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            # ì¡°ì‚¬
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 
            'ë¡œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ì„œ', 'ê»˜ì„œ', 'ë§Œ', 'ë¼ë„', 'ì´ë¼ë„',
            # ì˜ì¡´ëª…ì‚¬/ì–´ë¯¸
            'ê²ƒ', 'ê±°', 'ìˆ˜', 'ë“±', 'ë€', 'ëŒ€í•´', 'ê´€í•´', 'ìœ„í•´', 'ë•Œë¬¸',
            # ëŒ€ëª…ì‚¬
            'ê·¸', 'ì´', 'ì €', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ',
            # ì–´ë¯¸
            'í•˜ë‚˜ìš”', 'í•œë‹¤', 'í–ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤'
        }
        
        # í•œêµ­ì–´ ì¡°ì‚¬ (ì–´ë¯¸ì— ë¶™ëŠ” í˜•íƒœ)
        josa_suffixes = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 
                         'ë¡œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì—ì„œ', 'ê»˜ì„œ', 'ë§Œ', 'ë¼ë„', 'ì´ë¼ë„',
                         'ì—ê²Œ', 'í•œí…Œ', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ë§ˆì €', 'ì¡°ì°¨', 'ë°–ì—']
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±° + ì¡°ì‚¬ ë¶„ë¦¬ + ê¸¸ì´ í•„í„°ë§)
        keywords = []
        for token in tokens:
            # ì™„ì „ ì¼ì¹˜ ë¶ˆìš©ì–´ ì œê±°
            if token in stopwords:
                continue
            
            # ì¡°ì‚¬ ë¶„ë¦¬ ì²˜ë¦¬
            cleaned_token = token
            for josa in josa_suffixes:
                if token.endswith(josa) and len(token) > len(josa):
                    # ì¡°ì‚¬ ì œê±°
                    base = token[:-len(josa)]
                    # ì–´ê°„ì´ 2ê¸€ì ì´ìƒì¸ ê²½ìš°ë§Œ ìœ íš¨
                    if len(base) >= 2:
                        cleaned_token = base
                        break
            
            # ë„ˆë¬´ ì§§ì€ í† í° ì œê±° (1ê¸€ì)
            if len(cleaned_token) < 2:
                continue
            
            # ë¶ˆìš©ì–´ ì¬í™•ì¸ (ì¡°ì‚¬ ì œê±° í›„)
            if cleaned_token in stopwords:
                continue
            
            # ì¶”ê°€
            keywords.append(cleaned_token)
        
        logger.info(f"âœ… ê·œì¹™ ê¸°ë°˜: {len(tokens)}ê°œ í† í° â†’ {len(keywords)}ê°œ í‚¤ì›Œë“œ")
        
        return {
            'tokens': tokens,
            'keywords': keywords[:30],  # ìƒìœ„ 30ê°œ
            'pos_tags': []  # ê·œì¹™ ê¸°ë°˜ì—ì„œëŠ” í’ˆì‚¬ ì •ë³´ ì—†ìŒ
        }
    
    async def analyze_text_for_search(self, text: str) -> dict:
        """
        ê²€ìƒ‰ì„ ìœ„í•œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ (RAG ê²€ìƒ‰ ì „ìš©)
        
        ì–¸ì–´ ìë™ ê°ì§€:
        - ì˜ì–´: NLTK ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        - í•œêµ­ì–´: Kiwi ê¸°ë°˜ í˜•íƒœì†Œ ë¶„ì„
        - í˜¼í•©: ë‘ ë¶„ì„ ê²°ê³¼ ë³‘í•©
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            dict: {
                'language': str,  # 'ko', 'en', 'mixed'
                'keywords': List[str],  # í†µí•© í‚¤ì›Œë“œ
                'korean_keywords': List[str],  # í•œêµ­ì–´ í‚¤ì›Œë“œ
                'english_keywords': List[str],  # ì˜ì–´ í‚¤ì›Œë“œ
                'proper_nouns': [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
                'entities': {}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬
            }
        """
        logger.info(f"âœ… analyze_text_for_search í˜¸ì¶œ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
        
        # ì–¸ì–´ ê°ì§€
        is_english = self.english_nlp.is_english(text) if self.english_nlp else False
        has_korean = self._has_korean(text)
        
        if is_english and not has_korean:
            language = 'en'
        elif has_korean and not is_english:
            language = 'ko'
        else:
            language = 'mixed'
        
        logger.info(f"ğŸ“ ì–¸ì–´ ê°ì§€: {language} (í•œêµ­ì–´: {has_korean}, ì˜ì–´: {is_english})")
        
        # í•œêµ­ì–´ ë¶„ì„
        korean_analysis = await self.analyze_korean_text(text)
        korean_keywords = korean_analysis['keywords']
        
        # ì˜ì–´ ë¶„ì„
        english_keywords = []
        if self.english_nlp and (language == 'en' or language == 'mixed'):
            try:
                english_analysis = await self.english_nlp.analyze_english_text(text)
                english_keywords = english_analysis['keywords']
            except Exception as e:
                logger.warning(f"ì˜ì–´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # í‚¤ì›Œë“œ í†µí•© (ì¤‘ë³µ ì œê±°, ìˆœì„œ ìœ ì§€)
        all_keywords = []
        seen = set()
        for kw in korean_keywords + english_keywords:
            if kw not in seen:
                seen.add(kw)
                all_keywords.append(kw)
        
        logger.info(f"âœ… í†µí•© ë¶„ì„ ì™„ë£Œ: í•œêµ­ì–´ {len(korean_keywords)}ê°œ + "
                   f"ì˜ì–´ {len(english_keywords)}ê°œ = ì „ì²´ {len(all_keywords)}ê°œ í‚¤ì›Œë“œ")
        
        return {
            'language': language,
            'keywords': all_keywords[:50],  # ìµœëŒ€ 50ê°œ
            'korean_keywords': korean_keywords,
            'english_keywords': english_keywords,
            'proper_nouns': [],
            'entities': {}
        }
    
    def _has_korean(self, text: str) -> bool:
        """
        í…ìŠ¤íŠ¸ì— í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            bool: í•œêµ­ì–´ í¬í•¨ ì—¬ë¶€
        """
        import re
        # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„: \uAC00-\uD7A3
        korean_pattern = re.compile(r'[\uAC00-\uD7A3]')
        return bool(korean_pattern.search(text))
    
    def _create_dummy_embedding(self, text: str, dimension: Optional[int] = None) -> List[float]:
        """
        ë”ë¯¸ ì„ë² ë”© ë²¡í„° ìƒì„± (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            dimension: ë²¡í„° ì°¨ì› (ê¸°ë³¸ê°’: settings.vector_dimension)
            
        Returns:
            ì •ê·œí™”ëœ ë”ë¯¸ ì„ë² ë”© ë²¡í„°
        """
        if dimension is None:
            try:
                from app.core.config import settings
                dimension = settings.vector_dimension
            except:
                dimension = 1536  # ê¸°ë³¸ê°’
        
        # í…ìŠ¤íŠ¸ í•´ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œë“œ ìƒì„±
        text_hash = hashlib.md5(text.encode()).digest()
        seed = struct.unpack('I', text_hash[:4])[0]
        
        # ì‹œë“œë¥¼ ì´ìš©í•œ ì˜ì‚¬ ëœë¤ ë²¡í„° ìƒì„±
        random.seed(seed)
        vector = [random.uniform(-1.0, 1.0) for _ in range(dimension)]
        
        # ë²¡í„° ì •ê·œí™”
        magnitude = sum(x * x for x in vector) ** 0.5
        if magnitude > 0:
            vector = [x / magnitude for x in vector]
        
        return vector


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
korean_nlp_service = KoreanNLPService()
logger.info("âœ… korean_nlp_service ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")

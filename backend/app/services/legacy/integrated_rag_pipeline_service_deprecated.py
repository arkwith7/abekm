"""
ğŸš€ í†µí•© RAG íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤ 
============================

ë¬¸ì„œ ì—…ë¡œë“œ â†’ ì „ì²˜ë¦¬ â†’ NLP â†’ ë²¡í„°í™” â†’ ì €ì¥ì˜ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸
"""

import logging
from typing import Dict, List, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession
import time
from pathlib import Path

# ì„œë¹„ìŠ¤ imports
from app.services.document.processing.document_preprocessing_service import document_preprocessing_service
from app.services.core.korean_nlp_service import korean_nlp_service
from app.services.document.storage.vector_storage_service import vector_storage_service

logger = logging.getLogger(__name__)

class IntegratedRAGPipelineService:
    """í†µí•© RAG íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.vector_storage = vector_storage_service
    
    async def process_document_for_rag(
        self,
        session: AsyncSession,
        file_path: str,
        file_name: str,
        container_id: str,
        user_emp_no: str,
        file_bss_info_sno: int
    ) -> Dict[str, Any]:
        """
        ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        1. ë¬¸ì„œ ì „ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì²­í‚¹)
        2. í•œêµ­ì–´ NLP ë¶„ì„ (ì²­í¬ë³„)
        3. ë²¡í„° ì„ë² ë”© ìƒì„±
        4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš©)
        """
        start_time = time.time()
        
        result = {
            "success": False,
            "rag_ready": False,
            "processing_stats": {
                "chunks_created": 0,
                "nlp_processed": 0,
                "vectors_stored": 0,
                "total_processing_time": 0.0
            },
            "pipeline_steps": {
                "preprocessing": {"success": False},
                "nlp_analysis": {"success": False},
                "vector_storage": {"success": False}
            },
            "errors": []
        }
        
        try:
            # 1ë‹¨ê³„: ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì²­í‚¹
            logger.info(f"ğŸ“„ 1ë‹¨ê³„: ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹œì‘ - {file_name}")
            preprocessing_result = await document_preprocessing_service.process_document(
                file_path=file_path,
                file_extension=Path(file_path).suffix,
                container_id=container_id,
                user_emp_no=user_emp_no
            )
            
            if not preprocessing_result.get("success"):
                result["errors"].append(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {preprocessing_result.get('error')}")
                return result
            
            chunks = preprocessing_result.get("chunks", [])
            result["pipeline_steps"]["preprocessing"] = {
                "success": True,
                "chunks_count": len(chunks)
            }
            result["processing_stats"]["chunks_created"] = len(chunks)
            
            logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            # 2ë‹¨ê³„: ì²­í¬ë³„ í•œêµ­ì–´ NLP ë¶„ì„
            logger.info(f"ğŸ”¤ 2ë‹¨ê³„: í•œêµ­ì–´ NLP ë¶„ì„ ì‹œì‘")
            nlp_results = []
            
            for i, chunk in enumerate(chunks):
                try:
                    chunk_nlp = await korean_nlp_service.analyze_chunk_for_search(
                        chunk['content']
                    )
                    
                    if chunk_nlp.get("success"):
                        nlp_results.append(chunk_nlp)
                        result["processing_stats"]["nlp_processed"] += 1
                    else:
                        # ì‹¤íŒ¨í•œ ì²­í¬ë„ ê¸°ë³¸ êµ¬ì¡°ë¡œ ì¶”ê°€
                        nlp_results.append({
                            "success": False,
                            "korean_keywords": [],
                            "named_entities": [],
                            "embedding": None,
                            "error": chunk_nlp.get("error", "ë¶„ì„ ì‹¤íŒ¨")
                        })
                        result["errors"].append(f"ì²­í¬ {i} NLP ë¶„ì„ ì‹¤íŒ¨")
                
                except Exception as e:
                    nlp_results.append({
                        "success": False,
                        "korean_keywords": [],
                        "named_entities": [],
                        "embedding": None,
                        "error": str(e)
                    })
                    result["errors"].append(f"ì²­í¬ {i} NLP ì²˜ë¦¬ ì˜ˆì™¸: {str(e)}")
            
            result["pipeline_steps"]["nlp_analysis"] = {
                "success": len(nlp_results) > 0,
                "processed_chunks": len(nlp_results),
                "successful_chunks": result["processing_stats"]["nlp_processed"]
            }
            
            logger.info(f"âœ… NLP ë¶„ì„ ì™„ë£Œ: {result['processing_stats']['nlp_processed']}/{len(chunks)}ê°œ ì²­í¬")
            
            # 3ë‹¨ê³„: ë²¡í„° ìŠ¤í† ë¦¬ì§€ì— ì €ì¥
            logger.info(f"ğŸ”® 3ë‹¨ê³„: ë²¡í„° ìŠ¤í† ë¦¬ì§€ ì €ì¥ ì‹œì‘")
            storage_result = await self.vector_storage.store_processed_document(
                session=session,
                file_bss_info_sno=file_bss_info_sno,
                container_id=container_id,
                preprocessed_data=preprocessing_result,
                nlp_results=nlp_results
            )
            
            if storage_result.get("success"):
                result["pipeline_steps"]["vector_storage"] = {
                    "success": True,
                    "stored_chunks": storage_result.get("stored_chunks", 0),
                    "stored_vectors": storage_result.get("stored_vectors", 0),
                    "search_records": storage_result.get("search_records", 0)
                }
                result["processing_stats"]["vectors_stored"] = storage_result.get("stored_vectors", 0)
                
                logger.info(f"âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ: {storage_result.get('stored_vectors', 0)}ê°œ ë²¡í„°")
            else:
                result["errors"].append(f"ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {storage_result.get('error')}")
                logger.error(f"ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {storage_result.get('error')}")
            
            # ìµœì¢… ì„±ê³µ íŒë‹¨
            successful_steps = sum(1 for step in result["pipeline_steps"].values() if step.get("success"))
            result["success"] = successful_steps >= 2  # ì „ì²˜ë¦¬ + NLP ìµœì†Œ ì„±ê³µ
            result["rag_ready"] = successful_steps == 3  # ëª¨ë“  ë‹¨ê³„ ì„±ê³µ
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            result["processing_stats"]["total_processing_time"] = time.time() - start_time
            
            if result["rag_ready"]:
                logger.info(f"ğŸ‰ RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {file_name} - "
                           f"{result['processing_stats']['chunks_created']}ê°œ ì²­í¬, "
                           f"{result['processing_stats']['vectors_stored']}ê°œ ë²¡í„° ì €ì¥")
            else:
                logger.warning(f"âš ï¸ RAG íŒŒì´í”„ë¼ì¸ ë¶€ë¶„ ì™„ë£Œ: {file_name} - ì¼ë¶€ ë‹¨ê³„ ì‹¤íŒ¨")
            
            return result
            
        except Exception as e:
            result["errors"].append(f"íŒŒì´í”„ë¼ì¸ ì˜ˆì™¸: {str(e)}")
            result["processing_stats"]["total_processing_time"] = time.time() - start_time
            logger.error(f"RAG íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {file_name} - {str(e)}")
            return result
    
    async def test_pipeline_with_sample(self) -> Dict[str, Any]:
        """ìƒ˜í”Œ ë°ì´í„°ë¡œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        import tempfile
        import os
        
        test_text = """ì›…ì§„ì”½í¬ë¹… ì§€ì‹ê´€ë¦¬ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
        
ì´ ë¬¸ì„œëŠ” RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìƒ˜í”Œì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì²­í‚¹
2. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„  
3. ë²¡í„° ì„ë² ë”© ìƒì„±
4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì €ì¥

ì›…ì§„ì”½í¬ë¹…ì€ êµìœ¡ ì „ë¬¸ ê¸°ì—…ìœ¼ë¡œ AI ê¸°ë°˜ ì†”ë£¨ì…˜ì„ ê°œë°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.
"""
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            f.write(test_text)
            test_file_path = f.name
        
        try:
            from app.core.database import get_db
            async for session in get_db():
                result = await self.process_document_for_rag(
                    session=session,
                    file_path=test_file_path,
                    file_name="test_document.txt",
                    container_id="test_container",
                    user_emp_no="test_user",
                    file_bss_info_sno=999999  # í…ŒìŠ¤íŠ¸ìš© ID
                )
                break
            
            return result
            
        finally:
            if os.path.exists(test_file_path):
                os.unlink(test_file_path)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
integrated_rag_pipeline_service = IntegratedRAGPipelineService()

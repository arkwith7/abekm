"""
ğŸ”„ í†µí•© ì½˜í…ì¸  ì„œë¹„ìŠ¤ - 2ë¶€: ê²€ìƒ‰, RAG, ì±„íŒ… í†µí•©
====================================================

í‚¤ì›Œë“œ ê²€ìƒ‰, RAG ì‹œìŠ¤í…œ, ì±„íŒ… ì„¸ì…˜ ê´€ë¦¬ êµ¬í˜„
"""

    async def _keyword_search_unified(
        self,
        processed_query: Dict[str, Any],
        container_ids: List[str],
        max_results: int
    ) -> List[Dict[str, Any]]:
        """í†µí•© í‚¤ì›Œë“œ ê²€ìƒ‰ - vs_doc_contents_indexì˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        try:
            keywords = processed_query.get("keywords", [])
            if not keywords:
                keywords = [processed_query["original_text"]]
            
            # í‚¤ì›Œë“œë¥¼ PostgreSQL ì „ë¬¸ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜
            search_terms = " | ".join(keywords)  # OR ê²€ìƒ‰
            
            async with self.async_session_local() as session:
                container_filter = "', '".join(container_ids)
                
                query_sql = text(f"""
                    SELECT 
                        v.id as vector_id,
                        v.file_bss_info_sno,
                        v.chunk_text,
                        v.chunk_index,
                        v.chunk_size,
                        v.metadata_json,
                        v.knowledge_container_id,
                        f.file_lgc_nm,
                        f.file_psl_nm,
                        f.path,
                        f.korean_metadata,
                        f.created_at as file_created_at,
                        ts_rank(to_tsvector('korean', v.chunk_text), plainto_tsquery('korean', :search_terms)) as keyword_score
                    FROM vs_doc_contents_index v
                    JOIN tb_file_bss_info f ON v.file_bss_info_sno = f.file_bss_info_sno
                    WHERE v.knowledge_container_id IN ('{container_filter}')
                        AND f.del_yn = 'N'
                        AND to_tsvector('korean', v.chunk_text) @@ plainto_tsquery('korean', :search_terms)
                    ORDER BY keyword_score DESC
                    LIMIT {max_results}
                """)
                
                result = await session.execute(query_sql, {"search_terms": search_terms})
                
                results = []
                for row in result.fetchall():
                    # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                    metadata = {}
                    if row.metadata_json:
                        try:
                            metadata = json.loads(row.metadata_json)
                        except:
                            pass
                    
                    # í•œêµ­ì–´ ë©”íƒ€ë°ì´í„° íŒŒì‹±
                    korean_metadata = {}
                    if row.korean_metadata:
                        try:
                            korean_metadata = json.loads(row.korean_metadata)
                        except:
                            pass
                    
                    results.append({
                        "vector_id": row.vector_id,
                        "file_bss_info_sno": row.file_bss_info_sno,
                        "document_id": f"doc_{row.file_bss_info_sno}_{row.chunk_index}",
                        "title": row.file_lgc_nm or row.file_psl_nm,
                        "content": row.chunk_text,
                        "chunk_index": row.chunk_index,
                        "chunk_size": row.chunk_size,
                        "keyword_score": float(row.keyword_score),
                        "similarity_score": float(row.keyword_score),  # í†µì¼ì„±ì„ ìœ„í•´ 
                        "container_id": row.knowledge_container_id,
                        "file_path": row.path,
                        "metadata": metadata,
                        "korean_metadata": korean_metadata,
                        "file_created_at": row.file_created_at.isoformat() if row.file_created_at else None,
                        "search_method": "keyword"
                    })
                
                logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                return results
                
        except Exception as e:
            logger.error(f"í†µí•© í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _hybrid_search_unified(
        self,
        processed_query: Dict[str, Any],
        container_ids: List[str],
        max_results: int,
        threshold: float
    ) -> List[Dict[str, Any]]:
        """í†µí•© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ - ë²¡í„° + í‚¤ì›Œë“œ ê²°í•©"""
        try:
            # ë³‘ë ¬ë¡œ ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰
            import asyncio
            
            vector_results, keyword_results = await asyncio.gather(
                self._vector_search_unified(processed_query, container_ids, max_results * 2, threshold),
                self._keyword_search_unified(processed_query, container_ids, max_results * 2),
                return_exceptions=True
            )
            
            # ê²°ê³¼ í†µí•©
            combined_results = {}
            
            # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
            if not isinstance(vector_results, Exception):
                for result in vector_results:
                    doc_id = result["document_id"]
                    result["combined_score"] = result["similarity_score"] * self.vector_weight
                    result["search_methods"] = ["vector"]
                    combined_results[doc_id] = result
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
            if not isinstance(keyword_results, Exception):
                for result in keyword_results:
                    doc_id = result["document_id"]
                    keyword_contribution = result["keyword_score"] * self.keyword_weight
                    
                    if doc_id in combined_results:
                        # ì´ë¯¸ ë²¡í„° ê²€ìƒ‰ì—ì„œ ì°¾ì€ ë¬¸ì„œ
                        combined_results[doc_id]["combined_score"] += keyword_contribution
                        combined_results[doc_id]["search_methods"].append("keyword")
                        combined_results[doc_id]["keyword_score"] = result["keyword_score"]
                    else:
                        # í‚¤ì›Œë“œ ê²€ìƒ‰ì—ì„œë§Œ ì°¾ì€ ë¬¸ì„œ
                        result["combined_score"] = keyword_contribution
                        result["search_methods"] = ["keyword"]
                        combined_results[doc_id] = result
            
            # ê²°í•© ì ìˆ˜ë¡œ ì •ë ¬
            sorted_results = sorted(
                combined_results.values(),
                key=lambda x: x.get("combined_score", 0.0),
                reverse=True
            )
            
            # ì ìˆ˜ ì •ê·œí™”
            if sorted_results:
                max_score = max(r.get("combined_score", 0.0) for r in sorted_results)
                if max_score > 0:
                    for result in sorted_results:
                        result["similarity_score"] = result.get("combined_score", 0.0) / max_score
            
            final_results = sorted_results[:max_results]
            
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼: {len(final_results)}ê°œ")
            return final_results
            
        except Exception as e:
            logger.error(f"í†µí•© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    # =========================================================================
    # ğŸ’¬ 3. í†µí•© RAG ì‹œìŠ¤í…œ (Unified RAG System)
    # =========================================================================
    
    async def rag_search_and_context(
        self,
        query: str,
        user_emp_no: str,
        container_ids: Optional[List[str]] = None,
        max_chunks: int = 10,
        similarity_threshold: float = 0.7,
        context_window: int = 4000
    ) -> Dict[str, Any]:
        """
        RAGìš© ë¬¸ì„œ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        """
        try:
            # 1. ê³ í’ˆì§ˆ ë²¡í„° ê²€ìƒ‰ (RAGìš© ë†’ì€ ì„ê³„ê°’ ì‚¬ìš©)
            search_results = await self.unified_search(
                query=query,
                user_emp_no=user_emp_no,
                container_ids=container_ids,
                max_results=max_chunks,
                search_type="vector",
                similarity_threshold=similarity_threshold
            )
            
            if not search_results["results"]:
                return {
                    "success": False,
                    "context_text": "",
                    "chunks": [],
                    "total_tokens": 0,
                    "message": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }
            
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° í† í° ê´€ë¦¬
            context_parts = []
            chunks_used = []
            total_tokens = 0
            
            for i, result in enumerate(search_results["results"]):
                chunk_text = result["content"]
                chunk_tokens = len(chunk_text.split())  # ê°„ë‹¨í•œ í† í° ì¶”ì •
                
                if total_tokens + chunk_tokens > context_window:
                    break
                
                # ì»¨í…ìŠ¤íŠ¸ íŒŒíŠ¸ êµ¬ì„±
                metadata = result.get("metadata", {})
                source_info = f"ì¶œì²˜: {result['title']}"
                if metadata.get("page_number"):
                    source_info += f" (í˜ì´ì§€: {metadata['page_number']})"
                
                context_part = f"[ë¬¸ì„œ {i+1}] {source_info}\n{chunk_text}\n"
                context_parts.append(context_part)
                
                # ì‚¬ìš©ëœ ì²­í¬ ì •ë³´
                chunks_used.append({
                    "chunk_id": result["document_id"],
                    "title": result["title"],
                    "content_preview": chunk_text[:200] + "..." if len(chunk_text) > 200 else chunk_text,
                    "similarity_score": result["similarity_score"],
                    "page_number": metadata.get("page_number"),
                    "file_path": result.get("file_path"),
                    "container_id": result["container_id"]
                })
                
                total_tokens += chunk_tokens
            
            # 3. ìµœì¢… ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±
            context_text = "\n".join(context_parts)
            
            return {
                "success": True,
                "context_text": context_text,
                "chunks": chunks_used,
                "total_tokens": total_tokens,
                "max_chunks_used": len(chunks_used),
                "similarity_threshold_used": similarity_threshold,
                "search_query": query
            }
            
        except Exception as e:
            logger.error(f"RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "context_text": "",
                "chunks": [],
                "total_tokens": 0,
                "error": str(e)
            }

    # =========================================================================
    # ğŸ’¬ 4. í†µí•© ì±„íŒ… ì‹œìŠ¤í…œ (Unified Chat System)
    # =========================================================================
    
    async def create_chat_session(
        self,
        user_emp_no: str,
        session_name: Optional[str] = None,
        container_ids: Optional[List[str]] = None
    ) -> str:
        """ìƒˆ ì±„íŒ… ì„¸ì…˜ ìƒì„±"""
        try:
            session_id = f"chat_{user_emp_no}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            async with self.async_session_local() as session:
                query = text("""
                    INSERT INTO tb_chat_history (
                        session_id, user_emp_no, session_name, container_ids_json, 
                        created_at, updated_at, is_active
                    ) VALUES (
                        :session_id, :user_emp_no, :session_name, :container_ids,
                        NOW(), NOW(), true
                    )
                """)
                
                await session.execute(query, {
                    "session_id": session_id,
                    "user_emp_no": user_emp_no,
                    "session_name": session_name or f"ì±„íŒ… ì„¸ì…˜ {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                    "container_ids": json.dumps(container_ids) if container_ids else None
                })
                
                await session.commit()
            
            logger.info(f"ìƒˆ ì±„íŒ… ì„¸ì…˜ ìƒì„±: {session_id}")
            return session_id
            
        except Exception as e:
            logger.error(f"ì±„íŒ… ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def add_chat_message(
        self,
        session_id: str,
        user_emp_no: str,
        question: str,
        answer: str,
        context_chunks: Optional[List[Dict]] = None,
        search_stats: Optional[Dict] = None
    ) -> bool:
        """ì±„íŒ… ë©”ì‹œì§€ ì¶”ê°€"""
        try:
            async with self.async_session_local() as session:
                # 1. ì±„íŒ… íˆìŠ¤í† ë¦¬ì— Q&A ì €ì¥
                query = text("""
                    INSERT INTO tb_chat_history (
                        session_id, user_emp_no, question, answer, 
                        context_chunks_json, search_stats_json,
                        created_at, updated_at
                    ) VALUES (
                        :session_id, :user_emp_no, :question, :answer,
                        :context_chunks, :search_stats,
                        NOW(), NOW()
                    )
                """)
                
                await session.execute(query, {
                    "session_id": session_id,
                    "user_emp_no": user_emp_no,
                    "question": question,
                    "answer": answer,
                    "context_chunks": json.dumps(context_chunks) if context_chunks else None,
                    "search_stats": json.dumps(search_stats) if search_stats else None
                })
                
                # 2. ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì‹œê°„ ê°±ì‹ 
                update_query = text("""
                    UPDATE tb_chat_history 
                    SET updated_at = NOW()
                    WHERE session_id = :session_id AND user_emp_no = :user_emp_no
                        AND question IS NULL AND answer IS NULL
                """)
                
                await session.execute(update_query, {
                    "session_id": session_id,
                    "user_emp_no": user_emp_no
                })
                
                await session.commit()
            
            return True
            
        except Exception as e:
            logger.error(f"ì±„íŒ… ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def get_chat_history(
        self,
        session_id: str,
        user_emp_no: str,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        try:
            async with self.async_session_local() as session:
                query = text("""
                    SELECT 
                        id, session_id, question, answer, 
                        context_chunks_json, search_stats_json,
                        created_at
                    FROM tb_chat_history
                    WHERE session_id = :session_id 
                        AND user_emp_no = :user_emp_no
                        AND question IS NOT NULL
                        AND answer IS NOT NULL
                    ORDER BY created_at ASC
                    LIMIT :limit
                """)
                
                result = await session.execute(query, {
                    "session_id": session_id,
                    "user_emp_no": user_emp_no,
                    "limit": limit
                })
                
                history = []
                for row in result.fetchall():
                    context_chunks = []
                    search_stats = {}
                    
                    if row.context_chunks_json:
                        try:
                            context_chunks = json.loads(row.context_chunks_json)
                        except:
                            pass
                    
                    if row.search_stats_json:
                        try:
                            search_stats = json.loads(row.search_stats_json)
                        except:
                            pass
                    
                    history.append({
                        "id": row.id,
                        "session_id": row.session_id,
                        "question": row.question,
                        "answer": row.answer,
                        "context_chunks": context_chunks,
                        "search_stats": search_stats,
                        "timestamp": row.created_at.isoformat() if row.created_at else None
                    })
                
                return history
                
        except Exception as e:
            logger.error(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def get_user_chat_sessions(
        self,
        user_emp_no: str,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ìì˜ ì±„íŒ… ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
        try:
            async with self.async_session_local() as session:
                query = text("""
                    SELECT DISTINCT
                        session_id, session_name, container_ids_json,
                        MIN(created_at) as first_message,
                        MAX(updated_at) as last_activity,
                        COUNT(CASE WHEN question IS NOT NULL THEN 1 END) as message_count
                    FROM tb_chat_history
                    WHERE user_emp_no = :user_emp_no
                    GROUP BY session_id, session_name, container_ids_json
                    ORDER BY last_activity DESC
                    LIMIT :limit
                """)
                
                result = await session.execute(query, {
                    "user_emp_no": user_emp_no,
                    "limit": limit
                })
                
                sessions = []
                for row in result.fetchall():
                    container_ids = []
                    if row.container_ids_json:
                        try:
                            container_ids = json.loads(row.container_ids_json)
                        except:
                            pass
                    
                    sessions.append({
                        "session_id": row.session_id,
                        "session_name": row.session_name,
                        "container_ids": container_ids,
                        "first_message": row.first_message.isoformat() if row.first_message else None,
                        "last_activity": row.last_activity.isoformat() if row.last_activity else None,
                        "message_count": row.message_count
                    })
                
                return sessions
                
        except Exception as e:
            logger.error(f"ì±„íŒ… ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []

    # =========================================================================
    # ğŸ”§ 5. ê³µí†µ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
    # =========================================================================
    
    async def _get_accessible_containers(
        self,
        user_emp_no: str,
        requested_containers: Optional[List[str]] = None
    ) -> List[str]:
        """ì‚¬ìš©ìê°€ ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ"""
        try:
            # ê¶Œí•œ ì„œë¹„ìŠ¤ í™œìš© (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
            permission_service = PermissionService(None)  # sessionì€ ë‚´ë¶€ì—ì„œ ê´€ë¦¬
            
            if requested_containers:
                # ìš”ì²­ëœ ì»¨í…Œì´ë„ˆ ì¤‘ ì ‘ê·¼ ê°€ëŠ¥í•œ ê²ƒë§Œ í•„í„°ë§
                accessible = []
                for container_id in requested_containers:
                    # ê° ì»¨í…Œì´ë„ˆì— ëŒ€í•œ ê¶Œí•œ í™•ì¸ ë¡œì§ (ê°„ì†Œí™”)
                    accessible.append(container_id)  # ì‹¤ì œë¡œëŠ” ê¶Œí•œ ì²´í¬ í•„ìš”
                return accessible
            else:
                # ëª¨ë“  ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ë°˜í™˜
                return ["DEFAULT_CONTAINER"]  # ê¸°ë³¸ ì»¨í…Œì´ë„ˆ
                
        except Exception as e:
            logger.error(f"ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return ["DEFAULT_CONTAINER"]
    
    async def _process_search_query(self, query: str) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì „ì²˜ë¦¬"""
        try:
            # í•œêµ­ì–´ NLP ì²˜ë¦¬
            nlp_result = await korean_nlp_service.analyze_text(query)
            
            return {
                "original_text": query,
                "optimized_text": query,  # NLP ê²°ê³¼ë¡œ ìµœì í™”ëœ ì¿¼ë¦¬
                "keywords": nlp_result.get("keywords", [query]),
                "entities": nlp_result.get("entities", []),
                "intent": nlp_result.get("intent", "search")
            }
            
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                "original_text": query,
                "optimized_text": query,
                "keywords": [query],
                "entities": [],
                "intent": "search"
            }
    
    async def _format_search_results_unified(
        self,
        results: List[Dict[str, Any]],
        user_emp_no: str
    ) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í†µì¼ëœ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        formatted_results = []
        
        for result in results:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            formatted_result = {
                "document_id": result["document_id"],
                "title": result["title"],
                "content_preview": result["content"][:300] + "..." if len(result["content"]) > 300 else result["content"],
                "similarity_score": result["similarity_score"],
                "search_methods": result.get("search_methods", []),
                "container_id": result["container_id"],
                "file_path": result.get("file_path"),
                "metadata": {
                    "chunk_index": result.get("chunk_index"),
                    "chunk_size": result.get("chunk_size"),
                    "page_number": result.get("metadata", {}).get("page_number"),
                    "file_created_at": result.get("file_created_at"),
                    "search_timestamp": datetime.now().isoformat()
                }
            }
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            if result.get("korean_metadata"):
                formatted_result["korean_metadata"] = result["korean_metadata"]
            
            formatted_results.append(formatted_result)
        
        return formatted_results
    
    async def _log_search_activity(
        self,
        user_emp_no: str,
        query: str,
        result_count: int,
        search_type: str
    ) -> None:
        """ê²€ìƒ‰ í™œë™ ë¡œê·¸ (ì„ íƒì )"""
        try:
            # ê²€ìƒ‰ ë¡œê·¸ë¥¼ ë³„ë„ í…Œì´ë¸”ì— ì €ì¥í•  ìˆ˜ ìˆìŒ
            logger.info(f"ê²€ìƒ‰ ë¡œê·¸: ì‚¬ìš©ì={user_emp_no}, ì¿¼ë¦¬='{query}', ê²°ê³¼={result_count}ê°œ, íƒ€ì…={search_type}")
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
integrated_content_service = IntegratedContentService()

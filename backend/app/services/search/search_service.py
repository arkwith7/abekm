"""
í†µí•© ê²€ìƒ‰ ì„œë¹„ìŠ¤
ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ + í•œêµ­ì–´ ì „ë¬¸ê²€ìƒ‰ì„ í†µí•©í•œ ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì—”ì§„
"""
from typing import List, Dict, Any, Optional, Tuple
import logging
import asyncio
import json
import math
import time
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import and_, or_, func, text, desc, select
import numpy as np
from datetime import datetime

from app.core.database import get_async_session_local
from app.models import TbKnowledgeContainers, TbUserPermissions
from app.services.core.korean_nlp_service import korean_nlp_service
from app.services.core.embedding_service import EmbeddingService

try:
    # CLIP ê¸°ë°˜ ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ì„ë² ë”© ì„œë¹„ìŠ¤ (ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ìš©)
    from app.services.document.vision.image_embedding_service import image_embedding_service
except ImportError:  # pragma: no cover - ì„ íƒ êµ¬ì„± ìš”ì†Œê°€ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ì½”ë“œ
    image_embedding_service = None
from app.services.auth.permission_service import permission_service
from .natural_language_query_processor import natural_language_processor
from .query_pipeline import process_user_query  # í†µí•© íŒŒì´í”„ë¼ì¸
from app.core.config import settings

logger = logging.getLogger(__name__)


class SearchService:
    """í†µí•© ê²€ìƒ‰ ì„œë¹„ìŠ¤ - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.vector_weight = 0.4  # ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (í•œêµ­ì–´ ì„ë² ë”© í•œê³„ ê³ ë ¤)
        self.keyword_weight = 0.5  # í‚¤ì›Œë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (í•œêµ­ì–´ì—ì„œ ë” ì •í™•í•¨)
        self.fulltext_weight = 0.1  # ì „ë¬¸ê²€ìƒ‰ ê°€ì¤‘ì¹˜
        # í™˜ê²½ì„¤ì • ê¸°ë°˜ ì„ê³„ê°’ ì‚¬ìš© (.env â†’ settings.similarity_threshold)
        self.similarity_threshold = settings.similarity_threshold  # ê¸°ë³¸ê°’ì€ config.pyì˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        self.async_session_local = get_async_session_local()
        self._container_details_cache: Dict[str, Tuple[float, Dict[str, Any]]] = {}
        self._all_container_name_cache: Tuple[float, Dict[str, str]] = (0.0, {})
        self._container_cache_ttl = getattr(settings, "container_cache_ttl_seconds", 300)
        
    async def hybrid_search(
        self,
        query: str,
        user_emp_no: str,
        container_ids: Optional[List[str]] = None,
        max_results: int = 10,
        search_type: str = "hybrid",  # hybrid, vector_only, keyword_only
        filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            user_emp_no: ê²€ìƒ‰í•˜ëŠ” ì‚¬ìš©ì ì‚¬ë²ˆ
            container_ids: ê²€ìƒ‰ ëŒ€ìƒ ì»¨í…Œì´ë„ˆ ëª©ë¡ (Noneì´ë©´ ê¶Œí•œ ë‚´ ëª¨ë“  ì»¨í…Œì´ë„ˆ)
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            search_type: ê²€ìƒ‰ íƒ€ì…
            filters: ì¶”ê°€ í•„í„°
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # 1. ì‚¬ìš©ì ê¶Œí•œ í™•ì¸ ë° ê²€ìƒ‰ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ í™•ì¸
            accessible_containers = await self._get_accessible_containers(
                user_emp_no, container_ids
            )
            
            if not accessible_containers:
                return {
                    "results": [],
                    "total_count": 0,
                    "search_type": search_type,
                    "message": "ê²€ìƒ‰ ê¶Œí•œì´ ìˆëŠ” ì»¨í…Œì´ë„ˆê°€ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # 2. ì¿¼ë¦¬ ì „ì²˜ë¦¬
            processed_query = await self._preprocess_query(query)
            
            # 3. ê²€ìƒ‰ íƒ€ì…ì— ë”°ë¥¸ ê²€ìƒ‰ ìˆ˜í–‰
            if search_type == "vector_only":
                results = await self._vector_search(
                    processed_query, accessible_containers, max_results, filters
                )
            elif search_type == "keyword_only":
                results = await self._keyword_search(
                    processed_query, accessible_containers, max_results, filters
                )
            else:  # hybrid
                results = await self._hybrid_search_combined(
                    processed_query, accessible_containers, max_results, filters
                )
            
            # 4. ê²€ìƒ‰ ê¸°ë¡ ì €ì¥
            await self._save_search_history(
                user_emp_no, query, results, search_type, accessible_containers
            )
            
            # 5. íŒŒì¼ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (ê²€ìƒ‰ í™”ë©´ìš©)
            grouped_results = await self._group_results_by_file(results)
            
            # 6. ê²°ê³¼ í›„ì²˜ë¦¬
            formatted_results = await self._format_search_results(grouped_results, user_emp_no, query)
            
            # 7. ì»¨í…Œì´ë„ˆ ì´ë¦„ ë§¤í•‘
            accessible_container_names = await self._get_container_friendly_names(accessible_containers)
            
            return {
                "results": formatted_results,
                "total_count": len(formatted_results),
                "search_type": search_type,
                "accessible_containers": accessible_containers,
                "accessible_container_names": accessible_container_names,
                "query_processed": processed_query,
                "execution_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def _hybrid_search_combined(
        self,
        processed_query: Dict[str, Any],
        container_ids: List[str],
        max_results: int,
        filters: Optional[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ + ì „ë¬¸ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        """
        # ë³‘ë ¬ë¡œ ê° ê²€ìƒ‰ ë°©ì‹ ì‹¤í–‰
        vector_results, keyword_results, fulltext_results = await asyncio.gather(
            self._vector_search(processed_query, container_ids, max_results * 2, filters),
            self._keyword_search(processed_query, container_ids, max_results * 2, filters),
            self._fulltext_search(processed_query, container_ids, max_results * 2, filters),
            return_exceptions=True
        )
        
        # ê° ê²°ê³¼ì— ê²€ìƒ‰ ë°©ì‹ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        all_results = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        if not isinstance(vector_results, Exception):
            logger.info(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(vector_results)}")
            for result in vector_results:
                doc_id = result.get("search_doc_id", result.get("document_id"))  # ë‘ ê°€ì§€ í˜•íƒœ ëª¨ë‘ ì§€ì›
                container_id = result.get("knowledge_container_id", "")
                logger.debug(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ - doc_id: {doc_id}, container_id: {container_id}")
                score = result.get("similarity_score", 0.0) * self.vector_weight
                if doc_id not in all_results:
                    all_results[doc_id] = result.copy()
                    all_results[doc_id]["combined_score"] = score
                    all_results[doc_id]["search_methods"] = ["vector"]
                else:
                    all_results[doc_id]["combined_score"] += score
                    all_results[doc_id]["search_methods"].append("vector")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        if not isinstance(keyword_results, Exception):
            logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(keyword_results)}")
            for result in keyword_results:
                doc_id = result["search_doc_id"]
                container_id = result.get("knowledge_container_id", "")
                logger.debug(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ - doc_id: {doc_id}, container_id: {container_id}")
                score = result.get("keyword_score", 0.0) * self.keyword_weight
                if doc_id not in all_results:
                    all_results[doc_id] = result.copy()
                    all_results[doc_id]["combined_score"] = score
                    all_results[doc_id]["search_methods"] = ["keyword"]
                else:
                    all_results[doc_id]["combined_score"] += score
                    all_results[doc_id]["search_methods"].append("keyword")
        
        # ì „ë¬¸ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        if not isinstance(fulltext_results, Exception):
            logger.info(f"ì „ë¬¸ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(fulltext_results)}")
            for result in fulltext_results:
                doc_id = result["search_doc_id"]
                container_id = result.get("knowledge_container_id", "")
                logger.debug(f"ì „ë¬¸ê²€ìƒ‰ ê²°ê³¼ - doc_id: {doc_id}, container_id: {container_id}")
                score = result.get("fulltext_score", 0.0) * self.fulltext_weight
                if doc_id not in all_results:
                    all_results[doc_id] = result.copy()
                    all_results[doc_id]["combined_score"] = score
                    all_results[doc_id]["search_methods"] = ["fulltext"]
                else:
                    all_results[doc_id]["combined_score"] += score
                    all_results[doc_id]["search_methods"].append("fulltext")
        
        # NaN ê°’ ì •ë¦¬ ë° ê²°í•©ëœ ì ìˆ˜ë¡œ ì •ë ¬
        for result in all_results.values():
            # NaN ê°’ì„ 0.0ìœ¼ë¡œ êµì²´
            if "combined_score" in result:
                import math
                if math.isnan(result["combined_score"]):
                    result["combined_score"] = 0.0
            
            # ê°œë³„ ì ìˆ˜ë“¤ë„ NaN ì²´í¬
            for score_key in ["similarity_score", "keyword_score", "fulltext_score"]:
                if score_key in result:
                    if math.isnan(result[score_key]):
                        result[score_key] = 0.0
        
        sorted_results = sorted(
            all_results.values(),
            key=lambda x: x.get("combined_score", 0.0),
            reverse=True
        )
        
        # ê²€ìƒ‰ í’ˆì§ˆ í•„í„°ë§ ì ìš©
        sorted_results = self._apply_quality_filter(sorted_results, processed_query)
        
        # ìµœì¢… ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        if sorted_results:
            max_score = max(r.get("combined_score", 0.0) for r in sorted_results)
            min_score = min(r.get("combined_score", 0.0) for r in sorted_results)
            
            # ì •ê·œí™” (0-1 ë²”ìœ„)
            if max_score > min_score:
                for result in sorted_results:
                    original_score = result.get("combined_score", 0.0)
                    normalized_score = (original_score - min_score) / (max_score - min_score)
                    result["similarity_score"] = normalized_score
                    result["combined_score"] = normalized_score
                    logger.debug(f"ì ìˆ˜ ì •ê·œí™”: {original_score:.3f} -> {normalized_score:.3f}")
            else:
                # ëª¨ë“  ì ìˆ˜ê°€ ê°™ì€ ê²½ìš°
                for result in sorted_results:
                    result["similarity_score"] = 1.0
                    result["combined_score"] = 1.0
        
        return sorted_results[:max_results]
    
    async def _vector_search(
        self,
        processed_query: Dict[str, Any],
        container_ids: List[str],
        max_results: int,
        filters: Optional[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ - ì‹¤ì œ í…Œì´ë¸” êµ¬ì¡° ì‚¬ìš©"""
        try:
            # ğŸš€ ì„ë² ë”© ì…ë ¥ì€ ìì—°ì–´ ë¬¸ì¥ ì‚¬ìš© (íŒŒì´í”„ ë“± ì—°ì‚°ì ë¬¸ìì—´ ê¸ˆì§€)
            original_text = processed_query.get("original_text", "")
            normalized_text = processed_query.get("normalized_text") or original_text
            # ê²€ìƒ‰ ì‹œìŠ¤í…œ ë‚´ë¶€ì˜ fulltextìš© OR ë¬¸ìì—´ì€ ì„ë² ë”©ì— ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            optimized_text_for_fulltext = processed_query.get("search_query_string")
            if optimized_text_for_fulltext and optimized_text_for_fulltext != original_text:
                logger.info(
                    f"ìµœì í™”ëœ ê²€ìƒ‰ì–´(ì „ë¬¸/í‚¤ì›Œë“œìš©): '{original_text}' â†’ '{optimized_text_for_fulltext}'"
                )
            query_text = normalized_text or original_text
            logger.info(f"ì„ë² ë”© ì…ë ¥ ë¬¸ì¥: '{query_text}'")

            # ì–¸ì–´/ê¸¸ì´ ê¸°ë°˜ ë™ì  ì„ê³„ê°’ (í•œêµ­ì–´ ë‹¨ë¬¸ ë³´í˜¸)
            language = processed_query.get("language", "mixed")
            dyn_threshold = self.similarity_threshold
            if language == "ko":
                try:
                    text_len = len(query_text)
                except Exception:
                    text_len = 0
                if text_len > 0 and text_len < 6:
                    # ì§§ì€ í•œê¸€ ì§ˆì˜ëŠ” ì„ê³„ê°’ ì™„í™” (ìµœì†Œ 0.3 ë³´ì¥)
                    dyn_threshold = max(0.3, self.similarity_threshold - 0.1)
            logger.info(
                f"ë²¡í„° ê²€ìƒ‰ ì‹œì‘: '{query_text}', ì„ê³„ê°’: {dyn_threshold} (ê¸°ë³¸: {self.similarity_threshold})"
            )
            
            query_embedding = await self.embedding_service.get_embedding(query_text)
            
            async with self.async_session_local() as db:
                # vs_doc_contents_chunks í…Œì´ë¸”ì„ ì‚¬ìš©í•œ ë²¡í„° ê²€ìƒ‰ (ì²­í‚¹ëœ ì„ë² ë”©)
                container_id_list = "', '".join(container_ids)
                
                # ì„ë² ë”©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"
                
                # ğŸ”·ğŸŸ§ ë²¤ë”ë³„ ë²¡í„° ì»¬ëŸ¼ ì„ íƒ (ì°¨ì› ê¸°ë°˜ ìë™ íŒë³„)
                embedding_dim = len(query_embedding)
                vector_column = None
                provider_filter = ""
                
                if embedding_dim == 1536:
                    vector_column = "c.azure_embedding_1536"
                    provider_filter = "AND c.embedding_provider = 'azure'"
                    logger.info(f"[VECTOR-SEARCH] ğŸ”· Azure ë²¡í„° ì»¬ëŸ¼ ì‚¬ìš© (1536d)")
                elif embedding_dim == 1024:
                    vector_column = "c.aws_embedding_1024"
                    provider_filter = "AND c.embedding_provider = 'aws'"
                    logger.info(f"[VECTOR-SEARCH] ğŸŸ§ AWS ë²¡í„° ì»¬ëŸ¼ ì‚¬ìš© (1024d)")
                else:
                    # ë ˆê±°ì‹œ í´ë°± (ë™ì  ì°¨ì› ì»¬ëŸ¼)
                    vector_column = "c.chunk_embedding"
                    logger.warning(f"[VECTOR-SEARCH] âš ï¸ ë ˆê±°ì‹œ ë²¡í„° ì»¬ëŸ¼ í´ë°± ({embedding_dim}d)")
                
                query_sql = f"""
                    SELECT 
                        c.chunk_sno as id,
                        c.file_bss_info_sno,
                        c.chunk_text,
                        c.chunk_index,
                        c.chunk_size,
                        c.keywords as keywords_json,
                        c.knowledge_container_id,
                        c.metadata_json as metadata_json,
                        f.file_lgc_nm,
                        f.file_psl_nm,
                        f.path,
                        f.korean_metadata,
                        1 - ({vector_column} <=> '{embedding_str}'::vector) as similarity_score
                    FROM vs_doc_contents_chunks c
                    JOIN tb_file_bss_info f ON c.file_bss_info_sno = f.file_bss_info_sno
                    WHERE c.knowledge_container_id IS NOT NULL 
                        AND c.knowledge_container_id != '' 
                        AND c.knowledge_container_id NOT IN ('NONE', 'None', 'null', 'NULL')
                        AND (c.knowledge_container_id = 'DEFAULT_CONTAINER' OR c.knowledge_container_id IN ('{container_id_list}'))
                        AND f.del_yn = 'N'
                        AND {vector_column} IS NOT NULL
                        {provider_filter}
                        AND 1 - ({vector_column} <=> '{embedding_str}'::vector) >= {dyn_threshold}
                    ORDER BY similarity_score DESC
                    LIMIT {max_results * 2}
                """
                
                result = await db.execute(text(query_sql))
                
                results = []
                for row in result.fetchall():
                    similarity_score = float(row.similarity_score)
                    
                    # NaN ê°’ í•„í„°ë§
                    if math.isnan(similarity_score) or math.isinf(similarity_score):
                        logger.warning(f"ë²¡í„° ê²€ìƒ‰ì—ì„œ ì˜ëª»ëœ ì ìˆ˜ ë°œê²¬ (NaN/Inf): doc_id={getattr(row, 'id', 'unknown')}")
                        continue
                    
                    # ì„ê³„ê°’ í•„í„°ë§ (ë™ì  ì„ê³„ê°’ ê¸°ì¤€)
                    if similarity_score < dyn_threshold:
                        logger.debug(f"ì„ê³„ê°’ ë¯¸ë‹¬ë¡œ ì œì™¸: {similarity_score:.3f} < {dyn_threshold}")
                        continue
                    
                    metadata = {}
                    modality = "text"  # ê¸°ë³¸ê°’
                    chunk_id = row.id
                    source_object_ids = []
                    page_number = None
                    
                    if row.metadata_json:
                        try:
                            metadata = json.loads(row.metadata_json)
                            # metadata_jsonì—ì„œ modality ì¶”ì¶œ
                            modality = metadata.get("modality", "text")
                            # doc_chunk í…Œì´ë¸”ì˜ ì‹¤ì œ chunk_id ì‚¬ìš©
                            chunk_id = metadata.get("chunk_id", row.id)
                            # source_object_ids ì¶”ì¶œ (ì´ë¯¸ì§€ ê°ì²´ ID)
                            source_object_ids = metadata.get("source_object_ids", [])
                            # page_number ì¶”ì¶œ (ì´ë¯¸ì§€ í˜ì´ì§€ ë²ˆí˜¸)
                            page_number = metadata.get("page_number")
                        except:
                            metadata = {}
                    
                    korean_metadata = row.korean_metadata or {}
                    
                    results.append({
                        "search_doc_id": row.id,  # document_id ëŒ€ì‹  search_doc_id ì‚¬ìš©
                        "document_id": row.id,    # í˜¸í™˜ì„±ì„ ìœ„í•´ ë‘˜ ë‹¤ í¬í•¨
                        "chunk_id": chunk_id,     # doc_chunk í…Œì´ë¸”ì˜ ì‹¤ì œ chunk_id
                        "file_bss_info_sno": row.file_bss_info_sno,
                        "knowledge_container_id": row.knowledge_container_id,
                        "chunk_index": row.chunk_index,
                        "source_object_ids": source_object_ids,  # ì´ë¯¸ì§€ ê°ì²´ ID ë°°ì—´
                        "page_number": page_number,  # í˜ì´ì§€ ë²ˆí˜¸ (ì´ë¯¸ì§€ìš©)
                        "content": row.chunk_text,
                        "chunk_size": row.chunk_size,
                        "file_name": row.file_lgc_nm,
                        "file_path": row.path,
                        # í˜¼ë™ ë°©ì§€ë¥¼ ìœ„í•´ raw ìœ ì‚¬ë„ë¥¼ ë³„ë„ ë³´ê´€
                        "similarity_score": similarity_score,  # í•˜ì´ë¸Œë¦¬ë“œ ê²°í•© ì „ raw vector sim
                        "search_method": "vector",
                        "modality": modality,     # modality ì¶”ê°€
                        "metadata": {**metadata, **korean_metadata},
                        # ë¶„ì„/í•„í„° ì¼ê´€ì„± ìœ ì§€ë¥¼ ìœ„í•´ scores/raw_vector_similarity ì¶”ê°€
                        "scores": {"raw_vector_similarity": similarity_score},
                        "raw_vector_similarity": similarity_score
                    })
                
                logger.info(f"ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°œê²¬ (ì„ê³„ê°’: {dyn_threshold})")
                
                if results:
                    max_score = max(r.get("raw_vector_similarity", r["similarity_score"]) for r in results)
                    min_score = min(r.get("raw_vector_similarity", r["similarity_score"]) for r in results)
                    avg_score = sum(r.get("raw_vector_similarity", r["similarity_score"]) for r in results) / max(len(results), 1)
                    logger.info(f"ì ìˆ˜ ë²”ìœ„(ì›ì‹œ): {min_score:.3f} ~ {max_score:.3f}, í‰ê· : {avg_score:.3f}")
                
                return results
                
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _keyword_search(
        self,
        processed_query: Dict[str, Any],
        container_ids: List[str],
        max_results: int,
        filters: Optional[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        í‚¤ì›Œë“œ ê²€ìƒ‰ (Multilingual textsearch ê¸°ë°˜)
        
        ë³€ê²½ ì‚¬í•­ (2025-10-24):
        - í•œêµ­ì–´ + ì˜ì–´ dual tsvector ê²€ìƒ‰ ì§€ì›
        - language ê°ì§€í•˜ì—¬ ì ì ˆí•œ tsvector ì»¬ëŸ¼ ì„ íƒ
        - ts_rank()ë¡œ ì •í™•í•œ ìˆœìœ„ ê³„ì‚°
        
        ë³€ê²½ ì‚¬í•­ (2025-10-16):
        - kiwipiepy ì œê±°
        - textsearch_koë¥¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì—ë„ í™œìš©
        - ts_rank()ë¡œ ì •í™•í•œ ìˆœìœ„ ê³„ì‚°
        """
        try:
            query_text = processed_query["original_text"]
            language = processed_query.get("language", "mixed")  # ì–¸ì–´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            
            # textsearch ì¿¼ë¦¬ ìƒì„±
            # ì˜ˆ: "ì¸ìŠë¦° íŒí”„" â†’ "ì¸ìŠë¦° & íŒí”„"
            # ì˜ˆ: "Figure 1" â†’ "Figure & 1"
            ts_query = query_text.replace(' ', ' & ')
            
            container_id_list = "', '".join(container_ids)
            
            logger.info(f"[KEYWORD-SEARCH] ì¿¼ë¦¬: '{query_text}', ì–¸ì–´: {language}, ì»¨í…Œì´ë„ˆ: {len(container_ids)}ê°œ")
            
            async with self.async_session_local() as db:
                # ğŸŒ Multilingual textsearch ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰
                # í•œêµ­ì–´(korean) + ì˜ì–´(english) dual configuration ì§€ì›
                
                # ì–¸ì–´ë³„ ê²€ìƒ‰ ì¡°ê±´ êµ¬ì„±
                if language == "en":
                    # ì˜ì–´ ì „ìš© ê²€ìƒ‰
                    tsvector_condition = """
                        s.content_tsvector_en @@ to_tsquery('english', :ts_query)
                        OR s.keyword_tsvector_en @@ to_tsquery('english', :ts_query)
                    """
                    rank_calculation = """
                        COALESCE(
                            ts_rank(
                                s.content_tsvector_en, 
                                to_tsquery('english', :ts_query)
                            ) * 2.0,
                            0.0
                        ) +
                        COALESCE(
                            ts_rank(
                                s.keyword_tsvector_en, 
                                to_tsquery('english', :ts_query)
                            ) * 3.0,
                            0.0
                        )
                    """
                elif language == "ko":
                    # í•œêµ­ì–´ ì „ìš© ê²€ìƒ‰
                    tsvector_condition = """
                        s.content_tsvector @@ to_tsquery('korean', :ts_query)
                        OR s.keyword_tsvector @@ to_tsquery('korean', :ts_query)
                    """
                    rank_calculation = """
                        COALESCE(
                            ts_rank(
                                s.content_tsvector, 
                                to_tsquery('korean', :ts_query)
                            ) * 2.0,
                            0.0
                        ) +
                        COALESCE(
                            ts_rank(
                                s.keyword_tsvector, 
                                to_tsquery('korean', :ts_query)
                            ) * 3.0,
                            0.0
                        )
                    """
                else:  # mixed ë˜ëŠ” language ì •ë³´ ì—†ìŒ
                    # í•œêµ­ì–´ + ì˜ì–´ ë™ì‹œ ê²€ìƒ‰ (OR ì¡°ê±´)
                    tsvector_condition = """
                        s.content_tsvector @@ to_tsquery('korean', :ts_query)
                        OR s.keyword_tsvector @@ to_tsquery('korean', :ts_query)
                        OR s.content_tsvector_en @@ to_tsquery('english', :ts_query)
                        OR s.keyword_tsvector_en @@ to_tsquery('english', :ts_query)
                    """
                    rank_calculation = """
                        GREATEST(
                            COALESCE(
                                ts_rank(s.content_tsvector, to_tsquery('korean', :ts_query)) * 2.0,
                                0.0
                            ) +
                            COALESCE(
                                ts_rank(s.keyword_tsvector, to_tsquery('korean', :ts_query)) * 3.0,
                                0.0
                            ),
                            COALESCE(
                                ts_rank(s.content_tsvector_en, to_tsquery('english', :ts_query)) * 2.0,
                                0.0
                            ) +
                            COALESCE(
                                ts_rank(s.keyword_tsvector_en, to_tsquery('english', :ts_query)) * 3.0,
                                0.0
                            )
                        )
                    """
                
                query_sql = f"""
                    SELECT 
                        s.search_doc_id,
                        s.file_bss_info_sno,
                        s.knowledge_container_id,
                        0 as chunk_index,
                        s.full_content as content,
                        s.content_summary as main_text,
                        s.document_title,
                        s.has_images,
                        s.image_count,
                        f.file_lgc_nm,
                        f.path,
                        -- âœ… ì–¸ì–´ë³„ ts_rank() ê³„ì‚°
                        {rank_calculation} as keyword_score
                    FROM tb_document_search_index s
                    JOIN tb_file_bss_info f ON s.file_bss_info_sno = f.file_bss_info_sno
                    WHERE s.knowledge_container_id IS NOT NULL 
                        AND s.knowledge_container_id != '' 
                        AND s.knowledge_container_id NOT IN ('NONE', 'None', 'null', 'NULL')
                        AND (s.knowledge_container_id = 'DEFAULT_CONTAINER' OR s.knowledge_container_id IN ('{container_id_list}'))
                        AND f.del_yn = 'N'
                        AND s.indexing_status = 'indexed'
                        AND (
                            -- âœ… Multilingual textsearch ë§¤ì¹­ (ë©”ì¸)
                            {tsvector_condition}
                            OR 
                            -- âœ… ë³´ì¡°: ì§ì ‘ ë¬¸ìì—´ ë§¤ì¹­ (ë‹¨ìˆœ í‚¤ì›Œë“œ)
                            s.full_content ILIKE :like_pattern
                            OR s.document_title ILIKE :like_pattern
                        )
                    ORDER BY keyword_score DESC
                    LIMIT :max_results
                """
                
                like_pattern = f"%{query_text}%"
                
                result = await db.execute(
                    text(query_sql),
                    {
                        "ts_query": ts_query,
                        "like_pattern": like_pattern,
                        "max_results": max_results * 2
                    }
                )
                
                results = []
                for row in result.fetchall():
                    keyword_score = float(row.keyword_score) if row.keyword_score else 0.0
                    
                    # NaN í•„í„°ë§
                    if math.isnan(keyword_score) or math.isinf(keyword_score):
                        keyword_score = 0.0
                    
                    results.append({
                        "search_doc_id": row.search_doc_id,
                        "file_bss_info_sno": row.file_bss_info_sno,
                        "knowledge_container_id": row.knowledge_container_id,
                        "chunk_index": row.chunk_index,
                        "content": row.content[:500] if row.content else "",  # ë¯¸ë¦¬ë³´ê¸°
                        "main_text": row.main_text,
                        "document_title": row.document_title,
                        "has_images": row.has_images,
                        "image_count": row.image_count,
                        "keyword_score": keyword_score,
                        "file_name": row.file_lgc_nm,
                        "file_path": row.path,
                        "search_method": "keyword_textsearch",
                        "modality": "text"  # ë¬¸ì„œ ë ˆë²¨ ê²€ìƒ‰
                    })
                
                logger.info(f"[KEYWORD-SEARCH] ë¬¸ì„œ ë ˆë²¨ ê²°ê³¼: {len(results)}ê°œ")
                
                # ğŸ–¼ï¸ IMAGE chunk ê²€ìƒ‰ ì¶”ê°€ (ìº¡ì…˜ í…ìŠ¤íŠ¸ ë§¤ì¹­)
                image_chunk_results = await self._search_image_chunks_by_caption(
                    query_text, container_ids, db, language
                )
                
                if image_chunk_results:
                    logger.info(f"[KEYWORD-SEARCH] IMAGE chunk ê²°ê³¼: {len(image_chunk_results)}ê°œ")
                    results.extend(image_chunk_results)
                
                logger.info(f"[KEYWORD-SEARCH] ì „ì²´ ê²°ê³¼: {len(results)}ê°œ (ë¬¸ì„œ + IMAGE chunk)")
                return results[:max_results]
                
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _search_image_chunks_by_caption(
        self,
        query_text: str,
        container_ids: List[str],
        db: AsyncSession,
        language: str = "mixed"
    ) -> List[Dict[str, Any]]:
        """
        IMAGE chunkì˜ ìº¡ì…˜ í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰
        
        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            container_ids: ê²€ìƒ‰ ëŒ€ìƒ ì»¨í…Œì´ë„ˆ ID ëª©ë¡
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            language: ê²€ìƒ‰ ì–¸ì–´ (en/ko/mixed)
        
        Returns:
            IMAGE chunk ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            from app.models.document.multimodal_models import DocChunk, DocChunkSession
            
            container_id_list = "', '".join(container_ids)
            
            # ILIKE íŒ¨í„´ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ê²€ìƒ‰)
            like_pattern = f"%{query_text}%"
            
            logger.info(f"[IMAGE-CHUNK-SEARCH] ìº¡ì…˜ ê²€ìƒ‰: '{query_text}', ì–¸ì–´: {language}")
            
            # doc_chunk í…Œì´ë¸”ì—ì„œ modality='image'ì¸ ì²­í¬ ê²€ìƒ‰
            # content_textì— ìº¡ì…˜ì´ ì €ì¥ë˜ì–´ ìˆìŒ
            query_sql = f"""
                SELECT 
                    c.chunk_id,
                    c.file_bss_info_sno,
                    c.chunk_index,
                    c.source_object_ids,
                    c.page_range,
                    c.blob_key,
                    c.content_text as caption,
                    c.modality,
                    c.section_heading,
                    f.knowledge_container_id,
                    f.file_lgc_nm,
                    f.path,
                    -- ìº¡ì…˜ í…ìŠ¤íŠ¸ ë§¤ì¹­ ì ìˆ˜ (ë‹¨ìˆœ ILIKEì´ë¯€ë¡œ ê³ ì • ì ìˆ˜)
                    0.8 as keyword_score
                FROM doc_chunk c
                JOIN doc_chunk_session s ON c.chunk_session_id = s.chunk_session_id
                JOIN tb_file_bss_info f ON c.file_bss_info_sno = f.file_bss_info_sno
                WHERE c.modality = 'image'
                    AND f.knowledge_container_id IS NOT NULL
                    AND f.knowledge_container_id != ''
                    AND f.knowledge_container_id NOT IN ('NONE', 'None', 'null', 'NULL')
                    AND (f.knowledge_container_id = 'DEFAULT_CONTAINER' OR f.knowledge_container_id IN ('{container_id_list}'))
                    AND f.del_yn = 'N'
                    AND c.content_text ILIKE :like_pattern
                ORDER BY c.chunk_index
                LIMIT 50
            """
            
            result = await db.execute(
                text(query_sql),
                {"like_pattern": like_pattern}
            )
            
            image_results = []
            for row in result.fetchall():
                # page_rangeì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                page_number = None
                if row.page_range:
                    # PostgreSQL int4rangeëŠ” ë¬¸ìì—´ë¡œ ë°˜í™˜ë¨: "[5,6)"
                    try:
                        page_str = str(row.page_range).strip('[]()').split(',')[0]
                        page_number = int(page_str)
                    except (ValueError, IndexError):
                        page_number = None
                
                # blob_key ê°€ì ¸ì˜¤ê¸° (ì‹ ê·œ ë°ì´í„°ì—ë§Œ ì¡´ì¬)
                blob_key = getattr(row, 'blob_key', None)
                
                image_results.append({
                    "search_doc_id": row.chunk_id,  # IMAGE chunkì˜ ID
                    "chunk_id": row.chunk_id,
                    "file_bss_info_sno": row.file_bss_info_sno,
                    "knowledge_container_id": row.knowledge_container_id,
                    "chunk_index": row.chunk_index,
                    "source_object_ids": list(row.source_object_ids) if row.source_object_ids else [],  # ì´ë¯¸ì§€ ê°ì²´ ID ë°°ì—´
                    "page_number": page_number,  # í˜ì´ì§€ ë²ˆí˜¸
                    "blob_key": blob_key,  # Blob Storage ê²½ë¡œ (ì‹ ê·œ)
                    "content": row.caption or "",  # ìº¡ì…˜ í…ìŠ¤íŠ¸
                    "main_text": row.caption or "",
                    "document_title": row.section_heading or "Image",
                    "has_images": True,
                    "image_count": 1,
                    "keyword_score": 0.8,  # IMAGE chunk ë°œê²¬ ì‹œ ë†’ì€ ì ìˆ˜
                    "file_name": row.file_lgc_nm,
                    "file_path": row.path,
                    "search_method": "image_caption",
                    "modality": "image"  # IMAGE chunkì„ì„ ëª…ì‹œ
                })
            
            logger.info(f"[IMAGE-CHUNK-SEARCH] {len(image_results)}ê°œ IMAGE chunk ë°œê²¬")
            return image_results
            
        except Exception as e:
            logger.error(f"IMAGE chunk ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return []
    
    async def _fulltext_search(
        self,
        processed_query: Dict[str, Any],
        container_ids: List[str],
        max_results: int,
        filters: Optional[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        PostgreSQL ì „ë¬¸ê²€ìƒ‰ - Multilingual í†µí•© íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì‚¬ìš©
        
        ë³€ê²½ ì‚¬í•­ (2025-10-24):
        - í•œêµ­ì–´(korean) + ì˜ì–´(english) dual tsvector ê²€ìƒ‰ ì¶”ê°€
        """
        try:
            # í†µí•© íŒŒì´í”„ë¼ì¸ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ ê²°ê³¼ ì‚¬ìš©
            fulltext_query = processed_query.get("fulltext_query", "")
            filtered_keywords = processed_query.get("filtered_keywords", [])
            language = processed_query.get("language", "mixed")
            
            if not fulltext_query and not filtered_keywords:
                return []
            
            # ê²€ìƒ‰ì–´ ì¤€ë¹„
            search_terms = fulltext_query if fulltext_query else " ".join(filtered_keywords)
            
            logger.info(f"ğŸ“š ì „ë¬¸ê²€ìƒ‰ ì‹¤í–‰: '{search_terms}' (ì–¸ì–´: {language}, í‚¤ì›Œë“œ: {filtered_keywords})")
            
            async with self.async_session_local() as db:
                # tb_document_search_indexë¥¼ ì‚¬ìš©í•œ Multilingual ì „ë¬¸ê²€ìƒ‰
                # korean: í•œê¸€ í˜•íƒœì†Œ ë¶„ì„, english: ì˜ì–´ ê²€ìƒ‰
                container_id_list = "', '".join(container_ids)
                
                # í•œê¸€/ì˜ì–´ í˜¼ìš© ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±
                # OR ì¡°ê±´ìœ¼ë¡œ korean ë˜ëŠ” english êµ¬ì„±ì—ì„œ ë§¤ì¹­ë˜ë©´ ê²°ê³¼ ë°˜í™˜
                query_sql = f"""
                    SELECT 
                        s.search_doc_id,
                        s.file_bss_info_sno,
                        s.knowledge_container_id,
                        0 as chunk_index,
                        s.full_content as content,
                        s.content_summary as main_text,
                        s.document_type as doc_type,
                        GREATEST(
                            ts_rank(s.content_tsvector, plainto_tsquery('korean', :search_terms)),
                            ts_rank(s.content_tsvector_en, plainto_tsquery('english', :search_terms))
                        ) as fulltext_score,
                        s.last_updated,
                        f.file_lgc_nm,
                        f.path
                    FROM tb_document_search_index s
                    JOIN tb_file_bss_info f ON s.file_bss_info_sno = f.file_bss_info_sno
                    WHERE s.knowledge_container_id IS NOT NULL 
                        AND s.knowledge_container_id != '' 
                        AND s.knowledge_container_id NOT IN ('NONE', 'None', 'null', 'NULL')
                        AND (s.knowledge_container_id = 'DEFAULT_CONTAINER' OR s.knowledge_container_id IN ('{container_id_list}'))
                        AND f.del_yn = 'N'
                        AND s.indexing_status = 'indexed'
                        AND (
                            s.content_tsvector @@ plainto_tsquery('korean', :search_terms)
                            OR s.content_tsvector_en @@ plainto_tsquery('english', :search_terms)
                        )
                    ORDER BY fulltext_score DESC
                    LIMIT :max_results
                """
                
                result = await db.execute(
                    text(query_sql),
                    {
                        "search_terms": search_terms,
                        "max_results": max_results
                    }
                )
                
                results = []
                for row in result.fetchall():
                    results.append({
                        "search_doc_id": row.search_doc_id,
                        "file_bss_info_sno": row.file_bss_info_sno,
                        "knowledge_container_id": row.knowledge_container_id,
                        "chunk_index": row.chunk_index,
                        "content": row.content,
                        "main_text": row.main_text,
                        "doc_type": row.doc_type,
                        "fulltext_score": float(row.fulltext_score),
                        "search_method": "fulltext",
                        "file_name": row.file_lgc_nm,
                        "file_path": row.path
                    })
                
                return results
                
        except Exception as e:
            logger.error(f"ì „ë¬¸ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _preprocess_query(self, query: str) -> Dict[str, Any]:
        """
        ğŸš€ í†µí•© ì§ˆì˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        
        ë³€ê²½ ì‚¬í•­ (2025-10-17):
        - í†µí•© íŒŒì´í”„ë¼ì¸ (query_pipeline.process_user_query) ì‚¬ìš©
        - ì¼ê´€ëœ ë¶ˆìš©ì–´ ì œê±° (UNIFIED_STOPWORDS)
        - ì˜ë„ ê¸°ë°˜ ê²€ìƒ‰ ì „ëµ ì ìš©
        """
        try:
            # í†µí•© íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì§ˆì˜ ì²˜ë¦¬
            processed = await process_user_query(query, search_type="general")
            
            logger.info(f"âœ… í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ: {processed.processing_time_ms:.1f}ms")
            logger.info(f"  - ì˜ë„: {processed.intent} (confidence: {processed.intent_confidence:.2f})")
            logger.info(f"  - í‚¤ì›Œë“œ: {processed.keywords} â†’ {processed.filtered_keywords}")
            logger.info(f"  - ì „ë¬¸ê²€ìƒ‰ ì¿¼ë¦¬: '{processed.fulltext_query}'")
            
            # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„ ìœ„í•œ ë³€í™˜
            return {
                "original_text": processed.original_text,
                "normalized_text": processed.normalized_text,
                "language": processed.language,  # ì–¸ì–´ ì •ë³´ ì¶”ê°€ (ko/en/mixed)
                "intent": processed.intent,
                "main_keywords": processed.filtered_keywords,
                "keywords": processed.keywords,
                "filtered_keywords": processed.filtered_keywords,
                "fulltext_query": processed.fulltext_query,
                "keyword_query": processed.keyword_query,
                "search_operators": processed.filtered_keywords,
                "search_query_string": processed.fulltext_query,
                "weights": processed.weights,
                "similarity_threshold": processed.similarity_threshold,
                # ê¸°ì¡´ í•„ë“œ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
                "context_keywords": [],
                "optimized_keywords": processed.filtered_keywords,
                "expanded_keywords": processed.filtered_keywords
            }
            
        except Exception as e:
            logger.error(f"âŒ í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            # Fallback: ìµœì†Œ ê¸°ëŠ¥
            return {
                "original_text": query,
                "intent": "find_document",
                "main_keywords": query.split(),
                "keywords": query.split(),
                "filtered_keywords": query.split(),
                "fulltext_query": query,
                "search_operators": [query],
                "search_query_string": query,
                "context_keywords": [],
                "optimized_keywords": query.split(),
                "expanded_keywords": query.split()
            }

    
    async def _get_accessible_containers(
        self,
        user_emp_no: str,
        requested_containers: Optional[List[str]] = None
    ) -> List[str]:
        """ì‚¬ìš©ìê°€ ê²€ìƒ‰ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ëª©ë¡ ë°˜í™˜"""
        try:
            accessible = await permission_service.get_user_accessible_containers(
                user_emp_no, "VIEWER"
            )
            
            container_ids = [c["container_id"] for c in accessible]
            
            if requested_containers:
                # ìš”ì²­ëœ ì»¨í…Œì´ë„ˆ ì¤‘ ê¶Œí•œì´ ìˆëŠ” ê²ƒë§Œ í•„í„°ë§
                container_ids = [
                    cid for cid in requested_containers 
                    if cid in container_ids
                ]
            
            return container_ids
            
        except Exception as e:
            logger.error(f"ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _save_search_history(
        self,
        user_emp_no: str,
        query: str,
        results: List[Dict[str, Any]],
        search_type: str,
        container_ids: List[str]
    ):
        """ê²€ìƒ‰ ê¸°ë¡ ì €ì¥ - ì‹¤ì œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì— ë§ì¶¤"""
        try:
            async with self.async_session_local() as db:
                # ì‹¤ì œ í…Œì´ë¸” ì»¬ëŸ¼ì— ë§ì¶° ê²€ìƒ‰ ê¸°ë¡ ì €ì¥
                await db.execute(
                    text("""
                        INSERT INTO tb_chat_history (
                            session_id, user_emp_no, user_message, assistant_response,
                            search_results, accessible_containers
                        ) VALUES (
                            :session_id, :user_emp_no, :user_message, :assistant_response,
                            :search_results, :accessible_containers
                        )
                    """),
                    {
                        "session_id": f"search_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        "user_emp_no": user_emp_no,
                        "user_message": query,
                        "assistant_response": f"ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê±´ ë°œê²¬",
                        "search_results": json.dumps({
                            "search_type": search_type,
                            "result_count": len(results),
                            "container_ids": container_ids,
                            "results": self._clean_results_for_json(results[:5])  # ìµœëŒ€ 5ê°œ ê²°ê³¼ë§Œ ì €ì¥
                        }, default=str),  # NaN ë“±ì˜ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                        "accessible_containers": container_ids
                    }
                )
                await db.commit()
                
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            # ê²€ìƒ‰ ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨í•´ë„ ê²€ìƒ‰ ìì²´ëŠ” ê³„ì† ì§„í–‰
    
    async def _format_search_results(
        self,
        results: List[Dict[str, Any]],
        user_emp_no: str,
        query: str = ""
    ) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… - API ìŠ¤í‚¤ë§ˆì— ë§ê²Œ í¬ë§·íŒ…"""
        formatted = []
        
        # ì»¨í…Œì´ë„ˆ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‹¤ì œ ì´ë¦„ê³¼ ê²½ë¡œ ì¡°íšŒ)
        container_ids = []
        for r in results:
            container_id = r.get("knowledge_container_id") or r.get("container_id")
            if container_id:
                container_ids.append(container_id)
                logger.info(f"ğŸ” FORMAT_SEARCH_RESULTS - ê²°ê³¼ì—ì„œ ë°œê²¬ëœ container_id: {container_id}")
        
        logger.info(f"ğŸ” FORMAT_SEARCH_RESULTS - ìˆ˜ì§‘ëœ container_ids: {container_ids}")
        container_details = await self._get_container_details(list(set(container_ids)))
        logger.info(f"ğŸ” FORMAT_SEARCH_RESULTS - ì¡°íšŒëœ container_details: {container_details}")
        
        for result in results:
            # ê²€ìƒ‰ ë°©ë²• ê²°ì •
            search_methods = result.get("search_methods", [])
            if not search_methods:
                search_methods = [result.get("search_method", "unknown")]
            
            # match_type ê²°ì • (primary search method)
            match_type = "hybrid"
            if len(search_methods) == 1:
                if "vector" in search_methods[0]:
                    match_type = "vector"
                elif "keyword" in search_methods[0]:
                    match_type = "keyword"
                elif "fulltext" in search_methods[0]:
                    match_type = "fulltext"
            
            # similarity_score ê³„ì‚° ë° ì¶”ì¶œ
            similarity_score = 0.0
            scores = result.get("scores", {})
            
            # ë¨¼ì € combined_score í™•ì¸
            if result.get("combined_score"):
                similarity_score = result.get("combined_score", 0.0)
            elif isinstance(scores, dict) and scores.get("similarity_score"):
                similarity_score = scores.get("similarity_score", 0.0)
            elif result.get("similarity_score"):
                similarity_score = result.get("similarity_score", 0.0)
            else:
                # ê°œë³„ ì ìˆ˜ë“¤ì„ ì¡°í•©í•˜ì—¬ ê³„ì‚°
                similarity_score = (
                    result.get("similarity_score", 0.0) * 0.6 +
                    result.get("keyword_score", 0.0) * 0.3 +
                    result.get("fulltext_score", 0.0) * 0.1
                )
            
            # NaN ì²´í¬
            if math.isnan(similarity_score):
                similarity_score = 0.0
            
            # âœ… ìœ ì‚¬ë„ ì ìˆ˜ ì •ê·œí™”: 0.0-1.0 ë²”ìœ„ë¡œ ê°•ì œ ì¡°ì •
            similarity_score = self._normalize_similarity_score(similarity_score)
            
            # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜ (0-100%)
            similarity_percentage = similarity_score * 100
            
            # file_idë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            file_id = result.get("file_bss_info_sno") or result.get("file_id")
            if file_id is not None:
                file_id = str(file_id)
            else:
                file_id = ""
            
            # ì œëª© ê²°ì • - íŒŒì¼ ë‹¨ìœ„ ê²°ê³¼ì— ë§ê²Œ ê°œì„ 
            title = result.get("file_name", "")
            if not title:
                # íŒŒì¼ëª…ì´ ì—†ëŠ” ê²½ìš° ë‚´ìš©ì˜ ì²« ë¶€ë¶„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
                content = result.get("content", "")
                if content:
                    title = content[:50] + "..." if len(content) > 50 else content
                else:
                    title = "ì œëª© ì—†ìŒ"
            
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì œëª© ì •ë¦¬
            if title and "." in title:
                # í™•ì¥ìê°€ ìˆëŠ” ê²½ìš° í™•ì¥ì ì œê±°í•˜ì—¬ í‘œì‹œ
                title_without_ext = title.rsplit(".", 1)[0]
                if len(title_without_ext) > 100:
                    title = title_without_ext[:100] + "..."
                else:
                    title = title_without_ext
            elif title and len(title) > 100:
                title = title[:100] + "..."
            
            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° - íŒŒì¼ ë‹¨ìœ„ì— ë§ê²Œ ê°œì„ 
            content = result.get("content", "")
            content_preview = content[:300] + "..." if len(content) > 300 else content
            
            # ê²€ìƒ‰ í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ ì ìš©
            if content_preview and query:
                content_preview = self._highlight_keywords(content_preview, query)
            
            # ì²­í¬ ì •ë³´ ì¶”ê°€
            chunk_info = ""
            if result.get("file_level_result") and result.get("chunk_count", 0) > 1:
                chunk_info = f" (ì´ {result.get('chunk_count')}ê°œ ê´€ë ¨ ì„¹ì…˜)"
            
            # ì»¨í…Œì´ë„ˆ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            container_id = result.get("knowledge_container_id") or result.get("container_id", "")
            logger.info(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬: doc_id={result.get('search_doc_id')}, container_id={container_id}")
            logger.info(f"ğŸ” ì „ì²´ result í‚¤ë“¤: {list(result.keys())}")
            logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ details: {list(container_details.keys())}")
            
            container_detail = container_details.get(container_id, {})
            logger.info(f"ğŸ” ì»¨í…Œì´ë„ˆ {container_id}ì— ëŒ€í•œ detail: {container_detail}")
            
            container_name = container_detail.get("container_name", container_id)
            
            # ê³„ì¸µ ê²½ë¡œ êµ¬ì„± (org_path ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ container_name)
            container_path = container_detail.get("full_path", "")
            if not container_path:
                # full_pathê°€ ì—†ìœ¼ë©´ container_name ì‚¬ìš©
                container_path = container_name
            
            # ê²½ë¡œë¥¼ ì•„ì´ì½˜ê³¼ í•¨ê»˜ êµ¬ì„±
            container_path_with_icons = self._build_container_path_with_icons(container_path)
            
            logger.info(f"ğŸ” ìµœì¢… ì»¨í…Œì´ë„ˆ ì •ë³´: name={container_name}, path={container_path}, with_icons={container_path_with_icons}")
            
            # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ - ì»¨í…Œì´ë„ˆ ì •ë³´ - ID: {container_id}, ì´ë¦„: {container_name}, ê²½ë¡œ: {container_path}")
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ - ì•„ì´ì½˜ í¬í•¨ ê²½ë¡œ: {container_path_with_icons}")
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ - container_detail ì „ì²´: {container_detail}")
            
            # ì»¨í…Œì´ë„ˆ ì •ë³´ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if not container_id:
                logger.warning(f"ì»¨í…Œì´ë„ˆ IDê°€ ì—†ëŠ” ê²°ê³¼: {result}")
                container_path_with_icons = "ğŸ“‚ ê²½ë¡œ ì—†ìŒ"
            
            # ë©€í‹°ëª¨ë‹¬ í•„ë“œ ì¶”ê°€
            modality = result.get("modality", "text")
            has_images = result.get("has_images", False)
            image_count = result.get("image_count", 0)
            clip_score = result.get("clip_score")
            
            # document_id ê²°ì • (file_bss_info_sno ì‚¬ìš©)
            document_id = result.get("file_bss_info_sno") or result.get("document_id") or file_id
            
            # API ìŠ¤í‚¤ë§ˆì— ë§ëŠ” í¬ë§·
            formatted_result = {
                "file_id": file_id,
                "title": title + chunk_info,
                "content_preview": content_preview,
                "similarity_score": float(similarity_score),  # 0.0-1.0 ë²”ìœ„ì˜ ì •ê·œí™”ëœ ì ìˆ˜
                "match_type": match_type,
                "container_id": container_id,
                "container_name": container_name,  # ì‚¬ìš©ì ì¹œí™”ì ì¸ ì»¨í…Œì´ë„ˆ ì´ë¦„
                "container_path": container_path_with_icons,  # ì•„ì´ì½˜ í¬í•¨ ê³„ì¸µ ê²½ë¡œ
                "container_icon": "ğŸ“‚",  # ê¸°ë³¸ í´ë” ì•„ì´ì½˜
                "file_path": result.get("file_path"),
                "metadata": {
                    "document_id": str(document_id) if document_id else file_id,  # file_bss_info_snoë¥¼ document_idë¡œ ì‚¬ìš©
                    "chunk_index": result.get("chunk_index"),
                    "chunk_count": result.get("chunk_count", 1),
                    "file_level_result": result.get("file_level_result", False),
                    "keywords": result.get("keywords", []),
                    "proper_nouns": result.get("proper_nouns", []),
                    "corp_names": result.get("corp_names", []),
                    "document_type": self._get_document_type(result),
                    "search_methods": search_methods,
                    "scores": scores,
                    "last_updated": result.get("last_updated"),
                    "file_name": result.get("file_name")
                },
                # ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì¶”ê°€ í•„ë“œ
                "has_images": has_images,
                "image_count": image_count,
                "modality": modality,
            }
            
            # CLIP ì ìˆ˜ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            if clip_score is not None:
                formatted_result["clip_score"] = float(clip_score)
            
            # ì´ë¯¸ì§€ ì²­í¬ì¸ ê²½ìš° ì´ë¯¸ì§€ URL ì¶”ê°€
            if modality == "image":
                chunk_id = result.get("chunk_id")
                blob_key = result.get("blob_key")  # ì‹ ê·œ: blob_key ì§ì ‘ ì‚¬ìš©
                
                if chunk_id:
                    formatted_result["chunk_id"] = chunk_id
                    
                    # blob_keyê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš© (ì‹ ê·œ ë°©ì‹)
                    if blob_key:
                        formatted_result["image_blob_key"] = blob_key
                    else:
                        # blob_keyê°€ ì—†ìœ¼ë©´ ë™ì  ìƒì„± (êµ¬ ë°ì´í„° í˜¸í™˜ì„±)
                        source_object_ids = result.get("source_object_ids", [])
                        page_number = result.get("page_number")
                        doc_id = result.get("file_bss_info_sno") or result.get("document_id")
                        
                        if doc_id and source_object_ids and len(source_object_ids) > 0:
                            # Azure Blob Storage í‚¤ íŒ¨í„´: multimodal/{doc_id}/objects/image_{object_id}_{page_number}.png
                            object_id = source_object_ids[0]
                            page_num = page_number if page_number is not None else 1
                            formatted_result["image_blob_key"] = f"multimodal/{doc_id}/objects/image_{object_id}_{page_num}.png"

            # íŒŒì¼ ê·¸ë£¹í™” ë‹¨ê³„ì—ì„œ ì„ ì •ëœ ì¸ë„¤ì¼(ìˆì„ ê²½ìš°)ì„ ê·¸ëŒ€ë¡œ ë…¸ì¶œ
            thumb_blob = result.get("thumbnail_blob_key") or result.get("image_blob_key")
            thumb_chunk = result.get("thumbnail_chunk_id") or result.get("chunk_id")
            if thumb_blob:
                formatted_result["thumbnail_blob_key"] = thumb_blob
            if thumb_chunk:
                formatted_result["thumbnail_chunk_id"] = thumb_chunk
            
            logger.info(f"í¬ë§·ëœ ê²°ê³¼: {formatted_result}")
            formatted.append(formatted_result)
        
        return formatted


    async def _get_search_suggestions(
        self,
        query: str,
        user_emp_no: str,
        limit: int = 10
    ) -> List[str]:
        """
        ê²€ìƒ‰ ìë™ì™„ì„± ì œì•ˆ - full_contentì™€ document_titleì—ì„œ ì¶”ì¶œ
        
        ë³€ê²½ ì‚¬í•­ (2025-10-16):
        - keywords ì»¬ëŸ¼ ì œê±°ë¡œ ì¸í•´ full_contentì™€ document_titleì—ì„œ ì§ì ‘ ì¶”ì¶œ
        - pg_trgm ì¸ë±ìŠ¤ë¥¼ í™œìš©í•œ ìœ ì‚¬ë„ ê¸°ë°˜ ì œì•ˆ
        """
        try:
            accessible_containers = await self._get_accessible_containers(user_emp_no)
            
            if not accessible_containers:
                return []
            
            async with self.async_session_local() as db:
                # full_contentì™€ document_titleì—ì„œ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ ì¶”ì¶œ
                container_id_list = "', '".join(accessible_containers)
                query_sql = f"""
                    SELECT DISTINCT 
                        s.document_title as suggestion
                    FROM tb_document_search_index s
                    JOIN tb_file_bss_info f ON s.file_bss_info_sno = f.file_bss_info_sno
                    WHERE s.knowledge_container_id IS NOT NULL 
                        AND s.knowledge_container_id != '' 
                        AND s.knowledge_container_id NOT IN ('NONE', 'None', 'null', 'NULL')
                        AND (s.knowledge_container_id = 'DEFAULT_CONTAINER' OR s.knowledge_container_id IN ('{container_id_list}'))
                        AND f.del_yn = 'N'
                        AND s.indexing_status = 'indexed'
                        AND s.document_title IS NOT NULL
                        AND s.document_title ILIKE :query_pattern
                    ORDER BY suggestion
                    LIMIT :limit_count
                """
                
                result = await db.execute(
                    text(query_sql),
                    {
                        "query_pattern": f"%{query}%",
                        "limit_count": limit
                    }
                )
                
                suggestions = [row.suggestion for row in result.fetchall() if row.suggestion]
                return suggestions
                
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì œì•ˆ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _group_results_by_file(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒŒì¼ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
        ê° íŒŒì¼ë‹¹ ìµœê³  ì ìˆ˜ì˜ ì²­í¬ë§Œ ì„ íƒí•˜ì—¬ íŒŒì¼ ë‹¨ìœ„ ê²°ê³¼ ìƒì„±
        """
        try:
            file_groups = {}
            
            for result in results:
                file_id = result.get("file_bss_info_sno")
                if not file_id:
                    continue
                
                # ì ìˆ˜ ê³„ì‚° (combined_score ë˜ëŠ” ê°œë³„ ì ìˆ˜ ì‚¬ìš©)
                score = 0.0
                if "combined_score" in result:
                    score = result["combined_score"]
                else:
                    # ê°œë³„ ì ìˆ˜ë“¤ì„ ì¡°í•©
                    score = (
                        result.get("similarity_score", 0.0) * 0.6 +
                        result.get("keyword_score", 0.0) * 0.3 +
                        result.get("fulltext_score", 0.0) * 0.1
                    )
                
                # NaN ì²´í¬
                if math.isnan(score):
                    score = 0.0
                
                # íŒŒì¼ë³„ë¡œ ìµœê³  ì ìˆ˜ ì²­í¬ë§Œ ìœ ì§€
                if file_id not in file_groups or score > file_groups[file_id]["max_score"]:
                    # ëŒ€í‘œ ì²­í¬ ë‚´ìš© ìƒì„± (ì—¬ëŸ¬ ì²­í¬ì˜ ë‚´ìš©ì„ í•©ì¹  ìˆ˜ë„ ìˆìŒ)
                    representative_content = result.get("content", "")
                    
                    file_groups[file_id] = {
                        "max_score": score,
                        "representative_result": {
                            **result,
                            "combined_score": score,
                            "content": representative_content,
                            "chunk_count": 1,  # ë‚˜ì¤‘ì— ë™ì¼ íŒŒì¼ì˜ ì²­í¬ ìˆ˜ë¥¼ ì¹´ìš´íŠ¸
                            "file_level_result": True  # íŒŒì¼ ë ˆë²¨ ê²°ê³¼ì„ì„ í‘œì‹œ
                        }
                    }
            
            # íŒŒì¼ë³„ ì²­í¬ ê°œìˆ˜ ê³„ì‚°
            file_chunk_counts = {}
            for result in results:
                file_id = result.get("file_bss_info_sno")
                if file_id:
                    file_chunk_counts[file_id] = file_chunk_counts.get(file_id, 0) + 1
            
            # ì²­í¬ ê°œìˆ˜ ì •ë³´ ì¶”ê°€
            grouped_results = []
            for file_id, group_data in file_groups.items():
                result = group_data["representative_result"]
                result["chunk_count"] = file_chunk_counts.get(file_id, 1)

                # Thumbnail selection: prefer an image chunk within the same file group
                # Scan original results for same file_id and find an image modality chunk
                thumbnail_blob_key = None
                thumbnail_chunk_id = None
                for r in results:
                    if r.get("file_bss_info_sno") != file_id:
                        continue
                    # modality may be present or inside metadata_json
                    modality = r.get("modality")
                    if not modality and r.get("metadata"):
                        modality = r.get("metadata", {}).get("modality")
                    if modality == "image":
                        # prefer explicit chunk-level blob key if available
                        # check metadata for blob key/object id/page no
                        meta = r.get("metadata") or {}
                        obj_id = meta.get("object_id") or meta.get("objectIdx") or r.get("chunk_index")
                        page_no = meta.get("page_no", 1)
                        # common blob key patterns used by pipeline
                        if file_id:
                            thumbnail_blob_key = f"multimodal/{file_id}/objects/image_{obj_id}_{page_no}.png"
                            thumbnail_chunk_id = r.get("chunk_id") or r.get("search_doc_id") or r.get("document_id")
                            break

                if thumbnail_blob_key:
                    result["thumbnail_blob_key"] = thumbnail_blob_key
                    result["thumbnail_chunk_id"] = thumbnail_chunk_id

                grouped_results.append(result)
            
            # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
            grouped_results.sort(key=lambda x: x.get("combined_score", 0.0), reverse=True)
            
            logger.info(f"íŒŒì¼ ê·¸ë£¹í™” ì™„ë£Œ: {len(results)}ê°œ ì²­í¬ -> {len(grouped_results)}ê°œ íŒŒì¼")
            return grouped_results
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ê·¸ë£¹í™” ì‹¤íŒ¨: {str(e)}")
            return results  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ê²°ê³¼ ë°˜í™˜
    
    async def _get_search_analytics(self, period: str = "7d") -> Dict[str, Any]:
        """ê²€ìƒ‰ ë¶„ì„ ì •ë³´"""
        try:
            days = {"1d": 1, "7d": 7, "30d": 30, "90d": 90}[period]
            
            async with self.async_session_local() as db:
                # ê²€ìƒ‰ í†µê³„ ì¿¼ë¦¬ - ì‹¤ì œ í…Œì´ë¸” ì»¬ëŸ¼ ì‚¬ìš©
                analytics_sql = """
                    SELECT 
                        COUNT(*) as total_searches,
                        COUNT(DISTINCT user_emp_no) as unique_users,
                        0.1 as avg_response_time,
                        COUNT(CASE WHEN search_results IS NOT NULL 
                                   AND search_results::jsonb->>'result_count' != '0' 
                                   THEN 1 END) as successful_searches
                    FROM tb_chat_history 
                    WHERE user_message IS NOT NULL
                        AND search_results IS NOT NULL
                """
                
                result = await db.execute(text(analytics_sql))
                stats = result.fetchone()
                
                return {
                    "period": period,
                    "total_searches": stats.total_searches or 0,
                    "unique_users": stats.unique_users or 0,
                    "avg_response_time_ms": float(stats.avg_response_time or 0) * 1000,  # ì´ˆë¥¼ ë°€ë¦¬ì´ˆë¡œ ë³€í™˜
                    "success_rate": (stats.successful_searches / max(stats.total_searches, 1)) * 100,
                    "generated_at": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                "period": period,
                "total_searches": 0,
                "unique_users": 0,
                "avg_response_time_ms": 0.0,
                "success_rate": 0.0,
                "error": str(e)
            }


    async def vector_search_only(
        self,
        query: str,
        user_emp_no: str,
        limit: int = 10
    ) -> Dict[str, Any]:
        """ë²¡í„° ê²€ìƒ‰ ì „ìš© ë©”ì„œë“œ"""
        accessible_containers = await self._get_accessible_containers(user_emp_no)
        if not accessible_containers:
            return {"results": [], "total_count": 0, "message": "ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        processed_query = await self._preprocess_query(query)
        results = await self._vector_search(processed_query, accessible_containers, limit * 2, None)  # ê·¸ë£¹í™”ë¥¼ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
        grouped_results = await self._group_results_by_file(results)
        formatted_results = await self._format_search_results(grouped_results[:limit], user_emp_no, query)
        
        return {
            "results": formatted_results,
            "total_count": len(formatted_results),
            "search_type": "vector_only"
        }
    
    async def keyword_search_only(
        self,
        query: str,
        user_emp_no: str,
        limit: int = 10
    ) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ ì „ìš© ë©”ì„œë“œ"""
        accessible_containers = await self._get_accessible_containers(user_emp_no)
        if not accessible_containers:
            return {"results": [], "total_count": 0, "message": "ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        processed_query = await self._preprocess_query(query)
        results = await self._keyword_search(processed_query, accessible_containers, limit * 2, None)  # ê·¸ë£¹í™”ë¥¼ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
        grouped_results = await self._group_results_by_file(results)
        formatted_results = await self._format_search_results(grouped_results[:limit], user_emp_no, query)
        
        return {
            "results": formatted_results,
            "total_count": len(formatted_results),
            "search_type": "keyword_only"
        }
    
    async def get_search_suggestions(
        self,
        partial_query: str,
        user_emp_no: str,
        limit: int = 10
    ) -> List[str]:
        """ê²€ìƒ‰ ì œì•ˆ ë©”ì„œë“œ"""
        return await self._get_search_suggestions(partial_query, user_emp_no, limit)
    
    async def get_search_analytics(
        self,
        user_emp_no: str,
        period: str = "7d"
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ ë¶„ì„ ë©”ì„œë“œ"""
        return await self._get_search_analytics(period)
    
    async def reindex_document(
        self,
        file_id: str,
        user_emp_no: str
    ) -> Dict[str, Any]:
        """ë¬¸ì„œ ì¬ì¸ë±ì‹± ë©”ì„œë“œ"""
        try:
            logger.info(f"ë¬¸ì„œ ì¬ì¸ë±ì‹± ì‹œì‘: {file_id}, ì‚¬ìš©ì: {user_emp_no}")
            
            # 1. íŒŒì¼ ì •ë³´ ì¡°íšŒ
            async with self.async_session_local() as db:
                file_query = """
                    SELECT file_bss_info_sno, file_lgc_nm, path, knowledge_container_id
                    FROM tb_file_bss_info 
                    WHERE file_bss_info_sno = :file_id AND del_yn = 'N'
                """
                result = await db.execute(text(file_query), {"file_id": file_id})
                file_info = result.fetchone()
                
                if not file_info:
                    return {
                        "success": False,
                        "file_id": file_id,
                        "error": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    }
                
                # 2. ê¸°ì¡´ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì‚­ì œ
                await db.execute(
                    text("DELETE FROM tb_document_search_index WHERE file_bss_info_sno = :file_id"),
                    {"file_id": file_id}
                )
                
                # 3. ê¸°ì¡´ ë²¡í„° ì²­í¬ ì‚­ì œ
                await db.execute(
                    text("DELETE FROM vs_doc_contents_chunks WHERE file_bss_info_sno = :file_id"),
                    {"file_id": file_id}
                )
                
                await db.commit()
            
            # 4. ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰
            from app.services.document.pipeline.integrated_document_pipeline_service import integrated_document_pipeline_service
            
            pipeline_result = await integrated_document_pipeline_service.process_document_for_rag(
                file_path=file_info.path,
                file_name=file_info.file_lgc_nm,
                container_id=file_info.knowledge_container_id,
                user_emp_no=user_emp_no
            )
            
            if pipeline_result.get("success"):
                logger.info(f"ë¬¸ì„œ ì¬ì¸ë±ì‹± ì™„ë£Œ: {file_id}")
                return {
                    "success": True,
                    "file_id": file_id,
                    "message": "ì¬ì¸ë±ì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "status": "completed",
                    "pipeline_result": pipeline_result
                }
            else:
                return {
                    "success": False,
                    "file_id": file_id,
                    "error": f"íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {pipeline_result.get('error')}"
                }
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "file_id": file_id,
                "error": str(e)
            }

    def _clean_results_for_json(self, results):
        """JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ëŠ” ë©”ì„œë“œ"""
        cleaned_results = []
        for result in results:
            cleaned_result = {}
            for key, value in result.items():
                if isinstance(value, float):
                    if math.isnan(value) or math.isinf(value):
                        cleaned_result[key] = 0.0
                    else:
                        cleaned_result[key] = value
                else:
                    cleaned_result[key] = value
            cleaned_results.append(cleaned_result)
        return cleaned_results

    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ë©”ì„œë“œë“¤
    async def search_similar_documents(
        self,
        query: str,
        user_emp_no: str = "SYSTEM",
        limit: int = 10,
        threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë ˆê±°ì‹œ ë©”ì„œë“œ
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        """
        try:
            result = await self.hybrid_search(
                query=query,
                user_emp_no=user_emp_no,
                max_results=limit,
                search_type="hybrid"
            )
            return result.get("results", [])
        except Exception as e:
            logger.error(f"Legacy search method error: {str(e)}")
            return []

    async def unified_search(
        self,
        query: str,
        user_emp_no: str,
        container_ids: Optional[List[str]] = None,
        max_results: int = 10,
        search_type: str = "hybrid",
        filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        í†µí•©ê²€ìƒ‰ - íŒŒì¼ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”ëœ ê²€ìƒ‰ ê²°ê³¼
        í™”ë©´ í‘œì‹œìš© ê²€ìƒ‰ìœ¼ë¡œ ë™ì¼ íŒŒì¼ì˜ ì²­í¬ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì³ì„œ í‘œì‹œ
        """
        try:
            # ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ì²­í¬ ë‹¨ìœ„)
            chunk_results = await self.hybrid_search(
                query=query,
                user_emp_no=user_emp_no,
                container_ids=container_ids,
                max_results=max_results * 3,  # ë” ë§ì€ ì²­í¬ë¥¼ ê°€ì ¸ì™€ì„œ íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”
                search_type=search_type,
                filters=filters
            )
            
            # íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™” ë° ëŒ€í‘œ ì •ë³´ ìƒì„±
            file_groups = {}
            for result in chunk_results.get("results", []):
                file_id = result.get("file_id")
                if not file_id:
                    continue
                    
                if file_id not in file_groups:
                    # íŒŒì¼ì˜ ì²« ë²ˆì§¸ ì²­í¬ë¥¼ ëŒ€í‘œë¡œ ì„¤ì •
                    file_groups[file_id] = {
                        "file_id": file_id,
                        "title": result.get("metadata", {}).get("file_name", "ì œëª© ì—†ìŒ"),
                        "file_path": result.get("file_path"),
                        "container_id": result.get("container_id"),
                        "match_type": result.get("match_type"),
                        "max_similarity_score": result.get("similarity_score", 0.0),
                        "content_preview": result.get("content_preview", "")[:500],  # íŒŒì¼ ëŒ€í‘œ ë‚´ìš©
                        "chunk_count": 1,
                        "top_chunks": [result],  # ìƒìœ„ ì²­í¬ ì •ë³´ ë³´ê´€
                        "metadata": {
                            "file_name": result.get("metadata", {}).get("file_name"),
                            "document_type": result.get("metadata", {}).get("document_type"),
                            "keywords": result.get("metadata", {}).get("keywords", []),
                            "search_methods": result.get("metadata", {}).get("search_methods", []),
                            "last_updated": result.get("metadata", {}).get("last_updated")
                        }
                    }
                else:
                    # ë™ì¼ íŒŒì¼ì˜ ì¶”ê°€ ì²­í¬ ì²˜ë¦¬
                    file_group = file_groups[file_id]
                    file_group["chunk_count"] += 1
                    
                    # ë” ë†’ì€ ì ìˆ˜ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                    current_score = result.get("similarity_score", 0.0)
                    if current_score > file_group["max_similarity_score"]:
                        file_group["max_similarity_score"] = current_score
                        file_group["content_preview"] = result.get("content_preview", "")[:500]
                        file_group["match_type"] = result.get("match_type")
                    
                    # ìƒìœ„ 3ê°œ ì²­í¬ë§Œ ë³´ê´€
                    if len(file_group["top_chunks"]) < 3:
                        file_group["top_chunks"].append(result)
                    
                    # í‚¤ì›Œë“œ í†µí•©
                    existing_keywords = set(file_group["metadata"].get("keywords", []))
                    new_keywords = set(result.get("metadata", {}).get("keywords", []))
                    file_group["metadata"]["keywords"] = list(existing_keywords | new_keywords)
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ì œí•œ
            sorted_files = sorted(
                file_groups.values(),
                key=lambda x: x["max_similarity_score"],
                reverse=True
            )[:max_results]
            
            return {
                "results": sorted_files,
                "total_count": len(sorted_files),
                "search_type": f"unified_{search_type}",
                "accessible_containers": chunk_results.get("accessible_containers", []),
                "query_processed": chunk_results.get("query_processed", {}),
                "execution_time": datetime.now().isoformat(),
                "message": f"{len(sorted_files)}ê°œ íŒŒì¼ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            }
            
        except Exception as e:
            logger.error(f"í†µí•©ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            raise

    async def context_search(
        self,
        query: str,
        user_emp_no: str,
        container_ids: Optional[List[str]] = None,
        max_results: int = 20,
        search_type: str = "hybrid",
        filters: Optional[Dict[str, Any]] = None,
        include_references: bool = True
    ) -> Dict[str, Any]:
        """
        RAG ì»¨í…ìŠ¤íŠ¸ìš© ì²­í¬ ë‹¨ìœ„ ê²€ìƒ‰
        ì±—ë´‡ ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì •ë°€í•œ ì²­í¬ ë‹¨ìœ„ ê²€ìƒ‰
        """
        try:
            # ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ì²­í¬ ë‹¨ìœ„)
            chunk_results = await self.hybrid_search(
                query=query,
                user_emp_no=user_emp_no,
                container_ids=container_ids,
                max_results=max_results,
                search_type=search_type,
                filters=filters
            )
            
            # RAGìš© ìƒì„¸ ì •ë³´ ì¶”ê°€
            enhanced_results = []
            for result in chunk_results.get("results", []):
                enhanced_result = {
                    "chunk_id": result.get("metadata", {}).get("document_id"),
                    "file_id": result.get("file_id"),
                    "content": result.get("content_preview"),  # ì „ì²´ ì²­í¬ ë‚´ìš©
                    "similarity_score": result.get("similarity_score"),
                    "match_type": result.get("match_type"),
                    "container_id": result.get("container_id"),
                    "chunk_info": {
                        "chunk_index": result.get("metadata", {}).get("chunk_index"),
                        "file_name": result.get("metadata", {}).get("file_name"),
                        "file_path": result.get("file_path"),
                        "page_number": self._extract_page_number(result),
                        "section_title": self._extract_section_title(result)
                    },
                    "reference_info": {
                        "title": result.get("title"),
                        "source": f"{result.get('metadata', {}).get('file_name', 'Unknown')}",
                        "page": self._extract_page_number(result),
                        "section": self._extract_section_title(result),
                        "chunk_position": f"ì²­í¬ {result.get('metadata', {}).get('chunk_index', 0) + 1}"
                    } if include_references else None,
                    "metadata": {
                        "keywords": result.get("metadata", {}).get("keywords", []),
                        "proper_nouns": result.get("metadata", {}).get("proper_nouns", []),
                        "corp_names": result.get("metadata", {}).get("corp_names", []),
                        "search_methods": result.get("metadata", {}).get("search_methods", []),
                        "document_type": result.get("metadata", {}).get("document_type"),
                        "relevance_explanation": self._generate_relevance_explanation(result, query)
                    }
                }
                enhanced_results.append(enhanced_result)
            
            return {
                "results": enhanced_results,
                "total_count": len(enhanced_results),
                "search_type": f"context_{search_type}",
                "context_info": {
                    "total_chunks": len(enhanced_results),
                    "average_score": sum(r["similarity_score"] for r in enhanced_results) / max(len(enhanced_results), 1),
                    "score_distribution": self._calculate_score_distribution(enhanced_results),
                    "file_sources": list(set(r["chunk_info"]["file_name"] for r in enhanced_results if r["chunk_info"]["file_name"]))
                },
                "accessible_containers": chunk_results.get("accessible_containers", []),
                "query_processed": chunk_results.get("query_processed", {}),
                "execution_time": datetime.now().isoformat(),
                "message": f"RAG ì»¨í…ìŠ¤íŠ¸ìš© {len(enhanced_results)}ê°œ ì²­í¬ë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤."
            }
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            raise

    def _extract_page_number(self, result: Dict[str, Any]) -> Optional[int]:
        """ì²­í¬ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            metadata = result.get("metadata", {})
            # metadataì—ì„œ í˜ì´ì§€ ì •ë³´ ì°¾ê¸°
            if "page" in metadata:
                return metadata["page"]
            if "page_number" in metadata:
                return metadata["page_number"]
            
            # ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜ì´ì§€ ì¶”ì • (ì²­í¬ë‹¹ ì•½ 1/2 í˜ì´ì§€ë¡œ ê°€ì •)
            chunk_index = metadata.get("chunk_index", 0)
            return max(1, chunk_index // 2 + 1)
        except:
            return None

    def _extract_section_title(self, result: Dict[str, Any]) -> Optional[str]:
        """ì²­í¬ì—ì„œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ"""
        try:
            content = result.get("content_preview", "")
            # ë‚´ìš©ì˜ ì²« ì¤„ì´ ì œëª©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            lines = content.split('\n')
            if lines and len(lines[0]) < 100:  # ì œëª©ì€ ë³´í†µ ì§§ìŒ
                return lines[0].strip()
            return None
        except:
            return None

    def _generate_relevance_explanation(self, result: Dict[str, Any], query: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± ì„¤ëª… ìƒì„±"""
        try:
            match_type = result.get("match_type", "unknown")
            score = result.get("similarity_score", 0.0)
            
            if match_type == "vector":
                return f"ì˜ë¯¸ì  ìœ ì‚¬ë„ {score:.2f}ë¡œ ë§¤ì¹­"
            elif match_type == "keyword":
                return f"í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë°œê²¬"
            elif match_type == "fulltext":
                return f"ì „ë¬¸ê²€ìƒ‰ìœ¼ë¡œ ë°œê²¬"
            elif match_type == "hybrid":
                return f"ë³µí•© ê²€ìƒ‰ ì ìˆ˜ {score:.2f}ë¡œ ë§¤ì¹­"
            else:
                return f"ê²€ìƒ‰ ì ìˆ˜ {score:.2f}"
        except:
            return "ê´€ë ¨ ë‚´ìš©"

    async def _get_container_details(self, container_ids: List[str]) -> Dict[str, Dict[str, Any]]:
        """
        ì»¨í…Œì´ë„ˆ ìƒì„¸ ì •ë³´ ì¡°íšŒ - ì‚¬ìš©ìë³„ ê¶Œí•œ ì •ë³´ì™€ ê³„ì¸µ ê²½ë¡œ í¬í•¨
        """
        try:
            if not container_ids:
                logger.info("ì»¨í…Œì´ë„ˆ ID ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return {}

            now = time.time()
            container_details: Dict[str, Dict[str, Any]] = {}
            uncached_ids: List[str] = []

            for container_id in container_ids:
                cache_entry = self._container_details_cache.get(container_id)
                if cache_entry and now - cache_entry[0] <= self._container_cache_ttl:
                    container_details[container_id] = cache_entry[1]
                else:
                    uncached_ids.append(container_id)

            if not uncached_ids:
                logger.info("ì»¨í…Œì´ë„ˆ ì •ë³´ ìºì‹œ ì ì¤‘: %s", list(container_details.keys()))
                return container_details

            logger.info(f"ì»¨í…Œì´ë„ˆ ì •ë³´ ì¡°íšŒ ì‹œì‘(ë¯¸ìºì‹œ): {uncached_ids}")
            container_id_to_name = await self._get_all_container_names()

            async with self.async_session_local() as db:
                stmt = select(
                    TbKnowledgeContainers.container_id,
                    TbKnowledgeContainers.container_name,
                    TbKnowledgeContainers.parent_container_id,
                    TbKnowledgeContainers.org_level,
                    TbKnowledgeContainers.org_path,
                    TbKnowledgeContainers.container_type
                ).where(
                    TbKnowledgeContainers.is_active == True,
                    TbKnowledgeContainers.container_id.in_(uncached_ids)
                )

                result = await db.execute(stmt)
                rows = result.fetchall()

            fetched_ids = set()
            for row in rows:
                friendly_path = self._convert_path_ids_to_names(row.org_path, container_id_to_name)
                detail = {
                    "container_id": row.container_id,
                    "container_name": row.container_name,
                    "parent_container_id": row.parent_container_id,
                    "full_path": friendly_path or row.container_name,
                    "hierarchy_level": row.org_level or 1,
                    "container_type": row.container_type,
                }
                container_details[row.container_id] = detail
                self._container_details_cache[row.container_id] = (now, detail)
                fetched_ids.add(row.container_id)

            for container_id in uncached_ids:
                if container_id in fetched_ids:
                    continue
                logger.warning(f"ì»¨í…Œì´ë„ˆ {container_id}ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                detail = {
                    "container_id": container_id,
                    "container_name": container_id,
                    "parent_container_id": None,
                    "full_path": container_id,
                    "hierarchy_level": 1,
                    "container_type": "UNKNOWN"
                }
                container_details[container_id] = detail
                self._container_details_cache[container_id] = (now, detail)

            # ê¸°ë³¸ ì»¨í…Œì´ë„ˆ ì •ë³´ ì¶”ê°€
            if "DEFAULT_CONTAINER" in container_ids and "DEFAULT_CONTAINER" not in container_details:
                default_detail = {
                    "container_id": "DEFAULT_CONTAINER",
                    "container_name": "ê¸°ë³¸ ë¬¸ì„œ",
                    "parent_container_id": None,
                    "full_path": "ê¸°ë³¸ ë¬¸ì„œ",
                    "hierarchy_level": 1,
                    "container_type": "DEFAULT"
                }
                container_details["DEFAULT_CONTAINER"] = default_detail
                self._container_details_cache["DEFAULT_CONTAINER"] = (now, default_detail)

            return container_details

        except Exception as e:
            logger.error(f"ì»¨í…Œì´ë„ˆ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            fallback = {}
            for container_id in container_ids:
                fallback[container_id] = {
                    "container_id": container_id,
                    "container_name": container_id,
                    "parent_container_id": None,
                    "full_path": container_id,
                    "hierarchy_level": 1,
                    "container_type": "UNKNOWN"
                }
            return fallback

    async def _get_all_container_names(self) -> Dict[str, str]:
        """ì»¨í…Œì´ë„ˆ ID â†’ ì´ë¦„ ë§¤í•‘ (TTL ìºì‹±)"""
        cached_ts, cached_map = self._all_container_name_cache
        now = time.time()
        if cached_map and now - cached_ts <= self._container_cache_ttl:
            return cached_map

        async with self.async_session_local() as db:
            stmt = select(
                TbKnowledgeContainers.container_id,
                TbKnowledgeContainers.container_name
            ).where(TbKnowledgeContainers.is_active == True)
            result = await db.execute(stmt)
            mapping = {row.container_id: row.container_name for row in result.fetchall()}

        self._all_container_name_cache = (now, mapping)
        return mapping

    async def _get_container_friendly_names(self, container_ids: List[str]) -> List[str]:
        """ì»¨í…Œì´ë„ˆ IDë“¤ì„ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        try:
            logger.info(f"ì»¨í…Œì´ë„ˆ ì¹œí™”ì  ì´ë¦„ ë³€í™˜ ì‹œì‘: {container_ids}")
            container_details = await self._get_container_details(container_ids)
            friendly_names = []
            
            for container_id in container_ids:
                if container_id in container_details:
                    friendly_name = container_details[container_id]["container_name"]
                    friendly_names.append(friendly_name)
                    logger.info(f"ì»¨í…Œì´ë„ˆ {container_id} -> {friendly_name}")
                else:
                    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° ì»¨í…Œì´ë„ˆ ID ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    friendly_names.append(container_id)
                    logger.warning(f"ì»¨í…Œì´ë„ˆ {container_id}ì˜ ì¹œí™”ì  ì´ë¦„ì„ ì°¾ì§€ ëª»í•´ IDë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
            
            logger.info(f"ì»¨í…Œì´ë„ˆ ì¹œí™”ì  ì´ë¦„ ë³€í™˜ ì™„ë£Œ: {friendly_names}")
            return friendly_names
            
        except Exception as e:
            logger.error(f"ì»¨í…Œì´ë„ˆ ì¹œí™”ì  ì´ë¦„ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ì‹œ ì»¨í…Œì´ë„ˆ IDë“¤ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return container_ids

    def _build_container_path(self, container_id: str, all_containers: Dict[str, Dict]) -> str:
        """ì»¨í…Œì´ë„ˆ IDë¡œë¶€í„° ì „ì²´ ê²½ë¡œ êµ¬ì„±"""
        try:
            path_parts = []
            current_id = container_id
            
            while current_id and current_id in all_containers:
                container = all_containers[current_id]
                path_parts.insert(0, current_id)  # ì•ìª½ì— ì‚½ì…
                current_id = container.get("parent_container_id")
                
                # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if len(path_parts) > 10:
                    break
            
            return "/" + "/".join(path_parts)
            
        except Exception as e:
            logger.warning(f"ì»¨í…Œì´ë„ˆ ê²½ë¡œ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return f"/{container_id}"

    def _convert_path_to_display_names(self, org_path: str, all_containers: Dict[str, Dict]) -> str:
        """org_pathë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        try:
            if not org_path:
                return ""
                
            # org_pathì—ì„œ ì»¨í…Œì´ë„ˆ IDë“¤ ì¶”ì¶œ (ì˜ˆ: /WJ_ROOT/WJ_CEO/WJ_HR)
            path_parts = [part for part in org_path.split("/") if part]
            display_parts = []
            
            for container_id in path_parts:
                if container_id in all_containers:
                    value = all_containers[container_id]
                    if isinstance(value, dict):
                        display_name = value.get("container_name", container_id)
                    else:
                        display_name = value
                    display_parts.append(display_name)
                else:
                    display_parts.append(container_id)
            
            return " > ".join(display_parts)
            
        except Exception as e:
            logger.warning(f"ê²½ë¡œ í‘œì‹œëª… ë³€í™˜ ì‹¤íŒ¨: {e}")
            return org_path

    def _get_container_icon(self, is_final: bool = True) -> str:
        """ì»¨í…Œì´ë„ˆ ì•„ì´ì½˜ ë°˜í™˜"""
        if is_final:
            return "ğŸ“‚"  # ì—´ë¦° í´ë” ì•„ì´ì½˜ (ìµœì¢… ê²½ë¡œ)
        else:
            return "ğŸ“"  # ë‹«íŒ í´ë” ì•„ì´ì½˜ (ì¤‘ê°„ ê²½ë¡œ)

    def _calculate_score_distribution(self, results: List[Dict[str, Any]]) -> Dict[str, int]:
        """ì ìˆ˜ ë¶„í¬ ê³„ì‚°"""
        try:
            distribution = {"high": 0, "medium": 0, "low": 0}
            for result in results:
                score = result.get("similarity_score", 0.0)
                if score >= 0.8:
                    distribution["high"] += 1
                elif score >= 0.6:
                    distribution["medium"] += 1
                else:
                    distribution["low"] += 1
            return distribution
        except:
            return {"high": 0, "medium": 0, "low": 0}

    def _get_document_type(self, result: Dict[str, Any]) -> str:
        """íŒŒì¼ í™•ì¥ì ê¸°ë°˜ ë¬¸ì„œ íƒ€ì… ë°˜í™˜"""
        try:
            # ê¸°ì¡´ doc_typeì´ ìˆê³  'document'ê°€ ì•„ë‹ˆë©´ ì‚¬ìš©
            existing_type = result.get("document_type") or result.get("doc_type")
            if existing_type and existing_type != "document":
                return existing_type
            
            # íŒŒì¼ ê²½ë¡œë‚˜ ì´ë¦„ì—ì„œ í™•ì¥ì ì¶”ì¶œ
            file_path = result.get("file_path") or result.get("path") or ""
            file_name = result.get("file_lgc_nm") or result.get("file_name") or ""
            
            # í™•ì¥ì ê¸°ë°˜ íƒ€ì… ë§¤í•‘
            extension_map = {
                ".pdf": "PDF ë¬¸ì„œ",
                ".doc": "Word ë¬¸ì„œ", 
                ".docx": "Word ë¬¸ì„œ",
                ".xls": "Excel ë¬¸ì„œ",
                ".xlsx": "Excel ë¬¸ì„œ",
                ".ppt": "PowerPoint ë¬¸ì„œ",
                ".pptx": "PowerPoint ë¬¸ì„œ",
                ".txt": "í…ìŠ¤íŠ¸ ë¬¸ì„œ",
                ".hwp": "í•œê¸€ ë¬¸ì„œ",
                ".png": "ì´ë¯¸ì§€",
                ".jpg": "ì´ë¯¸ì§€",
                ".jpeg": "ì´ë¯¸ì§€",
                ".gif": "ì´ë¯¸ì§€",
                ".mp4": "ë™ì˜ìƒ",
                ".avi": "ë™ì˜ìƒ",
                ".zip": "ì••ì¶• íŒŒì¼",
                ".rar": "ì••ì¶• íŒŒì¼"
            }
            
            # íŒŒì¼ ê²½ë¡œì—ì„œ í™•ì¥ì ì¶”ì¶œ
            for ext, doc_type in extension_map.items():
                if file_path.lower().endswith(ext) or file_name.lower().endswith(ext):
                    return doc_type
            
            return "ë¬¸ì„œ"  # ê¸°ë³¸ê°’
            
        except Exception as e:
            logger.warning(f"ë¬¸ì„œ íƒ€ì… í™•ì¸ ì‹¤íŒ¨: {e}")
            return "ë¬¸ì„œ"

    def _calculate_similarity_percentage(self, similarity_score: float) -> float:
        """ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜"""
        try:
            if not similarity_score or math.isnan(similarity_score):
                return 0.0
            
            # ì ìˆ˜ê°€ ì´ë¯¸ í¼ì„¼íŠ¸ í˜•íƒœ(1ë³´ë‹¤ í° ê°’)ì¸ì§€ í™•ì¸
            if similarity_score > 1.0:
                # ì´ë¯¸ í¼ì„¼íŠ¸ í˜•íƒœì¸ ê²½ìš° 100ì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì œí•œ
                percentage = min(similarity_score, 100.0)
            else:
                # 0.0 ~ 1.0 ë²”ìœ„ì˜ ì ìˆ˜ë¥¼ 0.0 ~ 100.0 ë²”ìœ„ë¡œ ë³€í™˜
                percentage = similarity_score * 100.0
            
            # ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€ë§Œ í‘œì‹œ
            return round(percentage, 1)
            
        except Exception as e:
            logger.warning(f"ìœ ì‚¬ë„ í¼ì„¼íŠ¸ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
            return 0.0

    def _highlight_keywords(self, text: str, query: str, keywords: List[str] = None) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬"""
        try:
            if not text or not query:
                return text
            
            import re
            
            # í•˜ì´ë¼ì´íŠ¸í•  í‚¤ì›Œë“œ ëª©ë¡ ìƒì„±
            highlight_terms = set()
            
            # 1. ì›ë³¸ ì¿¼ë¦¬ ì¶”ê°€
            highlight_terms.add(query.strip().lower())
            
            # 2. ì¶”ê°€ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í¬í•¨
            if keywords:
                for keyword in keywords:
                    if keyword and keyword.strip():
                        highlight_terms.add(keyword.strip().lower())
            
            # 3. ì¿¼ë¦¬ë¥¼ ê³µë°±ìœ¼ë¡œ ë¶„í• í•œ ê°œë³„ ë‹¨ì–´ë“¤ ì¶”ê°€
            query_words = query.strip().split()
            for word in query_words:
                if len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ í•˜ì´ë¼ì´íŠ¸
                    highlight_terms.add(word.lower())
            
            # 4. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í™œìš©
            try:
                morphemes = korean_nlp_service.extract_morphemes(query)
                for morpheme in morphemes:
                    if len(morpheme) >= 2:
                        highlight_terms.add(morpheme.lower())
            except:
                pass
            
            # ë¹ˆ ë¬¸ìì—´ ì œê±°
            highlight_terms = {term for term in highlight_terms if term}
            
            if not highlight_terms:
                return text
            
            # ì •ê·œì‹ íŒ¨í„´ ìƒì„± (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆí•¨)
            # íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            escaped_terms = [re.escape(term) for term in highlight_terms]
            pattern = r'\b(' + '|'.join(escaped_terms) + r')\b'
            
            # í•˜ì´ë¼ì´íŠ¸ ì ìš©
            highlighted_text = re.sub(
                pattern, 
                r'<mark>\1</mark>', 
                text, 
                flags=re.IGNORECASE
            )
            
            return highlighted_text
            
        except Exception as e:
            logger.warning(f"í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return text

    def _normalize_similarity_score(self, score: float) -> float:
        """
        ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ 0.0-1.0 ë²”ìœ„ë¡œ ì •ê·œí™”
        
        Args:
            score: ì›ë³¸ ìœ ì‚¬ë„ ì ìˆ˜
            
        Returns:
            float: 0.0-1.0 ë²”ìœ„ë¡œ ì •ê·œí™”ëœ ì ìˆ˜
        """
        try:
            # NaN ë˜ëŠ” None ì²´í¬
            if score is None or math.isnan(score):
                return 0.0
            
            # ì´ë¯¸ 0.0-1.0 ë²”ìœ„ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
            if 0.0 <= score <= 1.0:
                return score
            
            # 1.0ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° 1.0ìœ¼ë¡œ í´ë¨í•‘
            if score > 1.0:
                logger.warning(f"ìœ ì‚¬ë„ ì ìˆ˜ê°€ 1.0ì„ ì´ˆê³¼í•©ë‹ˆë‹¤: {score:.3f} -> 1.0ìœ¼ë¡œ ì¡°ì •")
                return 1.0
            
            # 0.0 ë¯¸ë§Œì¸ ê²½ìš° 0.0ìœ¼ë¡œ í´ë¨í•‘
            if score < 0.0:
                logger.warning(f"ìœ ì‚¬ë„ ì ìˆ˜ê°€ 0.0 ë¯¸ë§Œì…ë‹ˆë‹¤: {score:.3f} -> 0.0ìœ¼ë¡œ ì¡°ì •")
                return 0.0
                
            return score
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ì ìˆ˜ ì •ê·œí™” ì˜¤ë¥˜: {e}")
            return 0.0

    def _build_container_path_with_icons(self, container_path: str) -> str:
        """
        ì»¨í…Œì´ë„ˆ ê²½ë¡œì— ì•„ì´ì½˜ì„ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©ì ì¹œí™”ì ì¸ ê²½ë¡œ ë¬¸ìì—´ ìƒì„±
        
        Args:
            container_path: ì›ë³¸ ì»¨í…Œì´ë„ˆ ê²½ë¡œ (ì˜ˆ: "/ì›…ì§„/CEOì§ì†/ì¸ì‚¬ì „ëµíŒ€")
            
        Returns:
            str: ì•„ì´ì½˜ì´ í¬í•¨ëœ ê²½ë¡œ (ì˜ˆ: "ğŸ“ ì›…ì§„ ğŸ“ CEOì§ì† ğŸ“‚ ì¸ì‚¬ì „ëµíŒ€")
        """
        try:
            if not container_path:
                return "ğŸ“‚ ê²½ë¡œ ì—†ìŒ"
            
            # ê²½ë¡œë¥¼ '/' ë˜ëŠ” ' > ' ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
            if '/' in container_path:
                path_parts = [part.strip() for part in container_path.split('/') if part.strip()]
            elif ' > ' in container_path:
                path_parts = [part.strip() for part in container_path.split(' > ') if part.strip()]
            else:
                # ë‹¨ì¼ ì»¨í…Œì´ë„ˆì¸ ê²½ìš°
                return f"ğŸ“‚ {container_path}"
            
            if not path_parts:
                return "ğŸ“‚ ê²½ë¡œ ì—†ìŒ"
            
            # ê° ê²½ë¡œ ë¶€ë¶„ì— ì•„ì´ì½˜ ì¶”ê°€
            icon_path_parts = []
            for i, part in enumerate(path_parts):
                if i == len(path_parts) - 1:
                    # ë§ˆì§€ë§‰ (í˜„ì¬) ì»¨í…Œì´ë„ˆëŠ” ì—´ë¦° í´ë” ì•„ì´ì½˜
                    icon_path_parts.append(f"ğŸ“‚ {part}")
                else:
                    # ìƒìœ„ ì»¨í…Œì´ë„ˆë“¤ì€ ë‹«íŒ í´ë” ì•„ì´ì½˜
                    icon_path_parts.append(f"ğŸ“ {part}")
            
            return " ".join(icon_path_parts)
            
        except Exception as e:
            logger.error(f"ì»¨í…Œì´ë„ˆ ê²½ë¡œ ì•„ì´ì½˜ êµ¬ì„± ì˜¤ë¥˜: {e}")
            return f"ğŸ“‚ {container_path}" if container_path else "ğŸ“‚ ê²½ë¡œ ì—†ìŒ"

    def _convert_path_ids_to_names(self, org_path: str, container_id_to_name: Dict[str, str]) -> str:
        """
        ì»¨í…Œì´ë„ˆ ID ê²½ë¡œë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì´ë¦„ ê²½ë¡œë¡œ ë³€í™˜
        
        Args:
            org_path: ì»¨í…Œì´ë„ˆ ID ê²½ë¡œ (ì˜ˆ: "/WJ_ROOT/WJ_CEO/WJ_HR")
            container_id_to_name: ì»¨í…Œì´ë„ˆ ID -> ì´ë¦„ ë§¤í•‘
            
        Returns:
            str: ë³€í™˜ëœ ê²½ë¡œ (ì˜ˆ: "ì›…ì§„/CEOì§ì†/ì¸ì‚¬ì „ëµíŒ€")
        """
        try:
            if not org_path:
                return ""
                
            # ê²½ë¡œ êµ¬ë¶„ìë¡œ ë¶„í• 
            if org_path.startswith('/'):
                org_path = org_path[1:]  # ë§¨ ì•ì˜ '/' ì œê±°
                
            path_parts = [part.strip() for part in org_path.split('/') if part.strip()]
            
            # ê° ì»¨í…Œì´ë„ˆ IDë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
            friendly_parts = []
            for container_id in path_parts:
                container_name = container_id_to_name.get(container_id, container_id)
                # ì´ëª¨ì§€ ì œê±° (ì´ë¯¸ ì´ë¦„ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´)
                if container_name.startswith(('ğŸ¢', 'ğŸ“', 'ğŸ“‚')):
                    # ì´ëª¨ì§€ì™€ ê³µë°± ì œê±°í•˜ì—¬ ìˆœìˆ˜í•œ ì´ë¦„ë§Œ ì¶”ì¶œ
                    clean_name = container_name[2:].strip()
                    friendly_parts.append(clean_name)
                else:
                    friendly_parts.append(container_name)
            
            return "/".join(friendly_parts)
            
        except Exception as e:
            logger.error(f"ì»¨í…Œì´ë„ˆ ê²½ë¡œ ë³€í™˜ ì˜¤ë¥˜: {e}")
            return org_path or ""
    
    def _apply_quality_filter(self, results: List[Dict[str, Any]], processed_query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í•„í„°ë§
        í‚¤ì›Œë“œ ë§¤ì¹˜ê°€ ì—†ê³  ë²¡í„° ì ìˆ˜ë§Œ ìˆëŠ” ê²½ìš° ê´€ë ¨ì„± ê²€ì¦
        
        ê°œì„ ì‚¬í•­:
        - ì´ë¯¸ì§€/í‘œ ì²­í¬ëŠ” í‚¤ì›Œë“œ í•„í„° ì™„í™” (ë‚´ìš© í…ìŠ¤íŠ¸ê°€ ì œí•œì )
        - ìœ ì‚¬ë„ ì„ê³„ê°’ ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§
        """
        try:
            query_keywords = processed_query.get("keywords", [])
            # ì›ë¬¸/ì •ê·œí™” í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶€ë¶„ì¼ì¹˜ í™•ì¸ì— ì‚¬ìš©
            query_text = (processed_query.get("normalized_text") or processed_query.get("original_text") or "").lower()
            
            if not query_keywords and not query_text:
                return results
            
            filtered_results = []
            
            for result in results:
                search_methods = result.get("search_methods", [])
                modality = result.get("modality", "text")
                
                # í‚¤ì›Œë“œë‚˜ ì „ë¬¸ê²€ìƒ‰ ë§¤ì¹˜ê°€ ìˆìœ¼ë©´ í†µê³¼
                if any(method in search_methods for method in ["keyword", "fulltext"]):
                    filtered_results.append(result)
                    continue
                
                # ë²¡í„° ê²€ìƒ‰ë§Œ ìˆëŠ” ê²½ìš° ì¶”ê°€ ê²€ì¦
                if "vector" in search_methods:
                    # ì •ê·œí™”/ê°€ì¤‘ì¹˜ ì´ì „ì˜ ì›ì‹œ ìœ ì‚¬ë„ ì‚¬ìš©
                    similarity = result.get("raw_vector_similarity", result.get("similarity_score", 0.0))
                    
                    # ì´ë¯¸ì§€/í‘œ ì²­í¬ëŠ” í‚¤ì›Œë“œ í•„í„° ì™„í™” (ìœ ì‚¬ë„ ì„ê³„ê°’ë§Œ ì²´í¬)
                    if modality in ['image', 'table']:
                        # ì´ë¯¸ì§€/í‘œëŠ” ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì´ë©´ í†µê³¼
                        if similarity >= self.similarity_threshold:
                            filtered_results.append(result)
                            logger.debug(f"ì´ë¯¸ì§€/í‘œ ì²­í¬ í¬í•¨: {result.get('file_name', 'unknown')} (modality={modality}, score={similarity:.3f})")
                        else:
                            logger.info(f"í’ˆì§ˆ í•„í„°ë§ìœ¼ë¡œ ì œì™¸: {result.get('file_name', 'unknown')} (modality={modality}, ë‚®ì€ ìœ ì‚¬ë„(raw)={similarity:.3f})")
                        continue
                    
                    # í…ìŠ¤íŠ¸ ì²­í¬ëŠ” ê¸°ì¡´ ë¡œì§ ìœ ì§€
                    # ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ (0.8 ì´ìƒ)ë©´ í†µê³¼
                    if similarity >= 0.8:
                        filtered_results.append(result)
                        continue
                    
                    # ì œëª©ì´ë‚˜ ë‚´ìš©ì—ì„œ ì¿¼ë¦¬ í‚¤ì›Œë“œ ë¶€ë¶„ ì¼ì¹˜ í™•ì¸
                    content = result.get("content", "").lower()
                    title = result.get("file_name", "").lower()
                    
                    # ì¿¼ë¦¬ í‚¤ì›Œë“œì™€ ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    has_partial_match = False
                    for keyword in query_keywords:
                        keyword_lower = keyword.lower()
                        if keyword_lower in content or keyword_lower in title:
                            has_partial_match = True
                            break
                    
                    # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì „ì²´ì™€ë„ í™•ì¸
                    if not has_partial_match and query_text:
                        if query_text in content or query_text in title:
                            has_partial_match = True
                    
                    if has_partial_match:
                        filtered_results.append(result)
                    else:
                        logger.info(f"í’ˆì§ˆ í•„í„°ë§ìœ¼ë¡œ ì œì™¸: {result.get('file_name', 'unknown')} (í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜, raw={similarity:.3f})")
                
            logger.info(f"í’ˆì§ˆ í•„í„°ë§: {len(results)}ê°œ -> {len(filtered_results)}ê°œ")
            return filtered_results
            
        except Exception as e:
            logger.error(f"í’ˆì§ˆ í•„í„°ë§ ì˜¤ë¥˜: {e}")
            return results
    
    async def multimodal_search(
        self,
        query: str,
        user_emp_no: str,
        image_query: Optional[str] = None,  # Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ (data:image/png;base64,...)
        container_ids: Optional[List[str]] = None,
        max_results: int = 10,
        filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ìˆ˜í–‰ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
        
        Args:
            query: í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¿¼ë¦¬
            user_emp_no: ì‚¬ìš©ì ì‚¬ì›ë²ˆí˜¸
            image_query: Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ (data:image/png;base64,iVBOR...)
            container_ids: ê²€ìƒ‰í•  ì»¨í…Œì´ë„ˆ ID ë¦¬ìŠ¤íŠ¸
            max_results: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜
            filters: ì¶”ê°€ í•„í„° ì¡°ê±´
        
        Returns:
            ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            import base64
            start_time = datetime.now()
            
            # Base64 ì´ë¯¸ì§€ ë””ì½”ë”©
            image_bytes: Optional[bytes] = None
            if image_query:
                try:
                    # data:image/png;base64,iVBOR... â†’ iVBOR... â†’ bytes
                    if ',' in image_query:
                        image_query = image_query.split(',', 1)[1]
                    image_bytes = base64.b64decode(image_query)
                    logger.info(f"[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ë””ì½”ë”© ì™„ë£Œ: {len(image_bytes)} bytes")
                except Exception as e:
                    logger.error(f"[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                    image_bytes = None
            
            # 1. í…ìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰ (í•˜ì´ë¸Œë¦¬ë“œ) - í…ìŠ¤íŠ¸ ì¿¼ë¦¬ê°€ ìˆì„ ë•Œë§Œ
            text_results: Dict[str, Any] = {"results": [], "total_count": 0, "search_metadata": {}, "filters_applied": {}}
            if query:
                text_results = await self.hybrid_search(
                    query=query,
                    user_emp_no=user_emp_no,
                    container_ids=container_ids,
                    max_results=max_results,
                    search_type="hybrid",
                    filters=filters
                )
            
            # 2. ì´ë¯¸ì§€ê°€ ìˆëŠ” ë¬¸ì„œ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
            if filters and filters.get('prefer_images', False):
                # ì´ë¯¸ì§€ê°€ ìˆëŠ” ë¬¸ì„œì— ê°€ì¤‘ì¹˜ ì¶”ê°€
                for result in text_results.get('results', []):
                    if result.get('has_images', False):
                        result['similarity_score'] = result.get('similarity_score', 0.0) * 1.2
                
                # ì¬ì •ë ¬
                text_results['results'] = sorted(
                    text_results['results'],
                    key=lambda x: x.get('similarity_score', 0.0),
                    reverse=True
                )[:max_results]
            
            # 3. ì´ë¯¸ì§€ ê²€ìƒ‰ (CLIP ë©€í‹°ëª¨ë‹¬)
            image_results: List[Dict[str, Any]] = []
            image_embedding: List[float] = []
            image_threshold = 0.8  # ìœ ì‚¬ë„ ì„ê³„ê°’ (0.8 = 80% ìœ ì‚¬ë„ ì´ìƒë§Œ ë°˜í™˜)
            if filters and isinstance(filters.get("image_similarity_threshold"), (int, float)):
                candidate = float(filters["image_similarity_threshold"])  # type: ignore[index]
                if 0.0 <= candidate <= 1.0:
                    image_threshold = candidate

            if image_bytes:  # ë””ì½”ë”©ëœ ë°”ì´íŠ¸ ë°ì´í„° ì‚¬ìš©
                logger.info(
                    "[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ê²€ìƒ‰ ìš”ì²­ ìˆ˜ì‹  - payload %d bytes",
                    len(image_bytes)
                )

                image_embedding = await self._generate_image_embedding(image_bytes)
                if image_embedding:
                    image_results = await self._search_by_image_embedding(
                        image_embedding=image_embedding,
                        user_emp_no=user_emp_no,
                        container_ids=container_ids,
                        max_results=max_results,
                        similarity_threshold=image_threshold
                    )
                else:
                    logger.warning("[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰ì„ ìƒëµí•©ë‹ˆë‹¤.")
            
            # 4. ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… (container_path ì¶”ê°€)
            if image_results:
                # ì»¨í…Œì´ë„ˆ ì •ë³´ ì¡°íšŒë¥¼ ìœ„í•œ container_id ìˆ˜ì§‘
                container_ids_to_fetch = [
                    str(r["container_id"]) for r in image_results 
                    if r.get("container_id")
                ]
                
                # ì»¨í…Œì´ë„ˆ ì •ë³´ ì¡°íšŒ
                container_details = {}
                if container_ids_to_fetch:
                    container_details = await self._get_container_details(container_ids_to_fetch)
                
                # ê° ì´ë¯¸ì§€ ê²°ê³¼ì— container_path ì¶”ê°€
                for result in image_results:
                    container_id = result.get("container_id")
                    if container_id and container_id in container_details:
                        detail = container_details[container_id]
                        container_path = detail.get("full_path", "")
                        if not container_path:
                            container_path = detail.get("container_name", "")
                        
                        container_path_with_icons = self._build_container_path_with_icons(container_path)
                        
                        result["container_name"] = detail.get("container_name", "ì•Œ ìˆ˜ ì—†ìŒ")
                        result["container_path"] = container_path_with_icons
                        result["container_icon"] = "ğŸ“‚"
                    else:
                        result["container_path"] = "ğŸ“‚ ê²½ë¡œ ì—†ìŒ"
                        result["container_icon"] = "ğŸ“‚"
                
                logger.info(f"[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ê²°ê³¼ í¬ë§·íŒ… ì™„ë£Œ: {len(image_results)}ê°œ")
            
            # 5. ê²°ê³¼ í†µí•©
            search_time = (datetime.now() - start_time).total_seconds()
            
            text_result_list = text_results.get('results') or []
            text_result_count = text_results.get('total_count', len(text_result_list))
            total_results = text_result_count + len(image_results)

            multimodal_results = {
                "success": True,
                "query": query or "[ì´ë¯¸ì§€ ê²€ìƒ‰]",
                "has_image_query": image_bytes is not None,
                "total_results": total_results,
                "results": text_result_list,
                "image_results": image_results,  # ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ (ì²­í¬ ë‹¨ìœ„)
                "search_metadata": {
                    **text_results.get('search_metadata', {}),
                    "multimodal_enabled": True,
                    "image_search_ready": image_embedding_service is not None,
                    "search_time_seconds": search_time,
                    "image_results_count": len(image_results),
                    "image_similarity_threshold": image_threshold,
                    "image_embedding_dimension": len(image_embedding) if image_embedding else None
                },
                "filters_applied": {
                    **text_results.get('filters_applied', {}),
                    "prefer_images": filters.get('prefer_images', False) if filters else False,
                    "image_query_provided": image_bytes is not None,
                    "image_similarity_threshold": image_threshold
                }
            }
            
            logger.info(f"[MULTIMODAL_SEARCH] ê²€ìƒ‰ ì™„ë£Œ - "
                       f"ì¿¼ë¦¬: '{query or '[ì´ë¯¸ì§€]'}', "
                       f"í…ìŠ¤íŠ¸ ê²°ê³¼: {text_result_count}ê°œ, ì´ë¯¸ì§€ ê²°ê³¼: {len(image_results)}ê°œ, "
                       f"ì‹œê°„: {search_time:.3f}ì´ˆ")
            
            return multimodal_results
            
        except Exception as e:
            logger.error(f"[MULTIMODAL_SEARCH] ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "total_results": 0,
                "results": []
            }
    
    async def _generate_image_embedding(self, image_data: bytes) -> List[float]:
        """
        ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (CLIP ê¸°ë°˜)

        Args:
            image_data: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°

        Returns:
            ì´ë¯¸ì§€ ì„ë² ë”© ë²¡í„°
        """
        if not image_data:
            logger.warning("[MULTIMODAL_SEARCH] ë¹ˆ ì´ë¯¸ì§€ ë°ì´í„°ë¡œ ì„ë² ë”©ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.")
            return []

        if image_embedding_service is None:
            logger.warning("[MULTIMODAL_SEARCH] CLIP ì„ë² ë”© ì„œë¹„ìŠ¤ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        try:
            embedding = await image_embedding_service.generate_image_embedding(image_bytes=image_data)
            if not embedding:
                logger.warning("[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                return []

            # numpy.ndarray ë“± ë¦¬ìŠ¤íŠ¸ í˜¸í™˜ íƒ€ì…ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜
            embedding_values = embedding if isinstance(embedding, list) else list(embedding)
            embedding_list = [float(x) for x in embedding_values]
            target_dim = getattr(settings, "clip_embedding_dimension", 512) or len(embedding_list)

            # CLIP ì„ë² ë”©ì€ 512ì°¨ì› ê¸°ì¤€ìœ¼ë¡œ íŒ¨ë”©/ìë¥´ê¸° ì²˜ë¦¬
            if len(embedding_list) < target_dim:
                embedding_list.extend([0.0] * (target_dim - len(embedding_list)))
            elif len(embedding_list) > target_dim:
                embedding_list = embedding_list[:target_dim]

            logger.info(f"[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì™„ë£Œ ({len(embedding_list)}d)")
            return embedding_list

        except Exception as exc:  # pragma: no cover - ì™¸ë¶€ ì„œë¹„ìŠ¤ ì˜¤ë¥˜ ë°©ì–´
            logger.error(f"[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {exc}")
            return []
    
    async def search_by_image_embedding(
        self,
        image_embedding: List[float],
        user_emp_no: str,
        container_ids: Optional[List[str]] = None,
        max_results: int = 10,
        similarity_threshold: float = 0.25
    ) -> List[Dict[str, Any]]:
        """ì™¸ë¶€ ì„œë¹„ìŠ¤ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥í•œ ê³µê°œ ë©”ì„œë“œ ë˜í¼."""
        return await self._search_by_image_embedding(
            image_embedding=image_embedding,
            user_emp_no=user_emp_no,
            container_ids=container_ids,
            max_results=max_results,
            similarity_threshold=similarity_threshold
        )

    async def _search_by_image_embedding(
        self,
        image_embedding: List[float],
        user_emp_no: str,
        container_ids: Optional[List[str]] = None,
        max_results: int = 10,
        similarity_threshold: float = 0.25
    ) -> List[Dict[str, Any]]:
        """
        ì´ë¯¸ì§€ ì„ë² ë”©ìœ¼ë¡œ doc_embedding.clip_vector ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            image_embedding: CLIP ì´ë¯¸ì§€ ì„ë² ë”© ë²¡í„°
            user_emp_no: ì‚¬ìš©ì ì‚¬ì›ë²ˆí˜¸
            container_ids: í•„í„°ë§í•  ì»¨í…Œì´ë„ˆ ëª©ë¡ (Noneì´ë©´ ê¶Œí•œ ë‚´ ì „ì²´)
            max_results: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ (0.0~1.0)

        Returns:
            ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì²­í¬ ë‹¨ìœ„)
        """
        if not image_embedding:
            logger.warning("[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ì„ë² ë”©ì´ ë¹„ì–´ ìˆì–´ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return []

        accessible_containers = await self._get_accessible_containers(user_emp_no, container_ids)
        if not accessible_containers:
            logger.info("[MULTIMODAL_SEARCH] ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆê°€ ì—†ì–´ ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ë²¡í„°ë¥¼ pgvector í¬ë§·ìœ¼ë¡œ ì§ë ¬í™”
        safe_embedding = [float(x) for x in image_embedding]
        vector_literal = "[" + ",".join(f"{value:.8f}" for value in safe_embedding) + "]"

        # ì»¨í…Œì´ë„ˆ í•„í„° ë¬¸ìì—´ êµ¬ì„± (ë‹¨ìˆœ í…ìŠ¤íŠ¸ í•„í„°ì´ë¯€ë¡œ ê¸°ë³¸ì ì¸ escaping ìˆ˜í–‰)
        container_filters = [cid.replace("'", "''") for cid in accessible_containers if cid]
        container_condition = ""
        if container_filters:
            joined = "','".join(container_filters)
            container_condition = f" AND fbf.knowledge_container_id IN ('{joined}')"

        query_sql = f"""
            SELECT
                de.embedding_id AS embedding_id,
                de.clip_vector <=> '{vector_literal}'::vector AS distance,
                1 - (de.clip_vector <=> '{vector_literal}'::vector) / 2 AS cosine_similarity,
                dc.chunk_id,
                dc.chunk_index,
                dc.content_text,
                dc.token_count,
                COALESCE(dc.modality, 'image') AS modality,
                fbf.file_bss_info_sno,
                fbf.file_lgc_nm AS file_name,
                fbf.path AS file_path,
                fbf.knowledge_container_id,
                kc.container_name,
                kc.org_path AS container_org_path
            FROM doc_embedding de
            JOIN doc_chunk dc ON de.chunk_id = dc.chunk_id
            JOIN tb_file_bss_info fbf ON dc.file_bss_info_sno = fbf.file_bss_info_sno
            LEFT JOIN tb_knowledge_containers kc ON fbf.knowledge_container_id = kc.container_id
            WHERE de.clip_vector IS NOT NULL
              AND fbf.del_yn = 'N'
              {container_condition}
            ORDER BY de.clip_vector <=> '{vector_literal}'::vector ASC
            LIMIT {max_results * 2}
        """

        try:
            async with self.async_session_local() as db:
                result = await db.execute(text(query_sql))
                rows = result.fetchall()

            image_results: List[Dict[str, Any]] = []
            similarity_scores = []  # ë””ë²„ê¹…ìš©
            for row in rows:
                # cosine distanceë¥¼ similarityë¡œ ë³€í™˜
                cosine_similarity = float(getattr(row, "cosine_similarity", 0.0))
                similarity_scores.append(cosine_similarity)  # ë””ë²„ê¹…ìš©

                if cosine_similarity < similarity_threshold:
                    continue

                distance_value = getattr(row, "distance", None)
                distance = float(distance_value) if distance_value is not None else None
                preview_text = row.content_text or "[ì´ë¯¸ì§€] ê´€ë ¨ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
                
                # ì»¨í…Œì´ë„ˆ ì •ë³´ êµ¬ì„±
                container_id = row.knowledge_container_id
                container_name = getattr(row, "container_name", None) or container_id
                container_org_path = getattr(row, "container_org_path", None)
                container_path = container_org_path or container_name
                
                # íŒŒì¼ëª… ì •ë³´ (ë¬¸ì„œ ì œëª©)
                file_name = row.file_name or "ì•Œ ìˆ˜ ì—†ìŒ"
                title = file_name  # ë¬¸ì„œëª…ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©

                image_results.append({
                    "chunk_id": row.chunk_id,
                    "embedding_id": row.embedding_id,
                    "file_id": row.file_bss_info_sno,
                    "chunk_index": row.chunk_index,
                    "content": preview_text,
                    "token_count": row.token_count,
                    "modality": row.modality,
                    "file_name": file_name,
                    "title": title,  # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œìš© ì œëª©
                    "file_path": row.file_path,
                    "container_id": container_id,
                    "container_name": container_name,
                    "container_path": container_path,
                    "similarity_score": cosine_similarity,
                    "distance": distance,
                    "clip_score": cosine_similarity,
                    "has_images": True,
                    "image_count": 1,
                    "metadata": {
                        "file_name": file_name,
                        "document_id": str(row.file_bss_info_sno),
                        "chunk_index": row.chunk_index,
                    }
                })

            # ë””ë²„ê¹…: ëª¨ë“  ìœ ì‚¬ë„ ì ìˆ˜ ë¡œê·¸
            if similarity_scores:
                logger.info(
                    "[MULTIMODAL_SEARCH] ìœ ì‚¬ë„ ì ìˆ˜ ë¶„í¬ - "
                    f"ìµœëŒ€: {max(similarity_scores):.4f}, "
                    f"ìµœì†Œ: {min(similarity_scores):.4f}, "
                    f"í‰ê· : {sum(similarity_scores)/len(similarity_scores):.4f}, "
                    f"ì´ {len(similarity_scores)}ê±´ ë¹„êµ"
                )

            logger.info(
                "[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ - ê²°ê³¼ %dê±´ (ì„ê³„ê°’ %.2f)",
                len(image_results),
                similarity_threshold
            )

            return image_results[:max_results]

        except Exception as exc:  # pragma: no cover - DB ì˜¤ë¥˜ ë“± ë°©ì–´
            logger.error(f"[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {exc}")
            return []


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
search_service = SearchService()

"""
í†µí•© ì§ˆì˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
ì¼ë°˜ ê²€ìƒ‰ + RAG ê²€ìƒ‰ ê³µí†µ ì‚¬ìš©

ì‚¬ìš© ì˜ˆì‹œ:
    # ì¼ë°˜ ê²€ìƒ‰
    from app.services.search.query_pipeline import process_user_query
    result = await process_user_query("í˜ì‹ ì— ëŒ€í•´ ë­ë¼ ì´ì•¼ê¸° í•˜ë‚˜ìš”", search_type="general")
    
    # RAG ê²€ìƒ‰
    result = await process_user_query("í˜ì‹ ì— ëŒ€í•´ ë­ë¼ ì´ì•¼ê¸° í•˜ë‚˜ìš”", search_type="rag")
"""

import time
import re
import logging
from typing import Dict, Any, Optional, List

from .query_models import ProcessedQuery, IntentType
from .query_config import (
    UNIFIED_STOPWORDS, 
    INTENT_SEARCH_STRATEGIES, 
    RAG_SEARCH_STRATEGIES,
    INTENT_PATTERNS,
    LANGUAGE_SETTINGS
)
from ..core.korean_nlp_service import korean_nlp_service
from ..core.embedding_service import embedding_service
from .spell_checker import apply_spell_correction

logger = logging.getLogger(__name__)


async def process_user_query(
    query: str,
    search_type: str = "general",  # "general" or "rag"
    **kwargs
) -> ProcessedQuery:
    """
    ì‚¬ìš©ì ì§ˆì˜ í†µí•© ì²˜ë¦¬
    
    Args:
        query: ì‚¬ìš©ì ì§ˆì˜ í…ìŠ¤íŠ¸
        search_type: "general" (ì¼ë°˜ ê²€ìƒ‰) or "rag" (RAG ê²€ìƒ‰)
    
    Returns:
        ProcessedQuery: ì²˜ë¦¬ëœ ì§ˆì˜
    
    ì²˜ë¦¬ ë‹¨ê³„:
        1. ì…ë ¥ ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        2. ì˜ë„ ë¶„ë¥˜ (keyword_search, document_search, qa_question, ...)
        3. í˜•íƒœì†Œ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
        4. ë¶ˆìš©ì–´ ì œê±° (UNIFIED_STOPWORDS ì‚¬ìš©)
        5. ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (fulltext, keyword, vector)
        6. ê²€ìƒ‰ ì „ëµ ì„¤ì • (ê°€ì¤‘ì¹˜, ì„ê³„ê°’)
    """
    start_time = time.time()
    
    try:
        logger.info(f"ğŸ” [QueryPipeline] ì§ˆì˜ ì²˜ë¦¬ ì‹œì‘: '{query[:50]}...' (type: {search_type})")
        
        # Step 1: ì…ë ¥ ì •ê·œí™”
        normalized_text = _normalize_text(query)
        language = _detect_language(normalized_text)
        logger.info(f"âœ“ ì •ê·œí™” ì™„ë£Œ: '{normalized_text}' (ì–¸ì–´: {language})")
        
        # Step 2: ìŠ¤í ë§ êµì • (ì˜ì–´/í˜¼í•© ì–´ì ˆë§Œ ëŒ€ìƒ)
        spell_corrections = {}
        if language in ("en", "mixed"):
            corrected_text, spell_corrections = apply_spell_correction(normalized_text)
            if spell_corrections:
                logger.info(f"âœ“ ì˜¤íƒˆì ë³´ì •: {spell_corrections}")
                normalized_text = corrected_text

        # Step 3: ì˜ë„ ë¶„ë¥˜
        intent_type, intent_confidence = _classify_intent(normalized_text)
        logger.info(f"âœ“ ì˜ë„ ë¶„ë¥˜: {intent_type} (confidence: {intent_confidence:.2f})")
        
        # Step 4: í˜•íƒœì†Œ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = await _extract_keywords(normalized_text)
        logger.info(f"âœ“ í‚¤ì›Œë“œ ì¶”ì¶œ: {len(keywords)}ê°œ â†’ {keywords}")
        
        # Step 5: ë¶ˆìš©ì–´ ì œê±°
        filtered_keywords = _filter_stopwords(keywords, intent_type, language)
        logger.info(f"âœ“ ë¶ˆìš©ì–´ ì œê±°: {len(keywords)}ê°œ â†’ {len(filtered_keywords)}ê°œ â†’ {filtered_keywords}")

        if spell_corrections:
            # êµì •ëœ ë‹¨ì–´ë¥¼ í‚¤ì›Œë“œì— ë³´ê°• (ì¤‘ë³µ ì œê±°)
            corrected_terms = list(spell_corrections.values())
            filtered_keywords = list(dict.fromkeys(filtered_keywords + corrected_terms))
            keywords = list(dict.fromkeys(keywords + corrected_terms))
            logger.info(f"âœ“ êµì • ë‹¨ì–´ ë³´ê°• í›„ í‚¤ì›Œë“œ: {filtered_keywords}")
        
        # Step 6: ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        fulltext_query = _generate_fulltext_query(filtered_keywords)
        keyword_query = _generate_keyword_query(filtered_keywords)
        logger.info(f"âœ“ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±: fulltext='{fulltext_query}'")
        
        # Step 7: ë²¡í„° ì„ë² ë”© ìƒì„± (ì˜µì…˜)
        vector_embedding = None
        if filtered_keywords and search_type in ["rag", "general"]:
            try:
                embedding_text = " ".join(filtered_keywords)
                vector_embedding = await embedding_service.get_embedding(embedding_text)
                logger.info(f"âœ“ ë²¡í„° ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš  ë²¡í„° ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        
        # Step 8: ê²€ìƒ‰ ì „ëµ ì„¤ì •
        strategy = _get_search_strategy(intent_type, search_type)
        
        processing_time = (time.time() - start_time) * 1000
        
        result = ProcessedQuery(
            original_text=query,
            normalized_text=normalized_text,
            language=language,
            intent=intent_type,
            intent_confidence=intent_confidence,
            keywords=keywords,
            filtered_keywords=filtered_keywords,
            fulltext_query=fulltext_query,
            keyword_query=keyword_query,
            vector_embedding=vector_embedding,
            weights=strategy["weights"],
            similarity_threshold=strategy["similarity_threshold"],
            max_results=strategy.get("max_results", 15),
            processing_time_ms=processing_time,
            spell_corrections=spell_corrections
        )
        
        logger.info(f"âœ… [QueryPipeline] ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.1f}ms")
        logger.debug(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {result.to_dict()}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ [QueryPipeline] ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        
        # Fallback: ìµœì†Œ ê¸°ëŠ¥ ì œê³µ
        processing_time = (time.time() - start_time) * 1000
        return ProcessedQuery(
            original_text=query,
            normalized_text=query,
            language="ko",
            intent=IntentType.KEYWORD_SEARCH.value,
            intent_confidence=0.5,
            keywords=query.split(),
            filtered_keywords=query.split(),
            fulltext_query=query,
            keyword_query=query,
            vector_embedding=None,
            weights={"vector": 0.4, "keyword": 0.4, "fulltext": 0.2},
            similarity_threshold=0.4,
            processing_time_ms=processing_time
        )


# ========================================================================
# Internal Helper Functions
# ========================================================================

def _normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    # ê³µë°± ì •ë¦¬
    cleaned = re.sub(r'\s+', ' ', text.strip())
    
    # ì´ëª¨ì§€ ì œê±°
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "]+", flags=re.UNICODE
    )
    cleaned = emoji_pattern.sub('', cleaned)
    
    # ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (?, !, . ë“±ì€ ìœ ì§€)
    cleaned = re.sub(r'[^\w\sê°€-í£\?\!\.\,\-]', ' ', cleaned)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned


def _detect_language(text: str) -> str:
    """ì–¸ì–´ ê°ì§€"""
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    total_chars = len(re.sub(r'\s+', '', text))
    
    if total_chars == 0:
        return "ko"
    
    korean_ratio = korean_chars / total_chars
    english_ratio = english_chars / total_chars
    
    if korean_ratio > 0.5:
        return "ko"
    elif english_ratio > 0.5:
        return "en"
    else:
        return "mixed"


def _classify_intent(text: str) -> tuple[str, float]:
    """
    ì˜ë„ ë¶„ë¥˜ (ê·œì¹™ ê¸°ë°˜)
    
    Returns:
        (intent_type, confidence)
    """
    scores = {}
    
    for intent_type, patterns in INTENT_PATTERNS.items():
        score = 0.0
        for pattern in patterns:
            if re.match(pattern, text, re.IGNORECASE):
                score += 0.3
        scores[intent_type] = score
    
    if scores:
        best_intent = max(scores.items(), key=lambda x: x[1])
        intent_type, score = best_intent
        
        if score > 0:
            confidence = min(0.5 + score, 0.9)
            return intent_type, confidence
    
    # ê¸°ë³¸ê°’
    return IntentType.KEYWORD_SEARCH.value, 0.5


async def _extract_keywords(text: str) -> List[str]:
    """í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„)"""
    try:
        analysis_result = await korean_nlp_service.analyze_text_for_search(text)
        keywords = analysis_result.get("keywords", [])
        return keywords
    except Exception as e:
        logger.warning(f"í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨, ë‹¨ìˆœ ë¶„ë¦¬ ì‚¬ìš©: {e}")
        return text.split()


def _filter_stopwords(
    keywords: List[str], 
    intent: str, 
    language: str
) -> List[str]:
    """
    ë¶ˆìš©ì–´ ì œê±°
    
    Args:
        keywords: ì›ë³¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        intent: ì˜ë„ íƒ€ì…
        language: ì–¸ì–´
    
    Returns:
        ë¶ˆìš©ì–´ê°€ ì œê±°ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    lang_settings = LANGUAGE_SETTINGS.get(language, LANGUAGE_SETTINGS["ko"])
    min_length = lang_settings["min_keyword_length"]
    
    filtered = []
    
    for word in keywords:
        # ë¶ˆìš©ì–´ ì²´í¬
        if word.lower() in UNIFIED_STOPWORDS:
            logger.debug(f"  - ë¶ˆìš©ì–´ ì œê±°: '{word}'")
            continue
        
        # ê¸¸ì´ ì²´í¬
        if len(word) < min_length:
            logger.debug(f"  - ê¸¸ì´ ë¶€ì¡±: '{word}' (< {min_length})")
            continue
        
        filtered.append(word)
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰ì¸ ê²½ìš° ëœ ì—„ê²©í•˜ê²Œ
    if intent == IntentType.KEYWORD_SEARCH.value and not filtered:
        # ë¶ˆìš©ì–´ ì œê±° ì—†ì´ ê¸¸ì´ë§Œ ì²´í¬
        filtered = [w for w in keywords if len(w) >= min_length]
    
    return filtered


def _generate_fulltext_query(keywords: List[str]) -> str:
    """
    ì „ë¬¸ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (tsquery)
    
    Args:
        keywords: í•„í„°ë§ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        "í‚¤ì›Œë“œ1 | í‚¤ì›Œë“œ2 | í‚¤ì›Œë“œ3" (OR ê²€ìƒ‰)
    """
    if not keywords:
        return ""
    
    return " | ".join(keywords)


def _generate_keyword_query(keywords: List[str]) -> str:
    """
    í‚¤ì›Œë“œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ILIKE)
    
    Args:
        keywords: í•„í„°ë§ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        "%í‚¤ì›Œë“œ1% OR %í‚¤ì›Œë“œ2%" í˜•ì‹
    """
    if not keywords:
        return ""
    
    return " OR ".join([f"%{kw}%" for kw in keywords])


def _get_search_strategy(intent: str, search_type: str) -> Dict[str, Any]:
    """
    ê²€ìƒ‰ ì „ëµ ì„ íƒ
    
    Args:
        intent: ì˜ë„ íƒ€ì…
        search_type: "general" or "rag"
    
    Returns:
        ê²€ìƒ‰ ì „ëµ (weights, threshold, etc.)
    """
    if search_type == "rag":
        return RAG_SEARCH_STRATEGIES.get(
            intent,
            RAG_SEARCH_STRATEGIES["document_search"]
        )
    else:
        return INTENT_SEARCH_STRATEGIES.get(
            intent,
            INTENT_SEARCH_STRATEGIES["document_search"]
        )

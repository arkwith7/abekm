"""
ğŸ¤– RAG ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤ 
======================

RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬:
- ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
- Claude 3.5 Sonnetì„ í™œìš©í•œ ë‹µë³€ ìƒì„±
- ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ë° ìµœì í™”
- ì‹¤ì‹œê°„ ì±„íŒ… ë° PPT ìƒì„± ì§€ì›
"""

import logging
import json
import time
from typing import Dict, List, Optional, Any, Generator
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

from sqlalchemy.ext.asyncio import AsyncSession

# Services
from backend.app.services.chat.rag_search_service import rag_search_service, RAGSearchParams
from backend.app.services.core.bedrock_service import bedrock_service

logger = logging.getLogger(__name__)

class ResponseMode(Enum):
    """ì‘ë‹µ ìƒì„± ëª¨ë“œ"""
    CHAT = "chat"           # ì¼ë°˜ ì±„íŒ…
    DETAILED = "detailed"   # ìƒì„¸ ì„¤ëª…
    SUMMARY = "summary"     # ìš”ì•½
    PPT = "ppt"            # PPT ìƒì„±ìš©

@dataclass
class RAGRequest:
    """RAG ìš”ì²­ ë§¤ê°œë³€ìˆ˜"""
    query: str
    container_ids: Optional[List[str]] = None
    response_mode: ResponseMode = ResponseMode.CHAT
    max_context_chunks: int = 8
    include_sources: bool = True
    stream_response: bool = False
    user_context: Optional[Dict[str, Any]] = None

@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ê²°ê³¼"""
    answer: str
    sources: List[Dict[str, Any]]
    context_used: str
    confidence_score: float
    processing_stats: Dict[str, Any]
    search_results: Optional[Dict[str, Any]] = None

class RAGResponseService:
    """RAG ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.search_service = rag_search_service
        self.bedrock_service = bedrock_service
        
        # ì‘ë‹µ ëª¨ë“œë³„ ì„¤ì •
        self.mode_configs = {
            ResponseMode.CHAT: {
                "max_tokens": 2000,
                "temperature": 0.7,
                "system_prompt": "ì¹œê·¼í•˜ê³  ì •í™•í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            },
            ResponseMode.DETAILED: {
                "max_tokens": 4000,
                "temperature": 0.3,
                "system_prompt": "ì „ë¬¸ì ì´ê³  ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            },
            ResponseMode.SUMMARY: {
                "max_tokens": 1000,
                "temperature": 0.2,
                "system_prompt": "í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            },
            ResponseMode.PPT: {
                "max_tokens": 3000,
                "temperature": 0.4,
                "system_prompt": "PowerPoint í”„ë ˆì  í…Œì´ì…˜ ìë£Œ ì‘ì„± ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            }
        }
    
    async def generate_rag_response(
        self,
        session: AsyncSession,
        request: RAGRequest
    ) -> RAGResponse:
        """
        RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        
        Args:
            session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            request: RAG ìš”ì²­ ë§¤ê°œë³€ìˆ˜
            
        Returns:
            RAG ì‘ë‹µ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ¤– RAG ì‘ë‹µ ìƒì„± ì‹œì‘: '{request.query[:50]}...' "
                       f"(ëª¨ë“œ: {request.response_mode.value})")
            
            # 1ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (í•„ìš” ì‹œ í•˜ì´ë¸Œë¦¬ë“œ í›„ë³´ ì¬ì‚¬ìš©)
            search_params = RAGSearchParams(
                query=request.query,
                container_ids=request.container_ids,
                max_chunks=request.max_context_chunks,
                use_reranking=True,
                context_window=4000
            )

            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í›„ë³´ ë¬¸ì„œê°€ ì „ë‹¬ëœ ê²½ìš° ë°”ë¡œ í•„í„°ë§ì— ì‚¬ìš©í•˜ì—¬ ì¬ê²€ìƒ‰ ë¹„ìš© ì¶•ì†Œ
            try:
                if request.user_context:
                    candidate_ids = None
                    for key in ("hybrid_candidates", "candidate_file_ids", "document_ids"):
                        if key in request.user_context and isinstance(request.user_context[key], list):
                            candidate_ids = request.user_context[key]
                            break
                    if candidate_ids:
                        from backend.app.services.chat.rag_search_service import RAGSearchService
                        # ì •ê·œí™”ëŠ” ì„œë¹„ìŠ¤ ë‚´ë¶€ì—ì„œ ì¬í™•ì¸ë˜ì§€ë§Œ, ì—¬ê¸°ì„œë„ ê°„ë‹¨ ë³´ì •
                        def _norm_ids(ids):
                            out = []
                            for x in ids:
                                sx = str(x)
                                import re
                                m = re.search(r"(?i)(?:doc[_-]|file[_-])?(\d+)", sx)
                                if m:
                                    out.append(int(m.group(1)))
                                else:
                                    try:
                                        out.append(int(sx))
                                    except Exception:
                                        pass
                            return out
                        normed = _norm_ids(candidate_ids)
                        if normed:
                            search_params.document_ids = normed
                            logger.info(f"â™»ï¸ í•˜ì´ë¸Œë¦¬ë“œ í›„ë³´ ë¬¸ì„œ ì¬ì‚¬ìš©: {len(normed)}ê°œ ë¬¸ì„œë¡œ í•„í„°ë§")
            except Exception:
                pass
            
            search_result = await self.search_service.search_for_rag_context(
                session=session,
                search_params=search_params
            )
            
            if not search_result.chunks:
                logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ UX ì¹œí™”ì  í´ë°± ì‘ë‹µ ìƒì„±")
                # ì—°ê´€ ë¬¸ì„œ ì¶”ì²œ ì‹œë„ (ì»¨í…Œì´ë„ˆ ë²”ìœ„ ë‚´)
                try:
                    recommendations = await self.search_service.recommend_related_documents(
                        query=request.query,
                        limit=5,
                        threshold=0.25,
                        db_session=session
                    )
                except Exception:
                    recommendations = []
                return await self._generate_fallback_response(request, recommendations)
            
            # 2ë‹¨ê³„: ì‘ë‹µ ìƒì„±
            if request.stream_response:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì‹¤ì‹œê°„ ì±„íŒ…ìš©)
                answer = await self._generate_streaming_response(request, search_result)
            else:
                # ì¼ë°˜ ì‘ë‹µ
                answer = await self._generate_standard_response(request, search_result)
            
            # 3ë‹¨ê³„: ì†ŒìŠ¤ ì •ë³´ êµ¬ì„±
            sources = self._build_sources_info(search_result.chunks, request.include_sources)
            
            # 4ë‹¨ê³„: ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence_score = self._calculate_confidence_score(search_result, answer)
            
            # 5ë‹¨ê³„: ì²˜ë¦¬ í†µê³„
            processing_stats = {
                "total_time": time.time() - start_time,
                "search_time": search_result.search_stats.get("search_time", 0),
                "generation_time": time.time() - start_time - search_result.search_stats.get("search_time", 0),
                "chunks_used": len(search_result.chunks),
                "context_tokens": search_result.total_tokens,
                "response_mode": request.response_mode.value,
                "reranking_applied": search_result.reranking_applied
            }
            
            logger.info(f"âœ… RAG ì‘ë‹µ ìƒì„± ì™„ë£Œ: {processing_stats['total_time']:.2f}ì´ˆ, "
                       f"ì‹ ë¢°ë„ {confidence_score:.2f}")
            
            return RAGResponse(
                answer=answer,
                sources=sources,
                context_used=search_result.context_text,
                confidence_score=confidence_score,
                processing_stats=processing_stats,
                search_results=search_result.search_stats
            )
            
        except Exception as e:
            logger.error(f"RAG ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return await self._generate_error_response(request, str(e))
    
    async def _generate_standard_response(
        self,
        request: RAGRequest,
        search_result
    ) -> str:
        """í‘œì¤€ ì‘ë‹µ ìƒì„±"""
        config = self.mode_configs[request.response_mode]
        
        # ì‘ë‹µ ëª¨ë“œë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if request.response_mode == ResponseMode.PPT:
            prompt = self._build_ppt_prompt(request, search_result)
        else:
            prompt = self._build_standard_prompt(request, search_result, config)
        
        # Claudeë¡œ ì‘ë‹µ ìƒì„±
        response = await self.bedrock_service.generate_text_claude(
            prompt=prompt,
            max_tokens=config["max_tokens"],
            temperature=config["temperature"]
        )
        
        return response
    
    async def _generate_streaming_response(
        self,
        request: RAGRequest,
        search_result
    ) -> str:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ì‹¤ì‹œê°„ ì±„íŒ…ìš©)"""
        # TODO: ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
        # í˜„ì¬ëŠ” í‘œì¤€ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
        return await self._generate_standard_response(request, search_result)
    
    def _build_standard_prompt(
        self,
        request: RAGRequest,
        search_result,
        config: Dict[str, Any]
    ) -> str:
        """í‘œì¤€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {request.query}

ì»¨í…ìŠ¤íŠ¸:
{search_result.context_text}

ë‹µë³€ ì§€ì¹¨:
- ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”
- ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”
"""
        
        if request.response_mode == ResponseMode.DETAILED:
            prompt += "- ìƒì„¸í•˜ê³  ì „ë¬¸ì ì¸ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”\n"
        elif request.response_mode == ResponseMode.SUMMARY:
            prompt += "- í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”\n"
        
        if request.user_context:
            prompt += f"\nì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸: {json.dumps(request.user_context, ensure_ascii=False)}\n"
        
        return prompt
    
    def _build_ppt_prompt(
        self,
        request: RAGRequest,
        search_result
    ) -> str:
        """PPT ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        return f"""ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ PowerPoint í”„ë ˆì  í…Œì´ì…˜ì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ë‚´ìš©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì£¼ì œ: {request.query}

ì°¸ê³  ìë£Œ:
{search_result.context_text}

ìš”êµ¬ì‚¬í•­:
1. í”„ë ˆì  í…Œì´ì…˜ ì œëª©ê³¼ ë¶€ì œëª© ì œì•ˆ
2. ìŠ¬ë¼ì´ë“œë³„ êµ¬ì„± (ì œëª©, ì£¼ìš” ë‚´ìš©, ì„¸ë¶€ ì„¤ëª…)
3. ê° ìŠ¬ë¼ì´ë“œë‹¹ 3-5ê°œì˜ í•µì‹¬ í¬ì¸íŠ¸
4. ì‹œê°ì  ìš”ì†Œ ì œì•ˆ (ì°¨íŠ¸, ì´ë¯¸ì§€, ë‹¤ì´ì–´ê·¸ë¨)
5. ë°œí‘œì ë…¸íŠ¸ í¬í•¨

ì¶œë ¥ í˜•ì‹:
```json
{{
    "presentation_title": "í”„ë ˆì  í…Œì´ì…˜ ì œëª©",
    "subtitle": "ë¶€ì œëª©",
    "slides": [
        {{
            "slide_number": 1,
            "title": "ìŠ¬ë¼ì´ë“œ ì œëª©",
            "content_points": ["í¬ì¸íŠ¸1", "í¬ì¸íŠ¸2", "í¬ì¸íŠ¸3"],
            "detailed_explanation": "ìƒì„¸ ì„¤ëª…",
            "visual_suggestions": "ì‹œê°ì  ìš”ì†Œ ì œì•ˆ",
            "speaker_notes": "ë°œí‘œì ë…¸íŠ¸"
        }}
    ]
}}
```

ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ì‹¤ìš©ì ì´ê³  ì „ë¬¸ì ì¸ í”„ë ˆì  í…Œì´ì…˜ì„ êµ¬ì„±í•´ì£¼ì„¸ìš”.
"""
    
    def _build_sources_info(
        self,
        chunks: List[Dict[str, Any]],
        include_details: bool
    ) -> List[Dict[str, Any]]:
        """ì†ŒìŠ¤ ì •ë³´ êµ¬ì„±"""
        sources = []
        
        for i, chunk in enumerate(chunks):
            source_info = {
                "index": i + 1,
                "file_name": chunk.get("file_name", "Unknown"),
                "similarity_score": chunk.get("similarity_score", 0),
                "chunk_type": chunk.get("chunk_type", "content")
            }
            
            if include_details:
                source_info.update({
                    "content_preview": chunk.get("content", "")[:200] + "...",
                    "metadata": chunk.get("metadata", {}),
                    "search_type": chunk.get("search_type", "unknown")
                })
            
            sources.append(source_info)
        
        return sources
    
    def _calculate_confidence_score(
        self,
        search_result,
        answer: str
    ) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ì ìˆ˜ (ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ)
            base_score = 0.5
            
            # ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
            if search_result.chunks:
                avg_similarity = sum(chunk.get("similarity_score", 0) for chunk in search_result.chunks) / len(search_result.chunks)
                base_score += avg_similarity * 0.3
            
            # ë‹µë³€ ê¸¸ì´ í‰ê°€ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
            answer_length = len(answer)
            if 100 <= answer_length <= 2000:
                base_score += 0.1
            elif answer_length < 50:
                base_score -= 0.2
            
            # ì»¨í…ìŠ¤íŠ¸ í™œìš©ë„ í‰ê°€
            if search_result.total_tokens > 0:
                base_score += 0.1
            
            # ë¦¬ë­í‚¹ ì ìš© ì—¬ë¶€
            if search_result.reranking_applied:
                base_score += 0.05
            
            return min(1.0, max(0.0, base_score))
            
        except Exception as e:
            logger.error(f"ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.5
    
    async def _generate_fallback_response(self, request: RAGRequest, recommendations: Optional[List[Dict[str, Any]]] = None) -> RAGResponse:
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ëŒ€ì²´ ì‘ë‹µ (ì¶”ì²œ ë¬¸ì„œ/ì§ˆë¬¸ ìœ ë„/PPT ì´ˆì•ˆ ì œì•ˆ í¬í•¨)"""
        rec_lines: List[str] = []
        if recommendations:
            for r in recommendations[:5]:
                name = r.get("file_name") or r.get("title") or str(r.get("file_id"))
                rec_lines.append(f"- {name} (ìœ ì‚¬ë„ {r.get('max_similarity', 0):.2f})")
        rec_text = "\n".join(rec_lines)

        guidance = (
            "ë‹¤ìŒì„ ì„ íƒí•˜ì‹œë©´ ë” ì •í™•íˆ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”:\n"
            "- ëŒ€ìƒ(ê³ ê°/ì œí’ˆ/ë¶€ì„œ)\n- ë¶„ëŸ‰(ìŠ¬ë¼ì´ë“œ ìˆ˜/í˜ì´ì§€)\n- í†¤ì•¤ë§¤ë„ˆ(ì „ë¬¸/ì¹œê·¼/ê°„ê²°)\n"
        )

        ppt_skeleton = (
            "ê°„ë‹¨í•œ PPT ì´ˆì•ˆì„ ì‹œì‘í•  ìˆ˜ë„ ìˆì–´ìš”:\n"
            "1) í‘œì§€: ì œëª©/ë¶€ì œëª©/ì‘ì„±ì\n"
            "2) ê°œìš”: ëª©ì /ë°°ê²½/ë²”ìœ„\n"
            "3) ë³¸ë¬¸: í•µì‹¬ ë©”ì‹œì§€ 3~5ê°œ (ê° 3 bullet)\n"
            "4) ê²°ë¡ : ìš”ì•½/ë‹¤ìŒ ë‹¨ê³„\n"
        )

        fallback_answer = (
            f"ì£„ì†¡í•©ë‹ˆë‹¤. '{request.query}'ì— ëŒ€í•œ ì§ì ‘ì ì¸ ì°¸ê³ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n"
            + ("ì¶”ì²œ ë¬¸ì„œ:\n" + rec_text + "\n\n" if rec_text else "")
            + guidance
            + ppt_skeleton
        )
        
        return RAGResponse(
            answer=fallback_answer,
            sources=[],
            context_used="",
            confidence_score=0.1,
            processing_stats={
                "total_time": 0.1,
                "search_time": 0.05,
                "generation_time": 0.05,
                "chunks_used": 0,
                "context_tokens": 0,
                "response_mode": request.response_mode.value,
                "is_fallback": True
            }
        )
    
    async def _generate_error_response(self, request: RAGRequest, error_msg: str) -> RAGResponse:
        """ì˜¤ë¥˜ ë°œìƒì‹œ ì‘ë‹µ"""
        error_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        return RAGResponse(
            answer=error_answer,
            sources=[],
            context_used="",
            confidence_score=0.0,
            processing_stats={
                "total_time": 0.0,
                "error": error_msg,
                "response_mode": request.response_mode.value
            }
        )

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rag_response_service = RAGResponseService()

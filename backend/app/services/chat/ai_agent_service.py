"""
AI Agent ê´€ë¦¬ ì„œë¹„ìŠ¤
- ë‹¤ì–‘í•œ AI Agent íƒ€ì…ë³„ ì²˜ë¦¬ ë¡œì§
- Agentë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
- ì„ íƒëœ ë¬¸ì„œ ê¸°ë°˜ RAG ì²˜ë¦¬
"""

import os
from pathlib import Path
from typing import List, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from app.schemas.chat import AgentSystemPrompt, AGENT_SYSTEM_PROMPTS, SelectedDocument
from app.services.chat.rag_search_service import rag_search_service, RAGSearchParams
from app.services.chat.query_classification_service import QueryClassificationService
from app.services.chat.conversation_context_service import conversation_context_service
from app.services.document.extraction.text_extractor_service import TextExtractorService
from app.services.chat.chat_attachment_service import chat_attachment_service
from loguru import logger


class AIAgentService:
    """AI Agent ê´€ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸°ë³¸ êµ¬ì„± (ë‚´ì¥ ìƒìˆ˜)
        self.agent_configs = AGENT_SYSTEM_PROMPTS
        # backend/prompts ê²½ë¡œ (ì„ íƒì  ì™¸ë¶€ íŒŒì¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•ìš©)
        self.prompts_dir = Path(__file__).parents[3] / "prompts"
        if not self.prompts_dir.exists():
            logger.warning(f"âš ï¸ backend/prompts ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {self.prompts_dir}")

        # RAG ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ì˜¤ë²„ë¼ì´ë“œ ì§€ì›)
        # ê´€ë ¨ì„± ì—†ëŠ” ë¬¸ì„œ í•„í„°ë§ì„ ìœ„í•œ ì—„ê²©í•œ ì„ê³„ê°’ ì‚¬ìš©
        self.rag_similarity_threshold = float(os.getenv("RAG_SIMILARITY_THRESHOLD", "0.4"))
        self.rag_max_chunks = int(os.getenv("RAG_MAX_CHUNKS", "10"))
        self.rag_use_reranking = os.getenv("RAG_USE_RERANKING", "true").lower() == "true"
        # ì§ˆë¬¸ ë¶„ë¥˜ê¸°
        self.classifier = QueryClassificationService()
        # í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°
        self.text_extractor = TextExtractorService()

        logger.info(
            f"ğŸ”§ RAG ì„¤ì • ë¡œë“œ: threshold={self.rag_similarity_threshold}, "
            f"max_chunks={self.rag_max_chunks}, reranking={self.rag_use_reranking}"
        )
        logger.info("ğŸ“ backend/prompts ë””ë ‰í† ë¦¬ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹œì‘")
        
        # íŒŒì¼ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        self._load_prompts_from_files()
    
    def _load_prompts_from_files(self):
        """íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œí•˜ì—¬ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë®ì–´ì“°ê¸°"""
        try:
            if not self.prompts_dir or not self.prompts_dir.exists():
                logger.debug("í”„ë¡¬í”„íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŒ, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                return
            
            # general.prompt íŒŒì¼ ë¡œë“œ
            general_prompt_path = self.prompts_dir / "general.prompt"
            if general_prompt_path.exists():
                general_prompt_content = general_prompt_path.read_text(encoding="utf-8").strip()
                # ê¸°ì¡´ general agent í”„ë¡¬í”„íŠ¸ ë®ì–´ì“°ê¸°
                if 'general' in self.agent_configs:
                    self.agent_configs['general'].system_prompt = general_prompt_content
                    logger.info(f"âœ… general.prompt íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ ({len(general_prompt_content)}ì)")
                else:
                    logger.warning("general agent ì„¤ì •ì´ ì—†ì–´ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ìŠ¤í‚µ")
            else:
                logger.debug("general.prompt íŒŒì¼ì´ ì—†ìŒ, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                
            # ë‹¤ë¥¸ agent íƒ€ì…ë“¤ë„ í•„ìš”ì‹œ ë¡œë“œ ê°€ëŠ¥
            # presentation.prompt, summarizer.prompt ë“±
            
        except Exception as e:
            logger.warning(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.info("ê¸°ë³¸ ë‚´ì¥ í”„ë¡¬í”„íŠ¸ë¥¼ ê³„ì† ì‚¬ìš©í•©ë‹ˆë‹¤")
    
    def reload_prompts(self):
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ë“¤ì„ ë‹¤ì‹œ ë¡œë“œ (í–¥í›„ í™•ì¥ìš©)"""
        self._load_prompts_from_files()
        logger.info("ğŸ”„ í”„ë¡¬í”„íŠ¸ ì¬ë¡œë”© ì™„ë£Œ (í˜„ì¬ ê¸°ë³¸ ë‚´ì¥ + íŒŒì¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• ë¯¸ì‚¬ìš©)")
    
    def get_agent_config(self, agent_type: str) -> AgentSystemPrompt:
        """Agent íƒ€ì…ì— ë”°ë¥¸ ì„¤ì • ë°˜í™˜"""
        return self.agent_configs.get(agent_type, self.agent_configs['general'])
    
    def get_all_agents(self) -> Dict[str, AgentSystemPrompt]:
        """ëª¨ë“  Agent ì„¤ì • ë°˜í™˜"""
        return self.agent_configs
    
    async def prepare_context_with_documents(
        self, 
        query: str, 
        selected_documents: Optional[List[SelectedDocument]],
        chat_history: Optional[List[Dict[str, str]]] = None,
        agent_type: str = 'general',
        container_ids: Optional[List[str]] = None,
        similarity_threshold: Optional[float] = None,
        session_id: Optional[str] = None,
        db_session = None,
        attachments: Optional[List[Dict[str, Any]]] = None  # ğŸ†• ì´ë¯¸ì§€ ì²¨ë¶€ ì •ë³´
    ) -> tuple[str, List[Dict[str, Any]], Dict[str, Any], Dict[str, Any]]:
        """
        ì„ íƒëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ë©€í‹°í„´ ëŒ€í™” ê¸°ë¡ ë°˜ì˜)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            selected_documents: ì„ íƒëœ ë¬¸ì„œ ëª©ë¡
            chat_history: ëŒ€í™” ê¸°ë¡
            agent_type: AI Agent íƒ€ì…
            container_ids: ì»¨í…Œì´ë„ˆ ID ëª©ë¡
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (Chat APIì—ì„œ ì „ë‹¬ë°›ì€ ê°’ ìš°ì„  ì‚¬ìš©)
            session_id: ì„¸ì…˜ ID (ì»¨í…ìŠ¤íŠ¸ ì„œë¹„ìŠ¤ìš©)
            db_session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            attachments: ì²¨ë¶€ëœ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (CLIP ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ìš©)
            
        Returns:
            tuple: (enhanced_query, references, context_info, rag_stats)
        """
        agent_config = self.get_agent_config(agent_type)
        system_prompt = agent_config.system_prompt
        
        # Chat APIì—ì„œ ì „ë‹¬ë°›ì€ ì„ê³„ê°’ì„ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        effective_threshold = similarity_threshold if similarity_threshold is not None else self.rag_similarity_threshold
        logger.info(f"ğŸšï¸ ìœ ì‚¬ë„ ì„ê³„ê°’: {effective_threshold} (API ì „ë‹¬ê°’: {similarity_threshold}, ê¸°ë³¸ê°’: {self.rag_similarity_threshold})")
        
        try:
            # ğŸ¯ í†µí•© ì§ˆì˜ ë¶„ì„: ì¬ì‘ì„± + ì˜ë„ ë¶„ë¥˜ + ë„êµ¬ ì„ íƒ
            conversation_history = []
            if session_id and db_session:
                conversation_history = await conversation_context_service._get_conversation_history(db_session, session_id)
            
            analysis_result = await conversation_context_service.analyze_query_with_intent(
                original_query=query,
                conversation_history=conversation_history,
                document_ids=[doc.id for doc in selected_documents] if selected_documents else None,
                container_ids=container_ids
            )
            
            logger.info(f"ğŸ¯ ì§ˆì˜ ë¶„ì„ ê²°ê³¼: intent={analysis_result['intent']}, confidence={analysis_result['confidence']:.2f}, tools={analysis_result['required_tools']}")
            logger.info(f"âœï¸ ì¬ì‘ì„± ì§ˆì˜: '{query[:50]}...' â†’ '{analysis_result['rewritten_query'][:50]}...'")
            
            # ì¬ì‘ì„±ëœ ì§ˆì˜ë¬¸ ì‚¬ìš©
            search_query = analysis_result['rewritten_query']
            context_metadata = {
                "analysis": analysis_result,
                "context_used": True
            }
            
            # ğŸ”§ ë„êµ¬ ë¼ìš°íŒ…
            intent = analysis_result['intent']
            required_tools = analysis_result['required_tools']
            
            # 1. ë„êµ¬ ë¯¸ì§€ì› ì²˜ë¦¬
            if intent == 'unsupported' or not required_tools:
                logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ìš”ì²­: {analysis_result['reasoning']}")
                return "", [], {
                    "rag_used": False, 
                    "unsupported": True,
                    "reason": analysis_result['reasoning']
                }, {
                    "rag_used": False,
                    "unsupported": True,
                    "message": "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë„êµ¬ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }
            
            # 2. ë¬¸ì„œ ë¡œë” (ìš”ì•½)
            if 'document_loader' in required_tools:
                logger.info(f"ğŸ“š document_loader ì‚¬ìš© - ì„ íƒ ë¬¸ì„œ ì›ë¬¸ ë¡œë“œ")
                if selected_documents and len(selected_documents) > 0:
                    return await self._load_documents_for_summarization(
                        selected_documents=selected_documents,
                        db_session=db_session,
                        max_chunks=self.rag_max_chunks
                    )
                else:
                    logger.warning(f"âš ï¸ document_loader ìš”ì²­ì´ì§€ë§Œ ì„ íƒëœ ë¬¸ì„œ ì—†ìŒ - ê²€ìƒ‰ìœ¼ë¡œ í´ë°±")
                    # í´ë°±: hybrid_searchë¡œ ì²˜ë¦¬
            
            # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì¼ë°˜ ì§ˆë¬¸, ë¹„êµ ë“±)
            # ê¸°ì¡´ ë¶„ë¥˜ ë¡œì§ ìœ ì§€ (í˜¸í™˜ì„±)
            classification = await self.classifier.classify_query(query)
            logger.info(f"ğŸ“‹ ê¸°ì¡´ ë¶„ë¥˜ (ì°¸ê³ ìš©): type={classification.query_type} need_rag={classification.needs_rag} conf={classification.confidence:.2f}")

            # ğŸ†• ì´ë¯¸ì§€ ì²¨ë¶€ ì‹œ ì§ˆì˜ë¬¸ ì¶”ê°€ ë³´ê°• (ì´ë¯¸ì§€ ê²€ìƒ‰ìš©)
            image_query_rewritten = False
            
            # ğŸ†• ë¬¸ì„œ ì²¨ë¶€ ì²˜ë¦¬ (Chat with File)
            attached_document_context = ""
            if attachments:
                # 1. ì´ë¯¸ì§€ ì²˜ë¦¬
                image_attachments = [
                    att for att in attachments 
                    if att.get('mime_type', '').startswith('image/')
                ]
                
                if image_attachments:
                    logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²¨ë¶€ ê°ì§€ ({len(image_attachments)}ê°œ) - ì§ˆì˜ë¬¸ ì¬ì‘ì„± ì‹œë„")
                    rewritten_query, rewrite_metadata = await conversation_context_service.rewrite_query_for_image_search(
                        original_query=query,
                        image_count=len(image_attachments),
                        selected_documents=selected_documents
                    )
                    
                    if rewrite_metadata.get("rewritten"):
                        search_query = rewritten_query
                        image_query_rewritten = True
                        context_metadata["image_rewrite"] = rewrite_metadata
                        logger.info(f"âœï¸ ì´ë¯¸ì§€ ì§ˆì˜ë¬¸ ì¬ì‘ì„± ì™„ë£Œ: '{query[:30]}...' â†’ '{search_query[:50]}...'")

                # 2. ë¬¸ì„œ ì²˜ë¦¬ (PDF, DOCX ë“±)
                doc_attachments = [
                    att for att in attachments 
                    if not att.get('mime_type', '').startswith('image/') and not att.get('mime_type', '').startswith('audio/')
                ]
                
                if doc_attachments:
                    logger.info(f"ğŸ“ ë¬¸ì„œ ì²¨ë¶€ ê°ì§€ ({len(doc_attachments)}ê°œ) - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì»¨í…ìŠ¤íŠ¸ ì£¼ì… ì‹œë„")
                    extracted_texts = []
                    
                    for doc_att in doc_attachments:
                        asset_id = doc_att.get('asset_id')
                        if not asset_id:
                            continue
                            
                        stored_file = chat_attachment_service.get(asset_id)
                        if not stored_file:
                            logger.warning(f"âš ï¸ ì²¨ë¶€ íŒŒì¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {asset_id}")
                            continue
                            
                        # íŒŒì¼ í¬ê¸° ì œí•œ (10MB)
                        MAX_FILE_SIZE = 10 * 1024 * 1024
                        if stored_file.size > MAX_FILE_SIZE:
                            logger.warning(f"âš ï¸ íŒŒì¼ í¬ê¸° ì´ˆê³¼ ({stored_file.size} bytes) - ì²˜ë¦¬ ê±´ë„ˆëœ€: {stored_file.file_name}")
                            extracted_texts.append(f"[íŒŒì¼: {stored_file.file_name}]\n(íŒŒì¼ì´ ë„ˆë¬´ ì»¤ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 10MB ì´í•˜ì˜ íŒŒì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤.)")
                            continue
                            
                        try:
                            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            extraction_result = await self.text_extractor.extract_text_from_file(
                                file_path=str(stored_file.path),
                                file_extension=Path(stored_file.file_name).suffix
                            )
                            
                            if extraction_result.get('success') and extraction_result.get('text'):
                                text_content = extraction_result['text']
                                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (30,000ì)
                                MAX_TEXT_LENGTH = 30000
                                if len(text_content) > MAX_TEXT_LENGTH:
                                    text_content = text_content[:MAX_TEXT_LENGTH] + "\n...(ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ìƒëµë¨)"
                                    
                                extracted_texts.append(f"[ì²¨ë¶€ íŒŒì¼ ë‚´ìš©: {stored_file.file_name}]\n{text_content}")
                                logger.info(f"âœ… ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {stored_file.file_name} ({len(text_content)}ì)")
                            else:
                                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {stored_file.file_name}")
                        except Exception as e:
                            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            
                    if extracted_texts:
                        attached_document_context = "\n\n".join(extracted_texts)
                        # ê²€ìƒ‰ ì¿¼ë¦¬ì— ë¬¸ì„œ ë‚´ìš©ì´ ìˆë‹¤ëŠ” íŒíŠ¸ ì¶”ê°€ (ì„ íƒ ì‚¬í•­)
                        # search_query += " (ì²¨ë¶€ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì¤˜)"
            
            # ë©€í‹°í„´ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì–´ ë³´ê°• (ì´ë¯¸ì§€ ì¬ì‘ì„±ì´ ì—†ì—ˆì„ ë•Œë§Œ)
            if not image_query_rewritten and classification.needs_rag and session_id and db_session:
                try:
                    enhanced_query, context_metadata = await conversation_context_service.enhance_query_with_context(
                        current_query=query,
                        session_id=session_id,
                        db_session=db_session
                    )
                    
                    if context_metadata.get("context_used"):
                        search_query = enhanced_query
                        topic_continuity = context_metadata.get("topic_continuity", 0.0)
                        logger.info(f"ğŸ”— ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì ìš©: ì—°ì†ì„±={topic_continuity:.2f}, ì›ë¬¸='{query[:50]}...' â†’ ê°•í™”='{search_query[:50]}...'")
                    else:
                        reason = context_metadata.get('reason', 'unknown')
                        if reason == 'no_explicit_reference':
                            logger.info(f"ğŸ“ ë…ë¦½ì  ì§ˆë¬¸ ê°ì§€ - ëª…ì‹œì  ì°¸ì¡° ì—†ìŒ: '{query[:30]}...'")
                        else:
                            logger.info(f"ğŸš« ì»¨í…ìŠ¤íŠ¸ ê°•í™” ìƒëµ: {reason}")
                        search_query = query  # ì›ë³¸ ì‚¬ìš©
                        
                except Exception as ctx_error:
                    logger.warning(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì„œë¹„ìŠ¤ ì˜¤ë¥˜, ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©: {ctx_error}")
                    search_query = query
                    
            else:
                # ì„¸ì…˜ IDë‚˜ DB ì„¸ì…˜ì´ ì—†ëŠ” ê²½ìš° ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©
                search_query = query
                context_metadata = {"context_used": False, "reason": "no_session_context"}
                logger.info("ğŸ“ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ - ë…ë¦½ì  ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬")

            if not classification.needs_rag and 'hybrid_search' not in required_tools:
                logger.info(f"RAG ë¶ˆí•„ìš”, ëŒ€í™” ê¸°ë¡ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„± ìœ ë„.")
                return "", [], {"rag_used": False, "query_classification": classification.query_type}, {"rag_used": False}

            if selected_documents and len(selected_documents) > 0:
                logger.info(f"ğŸ¯ Agent '{agent_type}' - ì„ íƒëœ ë¬¸ì„œ ê¸°ë°˜ RAG: {len(selected_documents)}ê°œ ë¬¸ì„œ")
                
                document_ids = [doc.id for doc in selected_documents]
                document_info = "\n".join([f"- {doc.fileName} ({doc.fileType})" for doc in selected_documents])
                
                rag_params = RAGSearchParams(
                    query=search_query,
                    document_ids=document_ids,
                    limit=self.rag_max_chunks,
                    threshold=effective_threshold,
                    similarity_threshold=effective_threshold,
                    search_mode='hybrid',
                    reranking=self.rag_use_reranking
                )
                
                enhanced_query = self._enhance_query_for_agent(search_query, agent_type, document_info)
            else:
                # ì„ íƒëœ ë¬¸ì„œê°€ ì—†ì„ ë•ŒëŠ” ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰
                logger.info(f"ï¿½ Agent '{agent_type}' - ì „ì²´ ë¬¸ì„œ ê¸°ë°˜ RAG ê²€ìƒ‰")
                
                rag_params = RAGSearchParams(
                    query=search_query,
                    document_ids=None,  # ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰
                    limit=self.rag_max_chunks,
                    threshold=effective_threshold,
                    similarity_threshold=effective_threshold,
                    search_mode='hybrid',
                    reranking=self.rag_use_reranking
                )
                
                document_info = "ì „ì²´ ë¬¸ì„œë¥¼ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰"
                enhanced_query = self._enhance_query_for_agent(search_query, agent_type, document_info)
            
            # ğŸ†• ì²¨ë¶€ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
            if attached_document_context:
                logger.info("ğŸ“ ì²¨ë¶€ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.")
                enhanced_query = f"""
[ì²¨ë¶€ëœ ë¬¸ì„œ ë‚´ìš©]
{attached_document_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{enhanced_query}
"""

            logger.info(f"ğŸ”§ RAG íŒŒë¼ë¯¸í„°: threshold={rag_params.similarity_threshold}, max_chunks={rag_params.limit}, reranking={rag_params.reranking}")
            
            # RAG ê²€ìƒ‰ ì‹¤í–‰ (ê°„ë‹¨ ìºì‹±: ë™ì¼ ì„¸ì…˜ ë‚´ ë™ì¼ ì¿¼ë¦¬/ë¬¸ì„œ ì…‹ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
            cache_key = None
            try:
                sel_ids = ",".join(sorted([str(d.id) for d in selected_documents])) if selected_documents else "ALL"
                cache_key = f"{session_id or 'no-session'}::{agent_type}::{search_query}::{sel_ids}::{container_ids or []}::{effective_threshold}"
            except Exception:
                cache_key = None
            if not hasattr(self, "_last_ctx_cache"):
                self._last_ctx_cache = {}
            if cache_key and cache_key in self._last_ctx_cache:
                logger.info("ğŸ§  RAG ê²°ê³¼ ìºì‹œ ì ì¤‘ - ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€")
                search_result = self._last_ctx_cache[cache_key]
            else:
                search_result = await rag_search_service.search_with_rag(
                    rag_params,
                    container_ids=container_ids,
                    attachments=attachments  # ğŸ†• ì´ë¯¸ì§€ ì²¨ë¶€ ì •ë³´ ì „ë‹¬
                )
                if cache_key:
                    self._last_ctx_cache[cache_key] = search_result
            
            # ë””ë²„ê·¸: search_result êµ¬ì¡° í™•ì¸
            logger.info(f"ğŸ” search_result íƒ€ì…: {type(search_result)}")
            logger.info(f"ğŸ” search_result í‚¤ë“¤: {list(search_result.keys()) if isinstance(search_result, dict) else 'Not a dict'}")
            
            # ê²€ìƒ‰ ì‹¤íŒ¨ ì²˜ë¦¬ (ì²­í¬ ìˆ˜ í™•ì¸)
            references = search_result.get('references', [])  # ì´ì œ used_chunksê°€ ë“¤ì–´ì˜´
            all_references = search_result.get('all_references', None)
            used_count = len(references) if references else 0
            total_count = len(all_references) if isinstance(all_references, list) else None
            logger.info(f"ğŸ” ì¶”ì¶œëœ references ìˆ˜(used): {used_count} / ì „ì²´ í›„ë³´: {total_count}")
            
            if len(references) == 0:
                logger.warning(f"ğŸ” RAG ê²€ìƒ‰ ì‹¤íŒ¨ - í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ê²€ìƒ‰ ì‹œë„")
                
                # í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ê²€ìƒ‰ ì‹œë„ (ì„ íƒëœ ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš°ë§Œ)
                if selected_documents and len(selected_documents) > 0:
                    document_ids = [doc.id for doc in selected_documents]
                    fallback_result = await self._try_keyword_fallback_search(
                        query, document_ids, container_ids
                    )
                    
                    if fallback_result and len(fallback_result.get('references', [])) > 0:
                        logger.info(f"âœ… í‚¤ì›Œë“œ í´ë°± ê²€ìƒ‰ ì„±ê³µ - ì²­í¬ ìˆ˜: {len(fallback_result['references'])}")
                        return system_prompt, fallback_result.get('references', []), fallback_result.get('context_info', {}), fallback_result.get('rag_stats', {})
                
                # í´ë°±ë„ ì‹¤íŒ¨í•œ ê²½ìš° ê²€ìƒ‰ ì‹¤íŒ¨ ì‘ë‹µ ìƒì„±
                logger.warning(f"ğŸ” í‚¤ì›Œë“œ í´ë°± ê²€ìƒ‰ë„ ì‹¤íŒ¨ - ê²€ìƒ‰ ì‹¤íŒ¨ ì‘ë‹µ ìƒì„±")
                fallback_response = await self._generate_search_failure_response(
                    query, selected_documents or [], agent_type
                )
                # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë¹ˆ ì°¸ê³ ìë£Œ ë°˜í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì°¸ê³ ìë£Œ í‘œì‹œí•˜ì§€ ì•Šë„ë¡ í•¨
                return fallback_response, [], {'search_failed': True, 'no_references': True}, {'chunks_found': 0, 'search_status': 'failed'}
            
            # 1) ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸/í†µê³„
            context_info = search_result.get('context_info', {})
            rag_stats = search_result.get('rag_stats', {})
            try:
                avg_sim = float(rag_stats.get('avg_similarity', 0.0) or 0.0)
            except Exception:
                avg_sim = 0.0

            # 2) ì €í’ˆì§ˆ ê°ì§€ ì‹œ ì „ì²´ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
            final_result = search_result
            if selected_documents and len(selected_documents) > 0:
                try:
                    low_quality = (avg_sim < 0.28) or (used_count < 2)
                    if low_quality:
                        # ì›ì¹™ 1 ì¤€ìˆ˜: ì„ íƒ ë¬¸ì„œê°€ ìˆì„ ë•ŒëŠ” ì „ì²´ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±í•˜ì§€ ì•Šê³  ì‹¤íŒ¨ ì•ˆë‚´ë¥¼ ë°˜í™˜
                        logger.info("ğŸ§© ì €í’ˆì§ˆ íŒë‹¨ â†’ ì„ íƒ ë¬¸ì„œ ìŠ¤ì½”í”„ ë‚´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•˜ê³  ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜")
                        failure_response = await self._generate_search_failure_response(
                            query, selected_documents, agent_type
                        )
                        return failure_response, [], {'search_failed': True, 'low_quality': True}, {
                            'chunks_found': used_count,
                            'search_status': 'failed_low_quality',
                            'avg_similarity': avg_sim
                        }
                except Exception as fb_err:
                    logger.warning(f"í´ë°± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {fb_err}")

            # 3) ê²Œì´íŒ…/ëª¨ë“œ ê²°ì • (ìµœì¢… used_count ê¸°ë°˜)
            ppt_intent = self._detect_ppt_intent(query)
            selected_mode = "full"
            gating_reason = ""
            if ppt_intent:
                if used_count >= 3:
                    selected_mode = "full"
                elif used_count >= 1:
                    selected_mode = "outline"
                    gating_reason = "ê·¼ê±° ì œí•œ(ì°¸ê³  1-2ê°œ)ìœ¼ë¡œ ì•„ì›ƒë¼ì¸ ëª¨ë“œ ì ìš©"
                else:
                    selected_mode = "decline"
                    gating_reason = "ì°¸ê³ ìë£Œ 0ê°œ"
            context_info = context_info or {}
            if isinstance(context_info, dict):
                context_info["selected_mode"] = selected_mode
                if gating_reason:
                    context_info["gating_reason"] = gating_reason

            # 4) í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
            if selected_documents and len(selected_documents) > 0:
                context_enhanced_prompt = (
                    f"{system_prompt}\n\n"
                    f"ì„ íƒëœ ë¬¸ì„œ ì •ë³´:\n{document_info}\n\n"
                    f"ğŸš¨ ì‹œìŠ¤í…œ ì œê³µ ì°¸ì¡°ë¬¸ì„œ ê°œìˆ˜: {used_count}ê°œ ğŸš¨\n"
                    f"ì•„ë˜ëŠ” ê²€ìƒ‰ìœ¼ë¡œ ìˆ˜ì§‘í•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‹µë³€ ì‹œ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.\n---\n"
                    f"{final_result.get('context_text', '')}"
                )
            else:
                context_enhanced_prompt = (
                    f"{system_prompt}\n\n"
                    f"ì „ì²´ ì§€ì‹ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
                    f"ğŸš¨ ì‹œìŠ¤í…œ ì œê³µ ì°¸ì¡°ë¬¸ì„œ ê°œìˆ˜: {used_count}ê°œ ğŸš¨\n"
                    f"ì•„ë˜ëŠ” ê²€ìƒ‰ìœ¼ë¡œ ìˆ˜ì§‘í•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‹µë³€ ì‹œ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.\n---\n"
                    f"{final_result.get('context_text', '')}"
                )

            # 5) context_infoì— í†µê³„ ë° ë©€í‹°í„´ ë©”íƒ€ë°ì´í„° ì£¼ì… ë° ë°˜í™˜
            ctx_info = final_result.get('context_info', {}) or {}
            try:
                if isinstance(ctx_info, dict):
                    ctx_info.setdefault('used_chunks', used_count)
                    if total_count is not None:
                        ctx_info.setdefault('total_chunks', total_count)
                    if 'selected_mode' in context_info:
                        ctx_info['selected_mode'] = context_info['selected_mode']
                    if 'gating_reason' in context_info:
                        ctx_info['gating_reason'] = context_info['gating_reason']
                    
                    # ë©€í‹°í„´ ì»¨í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    if isinstance(context_metadata, dict):
                        ctx_info['context_used'] = context_metadata.get('context_used', False)
                        ctx_info['multiterm_reason'] = context_metadata.get('reason', 'no_context')
                        
            except Exception:
                pass
            return context_enhanced_prompt, references, ctx_info, rag_stats or final_result.get('rag_stats', {})
                
        except Exception as e:
            logger.error(f"âŒ Agent context ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì¸ì‚¬ / ì¼ë°˜ ëŒ€í™”ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì´ˆë‹¨ë¬¸ì€ ë¶€ë“œëŸ¬ìš´ í´ë°±
            if len(query.strip()) <= 10:
                soft_fallback = "ì•ˆë…•í•˜ì„¸ìš”! ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë¬¸ì„œë‚˜ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë§ì”€í•´ì£¼ì„¸ìš”. ğŸ˜Š"
                return soft_fallback, [], {"rag_used": False, "error": str(e)}, {"rag_used": False}
            # ê¸°íƒ€ëŠ” ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            return system_prompt, [], {"rag_used": False, "error": str(e)}, {"rag_used": False}
    
    def _enhance_query_for_agent(
        self, 
        query: str, 
        agent_type: str, 
        document_info: Optional[str] = None
    ) -> str:
        """Agent íƒ€ì…ì— ë”°ë¥¸ ì§ˆë¬¸ ë³´ê°•"""
        
        enhancements = {
            'summarizer': "ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:",
            'keyword-extractor': "ë‹¤ìŒ ë‚´ìš©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:",
            'presentation': "ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ í”„ë ˆì  í…Œì´ì…˜ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”:",
            'template': "ë‹¤ìŒ ë‚´ìš©ì„ í…œí”Œë¦¿ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:",
            'knowledge-graph': "ë‹¤ìŒ ë‚´ìš©ì˜ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”:",
            'analyzer': "ë‹¤ìŒ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:",
            'insight': "ë‹¤ìŒ ë‚´ìš©ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”:",
            'report-generator': "ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:",
            'script-generator': "ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ ë°œí‘œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”:",
            'key-points': "ë‹¤ìŒ ë‚´ìš©ì˜ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:"
        }
        
        if agent_type in enhancements and agent_type != 'general':
            prefix = enhancements[agent_type]
            if document_info:
                return f"{prefix}\n\nì°¸ê³  ë¬¸ì„œ: {document_info}\n\nì§ˆë¬¸: {query}"
            else:
                return f"{prefix}\n\n{query}"
        
        return query

    def _detect_ppt_intent(self, query: str) -> bool:
        try:
            if not isinstance(query, str):
                return False
            q = query.lower()
            has_ppt = any(k in q for k in ["ppt", "pptx", "presentation", "í”„ë ˆì  í…Œì´ì…˜", "í”„ë¦¬ì  í…Œì´ì…˜", "ìŠ¬ë¼ì´ë“œ", "ë°œí‘œìë£Œ", "ì œí’ˆì†Œê°œ"])
            has_create = any(k in q for k in ["ë§Œë“¤", "ì‘ì„±", "ìƒì„±", "ì œì‘"])
            return bool(has_ppt and has_create)
        except Exception:
            return False

    def _build_non_rag_agent_response(self, query: str, qtype: str) -> str:
        """ì—ì´ì „íŠ¸ ê²½ë¡œì—ì„œ ì¸ì‚¬/ì¼ë°˜ëŒ€í™”/ì‹œìŠ¤í…œë¬¸ì˜ ë¶„ë¥˜ ì‹œ ì¦‰ì‹œ ì‘ë‹µ ìƒì„±"""
        if qtype == "greeting":
            return "ì•ˆë…•í•˜ì„¸ìš”! ì›…ì§„ ì§€ì‹ê´€ë¦¬ì‹œìŠ¤í…œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ğŸ˜Š\n\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        if qtype == "general_chat":
            # ê°„ë‹¨í•œ ì‘ë‹µë“¤ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ë‹µ
            query_lower = query.lower().strip()
            if "ë„¤" in query_lower or "ì‘" in query_lower:
                return "ë„¤, ê³„ì†í•´ì„œ ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”!"
            elif "ê³ ë§ˆ" in query_lower or "ê°ì‚¬" in query_lower:
                return "ì²œë§Œì—ìš”! ì–¸ì œë“  ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”."
            elif "ì¢‹" in query_lower:
                return "ê°ì‚¬í•©ë‹ˆë‹¤! ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."
            else:
                return "ë„¤, ì˜ ì•Œê² ìŠµë‹ˆë‹¤. ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!"
        if qtype == "system_inquiry":
            return (
                "ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ ì§€ì›í•˜ê³  ìˆì–´ìš”:\n\n"
                "- ğŸ“š ë¬¸ì„œ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µ\n"
                "- ğŸ“ ë¬¸ì„œ ìš”ì•½\n" 
                "- ğŸ“Š PPT ìë™ ìƒì„±\n"
                "- ğŸ” í‚¤ì›Œë“œ/ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ\n\n"
                "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
            )
        return "ë„¤, ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì‹œë©´ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì•„ ë„ì™€ë“œë¦´ê²Œìš”!"
    
    async def _generate_search_failure_response(
        self, 
        query: str, 
        selected_documents: List[SelectedDocument],
        agent_type: str = 'general'
    ) -> str:
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì•ˆë‚´ ì‘ë‹µ ìƒì„±"""
        try:
            # ê²€ìƒ‰ ì‹¤íŒ¨ ì „ìš© í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            failure_template = None
            try:
                if self.prompts_dir and self.prompts_dir.exists():
                    search_failure_prompt_path = self.prompts_dir / "search-failure.prompt"
                    if search_failure_prompt_path.exists():
                        failure_template = search_failure_prompt_path.read_text(encoding='utf-8').strip()
            except Exception as fe:
                logger.debug(f"search-failure.prompt ë¡œë“œ ì‹¤íŒ¨: {fe}")
            if not failure_template:
                failure_template = """ğŸ” **ê²€ìƒ‰ ê²°ê³¼**

{failure_lead}

## ğŸ“‹ ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ
{selected_documents}

## ğŸ’¡ **ê²€ìƒ‰ì„ ê°œì„ í•˜ë ¤ë©´**
- **ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ**ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”
- **ë‹¤ë¥¸ í‘œí˜„**ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”  
- **ë¬¸ì„œ ì œëª©ì´ë‚˜ ì„¹ì…˜ëª…**ì„ í¬í•¨í•´ë³´ì„¸ìš”

{suggestions_section}

ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ˜Š"""
            
            # ì„ íƒëœ ë¬¸ì„œ ì •ë³´ êµ¬ì„±
            document_list = "\n".join([
                f"ğŸ“„ {doc.fileName} ({doc.fileType})"
                for doc in selected_documents
            ])
            # ì„ íƒ ë¬¸ì„œ ìœ ë¬´ì— ë”°ë¥¸ ë¦¬ë“œ ë¬¸êµ¬ êµ¬ì„±
            if selected_documents and len(selected_documents) > 0:
                failure_lead = "ì„ íƒí•˜ì‹  ë¬¸ì„œë“¤ì„ ê²€í† í–ˆì§€ë§Œ, ì§ˆì˜í•˜ì‹  ë‚´ìš©ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤."
            else:
                failure_lead = "ì „ì²´ ë¬¸ì„œë¥¼ ê²€ìƒ‰í–ˆì§€ë§Œ, ì§ˆì˜í•˜ì‹  ë‚´ìš©ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤."

            # ë¬¸ì„œ ê¸°ë°˜ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± (ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±)
            suggestions: List[str] = []
            # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±° í›„ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            for doc in selected_documents[:3]:  # ìµœëŒ€ 3ê°œë§Œ í™œìš©
                base = doc.fileName.rsplit('.', 1)[0]
                # í•œê¸€/ì˜ë¬¸ í˜¼í•© ì •ë¦¬
                base_clean = base.replace('_', ' ').replace('-', ' ').strip()
                if not base_clean:
                    continue
                suggestions.extend([
                    f"'{base_clean}' ë¬¸ì„œì˜ í•µì‹¬ ìš”ì•½ì€?",
                    f"'{base_clean}' ë¬¸ì„œì—ì„œ ì£¼ìš” ì ˆì°¨ ë‹¨ê³„ëŠ”?",
                    f"'{base_clean}' ë¬¸ì„œì˜ ëª©ì ê³¼ ì ìš© ë²”ìœ„ë¥¼ ì„¤ëª…í•´ì¤˜",
                    f"'{base_clean}' ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•  ì‚°ì¶œë¬¼ì€?"
                ])

            # ì„ íƒëœ ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì œì•ˆ
            if not suggestions:
                suggestions = [
                    "ì–´ë–¤ ì œí’ˆ / í”„ë¡œì„¸ìŠ¤ / ë¬¸ì„œ ìœ í˜•ì¸ì§€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì ì–´ì£¼ì„¸ìš”",
                    "ë¬¸ì„œ ì œëª©ì— í¬í•¨ëœ ê³ ìœ  ìš©ì–´(ì˜ˆ: SOP, WI, ê·œê²©ëª…)ë¥¼ ì§ˆë¬¸ì— í¬í•¨í•´ë³´ì„¸ìš”",
                    "í•„ìš”í•œ ê²°ê³¼ í˜•íƒœ(ìš”ì•½, ì ˆì°¨, ì •ì˜ ë“±)ë¥¼ ëª…ì‹œí•´ë³´ì„¸ìš”"
                ]

            # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ Nê°œ ì œí•œ
            seen = set()
            unique_suggestions = []
            for s in suggestions:
                if s not in seen:
                    seen.add(s)
                    unique_suggestions.append(s)
            unique_suggestions = unique_suggestions[:6]

            suggestions_md = "\n".join(f"- {s}" for s in unique_suggestions)
            
            # ì¶”ì²œ ì§ˆë¬¸ ì„¹ì…˜ êµ¬ì„±
            suggestions_section = ""
            if suggestions_md:
                suggestions_section = f"""
## ğŸ¤” **ì´ëŸ° ì§ˆë¬¸ì€ ì–´ë– ì„¸ìš”?**
{suggestions_md}"""
            
            # ì—°ê´€ ë¬¸ì„œ ì¶”ì²œ (ì„ íƒ ë¬¸ì„œ ì œì™¸)
            related_docs_md = ""
            try:
                from app.services.chat.rag_search_service import rag_search_service
                exclude_ids = [doc.id for doc in selected_documents]
                recommendations = await rag_search_service.recommend_related_documents(
                    query=query,
                    exclude_document_ids=exclude_ids,
                    limit=5,
                    threshold=0.22
                )
                if recommendations:
                    # 1) ì‚¬ìš©ì ì§€ì • í…œí”Œë¦¿ (ë¬¸ìì—´ í¬í•¨ ì‹œ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                    custom_tpl = os.getenv("DOCUMENT_VIEWER_URL_TEMPLATE")  # ì˜ˆ: https://kms/viewer?doc={file_id}
                    link_mode = os.getenv("DOCUMENT_VIEWER_LINK_MODE", "scheme")  # scheme | template
                    scheme_prefix = os.getenv("DOCUMENT_VIEWER_SCHEME", "doc-open://file")
                    lines: List[str] = []
                    for r in recommendations:
                        file_id = r.get('file_id')
                        file_name = r.get('file_name') or 'ë¬¸ì„œ'
                        safe_name = file_name.replace(']', '\\]').replace('[', '\\[')
                        max_sim = r.get('max_similarity', 0.0)
                        pct = int(round(max_sim * 100))
                        ext = ''
                        if '.' in file_name:
                            ext = file_name.rsplit('.', 1)[-1]
                        if link_mode == 'template' and custom_tpl:
                            # í…œí”Œë¦¿ ì¹˜í™˜ (ë¯¸í¬í•¨ ì‹œ {file_id} ì¶”ê°€)
                            if '{file_id}' in custom_tpl:
                                url = custom_tpl.replace('{file_id}', str(file_id))
                            else:
                                sep = '&' if '?' in custom_tpl else '?'
                                url = f"{custom_tpl}{sep}fileId={file_id}"
                            # ë‹¨ìˆœ ì™¸ë¶€ ë§í¬ (ìƒˆíƒ­) - í”„ë¡ íŠ¸ ì¸í„°ì…‰íŠ¸ê°€ í•„ìš”í•˜ë©´ scheme ì‚¬ìš© ê¶Œì¥
                        else:
                            # ê¸°ë³¸: ì»¤ìŠ¤í…€ ìŠ¤í‚´ (í”„ë¡ íŠ¸ ë§ˆí¬ë‹¤ìš´ a íƒœê·¸ ì¸í„°ì…‰íŠ¸)
                            # ì¸ì½”ë”© (ê°„ë‹¨ ì²˜ë¦¬)
                            from urllib.parse import quote
                            q_name = quote(file_name)
                            q_ext = quote(ext)
                            url = f"{scheme_prefix}?docId={file_id}&name={q_name}&ext={q_ext}&sim={pct}"
                        # ìœ ì‚¬ë„ í‘œì‹œ: HTML span ëŒ€ì‹  ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš© (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ íŒ¨í„´ ë§¤ì¹­ ê°€ëŠ¥)
                        # íŒ¨í„´: (ìœ ì‚¬ë„ {pct}%)  -> ì˜ˆ: (ìœ ì‚¬ë„ 87%)
                        # í•„ìš” ì‹œ í”„ë¡ íŠ¸ì—ì„œ /\(ìœ ì‚¬ë„ (\d+)%\)/ íŒ¨í„´ìœ¼ë¡œ ë±ƒì§€ ìŠ¤íƒ€ì¼ ì ìš©
                        lines.append(f"- [{safe_name}]({url}) (ìœ ì‚¬ë„ {pct}%)")
                    related_docs_md = "\n".join(lines)
            except Exception as rec_err:
                logger.debug(f"ì—°ê´€ ë¬¸ì„œ ì¶”ì²œ ìŠ¤í‚µ: {rec_err}")

            # í…œí”Œë¦¿ placeholder ë³´í˜¸: ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ì•ˆì „ ì²˜ë¦¬
            # format í˜¸ì¶œ ì‹œ í•„ìš”í•œ placeholderë§Œ ì œê³µ
            format_kwargs: Dict[str, Any] = {"selected_documents": document_list, "failure_lead": failure_lead}
            if "{document_suggestions}" in failure_template:
                format_kwargs["document_suggestions"] = suggestions_md
            if "{suggestions_section}" in failure_template:
                format_kwargs["suggestions_section"] = suggestions_section
            if "{related_documents}" in failure_template:
                format_kwargs["related_documents"] = related_docs_md or "(ì—°ê´€ ë¬¸ì„œ í›„ë³´ ì—†ìŒ)"
            else:
                # í…œí”Œë¦¿ì— ì„¹ì…˜ì´ ì—†ë‹¤ë©´ ê¼¬ë¦¬ì— ì¶”ê°€
                if related_docs_md:
                    failure_template += f"\n\n### ğŸ”— ì—°ê´€ ë¬¸ì„œ í›„ë³´\n\n{related_docs_md}\n"
                if not "{suggestions_section}" in failure_template:
                    failure_template += suggestions_section
            
            try:
                response = failure_template.format(**format_kwargs)
            except KeyError as ke:
                # ì˜ˆìƒì¹˜ ëª»í•œ placeholderê°€ ì¶”ê°€ëœ ê²½ìš° ì•ˆì „ í´ë°±
                logger.warning(f"âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨ í…œí”Œë¦¿ í‚¤ ëˆ„ë½: {ke}. ì œê³µëœ í‚¤ë§Œ ì‚¬ìš©í•´ ì¬ì‹œë„")
                safe_template = failure_template
                for missing_key in ["document_suggestions", "selected_documents", "related_documents", "suggestions_section"]:
                    if f"{{{missing_key}}}" in safe_template and missing_key not in format_kwargs:
                        safe_template = safe_template.replace(f"{{{missing_key}}}", "")
                response = safe_template.format(**format_kwargs)
            
            logger.info(f"âœ… ê²€ìƒ‰ ì‹¤íŒ¨ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(selected_documents)}ê°œ ë¬¸ì„œ")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            # ìµœì†Œí•œì˜ ê¸°ë³¸ ë©”ì‹œì§€
            return f"""# âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜

ì£„ì†¡í•©ë‹ˆë‹¤. **"{query}"**ì— ëŒ€í•œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

---

## ğŸ”„ **ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”**
- ì ì‹œ í›„ ë™ì¼í•œ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”
- ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ í‘œí˜„ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”  
- ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”

---

ğŸ’¬ **ì´ìš©ì— ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤.**"""
    
    async def _try_keyword_fallback_search(
        self,
        query: str,
        document_ids: List[str],
        container_ids: Optional[List[str]] = None
    ) -> Optional[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ê²€ìƒ‰"""
        try:
            # ë” ê´€ëŒ€í•œ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¡œ ì¬ì‹œë„
            fallback_params = RAGSearchParams(
                query=query,
                limit=self.rag_max_chunks,
                threshold=0.15,  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
                similarity_threshold=0.15,
                search_mode='keyword',  # í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš©
                reranking=False,  # ë¦¬ë­í‚¹ ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
                document_ids=document_ids
            )
            
            logger.info(f"ğŸ”„ í´ë°± ê²€ìƒ‰ ì‹œë„: threshold=0.15, mode=keyword")
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰
            fallback_result = await rag_search_service.search_with_rag(
                fallback_params,
                container_ids=container_ids
            )
            
            return fallback_result
            
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ í´ë°± ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    async def _load_documents_for_summarization(
        self,
        selected_documents: List[SelectedDocument],
        db_session: AsyncSession,
        max_chunks: int = 50
    ) -> tuple[str, List[Dict[str, Any]], Dict[str, Any], Dict[str, Any]]:
        """
        ìš”ì•½ ìš”ì²­ ì‹œ ì„ íƒ ë¬¸ì„œì˜ ì›ë¬¸ì„ ì§ì ‘ ë¡œë“œ
        
        ğŸ”§ DocumentLoaderTool ì‚¬ìš©: ê²€ìƒ‰ ì—†ì´ ë¬¸ì„œ chunkë¥¼ ìˆœì„œëŒ€ë¡œ ë¡œë“œ
        """
        try:
            from app.tools.document.document_loader_tool import document_loader_tool
            
            document_ids = [int(doc.id) for doc in selected_documents]
            logger.info(f"ï¿½ [Summarization] DocumentLoaderTool ì‚¬ìš©: {len(document_ids)}ê°œ ë¬¸ì„œ")
            
            # DocumentLoaderToolë¡œ ë¬¸ì„œ ë¡œë“œ
            tool_result = await document_loader_tool._arun(
                document_ids=document_ids,
                db_session=db_session,
                max_chunks=max_chunks,
                user_emp_no=None  # ê¶Œí•œ í™•ì¸ì€ ì´ë¯¸ API ë ˆë²¨ì—ì„œ ì™„ë£Œ
            )
            
            if not tool_result.success or not tool_result.data:
                # chunkê°€ ì—†ìœ¼ë©´ ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€
                logger.warning(f"âš ï¸ ì„ íƒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {document_ids}")
                logger.warning(f"   ë„êµ¬ ì˜¤ë¥˜: {tool_result.errors}")
                
                doc_names = [doc.fileName for doc in selected_documents]
                failure_msg = f"""ì£„ì†¡í•©ë‹ˆë‹¤. ì„ íƒí•˜ì‹  ë¬¸ì„œì˜ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:

{chr(10).join(f'- {name}' for name in doc_names)}

ì´ ë¬¸ì„œê°€ ì•„ì§ ì²˜ë¦¬ ì¤‘ì´ê±°ë‚˜, ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‹¤ë¥¸ ë¬¸ì„œë¥¼ ì„ íƒí•˜ì‹œê±°ë‚˜, ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."""
                
                return failure_msg, [], {"chunks_count": 0, "documents_count": 0}, {"rag_used": False, "summarization_mode": True}
            
            # SearchChunkë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            chunks_data = []
            context_parts = []
            
            for chunk in tool_result.data:
                file_name = chunk.metadata.get("file_name", f"ë¬¸ì„œ {chunk.file_id}")
                page_number = chunk.metadata.get("page_number", "?")
                context_parts.append(f"[{file_name} - p.{page_number}]\n{chunk.content}")
                
                chunks_data.append({
                    "file_id": chunk.file_id,
                    "file_name": file_name,
                    "chunk_index": chunk.metadata.get("chunk_index", 0),
                    "page_number": page_number,
                    "content": chunk.content[:500],  # ë¯¸ë¦¬ë³´ê¸°ìš©
                    "similarity_score": chunk.score,
                    "search_type": chunk.match_type
                })
            
            context_text = "\n\n---\n\n".join(context_parts)
            
            logger.info(
                f"âœ… [Summarization] ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(chunks_data)}ê°œ ì²­í¬, "
                f"{len(context_text)}ì, latency={tool_result.metrics.latency_ms:.1f}ms"
            )
            
            context_info = {
                "chunks_count": len(chunks_data),
                "documents_count": len(set(c["file_id"] for c in chunks_data)),
                "total_tokens": len(context_text) // 4,  # ëŒ€ëµì ì¸ í† í° ìˆ˜
                "summarization_mode": True,
                "tool_used": "document_loader",
                "tool_latency_ms": tool_result.metrics.latency_ms
            }
            
            rag_stats = {
                "rag_used": True,
                "summarization_mode": True,
                "search_skipped": True,
                "direct_load": True,
                "tool_name": tool_result.tool_name,
                "tool_version": tool_result.tool_version
            }
            
            return context_text, chunks_data, context_info, rag_stats
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ìš© ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            failure_msg = f"""ì£„ì†¡í•©ë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ì˜¤ë¥˜: {str(e)}

ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."""
            
            return failure_msg, [], {"chunks_count": 0}, {"rag_used": False, "error": str(e)}
    
    def validate_agent_requirements(
        self, 
        agent_type: str, 
        selected_documents: List[SelectedDocument]
    ) -> tuple[bool, str]:
        """Agent ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
        
        agent_config = self.get_agent_config(agent_type)
        
        if agent_config.required_documents and len(selected_documents) == 0:
            return False, f"'{agent_config.name}' ì—ì´ì „íŠ¸ëŠ” ë¬¸ì„œ ì„ íƒì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        return True, "OK"
    
    def get_response_format_instruction(self, agent_type: str) -> str:
        """Agentë³„ ì‘ë‹µ í˜•ì‹ ì§€ì‹œì‚¬í•­"""
        
        agent_config = self.get_agent_config(agent_type)
        
        format_instructions = {
            'text': "",
            'markdown': "\n\nì‘ë‹µì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            'json': "\n\nì‘ë‹µì„ JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
        }
        
        return format_instructions.get(agent_config.output_format, "")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
ai_agent_service = AIAgentService()

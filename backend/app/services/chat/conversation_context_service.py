"""
ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì„œë¹„ìŠ¤
ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ì´ì „ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ RAG ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
"""

import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

from app.services.core.korean_nlp_service import korean_nlp_service

logger = logging.getLogger(__name__)

@dataclass
class ConversationContext:
    """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì •ë³´"""
    session_id: str
    turn_number: int
    accumulated_keywords: List[str]
    relevant_documents: List[str]  # ì´ì „ì— ìœ ìš©í–ˆë˜ ë¬¸ì„œ IDë“¤
    topic_continuity_score: float
    last_intent: str
    conversation_summary: str

class ConversationContextService:
    """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.max_history_turns = 5  # ìµœëŒ€ 5í„´ê¹Œì§€ ê³ ë ¤
        self.keyword_decay_factor = 0.8  # ì´ì „ í„´ì¼ìˆ˜ë¡ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê°ì†Œ
        self.document_relevance_threshold = 0.6
        
        # ëª…ì‹œì  ì°¸ì¡° í‘œí˜„ íŒ¨í„´
        self.explicit_reference_patterns = [
            # ì‹œê°„ì  ì°¸ì¡°
            "ì´ì „ì—", "ì•ì—ì„œ", "ë°©ê¸ˆ", "ì•„ê¹Œ", "ì „ì—", "ë¨¼ì €", 
            "ì²˜ìŒì—", "ìµœê·¼ì—", "ì§€ë‚œë²ˆì—", "ì˜ˆì „ì—", "ê³¼ê±°ì—",
            
            # ì§€ì‹œ ëŒ€ëª…ì‚¬ ë° ì§€ì‹œì–´
            "ê·¸ê²ƒ", "ê·¸ê±°", "ì´ê²ƒ", "ì´ê±°", "ê·¸", "ì´", "ì €ê²ƒ", "ì €ê±°", "ì €",
            "ê·¸ëŸ°", "ì´ëŸ°", "ì €ëŸ°", "ê·¸ê°™ì€", "ì´ê°™ì€", "ì €ê°™ì€",
            
            # ëŒ€í™” ì—°ê²° í‘œí˜„
            "ë§í•œ", "ì´ì•¼ê¸°í•œ", "ì–¸ê¸‰í•œ", "ì–˜ê¸°í•œ", "ì„¤ëª…í•œ", "ë‹µë³€í•œ",
            "ë§ì”€", "ì´ì•¼ê¸°", "ì–¸ê¸‰", "ì–˜ê¸°", "ì„¤ëª…", "ë‹µë³€",
            
            # ë¬¸ë§¥ ì—°ê²°ì–´
            "ê·¸ë˜ì„œ", "ê·¸ëŸ°ë°", "ê·¸ëŸ¬ë©´", "ê·¸ëŸ¼", "ê·¸ë ‡ë‹¤ë©´", "ë”°ë¼ì„œ",
            "ë˜í•œ", "ë˜", "ì¶”ê°€ë¡œ", "ë”ë¶ˆì–´", "ê²Œë‹¤ê°€", "ë§ˆì°¬ê°€ì§€ë¡œ",
            
            # ë‚´ìš© ê´€ë ¨ ì°¸ì¡°
            "ë‚´ìš©ì—ì„œ", "ë‚´ìš©", "ë¶€ë¶„ì—ì„œ", "ë¶€ë¶„", "ê´€ë ¨í•´ì„œ", "ê´€ë ¨í•˜ì—¬",
            "ëŒ€í•´ì„œ", "ëŒ€í•˜ì—¬", "ê´€í•´ì„œ", "ê´€í•˜ì—¬", "ê²ƒì— ëŒ€í•´", "ê²ƒì— ê´€í•´",
            
            # ëŒ€í™” íë¦„ ì°¸ì¡°
            "ê³„ì†í•´ì„œ", "ì´ì–´ì„œ", "ì—°ê²°í•´ì„œ", "ì—°ì¥í•´ì„œ", "ì¶”ê°€í•´ì„œ",
            "ë” ìì„¸íˆ", "ë” êµ¬ì²´ì ìœ¼ë¡œ", "ë³´ì¶©í•´ì„œ", "ë§ë¶™ì—¬ì„œ",
            
            # ğŸ†• ì²¨ë¶€ íŒŒì¼ ê´€ë ¨ ì°¸ì¡°
            "ì²¨ë¶€", "ì²¨ë¶€ëœ", "ì²¨ë¶€í•œ", "ì²¨ë¶€íŒŒì¼", "ì²¨ë¶€ íŒŒì¼",
            "ì—…ë¡œë“œ", "ì—…ë¡œë“œëœ", "ì—…ë¡œë“œí•œ", "ì˜¬ë¦°", "ì˜¬ë ¤ì§„",
            "ì´ë¯¸ì§€", "ì‚¬ì§„", "ê·¸ë¦¼", "íŒŒì¼", "ë¬¸ì„œ",
            "ìœ„", "ìœ„ì˜", "ì•„ë˜", "ì•„ë˜ì˜", "ë‹¤ìŒ", "ë‹¤ìŒì˜"
        ]
        
    async def enhance_query_with_context(
        self,
        current_query: str,
        session_id: str,
        db_session: AsyncSession
    ) -> Tuple[str, Dict[str, Any]]:
        """
        ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ í˜„ì¬ ì¿¼ë¦¬ ê°•í™”
        
        Returns:
            enhanced_query: ì»¨í…ìŠ¤íŠ¸ê°€ ë°˜ì˜ëœ ê°•í™”ëœ ì¿¼ë¦¬
            context_metadata: ì»¨í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°
        """
        try:
            # 0. ëª…ì‹œì  ì°¸ì¡° í‘œí˜„ í™•ì¸
            has_explicit_reference = self._has_explicit_reference(current_query)
            
            if not has_explicit_reference:
                logger.info(f"ğŸ“ ëª…ì‹œì  ì°¸ì¡° ì—†ìŒ - ë…ë¦½ì  ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬: '{current_query[:30]}...'")
                return current_query, {"context_used": False, "reason": "no_explicit_reference"}
            
            logger.info(f"ğŸ”— ëª…ì‹œì  ì°¸ì¡° íƒì§€ - ë©€í‹°í„´ ì»¨í…ìŠ¤íŠ¸ ì ìš©: '{current_query[:30]}...'")
            
            # 1. ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
            conversation_history = await self._get_conversation_history(db_session, session_id)
            
            if not conversation_history:
                return current_query, {"context_used": False, "reason": "no_history"}
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
            context = await self._analyze_conversation_context(conversation_history, current_query)
            
            # 3. ì¿¼ë¦¬ ê°•í™”
            enhanced_query = await self._enhance_query(current_query, context)
            
            context_metadata = {
                "context_used": True,
                "original_query": current_query,
                "enhanced_query": enhanced_query,
                "accumulated_keywords": context.accumulated_keywords,
                "relevant_documents": context.relevant_documents,
                "topic_continuity": context.topic_continuity_score,
                "last_intent": context.last_intent
            }
            
            logger.info(f"ğŸ”— ì¿¼ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê°•í™”: '{current_query}' â†’ '{enhanced_query}'")
            return enhanced_query, context_metadata
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì‹¤íŒ¨: {e}")
            return current_query, {"context_used": False, "error": str(e)}
    
    async def _get_conversation_history(
        self, 
        session: AsyncSession, 
        session_id: str
    ) -> List[Dict[str, Any]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        try:
            query = text("""
                SELECT 
                    user_message,
                    assistant_response,
                    created_date,
                    conversation_context
                FROM tb_chat_history 
                WHERE session_id = :session_id 
                ORDER BY created_date DESC 
                LIMIT :limit
            """)
            
            result = await session.execute(query, {
                "session_id": session_id,
                "limit": self.max_history_turns
            })
            
            history = []
            for row in result.fetchall():
                history.append({
                    "user_message": row.user_message,
                    "ai_response": row.assistant_response,
                    "created_at": row.created_date,
                    "metadata": row.conversation_context or {}
                })
            
            return list(reversed(history))  # ì‹œê°„ìˆœ ì •ë ¬
            
        except Exception as e:
            logger.error(f"âŒ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def _analyze_conversation_context(
        self,
        history: List[Dict[str, Any]],
        current_query: str
    ) -> ConversationContext:
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        try:
            # 1. ëˆ„ì  í‚¤ì›Œë“œ ì¶”ì¶œ
            accumulated_keywords = await self._extract_accumulated_keywords(history, current_query)
            
            # 2. ê´€ë ¨ ë¬¸ì„œ ì¶”ì¶œ
            relevant_documents = self._extract_relevant_documents(history)
            
            # 3. ì£¼ì œ ì—°ì†ì„± ì ìˆ˜ ê³„ì‚°
            topic_continuity = await self._calculate_topic_continuity(history, current_query)
            
            # 4. ë§ˆì§€ë§‰ ì˜ë„ ì¶”ì¶œ
            last_intent = self._extract_last_intent(history)
            
            # 5. ëŒ€í™” ìš”ì•½ ìƒì„±
            conversation_summary = self._generate_conversation_summary(history)
            
            return ConversationContext(
                session_id="",  # ì„¸ì…˜ IDëŠ” ìƒìœ„ì—ì„œ ê´€ë¦¬
                turn_number=len(history) + 1,
                accumulated_keywords=accumulated_keywords,
                relevant_documents=relevant_documents,
                topic_continuity_score=topic_continuity,
                last_intent=last_intent,
                conversation_summary=conversation_summary
            )
            
        except Exception as e:
            logger.error(f"âŒ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return ConversationContext("", 0, [], [], 0.0, "unknown", "")
    
    async def _extract_accumulated_keywords(
        self,
        history: List[Dict[str, Any]],
        current_query: str
    ) -> List[str]:
        """ëŒ€í™”ì—ì„œ ëˆ„ì ëœ í‚¤ì›Œë“œ ì¶”ì¶œ (ì£¼ì œ ì „í™˜ ì‹œ í‚¤ì›Œë“œ ì´ˆê¸°í™”)"""
        try:
            all_keywords = set()
            
            # í˜„ì¬ ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            current_analysis = await korean_nlp_service.analyze_korean_text(current_query)
            current_keywords = current_analysis.get("keywords", [])
            current_domain_keywords = set()
            
            # í˜„ì¬ ì§ˆë¬¸ì˜ ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ ë¶„ë¥˜
            domain_categories = {
                "medical": {"ì˜ë£Œ", "ë³‘ì›", "ì¹˜ë£Œ", "ì§ˆë³‘", "ì•½ë¬¼", "ì˜ì‚¬", "í™˜ì", "ê±´ê°•", "ì¸ìŠë¦°", "íŒí”„", "í˜ˆë‹¹", "ë‹¹ë‡¨"},
                "travel": {"ì—¬í–‰", "ê´€ê´‘", "í˜¸í…”", "í•­ê³µ", "ë¹„ì", "ì¼ë³¸", "ë„ì¿„", "êµí† ", "ì˜¤ì‚¬ì¹´", "ê´€ê´‘ì§€", "ìˆ™ì†Œ"},
                "technology": {"IT", "ì»´í“¨í„°", "ì†Œí”„íŠ¸ì›¨ì–´", "í”„ë¡œê·¸ë˜ë°", "ê°œë°œ", "ì‹œìŠ¤í…œ", "ë„¤íŠ¸ì›Œí¬", "AI"},
                "business": {"ì‚¬ì—…", "íšŒì‚¬", "ê²½ì˜", "ë§ˆì¼€íŒ…", "ì˜ì—…", "ì œí’ˆ", "ì„œë¹„ìŠ¤", "ê³ ê°", "ë§¤ì¶œ"},
                "education": {"êµìœ¡", "í•™êµ", "í•™ìŠµ", "ìˆ˜ì—…", "ê°•ì˜", "ì‹œí—˜", "ì¡¸ì—…", "ì…í•™", "ê³¼ì •"}
            }
            
            current_domain = "general"
            for domain, domain_kws in domain_categories.items():
                if any(kw in current_query.lower() or kw in current_keywords for kw in domain_kws):
                    current_domain = domain
                    current_domain_keywords = domain_kws
                    break
            
            # í˜„ì¬ ì§ˆë¬¸ í‚¤ì›Œë“œ ì¶”ê°€
            for keyword in current_keywords:
                all_keywords.add(keyword)
            
            # ì´ì „ ëŒ€í™”ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë„ë©”ì¸ ì¼ì¹˜ ì‹œì—ë§Œ)
            for i, exchange in enumerate(reversed(history)):
                weight = (self.keyword_decay_factor ** i)
                
                # ì´ì „ ë©”ì‹œì§€ ë„ë©”ì¸ í™•ì¸
                prev_message = exchange["user_message"]
                prev_domain = "general"
                for domain, domain_kws in domain_categories.items():
                    if any(kw in prev_message.lower() for kw in domain_kws):
                        prev_domain = domain
                        break
                
                # ê°™ì€ ë„ë©”ì¸ì´ê±°ë‚˜ ì¼ë°˜ì ì¸ ê²½ìš°ì—ë§Œ í‚¤ì›Œë“œ ëˆ„ì 
                if prev_domain == current_domain or (prev_domain == "general" and current_domain == "general"):
                    # ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    user_analysis = await korean_nlp_service.analyze_korean_text(exchange["user_message"])
                    user_keywords = user_analysis.get("keywords", [])
                    
                    for keyword in user_keywords:
                        if len(keyword) > 1:  # ë‹¨ì¼ ë¬¸ì ì œì™¸
                            # í˜„ì¬ ë„ë©”ì¸ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œë§Œ ëˆ„ì 
                            if current_domain == "general" or keyword in current_domain_keywords or any(kw in keyword for kw in current_domain_keywords):
                                all_keywords.add(keyword)
                else:
                    logger.info(f"ğŸš« ë„ë©”ì¸ ë¶ˆì¼ì¹˜ë¡œ í‚¤ì›Œë“œ ëˆ„ì  ì œì™¸: {prev_domain} vs {current_domain}")
                    break  # ë„ë©”ì¸ì´ ë‹¤ë¥´ë©´ ë” ì´ì „ ê¸°ë¡ì€ ë³´ì§€ ì•ŠìŒ
            
            # í‚¤ì›Œë“œ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_keywords = sorted(
                all_keywords, 
                key=lambda x: len(x) + (1 if x in current_keywords else 0),
                reverse=True
            )
            
            result_keywords = sorted_keywords[:10]
            logger.info(f"ğŸ”‘ ëˆ„ì  í‚¤ì›Œë“œ (ë„ë©”ì¸: {current_domain}): {result_keywords}")
            return result_keywords
            
        except Exception as e:
            logger.error(f"âŒ ëˆ„ì  í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_relevant_documents(self, history: List[Dict[str, Any]]) -> List[str]:
        """ì´ì „ ëŒ€í™”ì—ì„œ ìœ ìš©í–ˆë˜ ë¬¸ì„œ ID ì¶”ì¶œ"""
        document_scores = {}
        
        for exchange in history:
            metadata = exchange.get("metadata", {})
            references = metadata.get("references", [])
            
            for ref in references:
                doc_id = ref.get("document_id")
                if doc_id:
                    score = ref.get("similarity_score", 0)
                    if score > self.document_relevance_threshold:
                        document_scores[doc_id] = document_scores.get(doc_id, 0) + score
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë¬¸ì„œ ë°˜í™˜
        sorted_docs = sorted(
            document_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return [doc_id for doc_id, _ in sorted_docs[:5]]
    
    async def _calculate_topic_continuity(
        self,
        history: List[Dict[str, Any]],
        current_query: str
    ) -> float:
        """ì£¼ì œ ì—°ì†ì„± ì ìˆ˜ ê³„ì‚° (ì£¼ì œ ì „í™˜ ê°ì§€ ê°•í™”)"""
        if not history:
            return 0.0
        
        try:
            # ìµœê·¼ 2ê°œ ëŒ€í™”ì™€ í˜„ì¬ ì§ˆë¬¸ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
            recent_messages = []
            for exchange in history[-2:]:
                recent_messages.append(exchange["user_message"])
            
            if not recent_messages:
                return 0.0
            
            # ë„ë©”ì¸ ì¹´í…Œê³ ë¦¬ ê°ì§€ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ê·¸ë£¹
            domain_categories = {
                "medical": {"ì˜ë£Œ", "ë³‘ì›", "ì¹˜ë£Œ", "ì§ˆë³‘", "ì•½ë¬¼", "ì˜ì‚¬", "í™˜ì", "ê±´ê°•", "ì¸ìŠë¦°", "íŒí”„", "í˜ˆë‹¹", "ë‹¹ë‡¨", "ìˆ˜ìˆ ", "ì§„ë£Œ"},
                "travel": {"ì—¬í–‰", "ê´€ê´‘", "í˜¸í…”", "í•­ê³µ", "ë¹„ì", "ì¼ë³¸", "ë„ì¿„", "êµí† ", "ì˜¤ì‚¬ì¹´", "ê´€ê´‘ì§€", "ìˆ™ì†Œ", "ì—¬í–‰ì§€", "íŒ¨í‚¤ì§€", "íˆ¬ì–´"},
                "technology": {"IT", "ì»´í“¨í„°", "ì†Œí”„íŠ¸ì›¨ì–´", "í”„ë¡œê·¸ë˜ë°", "ê°œë°œ", "ì‹œìŠ¤í…œ", "ë„¤íŠ¸ì›Œí¬", "ë°ì´í„°ë² ì´ìŠ¤", "í´ë¼ìš°ë“œ", "AI"},
                "business": {"ì‚¬ì—…", "íšŒì‚¬", "ê²½ì˜", "ë§ˆì¼€íŒ…", "ì˜ì—…", "ì œí’ˆ", "ì„œë¹„ìŠ¤", "ê³ ê°", "ë§¤ì¶œ", "íˆ¬ì", "ê³„ì•½", "ì „ëµ"},
                "education": {"êµìœ¡", "í•™êµ", "í•™ìŠµ", "ìˆ˜ì—…", "ê°•ì˜", "ì‹œí—˜", "ì¡¸ì—…", "ì…í•™", "ê³¼ì •", "ì»¤ë¦¬í˜ëŸ¼", "í•™ìƒ", "êµì‚¬"}
            }
            
            # í˜„ì¬ ì§ˆë¬¸ ë„ë©”ì¸ ë¶„ì„
            current_analysis = await korean_nlp_service.analyze_korean_text(current_query)
            current_keywords = set(current_analysis.get("keywords", []))
            current_domain = self._detect_domain(current_query.lower(), current_keywords, domain_categories)
            
            # ì´ì „ ë©”ì‹œì§€ë“¤ì˜ ë„ë©”ì¸ ë¶„ì„
            prev_domains = []
            keyword_similarities = []
            
            for message in recent_messages:
                message_analysis = await korean_nlp_service.analyze_korean_text(message)
                message_keywords = set(message_analysis.get("keywords", []))
                prev_domain = self._detect_domain(message.lower(), message_keywords, domain_categories)
                prev_domains.append(prev_domain)
                
                # í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚°
                if current_keywords and message_keywords:
                    intersection = current_keywords.intersection(message_keywords)
                    union = current_keywords.union(message_keywords)
                    similarity = len(intersection) / len(union) if union else 0.0
                    keyword_similarities.append(similarity)
                else:
                    keyword_similarities.append(0.0)
            
            # ë„ë©”ì¸ ì¼ì¹˜ë„ ê³„ì‚°
            domain_consistency = sum(1 for prev_domain in prev_domains if prev_domain == current_domain and prev_domain != "general") / len(prev_domains)
            
            # í‚¤ì›Œë“œ ìœ ì‚¬ë„ í‰ê· 
            avg_keyword_similarity = sum(keyword_similarities) / len(keyword_similarities) if keyword_similarities else 0.0
            
            # ì£¼ì œ ì „í™˜ íŒ¨í„´ ê°ì§€
            topic_shift_phrases = ["ì´ì œ", "ë‹¤ìŒì€", "ê·¸ëŸ°ë°", "ëŒ€ì‹ ", "ë°”ê¿”ì„œ", "ìƒˆë¡œìš´", "ë‹¤ë¥¸", "ì „í˜€ ë‹¤ë¥¸"]
            has_shift_indicator = any(phrase in current_query for phrase in topic_shift_phrases)
            
            # ìµœì¢… ì—°ì†ì„± ì ìˆ˜ ê³„ì‚°
            continuity_score = (domain_consistency * 0.6 + avg_keyword_similarity * 0.4)
            
            # ì£¼ì œ ì „í™˜ ì§€ì‹œì–´ê°€ ìˆìœ¼ë©´ ì ìˆ˜ í¬ê²Œ ê°ì†Œ
            if has_shift_indicator:
                continuity_score *= 0.3
            
            # ì™„ì „íˆ ë‹¤ë¥¸ ë„ë©”ì¸ì´ë©´ ì—°ì†ì„± ë‚®ì¶¤
            if current_domain != "general" and all(pd != current_domain and pd != "general" for pd in prev_domains):
                continuity_score *= 0.2
                logger.info(f"ğŸ”„ ë„ë©”ì¸ ì „í™˜ ê°ì§€: {prev_domains} â†’ {current_domain}, ì—°ì†ì„±={continuity_score:.2f}")
            
            return min(1.0, continuity_score)
            
        except Exception as e:
            logger.error(f"âŒ ì£¼ì œ ì—°ì†ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _detect_domain(self, text: str, keywords: set, domain_categories: dict) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ë„ë©”ì¸ ì¹´í…Œê³ ë¦¬ ê°ì§€"""
        domain_scores = {}
        
        for domain, domain_keywords in domain_categories.items():
            score = 0
            # ì§ì ‘ í…ìŠ¤íŠ¸ ë§¤ì¹­
            for keyword in domain_keywords:
                if keyword in text:
                    score += 2
            # ì¶”ì¶œëœ í‚¤ì›Œë“œ ë§¤ì¹­
            for extracted_kw in keywords:
                if extracted_kw in domain_keywords:
                    score += 1
            domain_scores[domain] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë„ë©”ì¸ ë°˜í™˜ (ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°)
        max_domain = max(domain_scores.items(), key=lambda x: x[1])
        if max_domain[1] >= 2:  # ìµœì†Œ 2ì  ì´ìƒ
            return max_domain[0]
        return "general"
    
    def _extract_last_intent(self, history: List[Dict[str, Any]]) -> str:
        """ë§ˆì§€ë§‰ ì˜ë„ ì¶”ì¶œ"""
        if not history:
            return "unknown"
        
        last_message = history[-1]["user_message"].lower()
        
        # ê°„ë‹¨í•œ ì˜ë„ ë¶„ë¥˜
        if any(word in last_message for word in ["ìš”ì•½", "ì •ë¦¬", "ì„¤ëª…"]):
            return "summarization"
        elif any(word in last_message for word in ["ë¹„êµ", "ì°¨ì´", "ë‹¤ë¥¸"]):
            return "comparison"
        elif any(word in last_message for word in ["ë°©ë²•", "ì–´ë–»ê²Œ", "ì ˆì°¨"]):
            return "instruction"
        elif any(word in last_message for word in ["ì–¸ì œ", "ì‹œê¸°", "ì¼ì •"]):
            return "temporal"
        else:
            return "information_seeking"
    
    def _generate_conversation_summary(self, history: List[Dict[str, Any]]) -> str:
        """ëŒ€í™” ìš”ì•½ ìƒì„±"""
        if not history:
            return ""
        
        topics = []
        for exchange in history[-3:]:  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ
            user_msg = exchange["user_message"]
            if len(user_msg) > 10:
                topics.append(user_msg[:50] + "...")
        
        return " â†’ ".join(topics)
    
    async def _enhance_query(self, original_query: str, context: ConversationContext) -> str:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì¿¼ë¦¬ ê°•í™” (ì£¼ì œ ì „í™˜ ì‹œ ì»¨í…ìŠ¤íŠ¸ ë¬´ì‹œ)"""
        try:
            # ì£¼ì œ ì—°ì†ì„±ì´ ë‚®ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ í™œìš© ì œí•œ
            if context.topic_continuity_score < 0.3:
                logger.info(f"ğŸš« ì£¼ì œ ì „í™˜ ê°ì§€ (ì—°ì†ì„±={context.topic_continuity_score:.2f}) - ì»¨í…ìŠ¤íŠ¸ ê°•í™” ìƒëµ")
                return original_query
            
            # ğŸ†• ì§€ì‹œëŒ€ëª…ì‚¬ê°€ ë§ê³  ì—°ì†ì„±ì´ ë†’ìœ¼ë©´ LLM ê¸°ë°˜ ì¬ì‘ì„± ì‹œë„
            query_lower = original_query.lower()
            pronoun_count = sum(1 for p in ["ê·¸ê²ƒ", "ê·¸ê±°", "ì´ê²ƒ", "ì´ê±°", "ê·¸", "ì´"] if p in query_lower)
            
            if pronoun_count >= 2 and context.topic_continuity_score > 0.6:
                logger.info(f"ğŸ”„ ì§€ì‹œëŒ€ëª…ì‚¬ ë‹¤ìˆ˜ ê°ì§€ ({pronoun_count}ê°œ) - LLM ê¸°ë°˜ ì§ˆì˜ë¬¸ ì¬ì‘ì„± ì‹œë„")
                llm_rewritten = await self._rewrite_query_with_llm(original_query, context)
                if llm_rewritten and llm_rewritten != original_query:
                    logger.info(f"âœï¸ LLM ì¬ì‘ì„± ì„±ê³µ: '{original_query}' â†’ '{llm_rewritten}'")
                    return llm_rewritten
            
            enhanced_parts = [original_query]
            
            # 1. ëˆ„ì  í‚¤ì›Œë“œ ì¶”ê°€ (ì¤‘ìš”ë„ ë†’ì€ ìˆœ) - ì—°ì†ì„±ì´ ì¶©ë¶„í•  ë•Œë§Œ
            if context.accumulated_keywords and context.topic_continuity_score > 0.5:
                relevant_keywords = []
                for keyword in context.accumulated_keywords[:5]:
                    if keyword.lower() not in original_query.lower():
                        relevant_keywords.append(keyword)
                
                if relevant_keywords:
                    enhanced_parts.append(f"ê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(relevant_keywords[:3])}")
            
            # 2. ì£¼ì œ ì—°ì†ì„±ì´ ë†’ì€ ê²½ìš°ì—ë§Œ ì´ì „ ë§¥ë½ íŒíŠ¸ ì¶”ê°€
            if context.topic_continuity_score > 0.6:
                enhanced_parts.append(f"(ì´ì „ ëŒ€í™” ì£¼ì œì™€ ì—°ê´€)")
            
            # 3. ì˜ë„ë³„ íŒíŠ¸ ì¶”ê°€ - ì—°ì†ì„± ê³ ë ¤
            if context.topic_continuity_score > 0.4:
                if context.last_intent == "comparison" and "ë¹„êµ" not in original_query:
                    enhanced_parts.append("ë¹„êµ ë¶„ì„")
                elif context.last_intent == "summarization" and "ìš”ì•½" not in original_query:
                    enhanced_parts.append("ìš”ì•½ ì •ë¦¬")
            
            enhanced_query = " ".join(enhanced_parts)
            
            # ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ì œí•œ
            if len(enhanced_query) > 300:
                enhanced_query = enhanced_query[:300] + "..."
            
            return enhanced_query
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ê°•í™” ì‹¤íŒ¨: {e}")
            return original_query
    
    async def _rewrite_query_with_llm(self, original_query: str, context: ConversationContext) -> Optional[str]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì§€ì‹œëŒ€ëª…ì‚¬ê°€ í¬í•¨ëœ ì§ˆì˜ë¬¸ì„ ëª…í™•í•œ ì§ˆì˜ë¬¸ìœ¼ë¡œ ì¬ì‘ì„±
        
        Args:
            original_query: ì›ë³¸ ì§ˆë¬¸ (ì§€ì‹œëŒ€ëª…ì‚¬ í¬í•¨)
            context: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ì¬ì‘ì„±ëœ ì§ˆë¬¸ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            from app.core.config import settings
            
            # ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±
            context_summary = context.conversation_summary or ""
            if context.accumulated_keywords:
                context_summary += f"\nì´ì „ ëŒ€í™” ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(context.accumulated_keywords[:5])}"
            
            # LLM í”„ë¡¬í”„íŠ¸
            rewrite_prompt = f"""ë‹¹ì‹ ì€ ëŒ€í™” ë¬¸ë§¥ì„ ì´í•´í•˜ê³  ì§ˆë¬¸ì„ ëª…í™•í•˜ê²Œ ì¬ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.

ì´ì „ ëŒ€í™” ë‚´ìš©:
{context_summary}

í˜„ì¬ ì§ˆë¬¸:
{original_query}

ìœ„ ì§ˆë¬¸ì— í¬í•¨ëœ ì§€ì‹œëŒ€ëª…ì‚¬(ê·¸ê²ƒ, ì´ê²ƒ, ê·¸, ì´ ë“±)ë¥¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ì¸ ëª…ì‚¬ë¡œ ì¹˜í™˜í•˜ê³ , ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.

ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤."""

            # ì„¤ì •ëœ LLM ì œê³µìì— ë”°ë¼ í˜¸ì¶œ
            config = settings.get_query_rewrite_config()
            response = None
            
            if config["provider"] == "azure_openai":
                from app.services.ai_service import ai_service
                response = await ai_service.generate_completion(
                    prompt=rewrite_prompt,
                    max_tokens=config["max_tokens"],
                    temperature=config["temperature"]
                )
            elif config["provider"] == "bedrock":
                from app.services.core.bedrock_service import bedrock_service
                response = await bedrock_service.generate_text_claude(
                    prompt=rewrite_prompt,
                    max_tokens=config["max_tokens"],
                    temperature=config["temperature"]
                )
            
            if response and len(response.strip()) > 0:
                rewritten = response.strip()
                # ë„ˆë¬´ ê¸¸ë©´ ì›ë³¸ ë°˜í™˜
                if len(rewritten) > 300:
                    logger.warning(f"âš ï¸ ì¬ì‘ì„± ê²°ê³¼ê°€ ë„ˆë¬´ ê¹€ ({len(rewritten)}ì) - ì›ë³¸ ì‚¬ìš©")
                    return None
                logger.info(f"âœï¸ ì§ˆì˜ë¬¸ ì¬ì‘ì„± ì™„ë£Œ ({config['provider']}): '{original_query[:30]}...' â†’ '{rewritten[:50]}...'")
                return rewritten
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ LLM ì§ˆì˜ë¬¸ ì¬ì‘ì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _has_explicit_reference(self, query: str) -> bool:
        """
        ì§ˆë¬¸ì— ì´ì „ ëŒ€í™”ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì°¸ì¡°í•˜ëŠ” í‘œí˜„ì´ ìˆëŠ”ì§€ í™•ì¸
        
        Args:
            query: í˜„ì¬ ì§ˆë¬¸
            
        Returns:
            bool: ëª…ì‹œì  ì°¸ì¡° í‘œí˜„ì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False
        """
        try:
            query_lower = query.lower().strip()
            
            # 1. ëª…ì‹œì  ì°¸ì¡° íŒ¨í„´ í™•ì¸
            for pattern in self.explicit_reference_patterns:
                if pattern in query_lower:
                    logger.info(f"ğŸ¯ ëª…ì‹œì  ì°¸ì¡° íŒ¨í„´ íƒì§€: '{pattern}' in '{query[:50]}...'")
                    return True
            
            # 2. ë¬¸ì¥ êµ¬ì¡° ê¸°ë°˜ ì°¸ì¡° í™•ì¸
            reference_structures = [
                # "ê·¸ + ëª…ì‚¬" íŒ¨í„´
                "ê·¸ ë‚´ìš©", "ê·¸ ë‹µë³€", "ê·¸ ê²°ê³¼", "ê·¸ ë¬¸ì„œ", "ê·¸ ìë£Œ", "ê·¸ ì •ë³´",
                "ê·¸ ë°©ë²•", "ê·¸ ê³¼ì •", "ê·¸ ì‹œìŠ¤í…œ", "ê·¸ ì œí’ˆ", "ê·¸ ì„œë¹„ìŠ¤",
                
                # "ì´ + ëª…ì‚¬" íŒ¨í„´ (ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì§€ì¹­)
                "ì´ ë¬¸ì œ", "ì´ ì‚¬ì•ˆ", "ì´ ì£¼ì œ", "ì´ ê±´", "ì´ ì¼€ì´ìŠ¤",
                
                # ë¹„êµ/ëŒ€ì¡° í‘œí˜„ (ì´ì „ ë‚´ìš©ê³¼ ì—°ê²°)
                "ë°˜ë©´ì—", "ê·¸ì— ë¹„í•´", "ì´ì™€ ë‹¬ë¦¬", "ê·¸ì™€ ë°˜ëŒ€ë¡œ", "ëŒ€ì‹ ì—",
                
                # ì¶”ê°€ ì§ˆë¬¸ íŒ¨í„´
                "ë˜ ë­ê°€", "ë‹¤ë¥¸ ì ì€", "ì¶”ê°€ë¡œ ì•Œê³  ì‹¶ì€", "ë” ê¶ê¸ˆí•œ",
                "ê·¸ ì™¸ì—", "ê·¸ ë°–ì—", "í•œí¸ìœ¼ë¡œëŠ”", "ë‹¤ë¥¸ í•œí¸ìœ¼ë¡œëŠ”"
            ]
            
            for structure in reference_structures:
                if structure in query_lower:
                    logger.info(f"ğŸ¯ ì°¸ì¡° êµ¬ì¡° íƒì§€: '{structure}' in '{query[:50]}...'")
                    return True
            
            # 3. ë¬¸ë§¥ìƒ ì—°ì†ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ í™•ì¸
            continuity_indicators = [
                # ìˆœì„œ/ë‹¨ê³„ í‘œí˜„
                "ë‹¤ìŒìœ¼ë¡œ", "ê·¸ ë‹¤ìŒ", "ë‘ ë²ˆì§¸ë¡œ", "ë§ˆì§€ë§‰ìœ¼ë¡œ", "ì²« ë²ˆì§¸ë¡œ",
                
                # ê²°ê³¼/ê²°ë¡  í‘œí˜„  
                "ê²°ê³¼ì ìœ¼ë¡œ", "ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê²°êµ­", "ìµœì¢…ì ìœ¼ë¡œ",
                
                # ì¡°ê±´/ê°€ì • í‘œí˜„ (ì´ì „ ë‚´ìš© ê¸°ë°˜)
                "ë§Œì•½ ê·¸ë ‡ë‹¤ë©´", "ê·¸ ê²½ìš°", "ê·¸ëŸ° ìƒí™©ì—ì„œ", "ê·¸ëŸ´ ë•ŒëŠ”"
            ]
            
            for indicator in continuity_indicators:
                if indicator in query_lower:
                    logger.info(f"ğŸ¯ ì—°ì†ì„± ì§€ì‹œì–´ íƒì§€: '{indicator}' in '{query[:50]}...'")
                    return True
            
            # 4. ì§ˆë¬¸ì˜ ë…ë¦½ì„± í™•ì¸ (ëª…í™•í•œ ë…ë¦½ ì§ˆë¬¸ íŒ¨í„´)
            independent_patterns = [
                "ì´ë€", "ëŠ” ë¬´ì—‡", "ì— ëŒ€í•´", "ë€ ë¬´ì—‡", "ì˜ ì •ì˜", "ë¼ëŠ” ê²ƒ",
                "ì„¤ëª…í•´", "ì•Œë ¤ì¤˜", "ê°€ë¥´ì³", "ì†Œê°œí•´", "ì˜ˆì‹œ", "ì˜ˆë¥¼ ë“¤ì–´"
            ]
            
            # ë…ë¦½ì  ì§ˆë¬¸ì´ë©´ì„œ ë‹¤ë¥¸ ì°¸ì¡° í‘œí˜„ì´ ì—†ëŠ” ê²½ìš°
            is_likely_independent = any(pattern in query_lower for pattern in independent_patterns)
            if is_likely_independent and len(query.split()) <= 3:
                logger.info(f"ğŸ“ ë…ë¦½ì  ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨: '{query}'")
                return False
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ ëª…ì‹œì  ì°¸ì¡° íƒì§€ ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ False ë°˜í™˜ (ë…ë¦½ì  ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬)
            return False
    
    async def rewrite_query_for_image_search(
        self,
        original_query: str,
        image_count: int = 1,
        selected_documents: Optional[List[Any]] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """
        ì´ë¯¸ì§€ ì²¨ë¶€ ì‹œ ì§ˆì˜ë¬¸ ì¬ì‘ì„±
        "ì²¨ë¶€ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©" â†’ "ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ë‚´ìš©"
        
        Args:
            original_query: ì›ë³¸ ì§ˆë¬¸
            image_count: ì²¨ë¶€ëœ ì´ë¯¸ì§€ ê°œìˆ˜
            selected_documents: ì„ íƒëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            rewritten_query: ì¬ì‘ì„±ëœ ì§ˆë¬¸
            metadata: ì¬ì‘ì„± ë©”íƒ€ë°ì´í„°
        """
        try:
            query_lower = original_query.lower().strip()
            
            # ì´ë¯¸ì§€ ì°¸ì¡° íŒ¨í„´ ê°ì§€
            image_reference_patterns = [
                "ì²¨ë¶€", "ì—…ë¡œë“œ", "ì˜¬ë¦°", "ì´ë¯¸ì§€", "ì‚¬ì§„", "ê·¸ë¦¼"
            ]
            
            has_image_reference = any(pattern in query_lower for pattern in image_reference_patterns)
            
            if not has_image_reference:
                # ì´ë¯¸ì§€ ì°¸ì¡°ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
                return original_query, {"rewritten": False, "reason": "no_image_reference"}
            
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì°¸ì¡° ì§ˆë¬¸ ê°ì§€: '{original_query}'")
            
            # ì§ˆì˜ ìœ í˜• ë¶„ì„
            is_asking_content = any(word in query_lower for word in ["ë‚´ìš©", "ì„¤ëª…", "ì•Œë ¤", "ë¬´ì—‡", "ë­"])
            is_asking_comparison = any(word in query_lower for word in ["ë¹„êµ", "ì°¨ì´", "ìœ ì‚¬", "ê°™ì€", "ë‹¤ë¥¸"])
            is_asking_explanation = any(word in query_lower for word in ["ì™œ", "ì´ìœ ", "ì–´ë–»ê²Œ", "ë°©ë²•"])
            
            # ì¬ì‘ì„± í…œí”Œë¦¿ ì„ íƒ
            if is_asking_content:
                rewritten_query = "ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì™€ ì‹œê°ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì•„ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            elif is_asking_comparison:
                rewritten_query = "ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì™€ ì„ íƒëœ ë¬¸ì„œì— ìˆëŠ” ì´ë¯¸ì§€ë“¤ì„ ë¹„êµí•˜ì—¬ ìœ ì‚¬í•œ ì´ë¯¸ì§€ì™€ ê·¸ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            elif is_asking_explanation:
                rewritten_query = "ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì™€ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì•„ì„œ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            else:
                rewritten_query = "ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì™€ ì‹œê°ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ë° ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•´ì£¼ì„¸ìš”."
            
            # ì„ íƒëœ ë¬¸ì„œ ì •ë³´ ì¶”ê°€
            if selected_documents and len(selected_documents) > 0:
                doc_names = [doc.fileName if hasattr(doc, 'fileName') else str(doc) for doc in selected_documents[:2]]
                if len(doc_names) == 1:
                    rewritten_query += f" íŠ¹íˆ '{doc_names[0]}' ë¬¸ì„œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”."
                else:
                    rewritten_query += f" íŠ¹íˆ '{doc_names[0]}' ë“± ì„ íƒëœ ë¬¸ì„œë“¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”."
            
            logger.info(f"âœï¸ ì§ˆì˜ë¬¸ ì¬ì‘ì„±: '{original_query}' â†’ '{rewritten_query}'")
            
            return rewritten_query, {
                "rewritten": True,
                "original_query": original_query,
                "rewritten_query": rewritten_query,
                "image_count": image_count,
                "query_type": "image_search",
                "intent": "content" if is_asking_content else "comparison" if is_asking_comparison else "explanation"
            }
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì§ˆì˜ë¬¸ ì¬ì‘ì„± ì‹¤íŒ¨: {e}")
            return original_query, {"rewritten": False, "reason": "error", "error": str(e)}
    
    async def analyze_query_with_intent(
        self,
        original_query: str,
        conversation_history: List[Dict[str, Any]],
        document_ids: Optional[List[int]] = None,
        container_ids: Optional[List[int]] = None
    ) -> Dict[str, Any]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆì˜ë¬¸ ì¬ì‘ì„± + ì˜ë„ ë¶„ë¥˜ + í•„ìš” ë„êµ¬ íŒë‹¨
        
        Args:
            original_query: ì›ë³¸ ì§ˆì˜ë¬¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            document_ids: ì„ íƒëœ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            container_ids: ì„ íƒëœ ì»¨í…Œì´ë„ˆ ID ë¦¬ìŠ¤íŠ¸
            
        Returns:
            {
                "rewritten_query": str,  # ì¬ì‘ì„±ëœ ì§ˆì˜ë¬¸
                "intent": str,  # summarization | search | comparison | ppt_generation | unsupported
                "confidence": float,  # ì˜ë„ ë¶„ë¥˜ ì‹ ë¢°ë„ (0~1)
                "required_tools": List[str],  # document_loader | hybrid_search | ppt_generator
                "parameters": dict,  # ë„êµ¬ë³„ íŒŒë¼ë¯¸í„°
                "reasoning": str  # íŒë‹¨ ê·¼ê±°
            }
        """
        try:
            from app.core.config import settings
            import json
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
            context = await self._analyze_conversation_context(conversation_history, original_query)
            context_summary = context.conversation_summary or ""
            if context.accumulated_keywords:
                context_summary += f"\nì´ì „ ëŒ€í™” í‚¤ì›Œë“œ: {', '.join(context.accumulated_keywords[:5])}"
            
            # ë¬¸ì„œ/ì»¨í…Œì´ë„ˆ ì»¨í…ìŠ¤íŠ¸
            doc_context = ""
            if document_ids:
                doc_context = f"\nì„ íƒëœ ë¬¸ì„œ: {len(document_ids)}ê°œ (ID: {document_ids})"
            if container_ids:
                doc_context += f"\nì„ íƒëœ ì»¨í…Œì´ë„ˆ: {len(container_ids)}ê°œ (ID: {container_ids})"
            
            # í†µí•© ë¶„ì„ í”„ë¡¬í”„íŠ¸
            analysis_prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ëŠ” AIì…ë‹ˆë‹¤.

## ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
{context_summary}

## í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
{doc_context}

## ì‚¬ìš©ì ì§ˆì˜
{original_query}

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ë° ì¡°ê±´
1. **document_loader**: íŠ¹ì • ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ ë¡œë“œ
   - ì¡°ê±´: "ìš”ì•½", "ì •ë¦¬", "ë‚´ìš© í™•ì¸" ë“± + ë¬¸ì„œê°€ ëª…ì‹œì ìœ¼ë¡œ ì„ íƒë¨
   - ì˜ë„: summarization

2. **hybrid_search**: ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ íƒìƒ‰
   - ì¡°ê±´: ì¼ë°˜ì ì¸ ì§ˆë¬¸, ì •ë³´ ê²€ìƒ‰, ë¹„êµ ë¶„ì„
   - ì˜ë„: search, comparison

3. **ppt_generator**: PowerPoint í”„ë ˆì  í…Œì´ì…˜ ìƒì„±
   - ì¡°ê±´: "PPT ë§Œë“¤ì–´ì¤˜", "ìŠ¬ë¼ì´ë“œ ìƒì„±", "ë°œí‘œ ìë£Œ" ë“±
   - ì˜ë„: ppt_generation

4. **unsupported**: ìœ„ ë„êµ¬ë¡œ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥í•œ ìš”ì²­
   - ì¡°ê±´: ë„êµ¬ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ìš”ì²­

## ë¶„ì„ ì§€ì¹¨
- ì§€ì‹œëŒ€ëª…ì‚¬(ê·¸ê²ƒ, ì´ê²ƒ, ì²¨ë¶€, ìœ„ ë“±)ëŠ” êµ¬ì²´ì ìœ¼ë¡œ ëŒ€ì²´
- ë¬¸ì„œ ì„ íƒ + ìš”ì•½/ì •ë¦¬ ìš”ì²­ = document_loader ì‚¬ìš©
- íƒìƒ‰ì  ì§ˆë¬¸ = hybrid_search ì‚¬ìš©
- ì˜ë„ ì‹ ë¢°ë„ëŠ” 0~1 ë²”ìœ„ë¡œ í‰ê°€

## ì¶œë ¥ í˜•ì‹ (JSONë§Œ ë°˜í™˜, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´)
{{
  "rewritten_query": "ëª…í™•í•˜ê²Œ ì¬ì‘ì„±ëœ ì§ˆì˜ë¬¸",
  "intent": "summarization | search | comparison | ppt_generation | unsupported",
  "confidence": 0.95,
  "required_tools": ["document_loader"],
  "parameters": {{
    "document_ids": [5],
    "summarization_type": "comprehensive"
  }},
  "reasoning": "íŒë‹¨ ê·¼ê±° ì„¤ëª…"
}}"""

            # ì„¤ì •ëœ LLM ì œê³µìì— ë”°ë¼ í˜¸ì¶œ
            config = settings.get_query_rewrite_config()
            response_text = None
            
            if config["provider"] == "azure_openai":
                from app.services.ai_service import ai_service
                response_text = await ai_service.generate_completion(
                    prompt=analysis_prompt,
                    max_tokens=config["max_tokens"],
                    temperature=config["temperature"]
                )
            elif config["provider"] == "bedrock":
                from app.services.core.bedrock_service import bedrock_service
                response_text = await bedrock_service.generate_text_claude(
                    prompt=analysis_prompt,
                    max_tokens=config["max_tokens"],
                    temperature=config["temperature"]
                )
            
            if not response_text:
                raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # JSON íŒŒì‹±
            response_text = response_text.strip()
            # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            result = json.loads(response_text)
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            result.setdefault("rewritten_query", original_query)
            result.setdefault("intent", "search")
            result.setdefault("confidence", 0.5)
            result.setdefault("required_tools", ["hybrid_search"])
            result.setdefault("parameters", {})
            result.setdefault("reasoning", "ê¸°ë³¸ ê²€ìƒ‰ ì²˜ë¦¬")
            
            logger.info(f"ğŸ¯ ì§ˆì˜ ë¶„ì„ ì™„ë£Œ ({config['provider']}): intent={result['intent']}, confidence={result['confidence']}, tools={result['required_tools']}")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}, ì‘ë‹µ: {response_text[:200]}")
            return {
                "rewritten_query": original_query,
                "intent": "search",
                "confidence": 0.3,
                "required_tools": ["hybrid_search"],
                "parameters": {},
                "reasoning": f"JSON íŒŒì‹± ì‹¤íŒ¨ - ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±: {e}"
            }
        except Exception as e:
            logger.error(f"âŒ ì§ˆì˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "rewritten_query": original_query,
                "intent": "search",
                "confidence": 0.3,
                "required_tools": ["hybrid_search"],
                "parameters": {},
                "reasoning": f"ë¶„ì„ ì‹¤íŒ¨ - ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±: {e}"
            }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
conversation_context_service = ConversationContextService()

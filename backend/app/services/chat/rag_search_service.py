"""
ğŸ” RAG ì „ìš© ê²€ìƒ‰ ì„œë¹„ìŠ¤
====================

RAGë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° ìµœì í™”:
- ì˜ë„ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- ì‹œë§¨í‹± ìœ ì‚¬ë„ + í‚¤ì›Œë“œ ë§¤ì¹­
- ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ìµœì í™”
- ê²€ìƒ‰ ê²°ê³¼ ë¦¬ë­í‚¹
- ë©€í‹°í„´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í™œìš©
"""

import logging
import re
import json
import time
import os
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from dataclasses import dataclass
import copy

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text, and_, or_

# Multi-vendor AI services for embedding and reranking
from app.services.core.ai_service import ai_service
from app.services.core.korean_nlp_service import korean_nlp_service
from app.services.chat.conversation_context_service import conversation_context_service
from app.services.search.query_pipeline import process_user_query  # í†µí•© íŒŒì´í”„ë¼ì¸
from app.core.config import settings

logger = logging.getLogger(__name__)

@dataclass
class RAGSearchParams:
    """RAG ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜"""
    query: str
    container_ids: Optional[List[str]] = None
    document_ids: Optional[List[Any]] = None  # íŠ¹ì • ë¬¸ì„œë¡œ ì œí•œ (str/int í˜¼ìš© ì§€ì›)
    limit: int = 10
    threshold: float = 0.2  # (legacy) â€“ ì‚¬ìš© ì•ˆí•¨, similarity_threshold ì‚¬ìš©
    max_chunks: int = 10
    similarity_threshold: float = 0.25  # ê´€ë ¨ì„± í•„í„°ë§ ì„ê³„ê°’ (0.4 â†’ 0.25ë¡œ ì™„í™”, recall í–¥ìƒ)
    keyword_boost: float = 0.5  # í‚¤ì›Œë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì¦ê°€ (í•œêµ­ì–´ì—ì„œ ë” ì •í™•)
    semantic_boost: float = 0.4  # ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ê°ì†Œ (í•œêµ­ì–´ ì„ë² ë”© í•œê³„ ê³ ë ¤)
    use_reranking: bool = True
    reranking: bool = True
    context_window: int = 4000  # í† í° ìˆ˜
    search_mode: str = "hybrid"  # "semantic", "keyword", "hybrid"
    original_query: Optional[str] = None  # ë©€í‹°í„´ ê°•í™” ì „ ì‚¬ìš©ì ì›ë¬¸ ë³´ì¡´

@dataclass
class RAGSearchResult:
    """RAG ê²€ìƒ‰ ê²°ê³¼"""
    # ìµœì¢… í›„ë³´ ì²­í¬(í† í° êµ¬ì„± ì „, ì»·/ë¦¬ë­í‚¹ ì´í›„ ë‚¨ì€ ì „ì²´ í›„ë³´)
    chunks: List[Dict[str, Any]]
    # ì‹¤ì œ LLM ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ì²­í¬(í† í° ì œí•œ ë°˜ì˜)
    used_chunks: List[Dict[str, Any]]
    context_text: str
    total_tokens: int
    search_stats: Dict[str, Any]
    reranking_applied: bool

class RAGSearchService:
    """RAG ì „ìš© ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.ai_service = ai_service
        self.nlp_service = korean_nlp_service
        self.max_context_tokens = 4000
        self.chunk_overlap_ratio = 0.1
        # PPT ì˜ë„ ê°ì§€ìš© í‚¤ì›Œë“œ
        self._ppt_query_keywords = [
            "ppt", "pptx", "presentation", "ìŠ¬ë¼ì´ë“œ", "ë°œí‘œìë£Œ", "ë°œí‘œ ìë£Œ", "í”„ë ˆì  í…Œì´ì…˜", "í”„ë¦¬ì  í…Œì´ì…˜", "ì œí’ˆì†Œê°œ", "ì†Œê°œì„œ"
        ]
    
    def _detect_query_language(self, query: str) -> str:
        """
        ì¿¼ë¦¬ì˜ ì£¼ìš” ì–¸ì–´ ê°ì§€ (í•œêµ­ì–´/ì˜ì–´/í˜¼í•©)
        
        Returns:
            'ko': í•œêµ­ì–´ ìœ„ì£¼
            'en': ì˜ì–´ ìœ„ì£¼
            'mixed': í˜¼í•©
        """
        if not query:
            return 'ko'
        
        # í•œê¸€ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
        korean_chars = len([c for c in query if '\uac00' <= c <= '\ud7a3'])
        english_chars = len([c for c in query if c.isalpha() and c.isascii()])
        total_chars = korean_chars + english_chars
        
        if total_chars == 0:
            return 'ko'  # ê¸°ë³¸ê°’
        
        korean_ratio = korean_chars / total_chars
        
        if korean_ratio > 0.6:
            return 'ko'
        elif korean_ratio < 0.2:
            return 'en'
        else:
            return 'mixed'
    
    async def search_for_rag_context(
        self,
        session: AsyncSession,
        search_params: RAGSearchParams,
        session_id: Optional[str] = None,
        enable_multiturn_context: bool = True
    ) -> RAGSearchResult:
        """
        RAGë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        
        Args:
            session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            search_params: ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜
            
        Returns:
            RAG ê²€ìƒ‰ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì‹œì‘: '{search_params.query[:50]}...' "
                       f"(ëª¨ë“œ: {search_params.search_mode})")

            # --- ë¬¸ì„œ ID ì •í•©ì„± ë³´ì • (formatted/file_id â†’ ì‹¤ì œ file_bss_info_sno ì •ê·œí™”) ---
            if search_params.document_ids:
                search_params.document_ids = self._normalize_document_ids(search_params.document_ids)

            # --- ë©€í‹°í„´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í™œìš© (Option B ê°œì„ : ì›ë¬¸ ë³´ì¡´ + í•„ìš”ì‹œ 2ì°¨ ì‹œë„) ---
            if not search_params.original_query:
                search_params.original_query = search_params.query

            enhanced_query = search_params.query  # ê¸°ë³¸ê°’: ë³€ê²½ ì—†ìŒ
            context_metadata = {"context_used": False}

            if enable_multiturn_context and session_id:
                try:
                    enhanced_candidate, context_metadata = await conversation_context_service.enhance_query_with_context(
                        current_query=search_params.query,
                        session_id=session_id,
                        db_session=session
                    )
                    # ì›ë¬¸ê³¼ ë‹¤ë¥´ë©´ í›„ë³´ë¡œë§Œ ë³´ê´€ (1ì°¨ ì˜ë¯¸ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
                    if context_metadata.get("context_used") and enhanced_candidate != search_params.query:
                        enhanced_query = enhanced_candidate
                        logger.info(f"ğŸ”— ì»¨í…ìŠ¤íŠ¸ ê°•í™” í›„ë³´ í™•ë³´ (ì§€ì—° ì ìš©): '{search_params.query[:60]}' â†’ '{enhanced_query[:60]}'")
                except Exception as ctx_error:
                    logger.warning(f"âš ï¸ ë©€í‹°í„´ ì»¨í…ìŠ¤íŠ¸ ì ìš© ì‹¤íŒ¨, ì›ë³¸ ìš°ì„  ì‚¬ìš©: {ctx_error}")

            # --- Adaptive Threshold (ê´€ë ¨ì„± ì—†ëŠ” ë¬¸ì„œ í•„í„°ë§ ê°•í™”) ---
            base_threshold = search_params.similarity_threshold
            qlen = len(search_params.query)
            # ì§§ì€ ì¿¼ë¦¬ì¼ìˆ˜ë¡ ë” ì—„ê²©í•œ ì„ê³„ê°’ ì ìš© (ê´€ë ¨ì„± í™•ë³´)
            # âš ï¸ 2025-10-17: ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ recall í–¥ìƒ (0.268 ì •ë„ì˜ ìœ ì‚¬ë„ë„ ë§¤ì¹­ë˜ë„ë¡)
            if qlen < 15:
                adaptive = max(0.25, base_threshold - 0.10)  # 0.35 â†’ 0.25ë¡œ ì¶”ê°€ ì™„í™”
            elif qlen < 40:
                adaptive = max(0.22, base_threshold - 0.15)  # 0.28 â†’ 0.22ë¡œ ì¶”ê°€ ì™„í™” (ì¤‘ìš”!)
            elif qlen > 200:
                adaptive = max(0.20, base_threshold - 0.15)  # 0.25 â†’ 0.20ë¡œ ì¶”ê°€ ì™„í™”
            elif qlen > 120:
                adaptive = max(0.22, base_threshold - 0.15)  # 0.28 â†’ 0.22ë¡œ ì¶”ê°€ ì™„í™”
            else:
                adaptive = max(0.22, base_threshold - 0.15)  # 0.28 â†’ 0.22ë¡œ ì¶”ê°€ ì™„í™”
            # í•„í„° ì ìš© ìƒíƒœì—ì„œë„ ë†’ì€ í’ˆì§ˆ ìœ ì§€
            # ê¸°ë³¸ 0.22, ì»¨í…Œì´ë„ˆ í•„í„° 0.20, ë¬¸ì„œ í•„í„° 0.18
            min_floor = 0.22  # 0.28 â†’ 0.22ë¡œ ì™„í™” (ì¤‘ìš”!)
            if search_params.container_ids:
                min_floor = 0.20  # 0.25 â†’ 0.20
            if search_params.document_ids:
                min_floor = 0.18  # 0.22 â†’ 0.18ë¡œ ì™„í™” (0.268 ë§¤ì¹­ ê°€ëŠ¥)
            adaptive = max(min_floor, adaptive)
            if abs(adaptive - search_params.similarity_threshold) > 1e-6:
                logger.info(f"ğŸšï¸ Adaptive similarity threshold (revised): {search_params.similarity_threshold:.2f} -> {adaptive:.2f} (len={qlen})")
                search_params.similarity_threshold = adaptive
            
            # 1ë‹¨ê³„: ì§ˆì˜ ë¶„ì„ ë° ì„ë² ë”© ìƒì„±
            query_analysis = await self._analyze_query(search_params.query)
            
            # 2ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ 1ì°¨ (ì›ë¬¸ ê¸°ë°˜)
            search_results = await self._execute_hybrid_search(
                session=session,
                search_params=search_params,
                query_analysis=query_analysis
            )

            def has_semantic(results: List[Dict[str, Any]]) -> bool:
                for r in results:
                    if r.get("search_type") in ("semantic", "hybrid") and r.get("semantic_score", 0) > 0:
                        return True
                return False

            # 2ì°¨: ì˜ë¯¸ ê²°ê³¼ ì „ë¬´ & ê°•í™” ì¿¼ë¦¬ ì¡´ì¬ ì‹œ ê°•í™” ì¿¼ë¦¬ ì¬ê²€ìƒ‰ (threshold ì¬ì„¤ì •)
            if not has_semantic(search_results) and enhanced_query != search_params.query:
                logger.info("ğŸ” 1ì°¨ ì˜ë¯¸ê²€ìƒ‰ ì‹¤íŒ¨ â€“ ê°•í™” ì¿¼ë¦¬ë¡œ ì¬ì‹œë„")
                sp2 = copy.deepcopy(search_params)
                sp2.query = enhanced_query
                # ê¸¸ì´ ê¸°ë°˜ ì¬ì ìš© (ê°•í™” ì¿¼ë¦¬ë„ í’ˆì§ˆ ìœ ì§€)
                qlen2 = len(sp2.query)
                if qlen2 > 120 and sp2.similarity_threshold > 0.35:
                    sp2.similarity_threshold = max(0.35, sp2.similarity_threshold - 0.03)
                query_analysis2 = await self._analyze_query(sp2.query)
                search_results2 = await self._execute_hybrid_search(
                    session=session,
                    search_params=sp2,
                    query_analysis=query_analysis2
                )
                if has_semantic(search_results2):
                    logger.info("âœ… ê°•í™” ì¿¼ë¦¬ ì¬ì‹œë„ì—ì„œ ì˜ë¯¸ ê²°ê³¼ í™•ë³´ â€“ êµì²´ ì ìš©")
                    search_results = search_results2
                    search_params.query = sp2.query  # í†µê³„ ì¼ê´€ì„± ìœ„í•´ ë°˜ì˜
                else:
                    logger.info("âš ï¸ ê°•í™” ì¿¼ë¦¬ ì¬ì‹œë„ë„ ì˜ë¯¸ê²°ê³¼ ì—†ìŒ â€“ 1ì°¨ ê²°ê³¼ ìœ ì§€")
            
            # 2.9ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ë¶„í¬ ê¸°ë°˜ ì»·ë¼ì¸ ë™ì  ì ìš© (ê³¼ë„í•œ ì €í’ˆì§ˆ ì œê±°)
            search_results = self._apply_dynamic_cutline(search_results, search_params)

            # 3ë‹¨ê³„: ì¤‘ë³µ ì œê±° (ë™ì¼ íŒŒì¼ì˜ ë™ì¼ ì²­í¬ ì œê±°)
            search_results = self._remove_duplicates(search_results)
            
            # 3.2ë‹¨ê³„: PPT ì˜ë„ ê°ì§€ ë° ê°€ì¤‘ì¹˜ ë¶€ì—¬
            ppt_intent = self._detect_ppt_intent(search_params.query)
            if ppt_intent and search_results:
                search_results = self._boost_for_ppt_intent(search_results)

            # 3.5ë‹¨ê³„: ì°¸ê³ ìë£Œ í’ˆì§ˆ ê²€ì¦ (ì£¼ì œ ì¼ì¹˜ë„ í™•ì¸)
            if search_results:
                search_results = await self._validate_reference_quality(
                    original_query=search_params.original_query or search_params.query,
                    current_query=search_params.query,
                    results=search_results,
                    relax_filter=bool(search_params.document_ids) or ppt_intent
                )
            
            # 4ë‹¨ê³„: ë¦¬ë­í‚¹ (í•„ìš”ì‹œ)
            if search_params.use_reranking and len(search_results) > search_params.max_chunks:
                search_results = await self._rerank_results(
                    query=search_params.query,
                    results=search_results,
                    target_count=search_params.max_chunks
                )
                reranking_applied = True
            else:
                search_results = search_results[:search_params.max_chunks]
                reranking_applied = False
            
            # 5ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_text, total_tokens, used_chunks = await self._build_context(
                chunks=search_results,
                max_tokens=search_params.context_window,
                ppt_mode=ppt_intent
            )
            
            # 6ë‹¨ê³„: ê²°ê³¼ í†µê³„ (ë©€í‹°í„´ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í¬í•¨)
            from app.core.config import settings
            search_stats = {
                "query_length": len(search_params.query),
                "total_candidates": len(search_results),
                "final_chunks": len(search_results),
                "avg_similarity": sum(chunk.get("similarity_score", 0) for chunk in search_results) / len(search_results) if search_results else 0,
                "search_time": time.time() - start_time,
                "search_mode": search_params.search_mode,
                "has_korean_keywords": len(query_analysis.get("korean_keywords", [])) > 0,
                "embedding_dimension": len(query_analysis.get("embedding", [])) if query_analysis.get("embedding") else 0,
                # í”„ë¡œë°”ì´ë” ì •ë³´ ì¶”ê°€
                "provider": settings.get_current_llm_provider(),
                "embedding_provider": settings.get_current_embedding_provider(),
                "llm_model": settings.get_current_llm_model(),
                "embedding_model": settings.get_current_embedding_model(),
                # ë©€í‹°í„´ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
                "multiturn_context": context_metadata.get("context_used", False),
                "original_query": context_metadata.get("original_query", search_params.query),
                "enhanced_query": context_metadata.get("enhanced_query", search_params.query),
                "topic_continuity": context_metadata.get("topic_continuity", 0.0),
                "accumulated_keywords": context_metadata.get("accumulated_keywords", [])
            }
            
            logger.info(f"âœ… RAG ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ì²­í¬, "
                       f"{total_tokens}í† í°, {search_stats['search_time']:.2f}ì´ˆ")
            
            return RAGSearchResult(
                chunks=search_results,
                used_chunks=used_chunks,
                context_text=context_text,
                total_tokens=total_tokens,
                search_stats=search_stats,
                reranking_applied=reranking_applied
            )
            
        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return RAGSearchResult(
                chunks=[],
                used_chunks=[],
                context_text="",
                total_tokens=0,
                search_stats={"error": str(e), "search_time": time.time() - start_time},
                reranking_applied=False
            )

    def _detect_ppt_intent(self, query: str) -> bool:
        try:
            if not isinstance(query, str):
                return False
            q = query.lower()
            # PPT í‚¤ì›Œë“œì™€ ìƒì„± ì˜ë„ í‚¤ì›Œë“œ ë™ì‹œ í¬í•¨ ì‹œ PPT ì˜ë„
            creation = any(k in q for k in ["ë§Œë“¤", "ì‘ì„±", "ìƒì„±", "ì œì‘"])
            has_ppt = any(k in q for k in self._ppt_query_keywords)
            return bool(creation and has_ppt)
        except Exception:
            return False

    def _boost_for_ppt_intent(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """PPT ì˜ë„ì¼ ë•Œ í…œí”Œë¦¿/ìƒ˜í”Œ/PPT íŒŒì¼ ë° í—¤ë”ì„± ì²­í¬ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬."""
        boosted: List[Dict[str, Any]] = []
        for r in results:
            score = self._score_of(r)
            fname = (r.get("file_name") or "").lower()
            sec = (r.get("section_title") or "").lower()
            content = (r.get("content") or "").lower()
            boost = 0.0
            # íŒŒì¼ëª… ê¸°ë°˜ ë¶€ìŠ¤íŒ…
            if any(k in fname for k in [".ppt", ".pptx", "template", "ìƒ˜í”Œ", "sample", "í…œí”Œë¦¿", "ì†Œê°œì„œ"]):
                boost += 0.15
            # ì„¹ì…˜/ë‚´ìš©ì— ëª©ì°¨Â·ê°œìš”Â·ìš”ì•½Â·ìŠ¬ë¼ì´ë“œ ë“± í‚¤ì›Œë“œ
            if any(k in sec for k in ["ëª©ì°¨", "ê°œìš”", "ìš”ì•½", "outline", "overview", "ìŠ¬ë¼ì´ë“œ", "title", "ì œëª©"]):
                boost += 0.08
            elif any(k in content for k in ["ëª©ì°¨", "ê°œìš”", "ìš”ì•½", "outline", "overview"]):
                boost += 0.05
            r2 = r.copy()
            # combined_scoreê°€ ìˆìœ¼ë©´ ê±°ê¸°ì—, ì—†ìœ¼ë©´ similarity_score ê¸°ë°˜ìœ¼ë¡œ í•©ì‚°
            base = r2.get("combined_score", r2.get("similarity_score", score))
            r2["combined_score"] = base + boost
            boosted.append(r2)
        # ë¶€ìŠ¤íŒ… ë°˜ì˜í•˜ì—¬ ì¬ì •ë ¬
        boosted_sorted = sorted(boosted, key=lambda x: x.get("combined_score", 0.0), reverse=True)
        logger.info(f"ğŸ¯ PPT ì˜ë„ ë¶€ìŠ¤íŒ… ì ìš©: ìƒìœ„ ì ìˆ˜ {boosted_sorted[0].get('combined_score', 0):.2f} (ì´ {len(boosted_sorted)})")
        return boosted_sorted
    
    async def _analyze_query(self, query: str) -> Dict[str, Any]:
        """
        ì§ˆì˜ ë¶„ì„ - í†µí•© íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        
        ë³€ê²½ ì‚¬í•­ (2025-10-17):
        - í†µí•© íŒŒì´í”„ë¼ì¸ (process_user_query) ì‚¬ìš©
        - ì¼ê´€ëœ ë¶ˆìš©ì–´ ì œê±° (UNIFIED_STOPWORDS)
        - RAG ì „ìš© ê²€ìƒ‰ ì „ëµ ì ìš©
        """
        try:
            # í†µí•© íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì§ˆì˜ ì²˜ë¦¬ (RAG ëª¨ë“œ)
            processed = await process_user_query(query, search_type="rag")
            
            logger.info(f"âœ… RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ: {processed.processing_time_ms:.1f}ms")
            logger.info(f"  - ì˜ë„: {processed.intent} (confidence: {processed.intent_confidence:.2f})")
            logger.info(f"  - í‚¤ì›Œë“œ: {processed.keywords} â†’ {processed.filtered_keywords}")
            logger.info(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {processed.filtered_keywords} (ì´ {len(processed.filtered_keywords)}ê°œ)")
            
            # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„ ìœ„í•œ ë³€í™˜
            return {
                "original_query": query,
                "korean_keywords": processed.filtered_keywords,  # í•„í„°ë§ëœ í‚¤ì›Œë“œ
                "named_entities": [],  # TODO: ê°œì²´ëª… ì¸ì‹ ì¶”ê°€
                "pos_tags": [],  # TODO: í’ˆì‚¬ íƒœê¹… ì¶”ê°€
                "embedding": processed.vector_embedding,
                "query_type": self._classify_query_type_from_intent(processed.intent),
                "intent_keywords": processed.filtered_keywords
            }
            
        except Exception as e:
            logger.error(f"âŒ RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {"original_query": query, "error": str(e)}
    
    def _classify_query_type_from_intent(self, intent: str) -> str:
        """ì˜ë„ë¥¼ RAG ì§ˆì˜ ìœ í˜•ìœ¼ë¡œ ë³€í™˜"""
        intent_mapping = {
            "qa_question": "question",
            "document_search": "information",
            "comparison": "information",
            "summarization": "general",
            "keyword_search": "general"
        }
        return intent_mapping.get(intent, "general")
    
    def _classify_query_type(self, query: str, nlp_result: Dict[str, Any]) -> str:
        """ì§ˆì˜ ìœ í˜• ë¶„ë¥˜"""
        keywords = nlp_result.get("keywords", [])  # ğŸ‘ˆ keywordsë¡œ ìˆ˜ì •
        
        # ì§ˆë¬¸í˜• íŒ¨í„´
        question_patterns = ["ë¬´ì—‡", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ", "ëˆ„ê°€", "?"]
        if any(pattern in query for pattern in question_patterns):
            return "question"
        
        # ì •ë³´ ê²€ìƒ‰í˜•
        info_patterns = ["ì„¤ëª…", "ì •ë³´", "ë‚´ìš©", "ìë£Œ", "ë¬¸ì„œ"]
        if any(keyword in info_patterns for keyword in keywords):
            return "information"
        
        # ì ˆì°¨/ë°©ë²• ê²€ìƒ‰í˜•
        procedure_patterns = ["ë°©ë²•", "ì ˆì°¨", "ê³¼ì •", "ë‹¨ê³„", "í”„ë¡œì„¸ìŠ¤"]
        if any(keyword in procedure_patterns for keyword in keywords):
            return "procedure"
        
        return "general"
    
    def _extract_intent_keywords(self, nlp_result: Dict[str, Any]) -> List[str]:
        """ì˜ë„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = nlp_result.get("keywords", [])  # ğŸ‘ˆ keywordsë¡œ ìˆ˜ì •
        entities = nlp_result.get("proper_nouns", [])  # ğŸ‘ˆ proper_nounsë¡œ ìˆ˜ì •
        
        # ì¤‘ìš” ëª…ì‚¬ ë° ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
        intent_keywords = []
        for keyword in keywords:
            if len(keyword) > 1:  # 2ê¸€ì ì´ìƒ
                intent_keywords.append(keyword)
        
        for entity in entities:
            if entity not in intent_keywords:
                intent_keywords.append(entity)
        
        return intent_keywords[:10]  # ìƒìœ„ 10ê°œë§Œ
        
        return intent_keywords[:10]  # ìƒìœ„ 10ê°œë§Œ
    
    async def _execute_hybrid_search(
        self,
        session: AsyncSession,
        search_params: RAGSearchParams,
        query_analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            if search_params.search_mode == "semantic":
                return await self._semantic_search(session, search_params, query_analysis)
            elif search_params.search_mode == "keyword":
                return await self._keyword_search(session, search_params, query_analysis)
            else:  # hybrid
                return await self._hybrid_search(session, search_params, query_analysis)
                
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"ê²€ìƒ‰ ëª¨ë“œ: {search_params.search_mode}, ì¿¼ë¦¬: '{search_params.query[:100]}...'")
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ ì‹œìŠ¤í…œì´ ê³„ì† ë™ì‘í•˜ë„ë¡ í•¨
            return []
    
    async def _semantic_search(
        self,
        session: AsyncSession,
        search_params: RAGSearchParams,
        query_analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰ (Option B: ë‹¤ë‹¨ê³„ threshold ì™„í™” ì¬ì‹œë„)"""
        if not query_analysis.get("embedding"):
            logger.warning("ì„ë² ë”©ì´ ì—†ì–´ ì˜ë¯¸ì  ê²€ìƒ‰ ë¶ˆê°€")
            return []

        embedding_vector = query_analysis["embedding"]

        # ì»¨í…ìŠ¤íŠ¸/í•„í„° ìƒíƒœ ê¸°ë°˜ í•˜í•œê°’ ë³´ì •
        # âš ï¸ 2025-10-17: í•˜í•œê°’ ëŒ€í­ ì™„í™” (recall í–¥ìƒ)
        min_floor = 0.22  # 0.28 â†’ 0.22 (ì „ì²´ ê²€ìƒ‰ ëª¨ë“œ)
        if search_params.container_ids:
            min_floor = 0.20  # 0.25 â†’ 0.20
        if search_params.document_ids:
            min_floor = 0.18  # 0.20 â†’ 0.18 (0.268 ë§¤ì¹­ ê°€ëŠ¥)

        attempt_threshold = max(min_floor, search_params.similarity_threshold)
        attempts = 0
        all_results: List[Dict[str, Any]] = []

        # ğŸ”·ğŸŸ§ í”„ë¡œë°”ì´ë”ë³„ ë²¡í„° ì»¬ëŸ¼ ë™ì  ì„ íƒ
        provider = settings.get_current_embedding_provider()
        embedding_dim = len(embedding_vector)
        
        if provider == 'bedrock' or embedding_dim == 1024:
            # AWS Bedrock: Titan 1024d
            vector_column = "tdc.aws_embedding_1024"
            vector_not_null = "tdc.aws_embedding_1024 IS NOT NULL"
            logger.info(f"[RAG-SEARCH] ğŸŸ§ AWS Bedrock ë²¡í„° ê²€ìƒ‰ (aws_embedding_1024, {embedding_dim}d)")
        elif provider == 'azure_openai' or embedding_dim == 1536:
            # Azure OpenAI: text-embedding-3-small 1536d
            vector_column = "tdc.azure_embedding_1536"
            vector_not_null = "tdc.azure_embedding_1536 IS NOT NULL"
            logger.info(f"[RAG-SEARCH] ğŸ”· Azure OpenAI ë²¡í„° ê²€ìƒ‰ (azure_embedding_1536, {embedding_dim}d)")
        else:
            # ë ˆê±°ì‹œ í´ë°±
            vector_column = "tdc.chunk_embedding"
            vector_not_null = "tdc.chunk_embedding IS NOT NULL"
            logger.warning(f"[RAG-SEARCH] âš ï¸ ë ˆê±°ì‹œ ë²¡í„° ì»¬ëŸ¼ í´ë°± ({embedding_dim}d)")

        while attempts < 3:
            base_query = f"""
                SELECT 
                    tdc.file_bss_info_sno,
                    tdc.chunk_index,
                    tdc.chunk_text,
                    tdc.page_number,
                    tdc.section_title,
                    tdc.keywords,
                    tdc.named_entities,
                    tdc.knowledge_container_id,
                    1 - ({vector_column} <=> :embedding_vector) as similarity_score,
                    fbi.file_lgc_nm as file_name
                FROM vs_doc_contents_chunks tdc
                JOIN tb_file_bss_info fbi ON tdc.file_bss_info_sno = fbi.file_bss_info_sno
                WHERE {vector_not_null}
                AND tdc.del_yn = 'N'
                AND fbi.del_yn = 'N'
                AND 1 - ({vector_column} <=> :embedding_vector) > :threshold
            """

            conditions = []
            if search_params.container_ids:
                conditions.append("AND tdc.knowledge_container_id = ANY(:container_ids)")
            if search_params.document_ids:
                conditions.append("AND fbi.file_bss_info_sno = ANY(:document_ids)")
                logger.info(f"ğŸ” ë¬¸ì„œ ID í•„í„°ë§ ì ìš©: {search_params.document_ids}")
            if conditions:
                base_query += " " + " ".join(conditions)
            base_query += " ORDER BY similarity_score DESC LIMIT :limit"

            query_sql = text(base_query)
            params = {
                "embedding_vector": f"[{','.join(map(str, embedding_vector))}]",
                "threshold": attempt_threshold,
                "limit": search_params.max_chunks * 2
            }
            if search_params.container_ids:
                params["container_ids"] = search_params.container_ids
            if search_params.document_ids:
                try:
                    params["document_ids"] = [int(doc_id) for doc_id in search_params.document_ids]
                except ValueError:
                    params["document_ids"] = search_params.document_ids

            result = await session.execute(query_sql, params)
            rows = result.fetchall()
            logger.info(f"ğŸ” ì˜ë¯¸ì  ê²€ìƒ‰ SQL ì‹¤í–‰ ê²°ê³¼ (attempt {attempts+1}, threshold={attempt_threshold:.2f}): {len(rows)}ê°œ í–‰")

            all_results = []
            for row in rows:
                similarity_score = float(row[8])
                
                # NaN ê°’ í•„í„°ë§
                import math
                if math.isnan(similarity_score) or math.isinf(similarity_score):
                    logger.warning(f"RAG ê²€ìƒ‰ì—ì„œ ì˜ëª»ëœ ì ìˆ˜ ë°œê²¬ (NaN/Inf): file={row[9]}")
                    continue
                
                all_results.append({
                    "file_bss_info_sno": row[0],
                    "chunk_index": row[1],
                    "content": row[2],
                    "page_number": row[3] if row[3] else 1,
                    "section_title": row[4] if row[4] else "",
                    "keywords": row[5] if row[5] else "",
                    "named_entities": row[6] if row[6] else "",
                    "container_id": row[7],
                    "similarity_score": similarity_score,
                    "file_name": row[9],
                    "chunk_type": "content",
                    "search_type": "semantic",
                    "metadata": {
                        "page_number": row[3] if row[3] else 1,
                        "section_title": row[4] if row[4] else "",
                        "keywords": row[5].split(',') if row[5] else [],
                        "named_entities": row[6].split(',') if row[6] else []
                    }
                })

            if all_results:
                logger.info(f"ğŸ”® ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ í™•ë³´ (attempt {attempts+1}): {len(all_results)}ê°œ")
                break

            # ë‹¤ìŒ ì‹œë„ â€“ threshold ì™„í™”
            attempt_threshold = max(min_floor, attempt_threshold - 0.05)
            attempts += 1
            if attempts < 3:
                logger.info(f"ğŸ”„ ì˜ë¯¸ì  ê²€ìƒ‰ ì¬ì‹œë„ ì¤€ë¹„ (ìƒˆ threshold={attempt_threshold:.2f})")

        return all_results

    async def _validate_reference_quality(
        self,
        original_query: str,
        current_query: str,
        results: List[Dict[str, Any]],
        relax_filter: bool = False
    ) -> List[Dict[str, Any]]:
        """ì°¸ê³ ìë£Œ í’ˆì§ˆ ê²€ì¦ - ì£¼ì œ ë¶ˆì¼ì¹˜ í•„í„°ë§"""
        try:
            if not results:
                return results
            if relax_filter:
                logger.info("ğŸ§© ë„ë©”ì¸ í•„í„° ì™„í™”: ì‚¬ìš©ì ì„ íƒ ë¬¸ì„œ ë˜ëŠ” PPT ì˜ë„ ê°ì§€")
                return results
            
            # ë„ë©”ì¸ ì¹´í…Œê³ ë¦¬ ì •ì˜
            domain_categories = {
                "medical": {"ì˜ë£Œ", "ë³‘ì›", "ì¹˜ë£Œ", "ì§ˆë³‘", "ì•½ë¬¼", "ì˜ì‚¬", "í™˜ì", "ê±´ê°•", "ì¸ìŠë¦°", "íŒí”„", "í˜ˆë‹¹", "ë‹¹ë‡¨", "ìˆ˜ìˆ ", "ì§„ë£Œ", "ì˜ë£Œê¸°ê¸°"},
                "travel": {"ì—¬í–‰", "ê´€ê´‘", "í˜¸í…”", "í•­ê³µ", "ë¹„ì", "ì¼ë³¸", "ë„ì¿„", "êµí† ", "ì˜¤ì‚¬ì¹´", "ê´€ê´‘ì§€", "ìˆ™ì†Œ", "ì—¬í–‰ì§€", "íŒ¨í‚¤ì§€", "íˆ¬ì–´", "í•­ê³µê¶Œ"},
                "technology": {"IT", "ì»´í“¨í„°", "ì†Œí”„íŠ¸ì›¨ì–´", "í”„ë¡œê·¸ë˜ë°", "ê°œë°œ", "ì‹œìŠ¤í…œ", "ë„¤íŠ¸ì›Œí¬", "ë°ì´í„°ë² ì´ìŠ¤", "í´ë¼ìš°ë“œ", "AI", "ê¸°ìˆ "},
                "business": {"ì‚¬ì—…", "íšŒì‚¬", "ê²½ì˜", "ë§ˆì¼€íŒ…", "ì˜ì—…", "ì œí’ˆ", "ì„œë¹„ìŠ¤", "ê³ ê°", "ë§¤ì¶œ", "íˆ¬ì", "ê³„ì•½", "ì „ëµ", "ë¹„ì¦ˆë‹ˆìŠ¤"},
                "education": {"êµìœ¡", "í•™êµ", "í•™ìŠµ", "ìˆ˜ì—…", "ê°•ì˜", "ì‹œí—˜", "ì¡¸ì—…", "ì…í•™", "ê³¼ì •", "ì»¤ë¦¬í˜ëŸ¼", "í•™ìƒ", "êµì‚¬", "ì—°êµ¬"}
            }
            
            # í˜„ì¬ ì§ˆë¬¸ì˜ ë„ë©”ì¸ ê°ì§€
            current_domain = self._detect_query_domain(current_query.lower(), domain_categories)
            
            if current_domain == "general":
                logger.info("ğŸ” ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜ - ì°¸ê³ ìë£Œ í•„í„°ë§ ìƒëµ")
                return results
            
            # ê° ì°¸ê³ ìë£Œì˜ ë„ë©”ì¸ ê´€ë ¨ì„± ê²€ì‚¬
            validated_results = []
            filtered_count = 0
            
            for result in results:
                content = result.get("content", "").lower()
                file_name = result.get("file_name", "").lower()
                
                # ì°¸ê³ ìë£Œì˜ ë„ë©”ì¸ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                relevance_score = 0
                content_domain_keywords = domain_categories.get(current_domain, set())
                
                # ë‚´ìš©ì—ì„œ ë„ë©”ì¸ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                for keyword in content_domain_keywords:
                    if keyword in content:
                        relevance_score += 2
                    if keyword in file_name:
                        relevance_score += 1
                
                # ë‹¤ë¥¸ ë„ë©”ì¸ í‚¤ì›Œë“œ ì¡´ì¬ ì‹œ íŒ¨ë„í‹°
                for other_domain, other_keywords in domain_categories.items():
                    if other_domain != current_domain:
                        for keyword in other_keywords:
                            if keyword in content:
                                relevance_score -= 1
                
                # ì„ê³„ê°’ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (í˜„ì¬ ë„ë©”ì¸ í‚¤ì›Œë“œê°€ ìµœì†Œ 1ê°œ ì´ìƒ ìˆì–´ì•¼ í•¨)
                min_threshold = 1
                if relevance_score >= min_threshold:
                    result["domain_relevance_score"] = relevance_score
                    validated_results.append(result)
                else:
                    filtered_count += 1
                    logger.debug(f"ğŸš« ë„ë©”ì¸ ë¶ˆì¼ì¹˜ë¡œ í•„í„°ë§: {result.get('file_name', 'Unknown')} (score: {relevance_score})")
            
            if filtered_count > 0:
                logger.info(f"ğŸ” ë„ë©”ì¸ ê´€ë ¨ì„± í•„í„°ë§: {len(results)}ê°œ â†’ {len(validated_results)}ê°œ (ë„ë©”ì¸: {current_domain}, ì œì™¸: {filtered_count}ê°œ)")
            
            # í•„í„°ë§ í›„ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ìœ ì§€ (over-filtering ë°©ì§€)
            if len(validated_results) < max(1, len(results) * 0.3):
                logger.warning(f"âš ï¸ ê³¼ë„í•œ í•„í„°ë§ ê°ì§€ - ì›ë³¸ ê²°ê³¼ ìœ ì§€ ({len(validated_results)} < {len(results) * 0.3:.1f})")
                return results
            
            return validated_results
            
        except Exception as e:
            logger.error(f"âŒ ì°¸ê³ ìë£Œ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return results
    
    def _detect_query_domain(self, query: str, domain_categories: dict) -> str:
        """ì§ˆë¬¸ì—ì„œ ë„ë©”ì¸ ê°ì§€"""
        domain_scores = {}
        
        for domain, keywords in domain_categories.items():
            score = 0
            for keyword in keywords:
                if keyword in query:
                    score += 1
            domain_scores[domain] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë„ë©”ì¸ ë°˜í™˜ (ìµœì†Œ ì„ê³„ê°’ ì´ìƒ)
        max_domain = max(domain_scores.items(), key=lambda x: x[1])
        if max_domain[1] >= 1:  # ìµœì†Œ 1ê°œ í‚¤ì›Œë“œ ë§¤ì¹­
            return max_domain[0]
        return "general"
    
    async def _keyword_search(
        self,
        session: AsyncSession,
        search_params: RAGSearchParams,
        query_analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (Option A/B: ì •ê·œí™” + í•µì‹¬ì–´ AND + ê°€ì¤‘ rank)"""
        raw_keywords = query_analysis.get("korean_keywords", []) or []
        if not raw_keywords:
            logger.warning("í‚¤ì›Œë“œê°€ ì—†ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰ ë¶ˆê°€")
            return []

        original_user_query = (search_params.original_query or search_params.query).strip()

        # Stopword / ì¼ë°˜ì–´ (ëª¨ë‘ lower)
        exclude_words = {
            'ì´ì „','user','ì‚¬ìš©ì','ì§ˆë¬¸','ë¬¸ì˜','ë‹µë³€','ëŒ€í™”','ì±„íŒ…','í˜„ì¬','ì‹œìŠ¤í…œ','ìƒíƒœ','í™•ì¸','ì •ë³´','ë‚´ìš©','ë¬¸ì„œ','ìë£Œ','ê²€ìƒ‰','ê²°ê³¼',
            'ì¼ë³¸','ì—¬í–‰','ë„ì¿„','êµí† ','ì˜¤ì‚¬ì¹´','í›„ì§€ì‚°','ê´€ê´‘','ì—¬í–‰ì§€','í˜¸í…”','ê²½ì¹˜','í˜¸ìˆ˜','ë„ì‹œ','ì•„ë˜','ê³µì¡´','ë°©ë¬¸','ì¶”ì²œ','ê°ì‚¬','ê¸°ì˜¨','ì„¼ì†Œì§€','ì‚¬ì›','íƒ€ì›Œ','ê¸ˆê°ì‚¬','ì€ê°ì‚¬','ê±°ë¦¬','ìŒì‹','ë‚˜ì´íŠ¸','ë“±ì‚°','ì£¼ë³€','ë¹„ì','í•­ê³µê¶Œ','ìˆ™ì†Œ','ë£Œì¹¸','êµí†µ','íŒ¨ìŠ¤','ì˜ˆì ˆ','ì–¸ì–´','ì˜ì–´','í‘œí˜„','ê³„íš','ëª©ì ì§€','ìš”ì†Œ','í’ê²½','ì „í†µ','ëª…ì†Œ','ì‹ ì£¼ì¿ ','ì‹œë¶€ì•¼','ì•„ì‚¬ì¿ ì‚¬','ë„í†¤ë³´ë¦¬',
            'ë°ì´í„°','ëª©ë¡','ë¦¬ìŠ¤íŠ¸','í•­ëª©','ë°©ë²•','ê´€ë ¨','ê¸°ëŠ¥','ì„¤ì •','ê´€ë¦¬','ì„œë¹„ìŠ¤','ìš”ì²­','ì‘ë‹µ','ì²˜ë¦¬','ì‹¤í–‰','ë¡œê·¸','ì˜¤ë¥˜','ë¬¸ì œ','í•´ê²°','ë°©ì•ˆ','ì œì•ˆ','ì˜ê²¬','ìƒê°','í•˜ì´ë¸Œë¦¬ë“œ','ì˜ë¯¸ì ','í‚¤ì›Œë“œ','ê²€ìƒ‰ì–´','ê²°ê³¼ë¬¼','í† í°','ì²­í¬','ì»¨í…ìŠ¤íŠ¸','ì„¸ì…˜'
        }

        # í•µì‹¬ ë„ë©”ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ì›ë¬¸ ê·¸ëŒ€ë¡œ / ì†Œë¬¸ì)
        core_tokens = []
        for token in [t.strip() for t in re.split(r'[\s,/]+', original_user_query) if t.strip()]:
            lower = token.lower()
            # PPT ì‘ì„± ì˜ë„ ê´€ë ¨ í•µì‹¬ ë³´ì¡´ í† í°
            if any(k in lower for k in ['ppt', 'ì œí’ˆ', 'ì†Œê°œ', 'ì¸ìŠë¦°', 'íŒí”„', 'ìŠ¬ë¼ì´ë“œ']):
                if token not in core_tokens:
                    core_tokens.append(token)

        # ë³µí•©ì–´ ë¶„í•´ ë° ë™ì˜ì–´ í™•ì¥ í¬í•¨ ì •ê·œí™”
        normalized: List[str] = []
        for kw in raw_keywords:
            nk = kw.strip().lower()
            if len(nk) < 2:
                continue
            if nk in exclude_words:
                continue
            if nk.isdigit():
                continue
            # ë³µí•©ì–´ ë¶„í•´ (ì˜ˆ: 'ì œí’ˆì†Œê°œ' â†’ ['ì œí’ˆ','ì†Œê°œ'])
            parts = self._split_korean_compound(nk) or [nk]
            for p in parts:
                if p and p not in exclude_words:
                    normalized.append(p)
            # ë™ì˜ì–´ í™•ì¥ (ê³¼í•˜ì§€ ì•Šê²Œ ì†Œìˆ˜ë§Œ)
            for syn in self._expand_synonyms(nk):
                if syn not in exclude_words:
                    normalized.append(syn)

        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        seen = set()
        filtered_keywords = []
        for nk in normalized:
            if nk not in seen:
                seen.add(nk)
                filtered_keywords.append(nk)

        # í•µì‹¬ í† í°ì´ í•„í„°ë§ì—ì„œ ë¹ ì¡Œë‹¤ë©´ ë³„ë„ ë³´ê°•
        core_keywords = []
        
        # PPT ê´€ë ¨ ì¿¼ë¦¬ì—ì„œëŠ” core í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë„“íˆê¸° ìœ„í•´)
        is_ppt_related = search_params.query and any(term in search_params.query.lower() 
                                                    for term in ['ppt', 'powerpoint', 'í”„ë ˆì  í…Œì´ì…˜', 'ë°œí‘œìë£Œ', 'ì œí’ˆì†Œê°œì„œ', 'ì†Œê°œì„œ'])
        
        if not is_ppt_related:
            for ct in core_tokens:
                cl = ct.lower()
                if cl not in exclude_words and cl not in filtered_keywords:
                    core_keywords.append(cl)

        if not filtered_keywords and not core_keywords:
            logger.info(f"âš ï¸ í‚¤ì›Œë“œ í•„í„° í›„ ìœ íš¨ í‚¤ì›Œë“œ ì—†ìŒ: raw={raw_keywords}")
            return []

        # ìµœëŒ€ 3ê°œ ì¼ë°˜ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        main_keywords = filtered_keywords[:3]
        logger.info(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ëŒ€ìƒ(main): {main_keywords}, core: {core_keywords} (PPTê´€ë ¨: {is_ppt_related})")
        logger.debug(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜ - container_ids: {search_params.container_ids}, document_ids: {search_params.document_ids}")

        # rank_score êµ¬ì„±
        rank_parts = []
        for i, _ in enumerate(main_keywords):
            rank_parts.append(f"(CASE WHEN tdc.chunk_text ILIKE :kw_{i} THEN 1 ELSE 0 END)")
        core_weight = 2
        for j, _ in enumerate(core_keywords):
            rank_parts.append(f"(CASE WHEN tdc.chunk_text ILIKE :core_{j} THEN {core_weight} ELSE 0 END)")
        if not rank_parts:
            rank_expr = "1.0"  # fallback
        else:
            rank_expr = " + ".join(rank_parts)

        # WHERE ì ˆ êµ¬ì„±
        # ê¸°ë³¸: ( (kw OR kw OR ...) AND (core OR core ...) )
        # í•„í„°(ì»¨í…Œì´ë„ˆ/ë¬¸ì„œ)ê°€ ìˆëŠ” ê²½ìš°: ëŠìŠ¨í•œ ë§¤ì¹­ìœ¼ë¡œ 0ê±´ ë°©ì§€ â†’ (kw OR core) ë§Œ ìš”êµ¬
        kw_conditions = []
        for i, _ in enumerate(main_keywords):
            kw_conditions.append(f"tdc.chunk_text ILIKE :kw_{i}")
        kw_clause = " OR ".join(kw_conditions) if kw_conditions else ""

        core_conditions = []
        for j, _ in enumerate(core_keywords):
            core_conditions.append(f"tdc.chunk_text ILIKE :core_{j}")
        core_clause = " OR ".join(core_conditions)

        where_fragments = []
        loosen_match = bool(search_params.container_ids or search_params.document_ids)
        
        # PPT ê´€ë ¨ ì§ˆë¬¸ì˜ ê²½ìš° core í‚¤ì›Œë“œë¥¼ ë„ˆë¬´ ì—„ê²©í•˜ê²Œ ì ìš©í•˜ì§€ ì•ŠìŒ
        is_ppt_query = search_params.query and any(term in search_params.query.lower() 
                                                  for term in ['ppt', 'powerpoint', 'í”„ë ˆì  í…Œì´ì…˜', 'ë°œí‘œìë£Œ', 'ì œí’ˆì†Œê°œì„œ'])
        
        if loosen_match or is_ppt_query:
            # ëŠìŠ¨: kw ë˜ëŠ” core ì¤‘ í•˜ë‚˜ë¼ë„ ë§¤ì¹­ë˜ë©´ í›„ë³´ë¡œ
            oc = []
            if kw_clause:
                oc.append(f"({kw_clause})")
            if core_clause:
                oc.append(f"({core_clause})")
            where_clause = " OR ".join(oc) if oc else "1=1"
        else:
            if kw_clause:
                where_fragments.append(f"({kw_clause})")
            if core_clause:
                # í•µì‹¬ì–´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í•˜ë‚˜ëŠ” ë§¤ì¹­ë˜ë„ë¡ AND ê·¸ë£¹ìœ¼ë¡œ ì¶”ê°€
                where_fragments.append(f"({core_clause})")
            where_clause = " AND ".join(where_fragments) if where_fragments else "1=1"

        base_sql = f"""
            SELECT 
                tdc.file_bss_info_sno,
                tdc.chunk_index,
                tdc.chunk_text,
                tdc.page_number,
                tdc.section_title,
                tdc.keywords,
                tdc.named_entities,
                tdc.knowledge_container_id,
                {rank_expr} AS rank_score,
                fbi.file_lgc_nm AS file_name
            FROM vs_doc_contents_chunks tdc
            JOIN tb_file_bss_info fbi ON tdc.file_bss_info_sno = fbi.file_bss_info_sno
            WHERE {where_clause}
              AND tdc.del_yn = 'N'
              AND fbi.del_yn = 'N'
        """

        if search_params.container_ids and len(search_params.container_ids) > 0:
            base_sql += " AND tdc.knowledge_container_id = ANY(:container_ids)"
        if search_params.document_ids and len(search_params.document_ids) > 0:
            base_sql += " AND fbi.file_bss_info_sno = ANY(:document_ids)"

        base_sql += " ORDER BY rank_score DESC LIMIT :limit"

        query_sql = text(base_sql)
        params: Dict[str, Any] = {"limit": search_params.max_chunks * 2}
        for i, kw in enumerate(main_keywords):
            params[f"kw_{i}"] = f"%{kw}%"
        for j, ck in enumerate(core_keywords):
            params[f"core_{j}"] = f"%{ck}%"
        if search_params.container_ids:
            params["container_ids"] = search_params.container_ids
        if search_params.document_ids:
            params["document_ids"] = self._normalize_document_ids(search_params.document_ids)

        result = await session.execute(query_sql, params)
        rows = result.fetchall()
        
        logger.debug(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ SQL ì‹¤í–‰ë¨ - ë§¤ê°œë³€ìˆ˜: {params}")
        logger.info(f"ğŸ”¤ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {len(rows)}ê°œ (raw rows from DB)")
        
        if len(rows) == 0:
            logger.warning(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ 0ê°œ - SQL í™•ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ")
            logger.debug(f"ğŸ” ì‹¤í–‰ëœ SQL: {base_sql}")
            logger.debug(f"ğŸ” SQL ë§¤ê°œë³€ìˆ˜: {params}")

        search_results: List[Dict[str, Any]] = []
        for row in rows:
            rank_score = float(row[8]) if row[8] is not None else 0.0
            # ì •ê·œí™”: (ìµœëŒ€ ê°€ëŠ¥í•œ ì ìˆ˜ ëŒ€ë¹„) â€“ ìµœëŒ€ëŠ” len(main)+core_weight*len(core)
            denom = max(1.0, len(main_keywords) + core_weight * len(core_keywords))
            similarity_score = rank_score / denom
            search_results.append({
                "file_bss_info_sno": row[0],
                "chunk_index": row[1],
                "content": row[2],
                "page_number": row[3] if row[3] else 1,
                "section_title": row[4] if row[4] else "",
                "keywords": row[5] if row[5] else "",
                "named_entities": row[6] if row[6] else "",
                "container_id": row[7],
                "similarity_score": similarity_score,
                "file_name": row[9],
                "chunk_type": "content",
                "search_type": "keyword",
                "metadata": {
                    "page_number": row[3] if row[3] else 1,
                    "section_title": row[4] if row[4] else "",
                    "keywords": row[5].split(',') if row[5] else [],
                    "named_entities": row[6].split(',') if row[6] else []
                }
            })

        logger.info(f"ğŸ”¤ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ (core AND ì ìš© ì—¬ë¶€: {bool(core_keywords)})")
        return search_results
    
    async def _fulltext_search(
        self,
        session: AsyncSession,
        search_params: RAGSearchParams,
        query_analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        ì „ë¬¸ê²€ìƒ‰ (tsvector) - tb_document_search_index í™œìš©
        
        ë‹¤êµ­ì–´ ì§€ì›:
        - í•œêµ­ì–´: korean configuration (textsearch_ko í™•ì¥)
        - ì˜ì–´: english configuration (stemming, stopwords)
        - í˜¼í•©: korean + english dual search
        
        ì˜ˆì‹œ:
        - 'í˜ì‹ ' â†’ 'innovation', 'innovative' ìë™ ë§¤ì¹­ (textsearch_ko)
        - 'Ambidextrous Leadership' â†’ 'ambidextr', 'leadership' stemming ë§¤ì¹­
        """
        keywords = query_analysis.get("korean_keywords", [])
        if not keywords:
            logger.info("ğŸ“š ì „ë¬¸ê²€ìƒ‰: í‚¤ì›Œë“œ ì—†ìŒ - ê±´ë„ˆë›°ê¸°")
            return []
        
        # ì¡°ì‚¬ ì œê±° ë° ë¶ˆìš©ì–´ í•„í„°ë§
        stopwords = {'ë­', 'ë­ë¼', 'ë­ë¼ê³ ', 'í•˜ë‚˜ìš”', 'ìˆë‚˜ìš”', 'ìˆì–´ìš”', 'ëŒ€í•´', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œì„œ', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡'}
        filtered_keywords = []
        for kw in keywords:
            # ì¡°ì‚¬ ì œê±°
            cleaned_kw = self._remove_korean_josa(kw)
            if cleaned_kw and cleaned_kw.lower() not in stopwords and len(cleaned_kw) > 1:
                filtered_keywords.append(cleaned_kw)
        
        if not filtered_keywords:
            logger.info("ğŸ“š ì „ë¬¸ê²€ìƒ‰: ì¡°ì‚¬ ì œê±° í›„ í‚¤ì›Œë“œ ì—†ìŒ - ê±´ë„ˆë›°ê¸°")
            return []
        
        # ì–¸ì–´ ê°ì§€ (ìµœì í™”ëœ FTS configuration ì„ íƒìš©)
        query_language = self._detect_query_language(search_params.query)
        logger.info(f"ğŸ“š ì „ë¬¸ê²€ìƒ‰ ì‹œì‘: í‚¤ì›Œë“œ {keywords} â†’ í•„í„°ë§ í›„ {filtered_keywords}")
        logger.info(f"ğŸŒ ì¿¼ë¦¬ ì–¸ì–´ ê°ì§€: {query_language} (ko=í•œêµ­ì–´, en=ì˜ì–´, mixed=í˜¼í•©)")
        
        # ê²€ìƒ‰ì–´ ì¤€ë¹„ (OR ê²€ìƒ‰)
        search_terms = ' | '.join(filtered_keywords)
        
        # SQL ì¿¼ë¦¬ êµ¬ì„± (í•œêµ­ì–´ + ì˜ì–´ + simple ë©€í‹° ì–¸ì–´ ì§€ì›)
        base_sql = """
            WITH search_query AS (
                SELECT 
                    plainto_tsquery('korean', :search_terms) as query_korean,
                    plainto_tsquery('english', :search_terms) as query_english,
                    plainto_tsquery('simple', :search_terms) as query_simple
            )
            SELECT 
                dsi.file_bss_info_sno,
                dsi.search_doc_id,
                dsi.document_title as file_name,
                GREATEST(
                    ts_rank(dsi.content_tsvector, sq.query_korean),
                    ts_rank(dsi.content_tsvector_en, sq.query_english),
                    ts_rank(dsi.keyword_tsvector, sq.query_korean),
                    ts_rank(dsi.keyword_tsvector_en, sq.query_english),
                    ts_rank(dsi.content_tsvector, sq.query_simple)
                ) as rank,
                dsi.full_content,
                dsi.has_images,
                dsi.image_count,
                ts_headline('korean', 
                    COALESCE(substring(dsi.full_content, 1, 1000), ''), 
                    sq.query_korean,
                    'MaxWords=50, MinWords=20, ShortWord=3'
                ) as snippet
            FROM tb_document_search_index dsi
            CROSS JOIN search_query sq
            WHERE (
                dsi.content_tsvector @@ sq.query_korean 
                OR dsi.content_tsvector_en @@ sq.query_english
                OR dsi.keyword_tsvector @@ sq.query_korean
                OR dsi.keyword_tsvector_en @@ sq.query_english
                OR dsi.content_tsvector @@ sq.query_simple
            )
            AND dsi.file_bss_info_sno IS NOT NULL
        """
        
        # ë¬¸ì„œ ID í•„í„°ë§ ì¶”ê°€
        conditions = []
        params = {"search_terms": search_terms}
        
        if search_params.document_ids:
            conditions.append("AND dsi.file_bss_info_sno = ANY(:document_ids)")
            params["document_ids"] = self._normalize_document_ids(search_params.document_ids)
        
        if search_params.container_ids:
            conditions.append("AND dsi.knowledge_container_id = ANY(:container_ids)")
            params["container_ids"] = search_params.container_ids
        
        # ì¡°ê±´ ì¶”ê°€
        if conditions:
            base_sql += " " + " ".join(conditions)
        
        # ì •ë ¬ ë° ì œí•œ
        base_sql += """
            ORDER BY rank DESC
            LIMIT :limit
        """
        params["limit"] = 20  # ì „ë¬¸ê²€ìƒ‰ ê²°ê³¼ ì œí•œ
        
        try:
            # SQL ì‹¤í–‰
            result = await session.execute(text(base_sql), params)
            rows = result.fetchall()
            
            logger.info(f"ğŸ“š ì „ë¬¸ê²€ìƒ‰ SQL ì‹¤í–‰ ê²°ê³¼: {len(rows)}ê°œ ë¬¸ì„œ")
            
            if len(rows) == 0:
                logger.info(f"ğŸ“š ì „ë¬¸ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ê²€ìƒ‰ì–´: '{search_terms}'")
                return []
            
            # ë¬¸ì„œë³„ë¡œ ì²­í¬ ì¡°íšŒ
            search_results: List[Dict[str, Any]] = []
            
            for row in rows:
                # row[0]: file_bss_info_sno
                # row[1]: search_doc_id
                # row[2]: file_name
                # row[3]: rank (GREATEST ê²°ê³¼)
                # row[4]: full_text
                # row[5]: has_images
                # row[6]: image_count
                # row[7]: snippet
                file_bss_info_sno = row[0]
                rank_score = float(row[3]) if row[3] else 0.0
                snippet = row[7] if row[7] else ""
                
                # í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ë“¤ì„ ì¡°íšŒ (ìƒìœ„ 5ê°œë§Œ)
                chunk_sql = """
                    SELECT 
                        tdc.file_bss_info_sno,
                        tdc.chunk_index,
                        tdc.chunk_text,
                        tdc.page_number,
                        tdc.section_title,
                        tdc.keywords,
                        tdc.named_entities,
                        tdc.knowledge_container_id,
                        fbi.file_lgc_nm as file_name
                    FROM vs_doc_contents_chunks tdc
                    JOIN tb_file_bss_info fbi ON tdc.file_bss_info_sno = fbi.file_bss_info_sno
                    WHERE tdc.file_bss_info_sno = :file_id
                      AND tdc.del_yn = 'N'
                      AND fbi.del_yn = 'N'
                """
                
                # í‚¤ì›Œë“œë¡œ ì²­í¬ í•„í„°ë§ (ì²­í¬ ë‚´ìš©ì— í‚¤ì›Œë“œê°€ ìˆëŠ” ê²ƒë§Œ)
                chunk_conditions = []
                for i, kw in enumerate(filtered_keywords[:3]):  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                    chunk_conditions.append(f"tdc.chunk_text ILIKE :kw_{i}")
                
                if chunk_conditions:
                    chunk_sql += " AND (" + " OR ".join(chunk_conditions) + ")"
                
                chunk_sql += " ORDER BY tdc.chunk_index LIMIT 5"
                
                chunk_params = {"file_id": file_bss_info_sno}
                for i, kw in enumerate(filtered_keywords[:3]):
                    chunk_params[f"kw_{i}"] = f"%{kw}%"
                
                chunk_result = await session.execute(text(chunk_sql), chunk_params)
                chunk_rows = chunk_result.fetchall()
                
                # ì²­í¬ë“¤ì„ ê²°ê³¼ì— ì¶”ê°€
                for chunk_row in chunk_rows:
                    search_results.append({
                        "file_bss_info_sno": chunk_row[0],
                        "chunk_index": chunk_row[1],
                        "content": chunk_row[2],
                        "page_number": chunk_row[3] if chunk_row[3] else 1,
                        "section_title": chunk_row[4] if chunk_row[4] else "",
                        "keywords": chunk_row[5] if chunk_row[5] else "",
                        "named_entities": chunk_row[6] if chunk_row[6] else "",
                        "container_id": chunk_row[7],
                        "similarity_score": rank_score,  # ë¬¸ì„œ rank ì ìˆ˜ ì‚¬ìš©
                        "file_name": chunk_row[8],
                        "chunk_type": "content",
                        "search_type": "fulltext",
                        "metadata": {
                            "page_number": chunk_row[3] if chunk_row[3] else 1,
                            "section_title": chunk_row[4] if chunk_row[4] else "",
                            "keywords": chunk_row[5].split(',') if chunk_row[5] else [],
                            "named_entities": chunk_row[6].split(',') if chunk_row[6] else [],
                            "snippet": snippet
                        }
                    })
            
            logger.info(f"ğŸ“š ì „ë¬¸ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ì²­í¬")
            return search_results
            
        except Exception as e:
            logger.error(f"âŒ ì „ë¬¸ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def _remove_korean_josa(self, word: str) -> str:
        """í•œêµ­ì–´ ì¡°ì‚¬ ì œê±°"""
        josa_list = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë§Œ', 'ì—ê²Œ', 'í•œí…Œ', 'ì—ì„œ', 'ìœ¼ë¡œì„œ', 'ë¡œì„œ']
        for josa in josa_list:
            if word.endswith(josa) and len(word) > len(josa) + 1:
                return word[:-len(josa)]
        return word
    
    async def _hybrid_search(
        self,
        session: AsyncSession,
        search_params: RAGSearchParams,
        query_analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ì  + í‚¤ì›Œë“œ + ì „ë¬¸ê²€ìƒ‰)"""
        # ë³‘ë ¬ë¡œ ì„¸ ê°€ì§€ ê²€ìƒ‰ ì‹¤í–‰
        semantic_results = await self._semantic_search(session, search_params, query_analysis)
        keyword_results = await self._keyword_search(session, search_params, query_analysis)
        fulltext_results = await self._fulltext_search(session, search_params, query_analysis)
        
        # ê²°ê³¼ í†µí•© ë° ì ìˆ˜ ì¡°í•©
        combined_results = {}
        
        # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for result in semantic_results:
            key = f"{result['file_bss_info_sno']}_{result['chunk_index']}"
            combined_results[key] = {
                **result,
                "semantic_score": result["similarity_score"],
                "keyword_score": 0.0,
                "fulltext_score": 0.0,
                "combined_score": result["similarity_score"] * search_params.semantic_boost
            }
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ í†µí•©
        for result in keyword_results:
            key = f"{result['file_bss_info_sno']}_{result['chunk_index']}"
            if key in combined_results:
                # ê¸°ì¡´ ê²°ê³¼ì— í‚¤ì›Œë“œ ì ìˆ˜ ì¶”ê°€
                combined_results[key]["keyword_score"] = result["similarity_score"]
                combined_results[key]["combined_score"] = (
                    combined_results[key]["semantic_score"] * search_params.semantic_boost +
                    result["similarity_score"] * search_params.keyword_boost
                )
                combined_results[key]["search_type"] = "hybrid"
            else:
                # ìƒˆ ê²°ê³¼ ì¶”ê°€
                combined_results[key] = {
                    **result,
                    "semantic_score": 0.0,
                    "keyword_score": result["similarity_score"],
                    "fulltext_score": 0.0,
                    "combined_score": result["similarity_score"] * search_params.keyword_boost
                }
        
        # ì „ë¬¸ê²€ìƒ‰ ê²°ê³¼ í†µí•© (ê°€ì¤‘ì¹˜ 0.6 - í‚¤ì›Œë“œë³´ë‹¤ ë†’ê²Œ ì„¤ì •)
        fulltext_boost = 0.6
        for result in fulltext_results:
            key = f"{result['file_bss_info_sno']}_{result['chunk_index']}"
            if key in combined_results:
                # ê¸°ì¡´ ê²°ê³¼ì— ì „ë¬¸ê²€ìƒ‰ ì ìˆ˜ ì¶”ê°€
                combined_results[key]["fulltext_score"] = result["similarity_score"]
                combined_results[key]["combined_score"] += result["similarity_score"] * fulltext_boost
                combined_results[key]["search_type"] = "hybrid"
            else:
                # ìƒˆ ê²°ê³¼ ì¶”ê°€
                combined_results[key] = {
                    **result,
                    "semantic_score": 0.0,
                    "keyword_score": 0.0,
                    "fulltext_score": result["similarity_score"],
                    "combined_score": result["similarity_score"] * fulltext_boost
                }
        
        # í†µí•© ì ìˆ˜ë¡œ ì •ë ¬
        final_results = sorted(
            combined_results.values(),
            key=lambda x: x["combined_score"],
            reverse=True
        )
        
        logger.info(f"ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼: {len(final_results)}ê°œ "
                   f"(ì˜ë¯¸ì : {len(semantic_results)}, í‚¤ì›Œë“œ: {len(keyword_results)}, ì „ë¬¸ê²€ìƒ‰: {len(fulltext_results)})")
        
        # RAG ê²€ìƒ‰ í’ˆì§ˆ í•„í„°ë§ ì ìš©
        final_results = self._apply_rag_quality_filter(final_results, search_params, query_analysis)
        
        return final_results
    
    async def _rerank_results(
        self,
        query: str,
        results: List[Dict[str, Any]],
        target_count: int
    ) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ ë¦¬ë­í‚¹"""
        try:
            logger.info(f"ğŸ”„ ë¦¬ë­í‚¹ ì‹œì‘: {len(results)}ê°œ â†’ {target_count}ê°œ")
            
            # ì„¤ì •ëœ AI ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•œ ë¦¬ë­í‚¹
            rerank_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œ ì²­í¬ë“¤ì„ ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ë¬¸ì„œ ì²­í¬ë“¤:
"""
            
            for i, result in enumerate(results):
                content_preview = result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"]
                rerank_prompt += f"{i+1}. {content_preview}\n\n"
            
            rerank_prompt += """
ìœ„ ì²­í¬ë“¤ì„ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ë§Œ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
ì˜ˆ: 3, 1, 7, 2, 5
"""
            
            # ì„¤ì •ëœ AI ì„œë¹„ìŠ¤ì—ê²Œ ë¦¬ë­í‚¹ ìš”ì²­
            try:
                from app.services.core.ai_service import ai_service
                from app.core.config import settings
                from langchain_openai import AzureChatOpenAI
                from langchain.schema import HumanMessage
                
                # ë¦¬ë­í‚¹ ì „ìš© ì„¤ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ Settings ê¸°ë°˜ ê¸°ë³¸ê°’ ì‚¬ìš©
                rerank_endpoint = settings.rag_reranking_endpoint or settings.azure_openai_endpoint
                rerank_deployment = settings.rag_reranking_deployment or settings.azure_openai_llm_deployment
                rerank_api_key = settings.rag_reranking_api_key or settings.azure_openai_api_key
                rerank_api_version = settings.rag_reranking_api_version or settings.azure_openai_api_version

                if not (rerank_endpoint and rerank_deployment and rerank_api_key):
                    logger.info("âš ï¸ ë¦¬ë­í‚¹ ì „ìš© ì„¤ì • ì—†ìŒ - ê¸°ë³¸ Azure OpenAI ì„¤ì •ìœ¼ë¡œ fallback")
                    rerank_endpoint = settings.azure_openai_endpoint
                    rerank_deployment = settings.azure_openai_llm_deployment
                    rerank_api_key = settings.azure_openai_api_key
                    rerank_api_version = settings.azure_openai_api_version

                if not (rerank_endpoint and rerank_deployment and rerank_api_key):
                    raise ValueError("ë¦¬ë­í‚¹ì— ì‚¬ìš©í•  Azure OpenAI ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                deployment_lower = rerank_deployment.lower()

                # gpt-5, nano, o1, o3 ëª¨ë¸ì€ Reasoning ê³„ì—´ â†’ max_completion_tokens ì‚¬ìš©
                if 'gpt-5' in deployment_lower or 'nano' in deployment_lower or 'o1' in deployment_lower or 'o3' in deployment_lower:
                    logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ëª¨ë¸: {rerank_deployment} (Reasoning ê³„ì—´: max_completion_tokens ì‚¬ìš©)")
                    rerank_max_completion_tokens = settings.rag_reranking_max_completion_tokens
                    rerank_reasoning_effort = settings.rag_reranking_reasoning_effort
                    model_kwargs: Dict[str, Any] = {"max_completion_tokens": rerank_max_completion_tokens}
                    if rerank_reasoning_effort:
                        model_kwargs["reasoning_effort"] = rerank_reasoning_effort
                    rerank_llm = AzureChatOpenAI(
                        azure_endpoint=rerank_endpoint,
                        api_key=rerank_api_key,
                        api_version=rerank_api_version,
                        deployment_name=rerank_deployment,
                        model_kwargs=model_kwargs
                    )
                else:
                    logger.info(f"ğŸ”§ ë¦¬ë­í‚¹ ëª¨ë¸: {rerank_deployment} (temperature ì§€ì›)")
                    rerank_temperature = settings.rag_reranking_temperature
                    rerank_max_tokens = settings.rag_reranking_max_tokens
                    rerank_llm = AzureChatOpenAI(
                        azure_endpoint=rerank_endpoint,
                        api_key=rerank_api_key,
                        api_version=rerank_api_version,
                        deployment_name=rerank_deployment,
                        temperature=rerank_temperature,
                        max_tokens=rerank_max_tokens  # ì¼ë°˜ ëª¨ë¸ì€ max_tokens ì‚¬ìš©
                    )
                
                # ë¦¬ë­í‚¹ ì‹¤í–‰
                response = await rerank_llm.ainvoke([HumanMessage(content=rerank_prompt)])
                rerank_response = response.content if hasattr(response, 'content') else str(response)
            except Exception as ai_error:
                logger.warning(f"AI ì„œë¹„ìŠ¤ ë¦¬ë­í‚¹ ì‹¤íŒ¨, ê¸°ë³¸ ìˆœì„œ ì‚¬ìš©: {ai_error}")
                # í´ë°±: ê¸°ë³¸ ìœ ì‚¬ë„ ìˆœì„œ ì‚¬ìš©
                reranked_results = results[:target_count]
                for i, result in enumerate(reranked_results):
                    result["rerank_score"] = (target_count - i) / target_count
                logger.info(f"âœ… ë¦¬ë­í‚¹ ì™„ë£Œ: {len(reranked_results)}ê°œ ì„ íƒ (ê¸°ë³¸ ìˆœì„œ)")
                return reranked_results
            
            # ì‘ë‹µì—ì„œ ìˆœì„œ ì¶”ì¶œ
            reranked_order = self._parse_rerank_response(rerank_response, len(results))
            
            # ìƒˆë¡œìš´ ìˆœì„œë¡œ ê²°ê³¼ ì¬ì •ë ¬
            reranked_results = []
            for idx in reranked_order[:target_count]:
                if 0 <= idx < len(results):
                    result = results[idx].copy()
                    result["rerank_score"] = (target_count - len(reranked_results)) / target_count
                    reranked_results.append(result)
            
            logger.info(f"âœ… ë¦¬ë­í‚¹ ì™„ë£Œ: {len(reranked_results)}ê°œ ì„ íƒ")
            return reranked_results
            
        except Exception as e:
            logger.error(f"ë¦¬ë­í‚¹ ì‹¤íŒ¨: {str(e)}")
            # ë¦¬ë­í‚¹ ì‹¤íŒ¨ì‹œ ì›ë³¸ ì ìˆ˜ ìˆœìœ¼ë¡œ ë°˜í™˜
            return sorted(results, key=lambda x: x.get("combined_score", 0), reverse=True)[:target_count]
    
    def _parse_rerank_response(self, response: str, total_count: int) -> List[int]:
        """ë¦¬ë­í‚¹ ì‘ë‹µì—ì„œ ìˆœì„œ ì¶”ì¶œ"""
        try:
            # ìˆ«ìë§Œ ì¶”ì¶œ
            import re
            numbers = re.findall(r'\d+', response)
            reranked_order = []
            
            for num_str in numbers:
                num = int(num_str) - 1  # 0-based indexë¡œ ë³€í™˜
                if 0 <= num < total_count and num not in reranked_order:
                    reranked_order.append(num)
            
            # ëˆ„ë½ëœ ì¸ë±ìŠ¤ ì¶”ê°€
            for i in range(total_count):
                if i not in reranked_order:
                    reranked_order.append(i)
            
            return reranked_order
            
        except Exception as e:
            logger.error(f"ë¦¬ë­í‚¹ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return list(range(total_count))  # ì›ë³¸ ìˆœì„œ ìœ ì§€
    
    def _remove_duplicates(self, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±° (ë™ì¼ íŒŒì¼ì˜ ë™ì¼ ì²­í¬)"""
        seen = set()
        unique_results = []
        
        for result in search_results:
            # íŒŒì¼IDì™€ ì²­í¬ ì¸ë±ìŠ¤ë¡œ ì¤‘ë³µ í™•ì¸
            key = (result.get("file_bss_info_sno"), result.get("chunk_index"))
            
            if key not in seen:
                seen.add(key)
                unique_results.append(result)
                
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(search_results)}ê°œ â†’ {len(unique_results)}ê°œ")
        return unique_results

    # ---------------------- ì •ì±…/ìœ í‹¸: ì •ê·œí™”Â·ì»·ë¼ì¸Â·ë™ì˜ì–´ ----------------------
    def _normalize_document_ids(self, raw_ids: List[Any]) -> List[int]:
        """ë¬¸ì„œ ID ì…ë ¥ì„ file_bss_info_sno(int) ë°°ì—´ë¡œ ì •ê·œí™”.
        í—ˆìš© í˜•íƒœ: 123, "123", "doc_123_45", "DOC-123", "file-123" ë“±ì—ì„œ 123 ì¶”ì¶œ."""
        normalized: List[int] = []
        for rid in raw_ids:
            if isinstance(rid, int):
                normalized.append(rid)
                continue
            try:
                # ìˆœìˆ˜ ìˆ«ì ë¬¸ìì—´
                normalized.append(int(str(rid)))
                continue
            except Exception:
                pass
            s = str(rid)
            m = re.search(r"(?i)(?:doc[_-]|file[_-])?(\d+)", s)
            if m:
                try:
                    normalized.append(int(m.group(1)))
                except Exception:
                    continue
        return normalized

    def _split_korean_compound(self, text: str) -> List[str]:
        """ê°„ë‹¨ ë³µí•©ì–´ ë¶„í•´: ìì£¼ ì“°ì´ëŠ” ê²°í•©ì–´ë¥¼ ì–´ê·¼ìœ¼ë¡œ ë‚˜ëˆ” (ì˜ˆ: ì œí’ˆì†Œê°œ â†’ ì œí’ˆ, ì†Œê°œ)."""
        patterns = [
            ("ì œí’ˆì†Œê°œ", ["ì œí’ˆ", "ì†Œê°œ"]),
            ("íšŒì‚¬ì†Œê°œ", ["íšŒì‚¬", "ì†Œê°œ"]),
            ("ì œí’ˆì„¤ëª…", ["ì œí’ˆ", "ì„¤ëª…"]),
            ("ê¸°ìˆ ë¬¸ì„œ", ["ê¸°ìˆ ", "ë¬¸ì„œ"]),
            ("ìš”êµ¬ì‚¬í•­ì •ì˜", ["ìš”êµ¬ì‚¬í•­", "ì •ì˜"]),
        ]
        for p, parts in patterns:
            if p in text:
                return parts
        # ê³µë°±/ìŠ¬ë˜ì‹œ/ì‰¼í‘œ ë¶„ë¦¬ëŠ” ìƒìœ„ ë¡œì§ì—ì„œ ì²˜ë¦¬ë¨
        return []

    def _expand_synonyms(self, token: str) -> List[str]:
        """ë™ì˜ì–´/í‘œê¸° ë³€í˜• ì†ŒëŸ‰ í™•ì¥."""
        synmap = {
            "ì†Œê°œ": ["ì†Œê°œ", "ì†Œê°œì„œ", "ì†Œê°œìë£Œ", "overview", "introduction", "ì†Œê°œë¬¸"],
            "ì œí’ˆ": ["ì œí’ˆ", "ìƒí’ˆ", "product"],
            "ppt": ["ppt", "presentation", "ìŠ¬ë¼ì´ë“œ"],
            "ë¬¸ì„œ": ["ë¬¸ì„œ", "ìë£Œ", "document"],
        }
        return synmap.get(token, [])

    def _score_of(self, item: Dict[str, Any]) -> float:
        return float(item.get("combined_score", item.get("similarity_score", 0.0)))

    def _apply_dynamic_cutline(self, results: List[Dict[str, Any]], params: RAGSearchParams) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ë¶„í¬ ê¸°ë°˜ ì»·ë¼ì¸ ì ìš©.
        - ë„ˆë¬´ ë§ì€ ì €ì  ê²°ê³¼ ì œê±° (íŠ¹íˆ í•„í„° ì‚¬ìš© ì‹œ)
        - ìµœì†Œ ë³´ì¡´ ê°œìˆ˜ëŠ” max_chunks*2
        """
        if not results:
            return results
        keep_min = max(params.max_chunks * 2, 10)
        if len(results) <= keep_min:
            return results
        scores = [self._score_of(r) for r in results]
        scores_sorted = sorted(scores)
        median = scores_sorted[len(scores_sorted)//2]
        max_s = max(scores)
        # ì»·ë¼ì¸: medianê³¼ 0.9*max ì¤‘ ë” ë‚®ì€ ê°’, í•˜ì§€ë§Œ ìµœì†Œ í•˜í•œ ì ìš©
        min_floor = 0.45
        if params.container_ids:
            min_floor = 0.40
        if params.document_ids:
            min_floor = 0.30
        cut = max(min_floor, min(0.9 * max_s, median))
        filtered = [r for r in results if self._score_of(r) >= cut]
        if len(filtered) < keep_min:
            return results  # ê³¼ë„ í•„í„° ë°©ì§€
        logger.info(f"âœ‚ï¸ ë¶„í¬ ì»·ë¼ì¸ ì ìš©: {len(results)} â†’ {len(filtered)} (cut={cut:.2f}, median={median:.2f}, max={max_s:.2f})")
        return filtered
    
    async def _build_context(
        self,
        chunks: List[Dict[str, Any]],
        max_tokens: int,
        ppt_mode: bool = False
    ) -> Tuple[str, int, List[Dict[str, Any]]]:
        """RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - í† í° ì œí•œ ë‚´ì—ì„œ ìµœëŒ€í•œ ë§ì€ ì²­í¬ í™œìš©
        Returns: (context_text, total_tokens, used_chunks)
        """
        if not chunks:
            return "", 0, []
        
        context_parts = []
        current_tokens = 0
        used_chunks: List[Dict[str, Any]] = []
        
        # ì²­í¬ë³„ í† í° ìˆ˜ ë¯¸ë¦¬ ê³„ì‚° (í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë³´ì •)
        chunk_tokens = []
        for chunk in chunks:
            content = chunk.get("content", "")
            if not content:
                continue
            # í•œêµ­ì–´ í…ìŠ¤íŠ¸: ê¸€ì ìˆ˜ / 3 ì •ë„ê°€ í† í° ìˆ˜ì— ê°€ê¹Œì›€
            estimated_tokens = len(content) // 3
            metadata_tokens = 50
            chunk_tokens.append({
                "chunk": chunk,
                "content": content,
                "tokens": estimated_tokens + metadata_tokens
            })

        # PPT ëª¨ë“œ: í—¤ë”/ëª©ì°¨/ìš”ì•½ ì„±ê²©ì˜ ì§§ì€ ì²­í¬ë“¤ì„ ìš°ì„  ë°°ì¹˜í•´ ë‹¤ì–‘ì„± í™•ë³´
        if ppt_mode and chunk_tokens:
            def _ppt_priority(ct: Dict[str, Any]) -> float:
                ch = ct["chunk"]
                fname = (ch.get("file_name") or "").lower()
                sec = (ch.get("section_title") or "").lower()
                txt = (ct.get("content") or "").lower()
                pri = 0.0
                # íŒŒì¼ ìœ í˜• ìš°ì„  (í…œí”Œë¦¿/ìƒ˜í”Œ/PPT)
                if any(k in fname for k in [".ppt", ".pptx", "template", "ìƒ˜í”Œ", "sample", "í…œí”Œë¦¿", "ì†Œê°œì„œ"]):
                    pri += 1.0
                # í—¤ë”/ëª©ì°¨/ìš”ì•½
                if any(k in sec for k in ["ëª©ì°¨", "ê°œìš”", "ìš”ì•½", "outline", "overview", "title", "ì œëª©"]):
                    pri += 0.6
                elif any(k in txt for k in ["ëª©ì°¨", "ê°œìš”", "ìš”ì•½", "outline", "overview"]):
                    pri += 0.3
                # ì§§ì„ìˆ˜ë¡ ë” ìš°ì„ 
                pri += max(0.0, 0.6 - (ct["tokens"] / max_tokens))
                # ê¸°ë³¸ ìœ ì‚¬ë„ ì ìˆ˜ë„ ì•½ê°„ ë°˜ì˜
                pri += float(ch.get("combined_score", ch.get("similarity_score", 0.0))) * 0.3
                return pri
            chunk_tokens = sorted(chunk_tokens, key=_ppt_priority, reverse=True)
        
        # ì²« ë²ˆì§¸ ì²­í¬ê°€ ë„ˆë¬´ í¬ë©´ ì˜ë¼ì„œ ì‚¬ìš©
        if chunk_tokens and chunk_tokens[0]["tokens"] > max_tokens * 0.8:
            first_chunk = chunk_tokens[0]
            # ì²« ë²ˆì§¸ ì²­í¬ë¥¼ ìµœëŒ€ í† í°ì˜ 60%ê¹Œì§€ë§Œ ì‚¬ìš© (ë‚˜ë¨¸ì§€ ì²­í¬ë“¤ì„ ìœ„í•´ ì—¬ìœ  í™•ë³´)
            max_first_tokens = int(max_tokens * 0.6)
            content_limit = int(max_first_tokens * 3)  # í† í° -> ê¸€ì ìˆ˜ ë³€í™˜ (í•œêµ­ì–´ ê¸°ì¤€)
            
            truncated_content = first_chunk["content"][:content_limit] + "..."
            chunk_info = f"[ë¬¸ì„œ 1: {first_chunk['chunk'].get('file_name', 'Unknown')} - ìœ ì‚¬ë„: {first_chunk['chunk'].get('combined_score', first_chunk['chunk'].get('similarity_score', 0)):.2f}] (ì¼ë¶€ ë‚´ìš©)"
            context_part = f"{chunk_info}\n{truncated_content}\n\n"
            
            context_parts.append(context_part)
            current_tokens = max_first_tokens
            used_chunks.append(first_chunk["chunk"])  # ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ ê¸°ë¡
            
            logger.info(f"âš ï¸ ì²« ë²ˆì§¸ ì²­í¬ê°€ í¼ (ì›ë³¸ {first_chunk['tokens']}í† í°) â†’ {max_first_tokens}í† í°ìœ¼ë¡œ ì¶•ì†Œ")
            
            # ë‚˜ë¨¸ì§€ ì²­í¬ë“¤ë„ ì¶”ê°€
            for i, chunk_data in enumerate(chunk_tokens[1:], 2):
                if current_tokens + chunk_data["tokens"] > max_tokens:
                    logger.info(f"âš ï¸ í† í° ì œí•œ ë„ë‹¬: {current_tokens + chunk_data['tokens']} > {max_tokens}, ì²­í¬ {i}ë¶€í„° ìƒëµ")
                    break
                    
                chunk_info = f"[ë¬¸ì„œ {i}: {chunk_data['chunk'].get('file_name', 'Unknown')} - ìœ ì‚¬ë„: {chunk_data['chunk'].get('combined_score', chunk_data['chunk'].get('similarity_score', 0)):.2f}]"
                context_part = f"{chunk_info}\n{chunk_data['content']}\n\n"
                
                context_parts.append(context_part)
                current_tokens += chunk_data["tokens"]
                used_chunks.append(chunk_data["chunk"])  # ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ ê¸°ë¡
        else:
            # ì¼ë°˜ì ì¸ ê²½ìš°: ëª¨ë“  ì²­í¬ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ê°€
            for i, chunk_data in enumerate(chunk_tokens):
                required_tokens = chunk_data["tokens"]
                if current_tokens + required_tokens > max_tokens and i > 0:
                    remaining = max_tokens - current_tokens
                    if remaining <= 80:
                        logger.info(f"âš ï¸ í† í° ì œí•œ ë„ë‹¬: {current_tokens + required_tokens} > {max_tokens}, ì²­í¬ {i+1}ë¶€í„° ìƒëµ")
                        break

                    logger.info(
                        f"âœ‚ï¸ í† í° í•œë„ ì´ˆê³¼ë¡œ ì²­í¬ {i+1} ì¶•ì†Œ: í•„ìš” {required_tokens}í† í° â†’ ê°€ìš© {remaining}í† í°"
                    )
                    content_limit = max(remaining * 3, 0)
                    truncated_content = chunk_data["content"][:content_limit] + "..."
                    chunk_info = f"[ë¬¸ì„œ {i+1}: {chunk_data['chunk'].get('file_name', 'Unknown')} - ìœ ì‚¬ë„: {chunk_data['chunk'].get('combined_score', chunk_data['chunk'].get('similarity_score', 0)):.2f}] (ì¼ë¶€ ë‚´ìš©)"
                    context_part = f"{chunk_info}\n{truncated_content}\n\n"

                    context_parts.append(context_part)
                    current_tokens += remaining
                    used_chunks.append(chunk_data["chunk"])  # ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ ê¸°ë¡
                    break
                
                chunk_info = f"[ë¬¸ì„œ {i+1}: {chunk_data['chunk'].get('file_name', 'Unknown')} - ìœ ì‚¬ë„: {chunk_data['chunk'].get('combined_score', chunk_data['chunk'].get('similarity_score', 0)):.2f}]"
                context_part = f"{chunk_info}\n{chunk_data['content']}\n\n"
                
                context_parts.append(context_part)
                current_tokens += required_tokens
                used_chunks.append(chunk_data["chunk"])  # ì‹¤ì œ ì‚¬ìš©ëœ ì²­í¬ ê¸°ë¡
        
        context_text = "".join(context_parts)
        
        logger.info(f"ğŸ“ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {len(context_parts)}ê°œ ì²­í¬ (ì „ì²´ {len(chunks)}ê°œ ì¤‘), ì•½ {current_tokens}í† í°")
        
        return context_text, int(current_tokens), used_chunks
    
    async def search_with_rag(
        self,
        rag_params: RAGSearchParams,
        container_ids: Optional[List[str]] = None,
        db_session: Optional[AsyncSession] = None,
        attachments: Optional[List[Dict[str, Any]]] = None  # ğŸ†• ì´ë¯¸ì§€ ì²¨ë¶€ ì •ë³´
    ) -> Dict[str, Any]:
        """
        AI Agentë¥¼ ìœ„í•œ í†µí•© RAG ê²€ìƒ‰
        
        Args:
            rag_params: RAG ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            container_ids: ì»¨í…Œì´ë„ˆ ID ëª©ë¡
            db_session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ (ì„ íƒì )
            attachments: ì²¨ë¶€ëœ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° (CLIP ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ìš©)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        from app.core.database import get_db
        
        try:
            # ğŸ†• ì´ë¯¸ì§€ ì²¨ë¶€ê°€ ìˆê³  ë¬¸ì„œê°€ ì„ íƒëœ ê²½ìš° CLIP ê¸°ë°˜ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰
            if attachments and rag_params.document_ids:
                image_attachments = [
                    att for att in attachments 
                    if att.get('mime_type', '').startswith('image/')
                ]
                
                if image_attachments:
                    logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²¨ë¶€ ê°ì§€ - CLIP ê¸°ë°˜ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì‘ ({len(image_attachments)}ê°œ ì´ë¯¸ì§€, {len(rag_params.document_ids)}ê°œ ë¬¸ì„œ)")
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ í™•ë³´
                    if db_session:
                        session = db_session
                    else:
                        async for session in get_db():
                            break
                    
                    # CLIP ê¸°ë°˜ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
                    image_search_result = await self._search_by_image_similarity(
                        session=session,
                        image_attachments=image_attachments,
                        document_ids=rag_params.document_ids,
                        limit=rag_params.limit,
                        threshold=rag_params.similarity_threshold
                    )
                    
                    if image_search_result and len(image_search_result.get('references', [])) > 0:
                        logger.info(f"âœ… ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ ì„±ê³µ - {len(image_search_result['references'])}ê°œ ì²­í¬ ë°˜í™˜")
                        return image_search_result
                    else:
                        logger.warning("âš ï¸ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±")
            
            # ì»¨í…Œì´ë„ˆ ID ì„¤ì •
            if container_ids:
                rag_params.container_ids = container_ids
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ í™•ë³´
            if db_session:
                session = db_session
                search_result = await self.search_for_rag_context(session, rag_params)
            else:
                async for session in get_db():
                    search_result = await self.search_for_rag_context(session, rag_params)
                    break
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
            # referencesëŠ” ì‹¤ì œ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ëœ used_chunksë¡œ ì œí•œ
            # all_referencesëŠ” í† í° êµ¬ì„± ì „ ìµœì¢… í›„ë³´ ì „ì²´(chunks)
            return {
                "references": search_result.used_chunks,
                "all_references": search_result.chunks,
                "context_text": search_result.context_text,
                "context_info": {
                    "total_chunks": len(search_result.chunks),
                    "used_chunks": len(search_result.used_chunks),
                    "context_tokens": search_result.total_tokens,
                    "search_mode": rag_params.search_mode,
                    "reranking_applied": search_result.reranking_applied,
                    "document_filtering": bool(rag_params.document_ids),
                    "filtered_document_count": len(rag_params.document_ids) if rag_params.document_ids else 0
                },
                "rag_stats": {
                    "query_length": len(rag_params.query),
                    "total_candidates": len(search_result.chunks),
                    "final_chunks": len(search_result.used_chunks),
                    "avg_similarity": search_result.search_stats.get("avg_similarity", 0),
                    "search_time": search_result.search_stats.get("search_time", 0),
                    "search_mode": rag_params.search_mode,
                    "has_korean_keywords": search_result.search_stats.get("has_korean_keywords", False),
                    "embedding_dimension": settings.get_current_embedding_dimension(),
                    "embedding_provider": settings.get_current_embedding_provider(),
                    "llm_provider": settings.get_current_llm_provider()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return {
                "references": [],
                "all_references": [],
                "context_text": "",
                "context_info": {
                    "total_chunks": 0,
                    "used_chunks": 0,
                    "context_tokens": 0,
                    "search_mode": rag_params.search_mode,
                    "reranking_applied": False,
                    "document_filtering": bool(rag_params.document_ids),
                    "filtered_document_count": 0,
                    "error": str(e)
                },
                "rag_stats": {
                    "query_length": len(rag_params.query),
                    "total_candidates": 0,
                    "final_chunks": 0,
                    "avg_similarity": 0,
                    "search_time": 0,
                    "search_mode": rag_params.search_mode,
                    "has_korean_keywords": False,
                    "embedding_dimension": settings.get_current_embedding_dimension(),
                    "embedding_provider": settings.get_current_embedding_provider(),
                    "llm_provider": settings.get_current_llm_provider()
                }
            }

    async def recommend_related_documents(
        self,
        query: str,
        exclude_document_ids: Optional[List[str]] = None,
        limit: int = 5,
        threshold: float = 0.2,
        db_session: Optional[AsyncSession] = None
    ) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì§ˆì˜ì™€ ì—°ê´€ëœ ë¬¸ì„œë¥¼ ì¶”ì²œ (ë¬¸ì„œ ì „ì²´ ìŠ¤ì½”í”„).

        ì „ëµ:
        1) ì§ˆì˜ ì„ë² ë”© ìƒì„±
        2) ë‚®ì€ threshold ë¡œ ì „ì²´ ì²­í¬ì—ì„œ í›„ë³´ ì¶”ì¶œ (ìƒí•œ ë„‰ë„‰íˆ)
        3) ë¬¸ì„œ ë‹¨ìœ„ë¡œ max(similarity), ë§¤ì¹­ ì²­í¬ ìˆ˜ë¥¼ ê¸°ì¤€ ì •ë ¬
        4) ìƒìœ„ Nê°œ ë°˜í™˜
        """
        from app.core.database import get_db
        try:
            # ì„ë² ë”© ìƒì„± - ì„¤ì •ëœ ì„œë¹„ìŠ¤ ì‚¬ìš©
            from app.services.core.embedding_service import EmbeddingService
            embedding_service = EmbeddingService()
            embedding_vector = await embedding_service.get_embedding(query)
            if not embedding_vector:
                return []

            # ì„¸ì…˜ í™•ë³´
            if db_session:
                session = db_session
                close_after = False
            else:
                # ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ì—ì„œ í•˜ë‚˜ êº¼ëƒ„
                session_gen = get_db()
                session = None
                async for s in session_gen:
                    session = s
                    break
                close_after = True

            if session is None:
                return []

            # ğŸ”·ğŸŸ§ í”„ë¡œë°”ì´ë”ë³„ ë²¡í„° ì»¬ëŸ¼ ë™ì  ì„ íƒ
            provider = settings.get_current_embedding_provider()
            embedding_dim = len(embedding_vector)
            
            if provider == 'bedrock' or embedding_dim == 1024:
                # AWS Bedrock: Titan 1024d
                vector_column = "tdc.aws_embedding_1024"
                vector_not_null = "tdc.aws_embedding_1024 IS NOT NULL"
                logger.info(f"[RAG-DOC-SEARCH] ğŸŸ§ AWS Bedrock ë²¡í„° ê²€ìƒ‰ (aws_embedding_1024, {embedding_dim}d)")
            elif provider == 'azure_openai' or embedding_dim == 1536:
                # Azure OpenAI: text-embedding-3-small 1536d
                vector_column = "tdc.azure_embedding_1536"
                vector_not_null = "tdc.azure_embedding_1536 IS NOT NULL"
                logger.info(f"[RAG-DOC-SEARCH] ğŸ”· Azure OpenAI ë²¡í„° ê²€ìƒ‰ (azure_embedding_1536, {embedding_dim}d)")
            else:
                # ë ˆê±°ì‹œ í´ë°±
                vector_column = "tdc.chunk_embedding"
                vector_not_null = "tdc.chunk_embedding IS NOT NULL"
                logger.warning(f"[RAG-DOC-SEARCH] âš ï¸ ë ˆê±°ì‹œ ë²¡í„° ì»¬ëŸ¼ í´ë°± ({embedding_dim}d)")

            # í›„ë³´ ì²­í¬ì—ì„œ ë¬¸ì„œ ë‹¨ìœ„ ì§‘ê³„
            base_sql = f"""
                SELECT 
                    fbi.file_bss_info_sno AS file_id,
                    fbi.file_lgc_nm AS file_name,
                    MAX(1 - ({vector_column} <=> :embedding_vector)) AS max_similarity,
                    COUNT(*) AS matched_chunks
                FROM vs_doc_contents_chunks tdc
                JOIN tb_file_bss_info fbi ON tdc.file_bss_info_sno = fbi.file_bss_info_sno
                WHERE {vector_not_null}
                  AND fbi.del_yn = 'N'
                  AND 1 - ({vector_column} <=> :embedding_vector) > :threshold
            """
            conditions = []
            params: Dict[str, Any] = {
                "embedding_vector": f"[{','.join(map(str, embedding_vector))}]",
                "threshold": threshold,
                "candidate_limit": max(limit * 5, 25)  # ì¶©ë¶„í•œ í›„ë³´ í™•ë³´
            }
            if exclude_document_ids:
                try:
                    exclude_int = [int(x) for x in exclude_document_ids]
                except ValueError:
                    exclude_int = []
                if exclude_int:
                    conditions.append("AND NOT (fbi.file_bss_info_sno = ANY(:exclude_ids))")
                    params["exclude_ids"] = exclude_int

            if conditions:
                base_sql += " " + " ".join(conditions)

            # ê·¸ë£¹ & ì •ë ¬
            base_sql += """
                GROUP BY fbi.file_bss_info_sno, fbi.file_lgc_nm
                ORDER BY max_similarity DESC, matched_chunks DESC
                LIMIT :candidate_limit
            """
            from sqlalchemy import text
            result = await session.execute(text(base_sql), params)
            rows = result.fetchall()

            recommendations = []
            for row in rows[:limit]:
                import math
                max_sim = float(row.max_similarity)
                matched_count = int(row.matched_chunks)
                
                # NaN ê°’ í•„í„°ë§
                if math.isnan(max_sim) or math.isinf(max_sim):
                    logger.debug(f"ì—°ê´€ ë¬¸ì„œ ì¶”ì²œì—ì„œ NaN ì ìˆ˜ ìŠ¤í‚µ: {row.file_name}")
                    continue
                    
                recommendations.append({
                    "file_id": row.file_id,
                    "file_name": row.file_name,
                    "max_similarity": max_sim,
                    "matched_chunks": matched_count
                })

            logger.info(f"ğŸ”— ì—°ê´€ ë¬¸ì„œ ì¶”ì²œ: {len(recommendations)}ê°œ (limit={limit})")
            return recommendations
        except Exception as e:
            logger.warning(f"ì—°ê´€ ë¬¸ì„œ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return []
    
    def _apply_rag_quality_filter(
        self, 
        results: List[Dict[str, Any]], 
        search_params: RAGSearchParams,
        query_analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        RAG ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í•„í„°ë§
        í‚¤ì›Œë“œ ë§¤ì¹˜ê°€ ì—†ê³  ë‚®ì€ ë²¡í„° ì ìˆ˜ë§Œ ìˆëŠ” ê²½ìš° ê´€ë ¨ì„± ê²€ì¦
        
        ê°œì„  ì‚¬í•­ (2025-10-16):
        - í‚¤ì›Œë“œê°€ ì—†ì„ ë•ŒëŠ” ìœ ì‚¬ë„ë§Œìœ¼ë¡œ í•„í„°ë§
        - ê³¼ë„í•œ í•„í„°ë§ ë°©ì§€
        """
        try:
            query_keywords = query_analysis.get("korean_keywords", [])
            query_text = search_params.query.lower()
            
            # ì¿¼ë¦¬ ìì²´ê°€ ë¹„ì–´ìˆìœ¼ë©´ í•„í„°ë§ ì—†ì´ ë°˜í™˜
            if not query_keywords and not query_text:
                return results
            
            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ìœ ì‚¬ë„ë§Œìœ¼ë¡œ í•„í„°ë§
            if not query_keywords:
                logger.info("ğŸ” í‚¤ì›Œë“œ ì—†ìŒ - ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„°ë§ë§Œ ì ìš©")
                filtered_results = []
                for result in results:
                    similarity = result.get("similarity_score", 0.0)
                    # ë‚®ì€ ìœ ì‚¬ë„ë§Œ ì œì™¸ (0.3 ì´í•˜)
                    if similarity >= 0.3:
                        filtered_results.append(result)
                    else:
                        logger.info(f"RAG í’ˆì§ˆ í•„í„°ë§ìœ¼ë¡œ ì œì™¸: {result.get('file_name', 'unknown')} "
                                  f"(ë‚®ì€ ìœ ì‚¬ë„: {similarity:.3f})")
                logger.info(f"RAG í’ˆì§ˆ í•„í„°ë§ (ìœ ì‚¬ë„ ê¸°ë°˜): {len(results)}ê°œ -> {len(filtered_results)}ê°œ")
                return filtered_results
            
            filtered_results = []
            
            for result in results:
                search_type = result.get("search_type", "")
                
                # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ëŠ” í•­ìƒ í†µê³¼
                if search_type == "keyword":
                    filtered_results.append(result)
                    continue
                
                # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ëŠ” ì¶”ê°€ ê²€ì¦
                if search_type == "semantic":
                    similarity = result.get("similarity_score", 0.0)
                    
                    # ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ (0.6 ì´ìƒ)ë©´ í†µê³¼
                    if similarity >= 0.6:
                        filtered_results.append(result)
                        continue
                    
                    # ì œëª©ì´ë‚˜ ë‚´ìš©ì—ì„œ ì¿¼ë¦¬ í‚¤ì›Œë“œ ë¶€ë¶„ ì¼ì¹˜ í™•ì¸
                    content = result.get("content", "").lower()
                    title = result.get("file_name", "").lower()
                    
                    # ì¿¼ë¦¬ í‚¤ì›Œë“œì™€ ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    has_partial_match = False
                    for keyword in query_keywords:
                        keyword_lower = keyword.lower()
                        if len(keyword_lower) >= 2:  # 2ê¸€ì ì´ìƒë§Œ ê²€ì‚¬
                            if keyword_lower in content or keyword_lower in title:
                                has_partial_match = True
                                break
                    
                    # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì „ì²´ì™€ë„ í™•ì¸ (2ê¸€ì ì´ìƒ)
                    if not has_partial_match and len(query_text) >= 2:
                        if query_text in content or query_text in title:
                            has_partial_match = True
                    
                    if has_partial_match:
                        filtered_results.append(result)
                    else:
                        logger.info(f"RAG í’ˆì§ˆ í•„í„°ë§ìœ¼ë¡œ ì œì™¸: {result.get('file_name', 'unknown')} "
                                  f"(í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜, ìœ ì‚¬ë„: {similarity:.3f})")
                else:
                    # ì•Œ ìˆ˜ ì—†ëŠ” ê²€ìƒ‰ íƒ€ì…ì€ í†µê³¼
                    filtered_results.append(result)
            
            logger.info(f"RAG í’ˆì§ˆ í•„í„°ë§: {len(results)}ê°œ -> {len(filtered_results)}ê°œ")
            return filtered_results
            
        except Exception as e:
            logger.error(f"RAG í’ˆì§ˆ í•„í„°ë§ ì˜¤ë¥˜: {e}")
            return results

    async def _search_by_image_similarity(
        self,
        session: AsyncSession,
        image_attachments: List[Dict[str, Any]],
        document_ids: List[str],
        limit: int = 10,
        threshold: float = 0.3
    ) -> Optional[Dict[str, Any]]:
        """
        CLIP ê¸°ë°˜ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰
        
        Args:
            session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            image_attachments: ì´ë¯¸ì§€ ì²¨ë¶€ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            document_ids: ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            limit: ìµœëŒ€ ë°˜í™˜ ì²­í¬ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (references, context_text, context_info, rag_stats)
        """
        from app.models.document_chunk import DocumentChunk
        from app.services.clip_embedding_service import clip_embedding_service
        from sqlalchemy import select, and_, or_, text
        import numpy as np
        import time
        
        try:
            start_time = time.time()
            
            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ blob_url ê°€ì ¸ì˜¤ê¸°
            first_image = image_attachments[0]
            image_blob_url = first_image.get('blob_url')
            
            if not image_blob_url:
                logger.warning("âš ï¸ ì´ë¯¸ì§€ blob_urlì´ ì—†ìŒ")
                return None
            
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì¤‘: {image_blob_url}")
            
            # CLIPì„ í†µí•´ ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±
            query_embedding = await clip_embedding_service.create_image_embedding(image_blob_url)
            
            if query_embedding is None:
                logger.error("âŒ ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return None
            
            logger.info(f"âœ… ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ì™„ë£Œ (dimension: {len(query_embedding)})")
            
            # pgvectorë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì²­í¬ ìœ ì‚¬ë„ ê²€ìƒ‰
            # document_idê°€ document_idsì— í¬í•¨ë˜ê³ , image_embeddingì´ ìˆëŠ” ì²­í¬ë§Œ ê²€ìƒ‰
            query_text = text("""
                SELECT 
                    dc.id,
                    dc.document_id,
                    dc.chunk_index,
                    dc.content,
                    dc.image_path,
                    dc.image_url,
                    d.file_name,
                    d.file_type,
                    d.container_id,
                    1 - (dc.image_embedding <=> :query_embedding) as similarity_score
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE 
                    dc.document_id = ANY(:document_ids)
                    AND dc.image_embedding IS NOT NULL
                    AND (1 - (dc.image_embedding <=> :query_embedding)) >= :threshold
                ORDER BY similarity_score DESC
                LIMIT :limit
            """)
            
            result = await session.execute(
                query_text,
                {
                    "query_embedding": str(query_embedding),
                    "document_ids": document_ids,
                    "threshold": threshold,
                    "limit": limit
                }
            )
            
            rows = result.fetchall()
            search_time = time.time() - start_time
            
            logger.info(f"ğŸ” ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: {len(rows)}ê°œ ì²­í¬ ë°œê²¬ (ì†Œìš”ì‹œê°„: {search_time:.3f}ì´ˆ)")
            
            if not rows:
                return None
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            chunks = []
            for row in rows:
                chunk = {
                    "id": str(row[0]),
                    "document_id": str(row[1]),
                    "chunk_index": row[2],
                    "content": row[3] or "",
                    "image_path": row[4],
                    "image_url": row[5],
                    "file_name": row[6],
                    "file_type": row[7],
                    "container_id": str(row[8]) if row[8] else None,
                    "similarity_score": float(row[9]),
                    "search_type": "image_similarity"
                }
                chunks.append(chunk)
            
            # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
            context_parts = []
            for idx, chunk in enumerate(chunks, 1):
                context_parts.append(
                    f"[ë¬¸ì„œ {idx}: {chunk['file_name']} - í˜ì´ì§€ {chunk['chunk_index'] + 1}] "
                    f"(ì´ë¯¸ì§€ ìœ ì‚¬ë„: {chunk['similarity_score']:.3f})\n"
                    f"{chunk['content']}\n"
                )
            
            context_text = "\n".join(context_parts)
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                "references": chunks,
                "all_references": chunks,
                "context_text": context_text,
                "context_info": {
                    "total_chunks": len(chunks),
                    "used_chunks": len(chunks),
                    "context_tokens": len(context_text.split()),
                    "search_mode": "image_similarity",
                    "reranking_applied": False,
                    "document_filtering": True,
                    "filtered_document_count": len(document_ids),
                    "image_search": True
                },
                "rag_stats": {
                    "query_length": 0,
                    "total_candidates": len(chunks),
                    "final_chunks": len(chunks),
                    "avg_similarity": sum(c['similarity_score'] for c in chunks) / len(chunks),
                    "search_time": search_time,
                    "search_mode": "image_similarity",
                    "has_korean_keywords": False,
                    "embedding_dimension": len(query_embedding),
                    "embedding_provider": "clip",
                    "llm_provider": settings.get_current_llm_provider()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return None

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rag_search_service = RAGSearchService()

"""ì„¹ì…˜ ì¸ì‹ ì²­í‚¹ ìœ í‹¸ë¦¬í‹°
=================================

í•™ìˆ  ë…¼ë¬¸ì„ ìœ„í•œ ì„¹ì…˜ êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹:
1. ì„¹ì…˜ ê²½ê³„ë¥¼ ì¡´ì¤‘í•˜ì—¬ ì²­í‚¹ (ì„¹ì…˜ ë‚´ìš©ì´ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• ë˜ì§€ ì•Šë„ë¡)
2. References ì´í›„ ì½˜í…ì¸  ì œì™¸ (ì €ì ì‚¬ì§„, acknowledgments ë“±)
3. ì„¹ì…˜ íƒ€ì…ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ì²­í‚¹ ì „ëµ:
   - Abstract/Introduction/Conclusion: ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
   - Methods/Results/Discussion: ì„œë¸Œì„¹ì…˜ ë‹¨ìœ„ ë˜ëŠ” í† í° ê¸°ë°˜ ë¶„í• 
4. ê° ì²­í¬ì— ì„¹ì…˜ ë©”íƒ€ë°ì´í„° í¬í•¨ (ê²€ìƒ‰ ì‹œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ)
"""
from __future__ import annotations

from typing import List, Dict, Any, Optional, Set, Tuple
import logging

logger = logging.getLogger(__name__)

try:
    import tiktoken
    _TOKENIZER = tiktoken.get_encoding("cl100k_base")
except Exception:
    _TOKENIZER = None


def _count_tokens(text: str) -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    if not text:
        return 0
    if _TOKENIZER:
        try:
            return len(_TOKENIZER.encode(text))
        except Exception:
            return len(text.split())
    return len(text.split())


def should_exclude_section(section_type: str, section_title: str) -> bool:
    """
    ì„¹ì…˜ì„ ì²­í‚¹ì—ì„œ ì œì™¸í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨
    
    Args:
        section_type: í‘œì¤€ ì„¹ì…˜ íƒ€ì… (references, acknowledgments ë“±)
        section_title: ì„¹ì…˜ ì›ë³¸ ì œëª©
    
    Returns:
        True if ì œì™¸í•´ì•¼ í•¨, False otherwise
    """
    # References ì´í›„ ì„¹ì…˜ ì œì™¸
    exclude_types = {
        'references',
        'bibliography', 
        'acknowledgments',
        'acknowledgements',
        'appendix'
    }
    
    if section_type in exclude_types:
        return True
    
    # ì œëª© ê¸°ë°˜ ì¶”ê°€ í•„í„°ë§
    title_lower = section_title.lower()
    exclude_keywords = [
        'about the author',
        'author',
        'contributor',
        'biography',
        'photo',
        'funding',
        'conflict of interest',
        'ethical approval'
    ]
    
    for keyword in exclude_keywords:
        if keyword in title_lower:
            return True
    
    return False


def chunk_by_sections(
    sections: List[Dict[str, Any]],
    full_text: str,
    *,
    min_tokens: int = 80,
    target_tokens: int = 280,
    max_tokens: int = 420,
    overlap_tokens: int = 40,
) -> List[Dict[str, Any]]:
    """
    ì„¹ì…˜ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ì²­í‚¹
    
    Args:
        sections: AdaptiveSectionDetector.detect_sections()ì˜ ê²°ê³¼
        full_text: ì „ì²´ í…ìŠ¤íŠ¸ (markdown ë˜ëŠ” plain text)
        min_tokens: ìµœì†Œ í† í° ìˆ˜
        target_tokens: ëª©í‘œ í† í° ìˆ˜
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        overlap_tokens: ì²­í¬ ê°„ ì˜¤ë²„ë© í† í° ìˆ˜
    
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸: [{
            'content_text': str,
            'token_count': int,
            'char_count': int,
            'section_type': str,
            'section_title': str,
            'section_level': int,
            'chunk_index': int,  # ì„¹ì…˜ ë‚´ ì²­í¬ ìˆœì„œ
            'page_numbers': Set[int],
        }]
    """
    if not sections:
        logger.warning("[SECTION-CHUNK] ì„¹ì…˜ ì •ë³´ê°€ ì—†ì–´ ê¸°ë³¸ ì²­í‚¹ìœ¼ë¡œ í´ë°±")
        return _fallback_chunk(full_text, min_tokens, target_tokens, max_tokens)
    
    chunks = []
    references_reached = False
    
    logger.info(f"[SECTION-CHUNK] {len(sections)}ê°œ ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ ì‹œì‘")
    
    for section in sections:
        section_type = section.get('type', 'other')
        section_title = section.get('original_title', 'Unknown')
        section_level = section.get('level', 1)
        start_pos = section.get('start_pos', 0)
        end_pos = section.get('end_pos', len(full_text))
        page_start = section.get('page_start', 1)
        page_end = section.get('page_end', page_start)
        
        # References ë„ë‹¬ ì‹œ ì´í›„ ì„¹ì…˜ ëª¨ë‘ ì œì™¸
        if section_type == 'references':
            references_reached = True
            logger.info(f"[SECTION-CHUNK] âœ‚ï¸ References ì„¹ì…˜ ë„ë‹¬, ì´í›„ ì½˜í…ì¸  ì œì™¸: '{section_title}'")
            break
        
        # ì œì™¸ ëŒ€ìƒ ì„¹ì…˜ ì²´í¬
        if should_exclude_section(section_type, section_title):
            logger.info(f"[SECTION-CHUNK] â­ï¸ ì„¹ì…˜ ì œì™¸: '{section_title}' (type={section_type})")
            continue
        
        # ì„¹ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        section_text = full_text[start_pos:end_pos].strip()
        if not section_text:
            logger.debug(f"[SECTION-CHUNK] âš ï¸ ë¹ˆ ì„¹ì…˜: '{section_title}'")
            continue
        
        section_token_count = _count_tokens(section_text)
        
        # ì„¹ì…˜ íƒ€ì…ì— ë”°ë¥¸ ì²­í‚¹ ì „ëµ
        if section_type in ['abstract', 'introduction', 'conclusion']:
            # ì§§ì€ ì„¹ì…˜: ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
            if section_token_count <= max_tokens:
                chunks.append({
                    'content_text': section_text,
                    'token_count': section_token_count,
                    'char_count': len(section_text),
                    'section_type': section_type,
                    'section_title': section_title,
                    'section_level': section_level,
                    'chunk_index': 0,
                    'total_chunks': 1,
                    'page_numbers': set(range(page_start, page_end + 1)),
                    'chunking_strategy': 'single_section'
                })
                logger.info(
                    f"[SECTION-CHUNK] âœ… '{section_title}' â†’ ë‹¨ì¼ ì²­í¬ "
                    f"({section_token_count} tokens)"
                )
            else:
                # ë„ˆë¬´ ê¸´ ê²½ìš° í† í° ê¸°ë°˜ ë¶„í• 
                section_chunks = _split_section_by_tokens(
                    section_text,
                    section_type,
                    section_title,
                    section_level,
                    page_start,
                    page_end,
                    target_tokens,
                    max_tokens,
                    overlap_tokens
                )
                chunks.extend(section_chunks)
                logger.info(
                    f"[SECTION-CHUNK] âœ… '{section_title}' â†’ {len(section_chunks)}ê°œ ì²­í¬ "
                    f"(í† í° ê¸°ë°˜ ë¶„í• , {section_token_count} tokens)"
                )
        
        elif section_type in ['methods', 'results', 'discussion']:
            # ì¤‘ê°„ ê¸¸ì´ ì„¹ì…˜: ì„œë¸Œì„¹ì…˜ í™•ì¸ í›„ ì²˜ë¦¬
            subsections = section.get('subsections', [])
            
            if subsections and len(subsections) > 1:
                # ì„œë¸Œì„¹ì…˜ì´ ìˆìœ¼ë©´ ì„œë¸Œì„¹ì…˜ ë‹¨ìœ„ë¡œ ì²­í‚¹
                logger.info(
                    f"[SECTION-CHUNK] ğŸ” '{section_title}' â†’ {len(subsections)}ê°œ ì„œë¸Œì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹"
                )
                # TODO: ì„œë¸Œì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ êµ¬í˜„
                # í˜„ì¬ëŠ” í† í° ê¸°ë°˜ìœ¼ë¡œ í´ë°±
                section_chunks = _split_section_by_tokens(
                    section_text,
                    section_type,
                    section_title,
                    section_level,
                    page_start,
                    page_end,
                    target_tokens,
                    max_tokens,
                    overlap_tokens
                )
                chunks.extend(section_chunks)
            else:
                # ì„œë¸Œì„¹ì…˜ ì—†ìœ¼ë©´ í† í° ê¸°ë°˜ ë¶„í• 
                section_chunks = _split_section_by_tokens(
                    section_text,
                    section_type,
                    section_title,
                    section_level,
                    page_start,
                    page_end,
                    target_tokens,
                    max_tokens,
                    overlap_tokens
                )
                chunks.extend(section_chunks)
                logger.info(
                    f"[SECTION-CHUNK] âœ… '{section_title}' â†’ {len(section_chunks)}ê°œ ì²­í¬ "
                    f"({section_token_count} tokens)"
                )
        
        else:
            # ê¸°íƒ€ ì„¹ì…˜: í† í° ê¸°ë°˜ ë¶„í• 
            section_chunks = _split_section_by_tokens(
                section_text,
                section_type,
                section_title,
                section_level,
                page_start,
                page_end,
                target_tokens,
                max_tokens,
                overlap_tokens
            )
            chunks.extend(section_chunks)
            logger.info(
                f"[SECTION-CHUNK] âœ… '{section_title}' (type={section_type}) â†’ "
                f"{len(section_chunks)}ê°œ ì²­í¬ ({section_token_count} tokens)"
            )
    
    logger.info(
        f"[SECTION-CHUNK] ğŸ‰ ì²­í‚¹ ì™„ë£Œ: ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„± "
        f"(References ì´í›„ ì œì™¸: {references_reached})"
    )
    
    return chunks


def _split_section_by_tokens(
    section_text: str,
    section_type: str,
    section_title: str,
    section_level: int,
    page_start: int,
    page_end: int,
    target_tokens: int,
    max_tokens: int,
    overlap_tokens: int
) -> List[Dict[str, Any]]:
    """
    ì„¹ì…˜ì„ í† í° ê¸°ë°˜ìœ¼ë¡œ ë¶„í• 
    
    ë¬¸ë‹¨ ê²½ê³„ë¥¼ ì¡´ì¤‘í•˜ë©´ì„œ target_tokensì— ê°€ê¹ê²Œ ë¶„í• 
    """
    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    paragraphs = [p.strip() for p in section_text.split('\n\n') if p.strip()]
    
    if not paragraphs:
        return []
    
    chunks = []
    current_buffer = []
    current_token_count = 0
    chunk_index = 0
    
    for para in paragraphs:
        para_tokens = _count_tokens(para)
        
        # í˜„ì¬ ë²„í¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì¶”ê°€
        if not current_buffer:
            current_buffer.append(para)
            current_token_count = para_tokens
            continue
        
        # ì¶”ê°€í–ˆì„ ë•Œ max_tokensë¥¼ ì´ˆê³¼í•˜ë©´ í˜„ì¬ ì²­í¬ í”ŒëŸ¬ì‹œ
        if current_token_count + para_tokens > max_tokens:
            # í˜„ì¬ ë²„í¼ë¥¼ ì²­í¬ë¡œ ì €ì¥
            chunk_text = '\n\n'.join(current_buffer)
            chunks.append({
                'content_text': chunk_text,
                'token_count': current_token_count,
                'char_count': len(chunk_text),
                'section_type': section_type,
                'section_title': section_title,
                'section_level': section_level,
                'chunk_index': chunk_index,
                'page_numbers': set(range(page_start, page_end + 1)),
                'chunking_strategy': 'token_based'
            })
            chunk_index += 1
            
            # ì˜¤ë²„ë© ì²˜ë¦¬ (ë§ˆì§€ë§‰ ë¬¸ë‹¨ ì¼ë¶€ í¬í•¨)
            if overlap_tokens > 0 and current_buffer:
                last_para = current_buffer[-1]
                last_para_tokens = _count_tokens(last_para)
                if last_para_tokens <= overlap_tokens:
                    current_buffer = [last_para, para]
                    current_token_count = last_para_tokens + para_tokens
                else:
                    current_buffer = [para]
                    current_token_count = para_tokens
            else:
                current_buffer = [para]
                current_token_count = para_tokens
        
        # target_tokensì— ê°€ê¹Œìš°ë©´ ì¶”ê°€
        elif current_token_count + para_tokens >= target_tokens:
            current_buffer.append(para)
            chunk_text = '\n\n'.join(current_buffer)
            current_token_count += para_tokens
            
            chunks.append({
                'content_text': chunk_text,
                'token_count': current_token_count,
                'char_count': len(chunk_text),
                'section_type': section_type,
                'section_title': section_title,
                'section_level': section_level,
                'chunk_index': chunk_index,
                'page_numbers': set(range(page_start, page_end + 1)),
                'chunking_strategy': 'token_based'
            })
            chunk_index += 1
            current_buffer = []
            current_token_count = 0
        
        else:
            # ì•„ì§ targetì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ê³„ì† ì¶”ê°€
            current_buffer.append(para)
            current_token_count += para_tokens
    
    # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
    if current_buffer:
        chunk_text = '\n\n'.join(current_buffer)
        chunks.append({
            'content_text': chunk_text,
            'token_count': current_token_count,
            'char_count': len(chunk_text),
            'section_type': section_type,
            'section_title': section_title,
            'section_level': section_level,
            'chunk_index': chunk_index,
            'page_numbers': set(range(page_start, page_end + 1)),
            'chunking_strategy': 'token_based'
        })
    
    # total_chunks ë©”íƒ€ë°ì´í„° ì¶”ê°€
    for chunk in chunks:
        chunk['total_chunks'] = len(chunks)
    
    return chunks


def _fallback_chunk(
    text: str,
    min_tokens: int,
    target_tokens: int,
    max_tokens: int
) -> List[Dict[str, Any]]:
    """ì„¹ì…˜ ì •ë³´ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ í† í° ê¸°ë°˜ ì²­í‚¹"""
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    if not paragraphs:
        return []
    
    chunks = []
    current_buffer = []
    current_token_count = 0
    
    for para in paragraphs:
        para_tokens = _count_tokens(para)
        
        if not current_buffer:
            current_buffer.append(para)
            current_token_count = para_tokens
            continue
        
        if current_token_count + para_tokens > max_tokens:
            chunk_text = '\n\n'.join(current_buffer)
            chunks.append({
                'content_text': chunk_text,
                'token_count': current_token_count,
                'char_count': len(chunk_text),
                'section_type': 'unknown',
                'section_title': 'Unknown',
                'chunk_index': len(chunks),
                'chunking_strategy': 'fallback'
            })
            current_buffer = [para]
            current_token_count = para_tokens
        elif current_token_count + para_tokens >= target_tokens:
            current_buffer.append(para)
            chunk_text = '\n\n'.join(current_buffer)
            current_token_count += para_tokens
            chunks.append({
                'content_text': chunk_text,
                'token_count': current_token_count,
                'char_count': len(chunk_text),
                'section_type': 'unknown',
                'section_title': 'Unknown',
                'chunk_index': len(chunks),
                'chunking_strategy': 'fallback'
            })
            current_buffer = []
            current_token_count = 0
        else:
            current_buffer.append(para)
            current_token_count += para_tokens
    
    if current_buffer:
        chunk_text = '\n\n'.join(current_buffer)
        chunks.append({
            'content_text': chunk_text,
            'token_count': current_token_count,
            'char_count': len(chunk_text),
            'section_type': 'unknown',
            'section_title': 'Unknown',
            'chunk_index': len(chunks),
            'chunking_strategy': 'fallback'
        })
    
    return chunks


def filter_objects_before_references(
    sections: List[Dict[str, Any]],
    objects: List[Any]
) -> Tuple[List[Any], Optional[int]]:
    """
    References ì„¹ì…˜ ì´ì „ì˜ ê°ì²´ë“¤ë§Œ í•„í„°ë§
    
    Args:
        sections: ì„¹ì…˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        objects: ì¶”ì¶œëœ ê°ì²´ ë¦¬ìŠ¤íŠ¸ (í…Œì´ë¸”, ì´ë¯¸ì§€ ë“±)
    
    Returns:
        (filtered_objects, references_page): í•„í„°ë§ëœ ê°ì²´ ë¦¬ìŠ¤íŠ¸ì™€ References ì‹œì‘ í˜ì´ì§€
    """
    # References ì„¹ì…˜ ì°¾ê¸°
    references_section = None
    for section in sections:
        if section.get('type') == 'references':
            references_section = section
            break
    
    if not references_section:
        logger.info("[SECTION-FILTER] References ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨, ëª¨ë“  ê°ì²´ í¬í•¨")
        return objects, None
    
    references_page = references_section.get('page_start')
    references_pos = references_section.get('start_pos', float('inf'))
    
    # ğŸ†• page_startê°€ 1ì´ë©´ì„œ start_posê°€ í° ê²½ìš° (ì¦‰, ë¬¸ì„œ í›„ë°˜ë¶€) ì˜ì‹¬ìŠ¤ëŸ¬ì›€
    # ReferencesëŠ” ë³´í†µ ë¬¸ì„œ ëì— ìˆìœ¼ë¯€ë¡œ page=1ì€ ì˜ëª»ëœ ê°ì§€ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
    suspected_invalid_page = (references_page == 1 and references_pos > 30000)
    
    # page_startê°€ ì—†ê±°ë‚˜ ì˜ëª»ëœ ê²½ìš° None ì‚¬ìš© (bbox ê¸°ë°˜ ì¶”ì •ì— ì˜ì¡´)
    if not references_page or references_page <= 0 or references_page == float('inf') or suspected_invalid_page:
        if suspected_invalid_page:
            logger.warning(
                f"[SECTION-FILTER] References ì„¹ì…˜ page_start=1 ì˜ì‹¬ìŠ¤ëŸ¬ì›€ (pos={references_pos} > 30000), "
                f"í•„í„°ë§ ë¹„í™œì„±í™” (bbox ê¸°ë°˜ ì¶”ì • í•„ìš”)"
            )
        references_page = None
        logger.info(
            f"[SECTION-FILTER] References ì„¹ì…˜ ë°œê²¬ (page_start ë¯¸ì„¤ì •, í•„í„°ë§ ê±´ë„ˆëœ€), pos={references_pos}"
        )
    else:
        logger.info(
            f"[SECTION-FILTER] References ì„¹ì…˜ ì‹œì‘: page={references_page}, pos={references_pos}"
        )
    
    # í˜ì´ì§€ ë²ˆí˜¸ ë˜ëŠ” ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§
    filtered = []
    excluded_count = 0
    
    for obj in objects:
        obj_page = getattr(obj, 'page_no', None) or getattr(obj, 'page', None)
        obj_type = getattr(obj, 'object_type', 'unknown')
        
        # references_pageê°€ Noneì´ë©´ í•„í„°ë§í•˜ì§€ ì•Šê³  ëª¨ë‘ í¬í•¨
        if references_page is None:
            filtered.append(obj)
            continue
        
        # í˜ì´ì§€ ê¸°ë°˜ í•„í„°ë§
        if obj_page and obj_page < references_page:
            filtered.append(obj)
        elif obj_page and obj_page >= references_page:
            excluded_count += 1
            logger.debug(
                f"[SECTION-FILTER] ì œì™¸: {obj_type} (page={obj_page}, "
                f"references_page={references_page})"
            )
        else:
            # í˜ì´ì§€ ì •ë³´ ì—†ìœ¼ë©´ í¬í•¨ (ì•ˆì „í•œ ì„ íƒ)
            filtered.append(obj)
    
    logger.info(
        f"[SECTION-FILTER] í•„í„°ë§ ì™„ë£Œ: {len(filtered)}ê°œ í¬í•¨, {excluded_count}ê°œ ì œì™¸"
    )
    
    return filtered, references_page

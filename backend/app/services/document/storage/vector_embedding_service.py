"""
ğŸ”® ë²¡í„° ì„ë² ë”© ì„œë¹„ìŠ¤
==================

í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì˜ë¯¸ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì„œë¹„ìŠ¤
- ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ ì§€ì› (AWS Bedrock, OpenAI, Local)
- ì²­í‚¹ ë° ë²¡í„° ì €ì¥
- ìœ ì‚¬ë„ ê²€ìƒ‰
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
import json
from datetime import datetime
from app.core.config import settings

logger = logging.getLogger(__name__)

class VectorEmbeddingService:
    """ë²¡í„° ì„ë² ë”© ë° ì €ì¥ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.chunk_size = 1000  # ê¸°ë³¸ ì²­í¬ í¬ê¸°
        self.chunk_overlap = 200  # ì²­í¬ ì˜¤ë²„ë©
        self.max_chunks = 50  # ìµœëŒ€ ì²­í¬ ìˆ˜
        
    async def process_document_for_search(
        self, 
        text: str, 
        document_id: int,
        container_id: str
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            text: ë¬¸ì„œ í…ìŠ¤íŠ¸
            document_id: ë¬¸ì„œ ID
            container_id: ì»¨í…Œì´ë„ˆ ID
            
        Returns:
            Dict containing processing results
        """
        try:
            if not text or not text.strip():
                return self._empty_processing_result()
            
            # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self._chunk_text(text)
            
            # 2ë‹¨ê³„: ë²¡í„° ì„ë² ë”© ìƒì„± (í˜„ì¬ëŠ” ëª¨ì˜ ì²˜ë¦¬)
            embeddings = await self._create_embeddings(chunks)
            
            # 3ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ìƒì„±
            chunk_metadata = self._create_chunk_metadata(chunks, document_id, container_id)
            
            result = {
                "success": True,
                "chunk_count": len(chunks),
                "embedding_count": len(embeddings),
                "chunks": chunks,
                "embeddings": embeddings,
                "metadata": chunk_metadata,
                "processing_time": datetime.now().isoformat(),
                "total_tokens": sum(len(chunk.split()) for chunk in chunks)
            }
            
            logger.info(f"ë²¡í„° ì²˜ë¦¬ ì™„ë£Œ - ë¬¸ì„œ ID: {document_id}, ì²­í¬ ìˆ˜: {len(chunks)}")
            return result
            
        except Exception as e:
            logger.error(f"ë²¡í„° ì²˜ë¦¬ ì‹¤íŒ¨ - ë¬¸ì„œ ID: {document_id}, ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                **self._empty_processing_result()
            }
    
    def _chunk_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        if not text:
            return []
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = self._split_into_sentences(text)
        
        chunks = []
        current_chunk = ""
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ì‹œì‘
            if current_size + sentence_size > self.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
                current_size = sentence_size
            else:
                current_chunk += " " + sentence if current_chunk else sentence
                current_size += sentence_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        # ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œ
        if len(chunks) > self.max_chunks:
            chunks = chunks[:self.max_chunks]
            logger.warning(f"ì²­í¬ ìˆ˜ê°€ {self.max_chunks}ê°œë¡œ ì œí•œë¨")
        
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• """
        import re
        
        # í•œêµ­ì–´ ë¬¸ì¥ ë¶„í•  íŒ¨í„´
        sentence_endings = r'[.!?]+'
        sentences = re.split(sentence_endings, text)
        
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    async def _create_embeddings(self, chunks: List[str]) -> List[List[float]]:
        """ì²­í¬ë“¤ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„° ìƒì„± (í˜„ì¬ëŠ” ëª¨ì˜ ì²˜ë¦¬)"""
        embeddings = []
        
        for i, chunk in enumerate(chunks):
            # TODO: ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ í˜¸ì¶œ
            # í˜„ì¬ëŠ” ì²­í¬ ê¸¸ì´ì™€ í•´ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°€ìƒ ë²¡í„° ìƒì„±
            mock_vector = self._create_mock_vector(chunk, vector_dim=settings.get_current_embedding_dimension())
            embeddings.append(mock_vector)
        
        return embeddings
    
    def _create_mock_vector(self, text: str, vector_dim: int = None) -> List[float]:
        """ëª¨ì˜ ë²¡í„° ìƒì„± (ì‹¤ì œ êµ¬í˜„ ì „ê¹Œì§€ ì‚¬ìš©)"""
        if vector_dim is None:
            vector_dim = settings.get_current_embedding_dimension()
        import hashlib
        import struct
        
        # í…ìŠ¤íŠ¸ í•´ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œë“œ ìƒì„±
        text_hash = hashlib.md5(text.encode()).digest()
        seed = struct.unpack('I', text_hash[:4])[0]
        
        # ì‹œë“œë¥¼ ì´ìš©í•œ ì˜ì‚¬ ëœë¤ ë²¡í„° ìƒì„±
        import random
        random.seed(seed)
        
        vector = [random.uniform(-1.0, 1.0) for _ in range(vector_dim)]
        
        # ë²¡í„° ì •ê·œí™”
        magnitude = sum(x * x for x in vector) ** 0.5
        if magnitude > 0:
            vector = [x / magnitude for x in vector]
        
        return vector
    
    def _create_chunk_metadata(
        self, 
        chunks: List[str], 
        document_id: int, 
        container_id: str
    ) -> List[Dict[str, Any]]:
        """ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        metadata_list = []
        
        for i, chunk in enumerate(chunks):
            metadata = {
                "chunk_id": f"{document_id}_chunk_{i+1}",
                "document_id": document_id,
                "container_id": container_id,
                "chunk_sequence": i + 1,
                "chunk_text": chunk,
                "chunk_length": len(chunk),
                "word_count": len(chunk.split()),
                "created_at": datetime.now().isoformat(),
                "embedding_model": settings.get_current_embedding_model(),
                "embedding_dimension": settings.get_current_embedding_dimension()
            }
            metadata_list.append(metadata)
        
        return metadata_list
    
    def _empty_processing_result(self) -> Dict[str, Any]:
        """ë¹ˆ ì²˜ë¦¬ ê²°ê³¼ ë°˜í™˜"""
        return {
            "chunk_count": 0,
            "embedding_count": 0,
            "chunks": [],
            "embeddings": [],
            "metadata": [],
            "processing_time": datetime.now().isoformat(),
            "total_tokens": 0
        }
    
    async def store_vectors_to_database(
        self, 
        embeddings: List[List[float]], 
        metadata: List[Dict[str, Any]],
        session  # AsyncSession
    ) -> bool:
        """
        ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        TODO: PostgreSQL pgvector í…Œì´ë¸”ì— ì €ì¥ êµ¬í˜„
        """
        try:
            # TODO: TbDocumentSearchIndex ë° VsDocContentsChunks í…Œì´ë¸”ì— ì €ì¥
            # í˜„ì¬ëŠ” ë¡œê·¸ë§Œ ì¶œë ¥
            logger.info(f"ë²¡í„° ì €ì¥ ì‹œë®¬ë ˆì´ì…˜ - ë²¡í„° ìˆ˜: {len(embeddings)}, ë©”íƒ€ë°ì´í„° ìˆ˜: {len(metadata)}")
            
            for i, (embedding, meta) in enumerate(zip(embeddings, metadata)):
                logger.debug(f"ì²­í¬ {i+1} ì €ì¥ - ë²¡í„° ì°¨ì›: {len(embedding)}, í…ìŠ¤íŠ¸ ê¸¸ì´: {meta['chunk_length']}")
            
            return True
            
        except Exception as e:
            logger.error(f"ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    async def search_similar_documents(
        self, 
        query_text: str, 
        container_id: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        TODO: ì‹¤ì œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ êµ¬í˜„
        """
        try:
            # TODO: ì¿¼ë¦¬ ë²¡í„°í™” ë° ìœ ì‚¬ë„ ê²€ìƒ‰
            logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ - ì¿¼ë¦¬: {query_text[:50]}..., ì»¨í…Œì´ë„ˆ: {container_id}")
            
            # ëª¨ì˜ ê²€ìƒ‰ ê²°ê³¼
            mock_results = [
                {
                    "document_id": 1,
                    "chunk_id": "1_chunk_1",
                    "similarity_score": 0.85,
                    "chunk_text": "ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© ì˜ˆì‹œ...",
                    "container_id": container_id or "WJ_HR"
                }
            ]
            
            return mock_results
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
vector_embedding_service = VectorEmbeddingService()

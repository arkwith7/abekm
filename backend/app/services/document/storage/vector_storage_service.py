"""
ğŸ”® ë²¡í„° ì €ì¥ ì„œë¹„ìŠ¤ - vs_ ì¤‘ì‹¬ í†µí•© ì•„í‚¤í…ì²˜ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ë§ì¶¤)
================================================

ìµœì  í†µí•© ë°©ì•ˆ 1: vs_ ì¤‘ì‹¬ í™œìš© â­ [ê¶Œì¥]

ğŸ¯ ì—­í•  ë¶„ë‹´:
  Primary Vector Store: vs_doc_contents_index
    - ëª¨ë“  ë¬¸ì„œ ì²­í¬ ë²¡í„° ì €ì¥ (vector(1024))
    - ë¹ ë¥¸ ë²¡í„° ê²€ìƒ‰ ìµœì í™”
    - knowledge_container_id ì»¬ëŸ¼ìœ¼ë¡œ ê¶Œí•œ ê´€ë¦¬
  
  Metadata & Reference: tb_document_chunks  
    - ìƒì„¸ ë©”íƒ€ë°ì´í„° (í˜ì´ì§€, ì„¹ì…˜) (vector(768))
    - ì°¸ì¡° ì¶”ì  ("3í˜ì´ì§€ 2ë²ˆì§¸ ë‹¨ë½")
    - ê¶Œí•œ ë° ì»¨í…Œì´ë„ˆ ì •ë³´
  
  Hybrid Search: tb_search_documents
    - í‚¤ì›Œë“œ + ë²¡í„° ì¡°í•© ê²€ìƒ‰
    - tsvector ìµœì í™”
    - ì‹¤ì‹œê°„ ê²€ìƒ‰ ì„±ëŠ¥

PostgreSQL + pgvector í™œìš©, ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ë°˜ì˜
"""

import json
import logging
from typing import Dict, Any, List, Optional
import numpy as np
from datetime import datetime

# Database imports
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession
from app.core.database import get_async_session_local
from app.core.config import settings
from app.services.core.korean_nlp_service import korean_nlp_service

logger = logging.getLogger(__name__)

class VectorStorageService:
    """
    ğŸ”® vs_ ì¤‘ì‹¬ í†µí•© ë²¡í„° ì €ì¥ ì„œë¹„ìŠ¤ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ë§ì¶¤)
    ===================================================
    
    Primary: vs_doc_contents_index (ë²¡í„° ê²€ìƒ‰ - vector(1024))
    Metadata: tb_document_chunks (ì°¸ì¡° ì¶”ì  - vector(768))  
    Hybrid: tb_search_documents (í‚¤ì›Œë“œ+ë²¡í„°)
    """
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(VectorStorageService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """ë²¡í„° ì €ì¥ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ
        if VectorStorageService._initialized:
            return
            
        # ë™ì  ì°¨ì› ì„¤ì • - í˜„ì¬ ì„¤ì •ëœ ì„ë² ë”© ëª¨ë¸ì— ë”°ë¼ ê²°ì •
        self.embedding_dimension = settings.get_current_embedding_dimension()
        logger.info(f"ğŸ”® vs_ ì¤‘ì‹¬ ë²¡í„° ì €ì¥ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ - ì„ë² ë”© ì°¨ì›: {self.embedding_dimension}")
        logger.info(f"í˜„ì¬ ì„ë² ë”© ëª¨ë¸: {settings.get_current_embedding_model()}")
        print(f"âœ… VectorStorageService ì´ˆê¸°í™” ì„±ê³µ - vs_ ì¤‘ì‹¬ ì•„í‚¤í…ì²˜, ì°¨ì›: {self.embedding_dimension}")
        print(f"ğŸ“Š í˜„ì¬ ì„ë² ë”© ëª¨ë¸: {settings.get_current_embedding_model()}")
        
        # ì´ˆê¸°í™” ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
        VectorStorageService._initialized = True
    
    def get_dynamic_embedding_dimension(self) -> int:
        """ì‹¤ì‹œê°„ìœ¼ë¡œ í˜„ì¬ ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return settings.get_current_embedding_dimension()
    
    async def store_processed_document(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        preprocessed_data: Dict[str, Any],
        nlp_results: List[Dict[str, Any]],
        user_info: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ğŸ”® vs_ ì¤‘ì‹¬ í†µí•© ë¬¸ì„œ ë²¡í„° ì €ì¥ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ë°˜ì˜)
        
        Args:
            session: DB ì„¸ì…˜
            file_bss_info_sno: íŒŒì¼ ê¸°ë³¸ ì •ë³´ ID
            container_id: ì§€ì‹ ì»¨í…Œì´ë„ˆ ID (ê¶Œí•œ ê´€ë¦¬ìš©)
            preprocessed_data: ë¬¸ì„œ ì „ì²˜ë¦¬ ê²°ê³¼
            nlp_results: ì²­í¬ë³„ NLP ë¶„ì„ ê²°ê³¼
            user_info: ì—…ë¡œë“œ ì‚¬ìš©ì ì •ë³´ (ê¶Œí•œ ì„¤ì •ìš©)
        """
        try:
            # ê¶Œí•œ ì •ë³´ í™•ì¸
            access_permissions = await self._get_container_permissions(
                session, container_id, user_info
            )
            
            result = {
                "success": False,  # ê¸°ë³¸ê°’ì„ Falseë¡œ ì„¤ì •
                "primary_vectors": 0,      # vs_doc_contents_index ì €ì¥ ìˆ˜
                "metadata_chunks": 0,      # tb_document_chunks ì €ì¥ ìˆ˜
                "hybrid_records": 0,       # tb_search_documents ì €ì¥ ìˆ˜
                "errors": [],
                "container_id": container_id,
                "permissions": access_permissions,
                "architecture": "vs_primary"
            }
            
            chunks = preprocessed_data.get('chunks', [])
            
            for i, (chunk, nlp_result) in enumerate(zip(chunks, nlp_results)):
                try:
                    # 1ë‹¨ê³„: Primary Vector Store (vs_doc_contents_index)
                    primary_result = await self._store_primary_vector(
                        session, file_bss_info_sno, container_id, chunk, nlp_result, i
                    )
                    
                    if primary_result["success"]:
                        result["primary_vectors"] += 1
                    else:
                        result["errors"].append(f"ì²­í¬ {i} ì£¼ìš” ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {primary_result.get('error')}")
                    
                    # 2ë‹¨ê³„: Metadata & Reference (tb_document_chunks)
                    metadata_result = await self._store_metadata_chunk(
                        session, file_bss_info_sno, container_id, chunk, nlp_result, i
                    )
                    
                    if metadata_result["success"]:
                        result["metadata_chunks"] += 1
                    else:
                        result["errors"].append(f"ì²­í¬ {i} ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {metadata_result.get('error')}")
                    
                    # 3ë‹¨ê³„: Hybrid Search (tb_search_documents) - ì„ íƒì 
                    hybrid_result = await self._store_hybrid_search(
                        session, file_bss_info_sno, container_id, chunk, nlp_result, i
                    )
                    
                    if hybrid_result["success"]:
                        result["hybrid_records"] += 1
                    else:
                        result["errors"].append(f"ì²­í¬ {i} í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ì‹¤íŒ¨: {hybrid_result.get('error')}")
                    
                except Exception as e:
                    error_msg = f"ì²­í¬ {i} í†µí•© ì €ì¥ ì‹¤íŒ¨: {str(e)}"
                    result["errors"].append(error_msg)
                    logger.error(error_msg)
            
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            await self._update_file_metadata_integrated(
                session, file_bss_info_sno, len(chunks), preprocessed_data, access_permissions, result
            )
            
            # ì„±ê³µ ì—¬ë¶€ íŒì •: ì£¼ìš” ë²¡í„° ì €ì¥ ì„±ê³µ ê¸°ì¤€
            result["success"] = result["primary_vectors"] > 0
            
            logger.info(f"ğŸ”® vs_ ì¤‘ì‹¬ ë²¡í„° ì €ì¥ ì™„ë£Œ - ì»¨í…Œì´ë„ˆ: {container_id}, "
                       f"ì£¼ìš”ë²¡í„°: {result['primary_vectors']}, ë©”íƒ€ë°ì´í„°: {result['metadata_chunks']}, "
                       f"í•˜ì´ë¸Œë¦¬ë“œ: {result['hybrid_records']}")
            
            return result
            
        except Exception as e:
            logger.error(f"vs_ ì¤‘ì‹¬ ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "primary_vectors": 0,
                "metadata_chunks": 0,
                "hybrid_records": 0,
                "container_id": container_id
            }
    
    async def _store_primary_vector(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        chunk: Dict[str, Any],
        nlp_result: Dict[str, Any],
        chunk_index: int
    ) -> Dict[str, Any]:
        """vs_doc_contents_indexì— ì£¼ìš” ë²¡í„° ì €ì¥ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ë§ì¶¤)"""
        try:
            # ì„ë² ë”© ë²¡í„° ì¤€ë¹„ (ë™ì  ì°¨ì›)
            embedding = nlp_result.get('embedding')
            current_dim = self.get_dynamic_embedding_dimension()
            
            # ì°¨ì› ìë™ ì¡°ì • (í•„ìš”ì‹œ ìŠ¤ë§ˆíŠ¸ ì¶•ì†Œ ì ìš©)
            if embedding and len(embedding) != current_dim:
                logger.info(f"ì„ë² ë”© ì°¨ì› ì¡°ì •: {len(embedding)} â†’ {current_dim}")
                embedding = settings.apply_smart_dimension_reduction(embedding, current_dim)
            
            if not embedding or len(embedding) != current_dim:
                logger.warning(f"ë²¡í„° ì°¨ì› ë¬¸ì œ: ì˜ˆìƒ {current_dim}, ì‹¤ì œ {len(embedding) if embedding else 0}")
                return {"success": False, "error": "ë²¡í„° ì°¨ì› ë¬¸ì œ"}
            
            embedding_str = f"[{','.join(map(str, embedding))}]"
            
            # vs_doc_contents_index ì‹¤ì œ ìŠ¤í‚¤ë§ˆì— ë§ì¶˜ INSERT
            query = text("""
                INSERT INTO vs_doc_contents_index (
                    id, file_bss_info_sno, knowledge_container_id, chunk_index,
                    chunk_text, embedding, chunk_size, metadata_json, created_date
                ) VALUES (
                    :id, :file_sno, :container_id, :chunk_index,
                    :chunk_text, CAST(:embedding AS vector), :chunk_size, :metadata_json, NOW()
                )
            """)
            
            # ID ìƒì„± (íŒŒì¼SNO_ì²­í¬ì¸ë±ìŠ¤_ì»¨í…Œì´ë„ˆID ì¡°í•©)
            doc_id = f"{file_bss_info_sno}_{chunk_index}_{container_id}"
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "container_id": container_id,
                "chunk_type": chunk.get('chunk_type', 'content'),
                "page_number": chunk.get('page_number', 1),  # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                "keywords": nlp_result.get('korean_keywords', []),
                "named_entities": nlp_result.get('named_entities', [])
            }
            
            await session.execute(query, {
                "id": doc_id,
                "file_sno": file_bss_info_sno,
                "container_id": container_id,
                "chunk_index": chunk_index,
                "chunk_text": chunk.get('content', ''),
                "embedding": embedding_str,
                "chunk_size": chunk.get('size', len(chunk.get('content', ''))),
                "metadata_json": json.dumps(metadata, ensure_ascii=False)
            })
            
            return {"success": True, "doc_id": doc_id}
            
        except Exception as e:
            logger.error(f"ì£¼ìš” ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _store_metadata_chunk(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        chunk: Dict[str, Any],
        nlp_result: Dict[str, Any],
        chunk_index: int
    ) -> Dict[str, Any]:
        """tb_document_chunksì— ë©”íƒ€ë°ì´í„° ì €ì¥ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ë§ì¶¤)"""
        try:
            # ì„ë² ë”© ë²¡í„° ì •ê·œí™” (ì„¤ì •ëœ ì°¨ì›ì— ë§ì¶¤)
            original_embedding = nlp_result.get('embedding')
            embedding_str = None
            
            if original_embedding:
                expected_dimension = settings.vector_dimension
                if len(original_embedding) == expected_dimension:
                    # ì°¨ì›ì´ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    embedding_str = f"[{','.join(map(str, original_embedding))}]"
                elif len(original_embedding) > expected_dimension:
                    # ì¶•ì†Œ: ì•ìª½ ì°¨ì›ë§Œ ì‚¬ìš©
                    reduced_embedding = original_embedding[:expected_dimension]
                    embedding_str = f"[{','.join(map(str, reduced_embedding))}]"
                    logger.debug(f"ì„ë² ë”© ì°¨ì› ì¶•ì†Œ: {len(original_embedding)} -> {expected_dimension}")
                else:
                    # í™•ì¥: 0ìœ¼ë¡œ íŒ¨ë”©
                    padded_embedding = original_embedding + [0.0] * (expected_dimension - len(original_embedding))
                    embedding_str = f"[{','.join(map(str, padded_embedding))}]"
                    logger.debug(f"ì„ë² ë”© ì°¨ì› í™•ì¥: {len(original_embedding)} -> {expected_dimension}")
            else:
                logger.warning(f"ë©”íƒ€ë°ì´í„° ì €ì¥: ì„ë² ë”© ë²¡í„°ê°€ ì—†ìŒ")
            
            # ì°¸ì¡° ì •ë³´ ìƒì„± ("3í˜ì´ì§€ 2ë²ˆì§¸ ë‹¨ë½" í˜•íƒœ)
            page_number = chunk.get('page_number', 1)
            paragraph_index = chunk_index + 1
            reference_info = f"{page_number}í˜ì´ì§€ {paragraph_index}ë²ˆì§¸ ë‹¨ë½"
            
            # tb_document_chunks ì‹¤ì œ ìŠ¤í‚¤ë§ˆì— ë§ì¶˜ INSERT
            if embedding_str:
                query = text("""
                    INSERT INTO tb_document_chunks (
                        "FILE_BSS_INFO_SNO", "CHUNK_INDEX", "CHUNK_TEXT", 
                        "CHUNK_SIZE", "CHUNK_EMBEDDING", "PAGE_NUMBER",
                        "SECTION_TITLE", "KNOWLEDGE_CONTAINER_ID",
                        "CREATED_BY", "LAST_MODIFIED_BY"
                    ) VALUES (
                        :file_sno, :chunk_index, :chunk_text,
                        :chunk_size, CAST(:embedding AS vector), :page_number,
                        :section_title, :container_id,
                        :created_by, :modified_by
                    )
                """)
                
                await session.execute(query, {
                    "file_sno": file_bss_info_sno,
                    "chunk_index": chunk_index,
                    "chunk_text": chunk.get('content', ''),
                    "chunk_size": chunk.get('size', len(chunk.get('content', ''))),
                    "embedding": embedding_str,
                    "page_number": page_number,
                    "section_title": chunk.get('chunk_type', 'content'),
                    "container_id": container_id,
                    "created_by": "system",
                    "modified_by": "system"
                })
            else:
                # ë²¡í„° ì—†ì´ ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥
                query = text("""
                    INSERT INTO tb_document_chunks (
                        "FILE_BSS_INFO_SNO", "CHUNK_INDEX", "CHUNK_TEXT", 
                        "CHUNK_SIZE", "PAGE_NUMBER", "SECTION_TITLE", 
                        "KNOWLEDGE_CONTAINER_ID", "CREATED_BY", "LAST_MODIFIED_BY"
                    ) VALUES (
                        :file_sno, :chunk_index, :chunk_text,
                        :chunk_size, :page_number, :section_title,
                        :container_id, :created_by, :modified_by
                    )
                """)
                
                await session.execute(query, {
                    "file_sno": file_bss_info_sno,
                    "chunk_index": chunk_index,
                    "chunk_text": chunk.get('content', ''),
                    "chunk_size": chunk.get('size', len(chunk.get('content', ''))),
                    "page_number": page_number,
                    "section_title": chunk.get('chunk_type', 'content'),
                    "container_id": container_id,
                    "created_by": "system",
                    "modified_by": "system"
                })
            
            return {"success": True, "reference": reference_info}
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _store_hybrid_search(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        chunk: Dict[str, Any],
        nlp_result: Dict[str, Any],
        chunk_index: int
    ) -> Dict[str, Any]:
        """tb_search_documentsì— í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© ë ˆì½”ë“œ ì €ì¥"""
        try:
            # ì„ë² ë”© ë²¡í„° ì¤€ë¹„
            embedding = nlp_result.get('embedding')
            if embedding and len(embedding) == self.embedding_dimension:
                embedding_str = f"[{','.join(map(str, embedding))}]"
            else:
                embedding_str = None
                if embedding:
                    logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {self.embedding_dimension}, ì‹¤ì œ {len(embedding)}")
            
            # í‚¤ì›Œë“œ, ê³ ìœ ëª…ì‚¬ ë°°ì—´ ì¤€ë¹„
            keywords = nlp_result.get('korean_keywords', [])
            proper_nouns = nlp_result.get('named_entities', [])
            corp_names = []  # ì¶”í›„ íšŒì‚¬ëª… ì¶”ì¶œ ë¡œì§ ì¶”ê°€
            
            # PostgreSQL ë°°ì—´ í˜•íƒœë¡œ ë³€í™˜
            keywords_array = '{' + ','.join([f'"{k}"' for k in keywords if k]) + '}'
            proper_nouns_array = '{' + ','.join([f'"{p}"' for p in proper_nouns if p]) + '}'
            corp_names_array = '{}'
            
            content = chunk.get('content', '')
            
            # tb_search_documentsì— ì €ì¥
            if embedding_str:
                query = text("""
                    INSERT INTO tb_search_documents (
                        file_bss_info_sno, knowledge_container_id, chunk_index,
                        content, keywords, proper_nouns, corp_names,
                        content_vector, keyword_tsvector, content_tsvector
                    ) VALUES (
                        :file_sno, :container_id, :chunk_index,
                        :content, CAST(:keywords AS text[]), CAST(:proper_nouns AS text[]), CAST(:corp_names AS text[]),
                        CAST(:embedding AS vector), 
                        to_tsvector('korean', :content),
                        to_tsvector('korean', :content)
                    )
                """)
                
                await session.execute(query, {
                    "file_sno": file_bss_info_sno,
                    "container_id": container_id,
                    "chunk_index": chunk_index,
                    "content": content,
                    "keywords": keywords_array,
                    "proper_nouns": proper_nouns_array,
                    "corp_names": corp_names_array,
                    "embedding": embedding_str
                })
            else:
                # ë²¡í„° ì—†ì´ í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì €ì¥
                query = text("""
                    INSERT INTO tb_search_documents (
                        file_bss_info_sno, knowledge_container_id, chunk_index,
                        content, keywords, proper_nouns, corp_names,
                        keyword_tsvector, content_tsvector
                    ) VALUES (
                        :file_sno, :container_id, :chunk_index,
                        :content, CAST(:keywords AS text[]), CAST(:proper_nouns AS text[]), CAST(:corp_names AS text[]),
                        to_tsvector('korean', :content),
                        to_tsvector('korean', :content)
                    )
                """)
                
                await session.execute(query, {
                    "file_sno": file_bss_info_sno,
                    "container_id": container_id,
                    "chunk_index": chunk_index,
                    "content": content,
                    "keywords": keywords_array,
                    "proper_nouns": proper_nouns_array,
                    "corp_names": corp_names_array
                })
            
            return {"success": True}
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _get_container_permissions(
        self,
        session: AsyncSession,
        container_id: str,
        user_info: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ì§€ì‹ ì»¨í…Œì´ë„ˆ ê¶Œí•œ ì •ë³´ ì¡°íšŒ"""
        try:
            # ê¸°ë³¸ ê¶Œí•œ ì •ë³´
            permissions = {
                "container_id": container_id,
                "read_permission": True,
                "write_permission": True,
                "access_level": "full",
                "user_role": "system"
            }
            
            # ì‚¬ìš©ì ì •ë³´ê°€ ìˆìœ¼ë©´ ì‹¤ì œ ê¶Œí•œ ì¡°íšŒ
            if user_info and user_info.get("user_id"):
                # TODO: ì‹¤ì œ ê¶Œí•œ í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
                # í˜„ì¬ëŠ” ê¸°ë³¸ ê¶Œí•œ ë°˜í™˜
                permissions.update({
                    "user_id": user_info.get("user_id"),
                    "user_role": user_info.get("role", "user")
                })
            
            return permissions
            
        except Exception as e:
            logger.error(f"ì»¨í…Œì´ë„ˆ ê¶Œí•œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {
                "container_id": container_id,
                "read_permission": False,
                "write_permission": False,
                "access_level": "none",
                "error": str(e)
            }
    
    async def _update_file_metadata_integrated(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        chunk_count: int,
        preprocessed_data: Dict[str, Any],
        permissions: Dict[str, Any],
        storage_result: Dict[str, Any]
    ):
        """í†µí•© íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        try:
            # í†µí•© ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            integrated_metadata = {
                "vectorized": True,
                "architecture": "vs_primary",
                "chunk_count": chunk_count,
                "vector_dimension": self.embedding_dimension,
                "storage_summary": {
                    "primary_vectors": storage_result.get("primary_vectors", 0),
                    "metadata_chunks": storage_result.get("metadata_chunks", 0),
                    "hybrid_records": storage_result.get("hybrid_records", 0)
                },
                "access_permissions": permissions,
                "container_id": permissions.get("container_id"),
                "processing_status": "completed",
                "last_vectorized": datetime.now().isoformat()
            }
            
            # tb_file_bss_info ì—…ë°ì´íŠ¸
            query = text("""
                UPDATE tb_file_bss_info 
                SET chunk_count = :chunk_count,
                    korean_metadata = COALESCE(korean_metadata, '{}') || CAST(:metadata AS json)
                WHERE file_bss_info_sno = :file_sno
            """)
            
            await session.execute(query, {
                "chunk_count": chunk_count,
                "metadata": json.dumps(integrated_metadata, ensure_ascii=False),
                "file_sno": file_bss_info_sno
            })
            
        except Exception as e:
            logger.error(f"í†µí•© ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

    async def search_hybrid(
        self,
        session: AsyncSession,
        query_text: str,
        query_embedding: Optional[List[float]] = None,
        container_ids: Optional[List[str]] = None,
        limit: int = 10,
        similarity_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        ğŸ”® vs_ ì¤‘ì‹¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (vs_doc_contents_index + tb_search_documents)
        
        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
            container_ids: ê²€ìƒ‰í•  ì»¨í…Œì´ë„ˆ ID ëª©ë¡
            limit: ê²°ê³¼ ê°œìˆ˜ ì œí•œ
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        try:
            # vs_doc_contents_index ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰
            vector_results = []
            if query_embedding:
                vector_results = await self._search_primary_vectors(
                    session, query_embedding, container_ids, limit, similarity_threshold
                )
            
            # tb_search_documents ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰
            keyword_results = await self._search_keywords(
                session, query_text, container_ids, limit
            )
            
            # ê²°ê³¼ í†µí•© ë° ìŠ¤ì½”ì–´ë§
            integrated_results = self._integrate_search_results(
                vector_results, keyword_results, limit
            )
            
            logger.info(f"vs_ ì¤‘ì‹¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: ë²¡í„° {len(vector_results)}ê°œ, í‚¤ì›Œë“œ {len(keyword_results)}ê°œ, í†µí•© {len(integrated_results)}ê°œ")
            return integrated_results
            
        except Exception as e:
            logger.error(f"vs_ ì¤‘ì‹¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    async def _search_primary_vectors(
        self,
        session: AsyncSession,
        query_embedding: List[float],
        container_ids: Optional[List[str]] = None,
        limit: int = 10,
        similarity_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """vs_doc_contents_indexì—ì„œ ë²¡í„° ê²€ìƒ‰"""
        try:
            conditions = []
            params = {"query_embedding": f"[{','.join(map(str, query_embedding))}]", "limit": limit, "threshold": similarity_threshold}
            
            if container_ids:
                conditions.append("knowledge_container_id = ANY(:container_ids)")
                params["container_ids"] = container_ids
            
            where_clause = " AND ".join(conditions) if conditions else "1=1"
            
            query = text(f"""
                SELECT 
                    id, file_bss_info_sno, knowledge_container_id, chunk_index,
                    chunk_text, metadata_json,
                    1 - (embedding <-> :query_embedding::vector) as similarity
                FROM vs_doc_contents_index
                WHERE {where_clause}
                    AND embedding <-> :query_embedding::vector < :threshold
                ORDER BY similarity DESC
                LIMIT :limit
            """)
            
            result = await session.execute(query, params)
            rows = result.fetchall()
            
            return [
                {
                    "id": row[0],
                    "file_bss_info_sno": row[1],
                    "container_id": row[2],
                    "chunk_index": row[3],
                    "content": row[4],
                    "metadata": json.loads(row[5]) if row[5] else {},
                    "similarity": float(row[6]),
                    "source": "vector"
                }
                for row in rows
            ]
            
        except Exception as e:
            logger.error(f"ì£¼ìš” ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    async def _search_keywords(
        self,
        session: AsyncSession,
        query_text: str,
        container_ids: Optional[List[str]] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """tb_search_documentsì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        try:
            conditions = []
            params = {"query_text": query_text, "limit": limit}
            
            if container_ids:
                conditions.append("knowledge_container_id = ANY(:container_ids)")
                params["container_ids"] = container_ids
            
            where_clause = " AND ".join(conditions) if conditions else "1=1"
            
            query = text(f"""
                SELECT 
                    search_doc_id, file_bss_info_sno, knowledge_container_id, chunk_index,
                    content, keywords, proper_nouns,
                    ts_rank(content_tsvector, plainto_tsquery('korean', :query_text)) as keyword_score
                FROM tb_search_documents
                WHERE {where_clause}
                    AND content_tsvector @@ plainto_tsquery('korean', :query_text)
                ORDER BY keyword_score DESC
                LIMIT :limit
            """)
            
            result = await session.execute(query, params)
            rows = result.fetchall()
            
            return [
                {
                    "id": row[0],
                    "file_bss_info_sno": row[1],
                    "container_id": row[2],
                    "chunk_index": row[3],
                    "content": row[4],
                    "keywords": row[5] if row[5] else [],
                    "proper_nouns": row[6] if row[6] else [],
                    "keyword_score": float(row[7]),
                    "source": "keyword"
                }
                for row in rows
            ]
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    def _integrate_search_results(
        self,
        vector_results: List[Dict[str, Any]],
        keyword_results: List[Dict[str, Any]],
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        try:
            # ê²°ê³¼ í†µí•© (file_bss_info_sno + chunk_index ê¸°ì¤€)
            result_map = {}
            
            # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ê°€ì¤‘ì¹˜ 0.7)
            for result in vector_results:
                key = f"{result['file_bss_info_sno']}_{result['chunk_index']}"
                result["vector_score"] = result.get("similarity", 0) * 0.7
                result["keyword_score"] = 0
                result["hybrid_score"] = result["vector_score"]
                result_map[key] = result
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€/ë³‘í•© (ê°€ì¤‘ì¹˜ 0.3)
            for result in keyword_results:
                key = f"{result['file_bss_info_sno']}_{result['chunk_index']}"
                keyword_weighted = result.get("keyword_score", 0) * 0.3
                
                if key in result_map:
                    # ê¸°ì¡´ ê²°ê³¼ì— í‚¤ì›Œë“œ ì ìˆ˜ ì¶”ê°€
                    result_map[key]["keyword_score"] = keyword_weighted
                    result_map[key]["hybrid_score"] += keyword_weighted
                    # í‚¤ì›Œë“œ ì •ë³´ ë³‘í•©
                    if "keywords" in result:
                        result_map[key]["keywords"] = result["keywords"]
                    if "proper_nouns" in result:
                        result_map[key]["proper_nouns"] = result["proper_nouns"]
                else:
                    # í‚¤ì›Œë“œë§Œ ê²€ìƒ‰ëœ ê²°ê³¼
                    result["vector_score"] = 0
                    result["keyword_score"] = keyword_weighted
                    result["hybrid_score"] = keyword_weighted
                    result_map[key] = result
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬
            integrated_results = sorted(
                result_map.values(),
                key=lambda x: x["hybrid_score"],
                reverse=True
            )
            
            return integrated_results[:limit]
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê²°ê³¼ í†µí•© ì‹¤íŒ¨: {str(e)}")
            return []


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
vector_storage_service = VectorStorageService()

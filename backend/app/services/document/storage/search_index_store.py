"""
ğŸ” ë©€í‹°ëª¨ë‹¬ í†µí•©ê²€ìƒ‰ ì¸ë±ìŠ¤ ì €ì¥ ì„œë¹„ìŠ¤
=========================================

í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + í…Œì´ë¸” ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì§€ì›
- í‚¤ì›Œë“œ ê²€ìƒ‰ ìµœì í™” (Korean FTS)
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ + FTS)
- ì´ë¯¸ì§€ ê²€ìƒ‰ ì§€ì› (ì´ë¯¸ì§€ ì„ë² ë”© + ë©”íƒ€ë°ì´í„°)
- PostgreSQL FTS + GIN ì¸ë±ìŠ¤ + pgvector í™œìš©
"""

import json
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

from sqlalchemy import text, select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from app.models.document.unified_search_models import TbDocumentSearchIndex
from app.models.document.multimodal_models import DocEmbedding  # ì´ë¯¸ì§€ ì„ë² ë”© ì €ì¥
from app.core.config import settings

logger = logging.getLogger(__name__)

class SearchIndexStoreService:
    """ë©€í‹°ëª¨ë‹¬ í†µí•©ê²€ìƒ‰ ì¸ë±ìŠ¤ ì €ì¥ ì„œë¹„ìŠ¤ - í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ê²€ìƒ‰ ì§€ì›"""
    
    def __init__(self):
        self.max_content_length = 50000  # ìµœëŒ€ ë‚´ìš© ê¸¸ì´
        self.max_summary_length = 1000   # ìš”ì•½ ìµœëŒ€ ê¸¸ì´
        self.image_embedding_model = "openai-clip"  # ì´ë¯¸ì§€ ì„ë² ë”© ëª¨ë¸
        logger.info("ğŸ” SearchIndexStoreService ì´ˆê¸°í™” ì™„ë£Œ - ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì§€ì›")
    
    async def store_document_for_search(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        document_data: Dict[str, Any],
        nlp_analysis: Dict[str, Any],
        user_info: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œë¥¼ ë©€í‹°ëª¨ë‹¬ í†µí•©ê²€ìƒ‰ ì¸ë±ìŠ¤ì— ì €ì¥ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
        
        Args:
            session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            file_bss_info_sno: íŒŒì¼ ê¸°ë³¸ ì •ë³´ ì¼ë ¨ë²ˆí˜¸
            container_id: ì§€ì‹ ì»¨í…Œì´ë„ˆ ID
            document_data: ë¬¸ì„œ ì „ì²˜ë¦¬ ë°ì´í„° (ì œëª©, ë‚´ìš©, ìš”ì•½, ì´ë¯¸ì§€ ë“±)
            nlp_analysis: NLP ë¶„ì„ ê²°ê³¼ (í‚¤ì›Œë“œ, ê°œì²´ëª… ë“±)
            user_info: ì‚¬ìš©ì ì •ë³´ (ê¶Œí•œ ì„¤ì •ìš©)
        
        Returns:
            ì €ì¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ì •ë³´)
        """
        try:
            # 1. ê¸°ì¡´ ê²€ìƒ‰ ì¸ë±ìŠ¤ í™•ì¸ ë° ì‚­ì œ
            await self._remove_existing_index(session, file_bss_info_sno)
            
            # 2. ë¬¸ì„œ ë‚´ìš© ì¤€ë¹„
            document_title = document_data.get('title', document_data.get('file_name', 'Untitled'))
            full_content = self._prepare_full_content(document_data)
            content_summary = self._create_content_summary(full_content)
            
            # 3. NLP ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
            search_metadata = self._extract_search_metadata(nlp_analysis)
            
            # 4. ê¶Œí•œ ì •ë³´ ì„¤ì •
            access_info = self._determine_access_level(container_id, user_info)
            
            # 5. ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ìš©)
            images_metadata = document_data.get('images', [])
            has_images = document_data.get('has_images', False)
            image_count = document_data.get('image_count', 0)
            
            logger.info(f"[MULTIMODAL_SEARCH] ë¬¸ì„œ {file_bss_info_sno} - "
                       f"ì´ë¯¸ì§€: {image_count}ê°œ, "
                       f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_content)}")
            
            # 6. ìƒˆë¡œìš´ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
            search_index = TbDocumentSearchIndex(
                file_bss_info_sno=file_bss_info_sno,
                knowledge_container_id=container_id,
                document_title=document_title,
                full_content=full_content,
                content_summary=content_summary,
                
                # âŒ ì œê±°: kiwipiepy ê´€ë ¨ í•„ë“œ
                # keywords=search_metadata.get('keywords', []),
                # proper_nouns=search_metadata.get('proper_nouns', []),
                # corp_names=search_metadata.get('corp_names', []),
                
                # âœ… ìœ ì§€: ì£¼ì œ/ì¹´í…Œê³ ë¦¬
                main_topics=search_metadata.get('main_topics', []),
                
                # ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„°
                has_images=has_images,
                has_tables=document_data.get('has_tables', False),
                image_count=image_count,
                table_count=document_data.get('table_count', 0),
                
                # ë©”íƒ€ë°ì´í„°
                document_type=document_data.get('file_type', 'UNKNOWN').upper(),
                page_count=document_data.get('page_count'),
                content_length=len(full_content),
                language_code='ko',
                
                # ê¶Œí•œ ì •ë³´
                access_level=access_info['access_level'],
                is_public=access_info['is_public'],
                
                # ì‹œìŠ¤í…œ ì •ë³´
                indexing_status='indexed',
                search_weight=self._calculate_search_weight(document_data, search_metadata),
                created_date=datetime.now(),
                last_updated=datetime.now()
            )
            
            # 6. ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ìš©)
            if images_metadata and len(images_metadata) > 0:
                images_json = json.dumps(images_metadata, ensure_ascii=False)
                search_index.images_metadata = images_json  # JSON ì»¬ëŸ¼ì— ì €ì¥
                logger.info(f"[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ - {len(images_metadata)}ê°œ")
            
            # 7. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            session.add(search_index)
            await session.flush()  # ID ìƒì„±ì„ ìœ„í•´ flush
            
            logger.info(f"ğŸ” ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ - íŒŒì¼: {file_bss_info_sno}, "
                       f"ê²€ìƒ‰ID: {search_index.search_doc_id}, "
                       f"ì»¨í…Œì´ë„ˆ: {container_id}, "
                       f"ì´ë¯¸ì§€: {image_count}ê°œ")
            
            # 8. ì´ë¯¸ì§€ ì„ë² ë”© ì €ì¥ ì¤€ë¹„ (í–¥í›„ ì´ë¯¸ì§€ ê²€ìƒ‰ìš©)
            image_embeddings_saved = 0
            if has_images and images_metadata:
                # ì´ë¯¸ì§€ ì„ë² ë”©ì€ ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìƒì„±/ì €ì¥
                # í˜„ì¬ëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥í•˜ê³ , ì‹¤ì œ ì„ë² ë”©ì€ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹œ ì²˜ë¦¬
                logger.info(f"[MULTIMODAL_SEARCH] ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ëŒ€ê¸° ì¤‘ - {len(images_metadata)}ê°œ")
            
            return {
                "success": True,
                "search_doc_id": search_index.search_doc_id,
                "file_bss_info_sno": file_bss_info_sno,
                "container_id": container_id,
                "content_length": len(full_content),
                "image_count": image_count,
                "image_embeddings_saved": image_embeddings_saved,
                "indexing_status": "indexed",
                "multimodal_ready": has_images  # ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì¤€ë¹„ ì—¬ë¶€
            }
            
        except Exception as e:
            logger.error(f"ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "file_bss_info_sno": file_bss_info_sno,
                "container_id": container_id
            }
    
    async def update_search_statistics(
        self,
        session: AsyncSession,
        search_doc_id: int,
        search_query: str
    ) -> None:
        """ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            query = text("""
                UPDATE tb_document_search_index 
                SET search_count = search_count + 1,
                    last_searched_at = NOW()
                WHERE search_doc_id = :search_doc_id
            """)
            
            await session.execute(query, {"search_doc_id": search_doc_id})
            logger.debug(f"ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸: ë¬¸ì„œ {search_doc_id}")
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    async def keyword_search(
        self,
        session: AsyncSession,
        query_text: str,
        container_ids: Optional[List[str]] = None,
        access_level: str = 'normal',
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        try:
            # ì»¨í…Œì´ë„ˆ í•„í„° ì¡°ê±´
            container_filter = ""
            params = {"query_text": query_text, "access_level": access_level, "limit": limit}
            
            if container_ids:
                container_filter = "AND knowledge_container_id = ANY(:container_ids)"
                params["container_ids"] = container_ids
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¿¼ë¦¬
            query = text(f"""
                SELECT 
                    search_doc_id,
                    file_bss_info_sno,
                    knowledge_container_id,
                    document_title,
                    content_summary,
                    keywords,
                    proper_nouns,
                    document_type,
                    -- í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                    ts_rank(keyword_tsvector, plainto_tsquery('korean', :query_text)) as keyword_score,
                    -- ë‚´ìš© ë§¤ì¹­ ì ìˆ˜  
                    ts_rank(content_tsvector, plainto_tsquery('korean', :query_text)) as content_score,
                    last_updated
                FROM tb_document_search_index
                WHERE (keyword_tsvector @@ plainto_tsquery('korean', :query_text)
                       OR content_tsvector @@ plainto_tsquery('korean', :query_text))
                  AND access_level <= :access_level
                  AND indexing_status = 'indexed'
                  {container_filter}
                ORDER BY 
                    GREATEST(keyword_score, content_score) DESC,
                    search_weight DESC,
                    last_updated DESC
                LIMIT :limit
            """)
            
            result = await session.execute(query, params)
            rows = result.fetchall()
            
            # ê²°ê³¼ í¬ë§·íŒ…
            search_results = []
            for row in rows:
                search_results.append({
                    "search_doc_id": row[0],
                    "file_bss_info_sno": row[1],
                    "container_id": row[2],
                    "title": row[3],
                    "content_preview": row[4],
                    "keywords": row[5] if row[5] else [],
                    "proper_nouns": row[6] if row[6] else [],
                    "document_type": row[7],
                    "keyword_score": float(row[8]) if row[8] else 0.0,
                    "content_score": float(row[9]) if row[9] else 0.0,
                    "relevance_score": max(float(row[8]) if row[8] else 0.0, 
                                         float(row[9]) if row[9] else 0.0),
                    "last_updated": row[10].isoformat() if row[10] else None,
                    "search_type": "keyword"
                })
            
            logger.info(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def hybrid_search(
        self,
        session: AsyncSession,
        query_text: str,
        container_ids: Optional[List[str]] = None,
        access_level: str = 'normal',
        limit: int = 20,
        keyword_weight: float = 0.4,
        content_weight: float = 0.6
    ) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ë‚´ìš©)"""
        try:
            # ì»¨í…Œì´ë„ˆ í•„í„° ì¡°ê±´
            container_filter = ""
            params = {
                "query_text": query_text, 
                "access_level": access_level, 
                "limit": limit,
                "keyword_weight": keyword_weight,
                "content_weight": content_weight
            }
            
            if container_ids:
                container_filter = "AND knowledge_container_id = ANY(:container_ids)"
                params["container_ids"] = container_ids
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¿¼ë¦¬
            query = text(f"""
                SELECT 
                    search_doc_id,
                    file_bss_info_sno,
                    knowledge_container_id,
                    document_title,
                    content_summary,
                    keywords,
                    proper_nouns,
                    main_topics,
                    document_type,
                    -- ê°œë³„ ì ìˆ˜
                    ts_rank(keyword_tsvector, plainto_tsquery('korean', :query_text)) as keyword_score,
                    ts_rank(content_tsvector, plainto_tsquery('korean', :query_text)) as content_score,
                    -- í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (ê°€ì¤‘ì¹˜ ì ìš©)
                    (ts_rank(keyword_tsvector, plainto_tsquery('korean', :query_text)) * :keyword_weight +
                     ts_rank(content_tsvector, plainto_tsquery('korean', :query_text)) * :content_weight) as hybrid_score,
                    search_weight,
                    last_updated
                FROM tb_document_search_index
                WHERE (keyword_tsvector @@ plainto_tsquery('korean', :query_text)
                       OR content_tsvector @@ plainto_tsquery('korean', :query_text))
                  AND access_level <= :access_level
                  AND indexing_status = 'indexed'
                  {container_filter}
                ORDER BY 
                    hybrid_score DESC,
                    search_weight DESC,
                    last_updated DESC
                LIMIT :limit
            """)
            
            result = await session.execute(query, params)
            rows = result.fetchall()
            
            # ê²°ê³¼ í¬ë§·íŒ…
            search_results = []
            for row in rows:
                search_results.append({
                    "search_doc_id": row[0],
                    "file_bss_info_sno": row[1],
                    "container_id": row[2],
                    "title": row[3],
                    "content_preview": row[4],
                    "keywords": row[5] if row[5] else [],
                    "proper_nouns": row[6] if row[6] else [],
                    "main_topics": row[7] if row[7] else [],
                    "document_type": row[8],
                    "keyword_score": float(row[9]) if row[9] else 0.0,
                    "content_score": float(row[10]) if row[10] else 0.0,
                    "hybrid_score": float(row[11]) if row[11] else 0.0,
                    "search_weight": row[12],
                    "last_updated": row[13].isoformat() if row[13] else None,
                    "search_type": "hybrid"
                })
            
            logger.info(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    # ==============================================
    # ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œë“¤
    # ==============================================
    
    async def _remove_existing_index(self, session: AsyncSession, file_bss_info_sno: int):
        """ê¸°ì¡´ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì œê±°"""
        query = text("""
            DELETE FROM tb_document_search_index 
            WHERE file_bss_info_sno = :file_sno
        """)
        await session.execute(query, {"file_sno": file_bss_info_sno})
    
    def _prepare_full_content(self, document_data: Dict[str, Any]) -> str:
        """ë¬¸ì„œ ì „ì²´ ë‚´ìš© ì¤€ë¹„"""
        # ì²­í¬ê°€ ìˆëŠ” ê²½ìš° í•©ì¹˜ê¸°
        if 'chunks' in document_data:
            chunks = document_data['chunks']
            # ì²­í¬ê°€ ë¬¸ìì—´ ë°°ì—´ì¸ ê²½ìš°
            if chunks and isinstance(chunks[0], str):
                full_text = ' '.join(chunks)
            # ì²­í¬ê°€ ë”•ì…”ë„ˆë¦¬ ë°°ì—´ì¸ ê²½ìš°
            elif chunks and isinstance(chunks[0], dict):
                full_text = ' '.join([chunk.get('content', '') for chunk in chunks])
            else:
                full_text = ''
        else:
            # full_content, full_text, content í‚¤ ìˆœì„œë¡œ ì°¾ê¸°
            full_text = document_data.get('full_content', 
                                         document_data.get('full_text', 
                                                          document_data.get('content', '')))
        
        # ê¸¸ì´ ì œí•œ
        if len(full_text) > self.max_content_length:
            full_text = full_text[:self.max_content_length] + '...'
        
        return full_text.strip()
    
    def _create_content_summary(self, full_content: str) -> str:
        """ë‚´ìš© ìš”ì•½ ìƒì„±"""
        if len(full_content) <= self.max_summary_length:
            return full_content
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        sentences = full_content.split('. ')
        summary = ""
        for sentence in sentences:
            if len(summary + sentence) <= self.max_summary_length - 10:
                summary += sentence + '. '
            else:
                break
        
        return summary.strip() + '...' if summary else full_content[:self.max_summary_length] + '...'
    
    def _extract_search_metadata(self, nlp_analysis: Dict[str, Any]) -> Dict[str, List[str]]:
        """
        NLP ë¶„ì„ ê²°ê³¼ì—ì„œ ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (Simplified)
        
        ë³€ê²½ ì‚¬í•­ (2025-10-16):
        - kiwipiepy ê´€ë ¨ í•„ë“œ ì œê±°
        - keywords, proper_nouns, corp_names ì œê±°
        - ì£¼ì œ/ì¹´í…Œê³ ë¦¬ë§Œ ìœ ì§€
        """
        return {
            'main_topics': nlp_analysis.get('topics', nlp_analysis.get('categories', []))[:10]  # ìµœëŒ€ 10ê°œ
        }
    
    def _determine_access_level(self, container_id: str, user_info: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """ì ‘ê·¼ ê¶Œí•œ ë ˆë²¨ ê²°ì •"""
        # ê¸°ë³¸ ê¶Œí•œ ì„¤ì • (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì»¨í…Œì´ë„ˆë³„ ê¶Œí•œ ì •ì±… ì ìš©)
        if container_id.endswith('_public'):
            return {'access_level': 'public', 'is_public': True}
        elif container_id.endswith('_hr'):
            return {'access_level': 'restricted', 'is_public': False}
        else:
            return {'access_level': 'normal', 'is_public': False}
    
    def _calculate_search_weight(self, document_data: Dict[str, Any], search_metadata: Dict[str, Any]) -> int:
        """
        ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ê³„ì‚° (Simplified)
        
        ë³€ê²½ ì‚¬í•­ (2025-10-16):
        - í‚¤ì›Œë“œ ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì œê±°
        - ë¬¸ì„œ íƒ€ì…ê³¼ ë‚´ìš© ê¸¸ì´ë§Œ ì‚¬ìš©
        """
        weight = 1
        
        # ë¬¸ì„œ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        doc_type = document_data.get('file_type', '').upper()
        if doc_type in ['PDF', 'DOCX']:
            weight += 2
        elif doc_type in ['PPTX', 'XLSX']:
            weight += 1
        
        # ë‚´ìš© ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        content_length = document_data.get('content_length', len(document_data.get('content', '')))
        if content_length > 10000:
            weight += 2
        elif content_length > 5000:
            weight += 1
        
        return min(weight, 10)  # ìµœëŒ€ ê°€ì¤‘ì¹˜ 10

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
search_index_store_service = SearchIndexStoreService()

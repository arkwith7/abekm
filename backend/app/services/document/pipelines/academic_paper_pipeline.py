"""
í•™ìˆ  ë…¼ë¬¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

í•™ìˆ  ë…¼ë¬¸ íŠ¹í™” ì²˜ë¦¬:
- ì„¹ì…˜ ìë™ ê°ì§€ (Abstract, Introduction, Methods, Results, Discussion, Conclusion, References)
- Figure/Table ìº¡ì…˜ ìš°ì„  ì¶”ì¶œ
- Abstract, Conclusion ì„¹ì…˜ ìš°ì„  ì²˜ë¦¬
- References ì„¹ì…˜ íŒŒì‹±
- ìˆ˜ì‹ ì¶”ì¶œ (ì˜µì…˜)
"""
from typing import Dict, Any, Callable, Optional
import logging
import json
from datetime import datetime

from app.services.document.pipelines.general_pipeline import GeneralPipeline
from app.services.document.extraction.adaptive_section_detector import AdaptiveSectionDetector
from app.core.database import get_async_session_local
from app.services.document.processing.bibliography_service import BibliographyService

logger = logging.getLogger(__name__)


class AcademicPaperPipeline(GeneralPipeline):
    """
    í•™ìˆ  ë…¼ë¬¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    
    í˜„ì¬ëŠ” GeneralPipelineì„ ìƒì†ë°›ì•„ ê¸°ë³¸ ê¸°ëŠ¥ ì‚¬ìš©.
    í–¥í›„ ë…¼ë¬¸ íŠ¹í™” ê¸°ëŠ¥ ì¶”ê°€ ì˜ˆì •:
    - ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ (Abstract, Introduction, Methodology, Results, Discussion, Conclusion)
    - Figure/Table ìº¡ì…˜ ìš°ì„  ì²˜ë¦¬
    - References íŒŒì‹±
    - ìˆ˜ì‹(LaTeX) ì¶”ì¶œ
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ì ì‘í˜• ì„¹ì…˜ ê°ì§€ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.section_detector = AdaptiveSectionDetector()
        
        # ë…¼ë¬¸ íŠ¹í™” ì˜µì…˜
        self.extract_figures = self._get_option("extract_figures", True)
        self.parse_references = self._get_option("parse_references", True)
        self.extract_equations = self._get_option("extract_equations", False)
        self.priority_sections = self._get_option("priority_sections", ["abstract", "conclusion"])
        self.figure_caption_required = self._get_option("figure_caption_required", True)
        
        logger.info(f"ğŸ“š [AcademicPaperPipeline] ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ğŸ–¼ï¸ Figure ì¶”ì¶œ: {self.extract_figures}")
        logger.info(f"   ğŸ“– References íŒŒì‹±: {self.parse_references}")
        logger.info(f"   ğŸ”¢ ìˆ˜ì‹ ì¶”ì¶œ: {self.extract_equations}")
        logger.info(f"   â­ ìš°ì„  ì„¹ì…˜: {self.priority_sections}")

    def _get_storage_adapter(
        self,
    ) -> tuple[str, Optional[Callable[[str], str]], Optional[Callable[[str, bytes, str], None]]]:
        """ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œë³„ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ/ì—…ë¡œë“œ í—¬í¼"""
        from app.core.config import settings

        backend = getattr(settings, 'storage_backend', 's3').lower()

        if backend == 'azure_blob':
            from app.services.core.azure_blob_service import get_azure_blob_service

            blob_service = get_azure_blob_service()

            def download_text(key: str) -> str:
                return blob_service.download_text(key, purpose='intermediate')

            def upload_bytes(key: str, data: bytes, content_type: str = 'text/plain; charset=utf-8') -> None:
                blob_service.upload_bytes(data, key, purpose='intermediate')

            return backend, download_text, upload_bytes

        if backend == 's3':
            from app.services.core.aws_service import S3Service

            s3_service = S3Service()

            def download_text(key: str) -> str:
                # S3ì—ì„œëŠ” intermediate/ prefixê°€ ìë™ìœ¼ë¡œ ë¶™ìœ¼ë¯€ë¡œ ì¶”ê°€
                full_key = f"intermediate/{key}" if not key.startswith('intermediate/') else key
                return s3_service.download_text(full_key)

            def upload_bytes(key: str, data: bytes, content_type: str = 'text/plain; charset=utf-8') -> None:
                # upload_bytesëŠ” ì´ë¯¸ purpose='intermediate'ë¡œ í˜¸ì¶œë˜ë¯€ë¡œ prefix ìë™ ì¶”ê°€ë¨
                full_key = f"intermediate/{key}" if not key.startswith('intermediate/') else key
                put_params = {
                    'Bucket': s3_service.bucket_name,
                    'Key': full_key,
                    'Body': data
                }
                if content_type:
                    put_params['ContentType'] = content_type
                s3_service.s3_client.put_object(**put_params)

            return backend, download_text, upload_bytes

        return backend, None, None
    
    async def process(self) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ + í•™ìˆ ë…¼ë¬¸ ì„¹ì…˜ ê°ì§€"""
        logger.info(f"ğŸš€ [AcademicPaperPipeline] íŒŒì´í”„ë¼ì¸ ì‹œì‘: {self.file_name}")
        
        # ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = await super().process()
        
        if not result.get("success"):
            return result
        
        statistics = result.get("statistics") or {}
        section_meta = statistics.get("section_chunking") if isinstance(statistics, dict) else None

        if section_meta and section_meta.get("stored_to_blob"):
            logger.info("[AcademicPaperPipeline] ì„¹ì…˜ ê°ì§€ê°€ ì²­í‚¹ ë‹¨ê³„ì—ì„œ ì™„ë£Œë˜ì–´ ì¬ì‹¤í–‰ ìƒëµ")
            logger.info("[AcademicPaperPipeline] âš ï¸ ì„œì§€ì •ë³´ upsertëŠ” ì²­í‚¹ ë‹¨ê³„ì—ì„œ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ì‹¤í–‰ í•„ìš”")
            # ì²­í‚¹ ë‹¨ê³„ì—ì„œ ì„¹ì…˜ì€ ê°ì§€í–ˆì§€ë§Œ ì„œì§€ì •ë³´ëŠ” ì €ì¥ ì•ˆ í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ì²˜ë¦¬
            try:
                await self._upsert_bibliography_only()
            except Exception as e:
                logger.error(f"âš ï¸ [AcademicPaperPipeline] ì„œì§€ì •ë³´ upsert ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}", exc_info=True)
        else:
            # ì„¹ì…˜ ê°ì§€ ì¶”ê°€ ì²˜ë¦¬
            logger.info("[AcademicPaperPipeline] ì„¹ì…˜ ê°ì§€ ë° ì„œì§€ì •ë³´ upsert ì‹œì‘")
            try:
                await self._detect_and_save_sections()
            except Exception as e:
                logger.error(f"âš ï¸ [AcademicPaperPipeline] ì„¹ì…˜ ê°ì§€ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}", exc_info=True)
        
        logger.info(f"âœ… [AcademicPaperPipeline] íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        return result
    
    async def _detect_and_save_sections(self):
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ ì„¹ì…˜ ê°ì§€ (ë™ì  ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ)"""
        # ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ
        try:
            storage_backend, download_text, upload_bytes = self._get_storage_adapter()
            blob_key = f"multimodal/{self.document_id}/extraction_full_text.txt"

            if not download_text:
                logger.warning(f"[SECTION-DETECT] ì§€ì›ë˜ì§€ ì•ŠëŠ” ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ: {storage_backend}")
                return

            try:
                full_text = download_text(blob_key)
            except Exception as exc:
                logger.error(f"[SECTION-DETECT] ì „ì²´ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({storage_backend}): {exc}")
                return
            
            if not full_text:
                logger.warning("[SECTION-DETECT] ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return
            
            # ì„¹ì…˜ ê°ì§€
            sections = self.section_detector.detect_sections(full_text)
            
            if sections:
                # ì„¹ì…˜ ìš”ì•½ í†µê³„
                summary = self.section_detector.get_section_summary(sections)
                logger.info(
                    f"[SECTION-DETECT] {summary['total_sections']}ê°œ ì„¹ì…˜ ê°ì§€: "
                    f"{', '.join(summary['sections_found'])}"
                )
                logger.info(
                    f"[SECTION-DETECT] Abstract: {summary['abstract_words']}ë‹¨ì–´, "
                    f"References ì‹œì‘: {summary['references_start_page']}í˜ì´ì§€"
                )
                
                # Blobì— ì„¹ì…˜ ì •ë³´ ì €ì¥
                sections_blob_path = f"multimodal/{self.document_id}/sections.json"
                sections_data = {
                    "sections": sections,
                    "summary": summary,
                    "detected_at": datetime.now().isoformat()
                }
                if upload_bytes:
                    upload_bytes(
                        sections_blob_path,
                        json.dumps(sections_data, ensure_ascii=False, indent=2).encode("utf-8"),
                        content_type='application/json; charset=utf-8'
                    )
                    logger.info(f"[SECTION-DETECT] ì„¹ì…˜ ì •ë³´ ì €ì¥({storage_backend}): {sections_blob_path}")
                else:
                    logger.warning("[SECTION-DETECT] ì—…ë¡œë“œ í—¬í¼ê°€ ì—†ì–´ ì„¹ì…˜ ì •ë³´ë¥¼ ì €ì¥í•˜ì§€ ëª»í•¨")

                # í•™ìˆ  ë…¼ë¬¸ ì„œì§€ì •ë³´ ìµœì†Œ upsert (ì œëª©/ì´ˆë¡/DOI/ì—°ë„)
                try:
                    async_session_local = get_async_session_local()
                    async with async_session_local() as session:
                        biblio = BibliographyService(session)
                        # ì²« í˜ì´ì§€ í…ìŠ¤íŠ¸(ìˆë‹¤ë©´) ì¶”ì¶œ
                        first_page_text = None
                        try:
                            pages = sections_data.get("summary", {}).get("pages", []) or []
                        except Exception:
                            pages = []
                        # í˜„ì¬ ì €ì¥ êµ¬ì¡°ì—” í˜ì´ì§€ ì›ë¬¸ì´ ìš”ì•½ì— ì—†ìœ¼ë¯€ë¡œ None ì²˜ë¦¬
                        upsert_res = await biblio.upsert_document_metadata(
                            file_bss_info_sno=self.document_id,
                            full_text=full_text,
                            sections_summary=sections_data,  # ì „ì²´ sections.json ë°ì´í„° ì „ë‹¬
                            first_page_text=first_page_text,
                        )
                        if not upsert_res.get("success"):
                            logger.warning(f"[BIBLIO] Upsert failed (non-fatal): {upsert_res.get('error')}")
                        else:
                            logger.info(
                                f"[BIBLIO] Upsert success: doi={upsert_res.get('doi')}, year={upsert_res.get('year')}, title={upsert_res.get('title')}"
                            )
                except Exception as e:
                    logger.warning(f"[BIBLIO] Upsert exception (non-fatal): {e}")
            else:
                logger.warning("[SECTION-DETECT] ì„¹ì…˜ì„ ê°ì§€í•˜ì§€ ëª»í•¨")
                
        except Exception as e:
            logger.error(f"[SECTION-DETECT] ì„¹ì…˜ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            raise
    
    async def _upsert_bibliography_only(self):
        """
        ì„¹ì…˜ ê°ì§€ê°€ ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ì„œì§€ì •ë³´ë§Œ upsert
        """
        try:
            storage_backend, download_text, _ = self._get_storage_adapter()
            if not download_text:
                logger.warning(f"[BIBLIO-ONLY] ì§€ì›ë˜ì§€ ì•ŠëŠ” ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ: {storage_backend}")
                return

            blob_path = f"multimodal/{self.document_id}/extraction_full_text.txt"
            full_text = download_text(blob_path)
            
            if not full_text:
                logger.warning("[BIBLIO-ONLY] ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return
            
            # ì„¹ì…˜ ì •ë³´ ë¡œë“œ (ìˆë‹¤ë©´)
            sections_blob_path = f"multimodal/{self.document_id}/sections.json"
            try:
                sections_json = download_text(sections_blob_path)
                sections_data = json.loads(sections_json) if sections_json else {}
                # ì „ì²´ sections_dataë¥¼ ì „ë‹¬ (sections ë°°ì—´ê³¼ summary ëª¨ë‘ í¬í•¨)
            except Exception as e:
                logger.warning(f"[BIBLIO-ONLY] ì„¹ì…˜ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
                sections_data = None
            
            # ì„œì§€ì •ë³´ upsert
            async_session_local = get_async_session_local()
            async with async_session_local() as session:
                biblio = BibliographyService(session)
                upsert_res = await biblio.upsert_document_metadata(
                    file_bss_info_sno=self.document_id,
                    full_text=full_text,
                    sections_summary=sections_data,  # ì „ì²´ sections.json ë°ì´í„° ì „ë‹¬
                    first_page_text=None,
                )
                if not upsert_res.get("success"):
                    logger.warning(f"[BIBLIO-ONLY] Upsert failed: {upsert_res.get('error')}")
                else:
                    title = upsert_res.get('title') or ''
                    logger.info(
                        f"[BIBLIO-ONLY] âœ… Upsert success: doi={upsert_res.get('doi')}, "
                        f"year={upsert_res.get('year')}, title={title[:50]}..."
                    )
        except Exception as e:
            logger.error(f"[BIBLIO-ONLY] ì„œì§€ì •ë³´ upsert ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
    
    # ğŸ”œ í–¥í›„ ì¶”ê°€ ë©”ì„œë“œë“¤ (placeholder)
    
    # def _filter_figures_with_caption(self, extracted_objects: List[Dict]) -> List[Dict]:
    #     """
    #     ìº¡ì…˜ì´ ìˆëŠ” Figureë§Œ í•„í„°ë§
    #     """
    #     filtered = []
    #     for obj in extracted_objects:
    #         if obj.get("object_type") == "image":
    #             if not self.figure_caption_required or obj.get("content_text"):
    #                 filtered.append(obj)
    #         else:
    #             filtered.append(obj)
    #     return filtered
    
    # def _apply_section_priority(self, chunks: List[Dict]) -> List[Dict]:
    #     """
    #     ì„¹ì…˜ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì ìš©
    #     """
    #     for chunk in chunks:
    #         section = chunk.get("section", "").lower()
    #         if section in self.priority_sections:
    #             chunk["priority"] = "high"
    #         else:
    #             chunk["priority"] = "normal"
    #     return chunks

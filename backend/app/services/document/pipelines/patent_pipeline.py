"""
íŠ¹í—ˆ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

íŠ¹í—ˆ ë¬¸ì„œ íŠ¹í™” ì²˜ë¦¬:
- ì •í˜•í™”ëœ ì„¹ì…˜ ìë™ ê°ì§€ (ì²­êµ¬í•­, ëª…ì„¸ì„œ, ë„ë©´ ë“±)
- ì²­êµ¬í•­ ìš°ì„  ì²˜ë¦¬ (ë…ë¦½í•­/ì¢…ì†í•­ ë¶„ë¦¬)
- ì„¹ì…˜ë³„ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° ì €ì¥
- íŠ¹í—ˆ ì„œì§€ì •ë³´ ì¶”ì¶œ (ì¶œì›ë²ˆí˜¸, ë°œëª…ì, IPC ë“±)
"""
from typing import Dict, Any, Optional, Callable, Tuple
import logging
import json
from datetime import datetime

from app.services.document.pipelines.general_pipeline import GeneralPipeline
from app.services.document.extraction.patent_section_detector import PatentSectionDetector, PatentSection
from app.core.database import get_async_session_local

logger = logging.getLogger(__name__)


class PatentPipeline(GeneralPipeline):
    """
    íŠ¹í—ˆ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    
    GeneralPipelineì„ ìƒì†ë°›ì•„ ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ìˆ˜í–‰ í›„
    íŠ¹í—ˆ íŠ¹í™” ê¸°ëŠ¥ ì¶”ê°€:
    - íŠ¹í—ˆ ì„¹ì…˜ ê°ì§€ (ì²­êµ¬í•­, ë°°ê²½, ìƒì„¸ì„¤ëª… ë“±)
    - ì²­êµ¬í•­ ê°œë³„ í•­ ë¶„ë¦¬ ë° ìš°ì„  ì²˜ë¦¬
    - ì„¹ì…˜ë³„ ë©”íƒ€ë°ì´í„° ì €ì¥
    - íŠ¹í—ˆ ì„œì§€ì •ë³´ DB ì €ì¥
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # íŠ¹í—ˆ ì„¹ì…˜ ê°ì§€ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.section_detector = PatentSectionDetector()
        
        # íŠ¹í—ˆ íŠ¹í™” ì˜µì…˜
        self.extract_claims = self._get_option("extract_claims", True)
        self.parse_citations = self._get_option("parse_citations", False)
        self.technical_field_extraction = self._get_option("technical_field_extraction", True)
        self.priority_claims = self._get_option("priority_claims", True)
        
        logger.info(f"ğŸ“œ [PatentPipeline] ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   âš–ï¸ ì²­êµ¬í•­ ì¶”ì¶œ: {self.extract_claims}")
        logger.info(f"   ğŸ”— ì¸ìš© íŠ¹í—ˆ íŒŒì‹±: {self.parse_citations}")
        logger.info(f"   ğŸ”¬ ê¸°ìˆ ë¶„ì•¼ ì¶”ì¶œ: {self.technical_field_extraction}")
        logger.info(f"   â­ ì²­êµ¬í•­ ìš°ì„  ì²˜ë¦¬: {self.priority_claims}")

    def _get_storage_adapter(
        self,
    ) -> tuple[str, Optional[Callable[[str], str]], Optional[Callable[[str, bytes, str], None]]]:
        """ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œë³„ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ/ì—…ë¡œë“œ í—¬í¼"""
        from app.core.config import settings

        backend = getattr(settings, 'storage_backend', 's3').lower()

        if backend == 'azure_blob':
            from app.services.core.azure_blob_service import get_azure_blob_service

            blob_service = get_azure_blob_service()

            def download_text(key: str) -> str:
                return blob_service.download_text(key, purpose='intermediate')

            def upload_bytes(key: str, data: bytes, content_type: str = 'text/plain; charset=utf-8') -> None:
                blob_service.upload_bytes(data, key, purpose='intermediate')

            return backend, download_text, upload_bytes

        if backend == 's3':
            from app.services.core.aws_service import S3Service

            s3_service = S3Service()

            def download_text(key: str) -> str:
                # S3ì—ì„œëŠ” intermediate/ prefixê°€ ìë™ìœ¼ë¡œ ë¶™ìœ¼ë¯€ë¡œ ì¶”ê°€
                full_key = f"intermediate/{key}" if not key.startswith('intermediate/') else key
                return s3_service.download_text(full_key)

            def upload_bytes(key: str, data: bytes, content_type: str = 'text/plain; charset=utf-8') -> None:
                # upload_bytesëŠ” ì´ë¯¸ purpose='intermediate'ë¡œ í˜¸ì¶œë˜ë¯€ë¡œ prefix ìë™ ì¶”ê°€ë¨
                full_key = f"intermediate/{key}" if not key.startswith('intermediate/') else key
                put_params = {
                    'Bucket': s3_service.bucket_name,
                    'Key': full_key,
                    'Body': data
                }
                if content_type:
                    put_params['ContentType'] = content_type
                s3_service.s3_client.put_object(**put_params)

            return backend, download_text, upload_bytes

        return backend, None, None
    
    async def process(self) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ + íŠ¹í—ˆ ì„¹ì…˜ ê°ì§€ ë° ì²­í‚¹ ë©”íƒ€ë°ì´í„° ë³´ê°•"""
        logger.info(f"ğŸš€ [PatentPipeline] íŒŒì´í”„ë¼ì¸ ì‹œì‘: {self.file_name}")
        
        # 1. ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì¶”ì¶œ, ì²­í‚¹, ì„ë² ë”©, ì¸ë±ì‹±)
        result = await super().process()
        
        if not result.get("success"):
            return result
        
        # 2. íŠ¹í—ˆ ì„¹ì…˜ ê°ì§€ ì¶”ê°€ ì²˜ë¦¬ (í›„ì²˜ë¦¬)
        logger.info("[PatentPipeline] íŠ¹í—ˆ ì„¹ì…˜ ê°ì§€ ë° ì²­í‚¹ ë©”íƒ€ë°ì´í„° ë³´ê°• ì‹œì‘")
        try:
            sections_data, full_text = await self._detect_and_save_patent_sections()
            
            # 3. ì„¹ì…˜ ì •ë³´ë¥¼ ì²­í¬ì— ë§¤í•‘ (section_heading ì—…ë°ì´íŠ¸)
            if sections_data and sections_data.get("sections") and full_text:
                await self._enrich_chunks_with_sections(sections_data["sections"], full_text)
            
        except Exception as e:
            logger.error(f"âš ï¸ [PatentPipeline] íŠ¹í—ˆ ì„¹ì…˜ ê°ì§€ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}", exc_info=True)
        
        logger.info(f"âœ… [PatentPipeline] íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        return result
    
    async def _detect_and_save_patent_sections(self) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹í—ˆ ì„¹ì…˜ ê°ì§€ ë° ì €ì¥ (ë™ì  ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ)"""
        # ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ë¡œë“œ
        try:
            storage_backend, download_text, upload_bytes = self._get_storage_adapter()
            blob_key = f"multimodal/{self.document_id}/extraction_full_text.txt"

            if not download_text:
                logger.warning(f"[PATENT-SECTION] ì§€ì›ë˜ì§€ ì•ŠëŠ” ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ: {storage_backend}")
                return None

            try:
                full_text = download_text(blob_key)
            except Exception as exc:
                logger.error(f"[PATENT-SECTION] ì „ì²´ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({storage_backend}): {exc}")
                return None
            
            if not full_text:
                logger.warning("[PATENT-SECTION] ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
            
            # íŠ¹í—ˆ ì„¹ì…˜ ê°ì§€
            sections = self.section_detector.detect_sections(full_text)
            
            if sections:
                # ì„¹ì…˜ ìš”ì•½ í†µê³„
                summary = self.section_detector.get_section_summary(sections)
                logger.info(
                    f"[PATENT-SECTION] {summary['total_sections']}ê°œ ì„¹ì…˜ ê°ì§€: "
                    f"{', '.join(summary['sections_found'])}"
                )
                logger.info(
                    f"[PATENT-SECTION] ì²­êµ¬í•­: {summary['claims_count']}ê°œ í•­"
                )
                
                # ì„ í–‰ê¸°ìˆ ë¬¸í—Œ(ì¸ìš©ë¬¸í—Œ) ì¶”ì¶œ (ìˆìœ¼ë©´ êµ¬ì¡°í™”í•´ì„œ í•¨ê»˜ ì €ì¥)
                prior_art_citations: list[str] = []
                try:
                    for s in sections:
                        if s.section_type == "prior_art":
                            prior_art_citations = self.section_detector.extract_prior_art_citations(s.content)
                            break
                except Exception as exc:
                    logger.warning(f"[PATENT-SECTION] ì„ í–‰ê¸°ìˆ ë¬¸í—Œ ì¸ìš© ì¶”ì¶œ ì‹¤íŒ¨ (non-fatal): {exc}")

                # ì„¹ì…˜ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™” (PatentSection â†’ dict)
                sections_data = {
                    "sections": [self._section_to_dict(s) for s in sections],
                    "summary": summary,
                    "prior_art_citations": prior_art_citations,
                    "detected_at": datetime.now().isoformat()
                }
                
                # Blobì— ì„¹ì…˜ ì •ë³´ ì €ì¥
                sections_blob_path = f"multimodal/{self.document_id}/patent_sections.json"
                if upload_bytes:
                    upload_bytes(
                        sections_blob_path,
                        json.dumps(sections_data, ensure_ascii=False, indent=2).encode("utf-8"),
                        content_type='application/json; charset=utf-8'
                    )
                    logger.info(f"[PATENT-SECTION] ì„¹ì…˜ ì •ë³´ ì €ì¥({storage_backend}): {sections_blob_path}")
                else:
                    logger.warning("[PATENT-SECTION] ì—…ë¡œë“œ í—¬í¼ê°€ ì—†ì–´ ì„¹ì…˜ ì •ë³´ë¥¼ ì €ì¥í•˜ì§€ ëª»í•¨")

                # íŠ¹í—ˆ ì„œì§€ì •ë³´ ì €ì¥ (í–¥í›„ êµ¬í˜„)
                # await self._save_patent_bibliographic_info(full_text, sections_data)
                
                return sections_data, full_text
            else:
                logger.warning("[PATENT-SECTION] íŠ¹í—ˆ ì„¹ì…˜ì„ ê°ì§€í•˜ì§€ ëª»í•¨")
                return None, full_text
                
        except Exception as e:
            logger.error(f"[PATENT-SECTION] ì„¹ì…˜ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            raise
    
    def _section_to_dict(self, section: PatentSection) -> Dict[str, Any]:
        """PatentSection ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        result = {
            "section_type": section.section_type,
            "title": section.title,
            "start_pos": section.start_pos,
            "end_pos": section.end_pos,
            "content": section.content[:500] + "..." if len(section.content) > 500 else section.content,  # ìš”ì•½ë§Œ ì €ì¥
            "content_length": len(section.content),
            "priority": section.priority
        }
        
        # í•˜ìœ„ ì„¹ì…˜ (ì²­êµ¬í•­ì˜ ê²½ìš° ê°œë³„ í•­)
        if section.subsections:
            result["subsections"] = [self._section_to_dict(sub) for sub in section.subsections]
        
        return result
    
    async def _enrich_chunks_with_sections(self, sections: list[Dict[str, Any]], full_text: str):
        """
        ì²­í¬ì— ì„¹ì…˜ ì •ë³´ ë§¤í•‘ (section_heading ì—…ë°ì´íŠ¸)
        
        ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ ì‹œì‘ ìœ„ì¹˜ë¥¼ íŠ¹í—ˆ ì„¹ì…˜ ë²”ìœ„ì™€ ë¹„êµí•˜ì—¬
        í•´ë‹¹ ì„¹ì…˜ì˜ ì œëª©ì„ section_headingì— ì„¤ì •
        """
        try:
            from app.models.document.multimodal_models import DocChunk
            from sqlalchemy import select, update
            
            async_session_local = get_async_session_local()
            async with async_session_local() as session:
                # DBì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ ì¡°íšŒ
                stmt = select(DocChunk).where(DocChunk.file_bss_info_sno == self.document_id)
                result = await session.execute(stmt)
                chunks = list(result.scalars().all())
                
                if not chunks:
                    logger.warning(f"[PATENT-CHUNK-ENRICH] ë¬¸ì„œ ID {self.document_id}ì— ëŒ€í•œ ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return

                # ì²­í¬ë¥¼ ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬ (ìˆœì°¨ì  ì²˜ë¦¬ë¥¼ ìœ„í•´)
                chunks.sort(key=lambda x: x.chunk_index)
                
                # 1. ê° ì²­í¬ì˜ ìœ„ì¹˜ ì°¾ê¸° (Two-Pass Approach)
                chunk_positions = []  # (chunk, start_pos, end_pos)
                current_search_pos = 0
                
                for chunk in chunks:
                    # NOTE: table ì²­í¬ì—ë„ ì„¹ì…˜ í—¤ë”©(ì˜ˆ: (54) ë°œëª…ì˜ ëª…ì¹­)ì´ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
                    #       text/table ëª¨ë‘ë¥¼ ëŒ€ìƒìœ¼ë¡œ ìœ„ì¹˜ íƒìƒ‰ ë° ì„¹ì…˜ ë§¤í•‘ì„ ìˆ˜í–‰í•œë‹¤.
                    if chunk.modality not in {"text", "table"}:
                        chunk_positions.append((chunk, -1, -1))
                        continue
                        
                    chunk_text = chunk.content_text
                    if not chunk_text:
                        chunk_positions.append((chunk, -1, -1))
                        continue
                        
                    # ìœ„ì¹˜ ì°¾ê¸° ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
                    chunk_start_pos = -1
                    
                    # 1. ì •í™•í•œ ë§¤ì¹­
                    if len(chunk_text) > 10:
                        chunk_start_pos = full_text.find(chunk_text, current_search_pos)
                    
                    # 2. ì•ë¶€ë¶„ ë§¤ì¹­ (50ì)
                    if chunk_start_pos == -1 and len(chunk_text) >= 50:
                        chunk_start_pos = full_text.find(chunk_text[:50], current_search_pos)
                        
                    # 3. ì¤‘ê°„ë¶€ë¶„ ë§¤ì¹­
                    if chunk_start_pos == -1 and len(chunk_text) > 100:
                        mid_idx = len(chunk_text) // 2
                        mid_text = chunk_text[mid_idx : mid_idx + 50]
                        mid_pos = full_text.find(mid_text, current_search_pos)
                        if mid_pos != -1:
                            chunk_start_pos = max(current_search_pos, mid_pos - mid_idx)
                            
                    # 4. ê³µë°± ìœ ì—° ë§¤ì¹­
                    if chunk_start_pos == -1:
                        try:
                            import re
                            clean_start = "".join(c for c in chunk_text[:30] if c.isalnum())
                            if clean_start:
                                pattern_str = r"\s*".join(list(map(re.escape, clean_start)))
                                match = re.search(pattern_str, full_text[current_search_pos:])
                                if match:
                                    chunk_start_pos = current_search_pos + match.start()
                        except Exception:
                            pass
                    
                    if chunk_start_pos != -1:
                        chunk_end_pos = chunk_start_pos + len(chunk_text)
                        chunk_positions.append((chunk, chunk_start_pos, chunk_end_pos))
                        # ë‹¤ìŒ ê²€ìƒ‰ ìœ„ì¹˜ ì—…ë°ì´íŠ¸ (ê²¹ì¹¨ ê³ ë ¤í•˜ì—¬ ì ˆë°˜ë§Œ ì „ì§„)
                        current_search_pos = chunk_start_pos + (len(chunk_text) // 2)
                    else:
                        chunk_positions.append((chunk, -1, -1))
                
                # 2. ìœ„ì¹˜ ë³´ê°„ (Interpolation)
                last_valid_end = 0
                for i, (chunk, start, end) in enumerate(chunk_positions):
                    if start == -1:
                        # ìœ„ì¹˜ë¥¼ ëª» ì°¾ì€ ê²½ìš°: ì´ì „ ìœ íš¨ ìœ„ì¹˜ ë°”ë¡œ ë’¤ë¡œ ê°€ì •
                        if chunk.modality in {"text", "table"} and chunk.content_text:
                            interpolated_start = last_valid_end
                            interpolated_end = interpolated_start + len(chunk.content_text)
                            chunk_positions[i] = (chunk, interpolated_start, interpolated_end)
                            # ë‹¤ìŒ ì²­í¬ë¥¼ ìœ„í•´ last_valid_end ì—…ë°ì´íŠ¸
                            last_valid_end = interpolated_end
                    else:
                        last_valid_end = end
                
                # 3. ì„¹ì…˜ ë§¤í•‘
                update_count = 0
                for chunk, start, end in chunk_positions:
                    if start == -1:
                        continue

                    # ì„¹ì…˜ ì„ íƒ: ê¸°ë³¸ì€ overlap ìµœëŒ€ (ê¸°ì¡´ í’ˆì§ˆ ìœ ì§€)
                    matching_section = None
                    best_overlap = 0

                    for section in sections:
                        s_start = int(section.get("start_pos", 0))
                        s_end = int(section.get("end_pos", 0))
                        overlap = min(end, s_end) - max(start, s_start)
                        if overlap > best_overlap:
                            best_overlap = overlap
                            matching_section = section

                    # overlapì´ 0ì´ë©´ fallback: center/start ê¸°ì¤€
                    if not matching_section:
                        center_pos = (start + end) // 2
                        for section in sections:
                            if section["start_pos"] <= center_pos < section["end_pos"]:
                                matching_section = section
                                break
                        if not matching_section:
                            for section in sections:
                                if section["start_pos"] <= start < section["end_pos"]:
                                    matching_section = section
                                    break

                    # íŠ¹ì • ì„¹ì…˜(ì§§ê±°ë‚˜ í‘œ/ë„ì–´ì“°ê¸° ë³€í˜•ìœ¼ë¡œ chunkê°€ ì„ì´ëŠ” ê²½ìš°) ë³´ì •
                    # - ë°œëª…ì˜ ëª…ì¹­: í‘œ(table) ì²­í¬ì— í¬í•¨ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
                    # - ê¸°ìˆ ë¶„ì•¼: 'ê¸° ìˆ  ë¶„ ì•¼' ì²˜ëŸ¼ ë„ì–´ì“°ê¸° ë³€í˜•ì´ í”í•¨
                    try:
                        import re

                        chunk_text = chunk.content_text or ""

                        if chunk_text:
                            # 'ë°œëª…ì˜ ëª…ì¹­'ì€ (54) í‘œê¸°/í…Œì´ë¸” íŒŒì´í”„ë¼ì¸ì—ì„œ ìì£¼ ë“±ì¥
                            if re.search(r"(?:^|\n|\|)\s*(?:\(54\)\s*)?ë°œ\s*ëª…\s*ì˜\s*ëª…\s*ì¹­", chunk_text):
                                forced = next((s for s in sections if s.get("title") == "ë°œëª…ì˜ ëª…ì¹­"), None)
                                if forced:
                                    matching_section = forced

                            # 'ê¸°ìˆ ë¶„ì•¼'ëŠ” ë³¸ë¬¸ì—ë„ 'ê¸°ìˆ  ë¶„ì•¼ì—ì„œ ...'ë¡œ í”íˆ ë“±ì¥í•˜ë¯€ë¡œ
                            # í—¤ë”© í˜•íƒœ(ë¼ì¸ ì‹œì‘ + ë’¤ì— ê°œí–‰/ëŒ€ê´„í˜¸ ë“±)ë§Œ ë§¤ì¹­
                            if re.search(r"(?:^|\n)\s*ê¸°\s*ìˆ \s*ë¶„\s*ì•¼\s*(?:\n|\[)", chunk_text):
                                forced = next((s for s in sections if s.get("title") == "ê¸°ìˆ ë¶„ì•¼"), None)
                                if forced:
                                    matching_section = forced
                    except Exception:
                        pass

                    if matching_section:
                        chunk.section_heading = matching_section["title"]
                        update_count += 1
                
                # DB ì»¤ë°‹
                await session.commit()
                
                logger.info(f"[PATENT-CHUNK-ENRICH] âœ… {update_count}ê°œ ì²­í¬ì— ì„¹ì…˜ ì •ë³´ ë§¤í•‘ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"[PATENT-CHUNK-ENRICH] ì²­í¬ ë³´ê°• ì‹¤íŒ¨: {e}", exc_info=True)
    
    async def _save_patent_bibliographic_info(self, full_text: str, sections_data: Dict):
        """
        íŠ¹í—ˆ ì„œì§€ì •ë³´ DB ì €ì¥
        
        ğŸ”œ í–¥í›„ êµ¬í˜„:
        - ì¶œì›ë²ˆí˜¸ ì¶”ì¶œ (ì •ê·œì‹)
        - ë°œëª…ì/ì¶œì›ì¸ ì¶”ì¶œ (ì²« í˜ì´ì§€ íŒŒì‹±)
        - IPC ë¶„ë¥˜ ì¶”ì¶œ
        - ì¶œì›ì¼/ë“±ë¡ì¼ ì¶”ì¶œ
        - TbPatentBibliographicInfo í…Œì´ë¸”ì— ì €ì¥
        """
        try:
            logger.info("[PATENT-BIBLIO] íŠ¹í—ˆ ì„œì§€ì •ë³´ ì¶”ì¶œ ì‹œì‘")
            
            # ì¶œì›ë²ˆí˜¸ íŒ¨í„´ (ì˜ˆ: "ì¶œì›ë²ˆí˜¸: 10-2023-0012345")
            application_number = self._extract_application_number(full_text)
            if application_number:
                logger.info(f"[PATENT-BIBLIO] ì¶œì›ë²ˆí˜¸ ë°œê²¬: {application_number}")
            
            # í–¥í›„ DB ì €ì¥ ë¡œì§ ì¶”ê°€
            # async_session_local = get_async_session_local()
            # async with async_session_local() as session:
            #     # TbPatentBibliographicInfo ìƒì„±/ì—…ë°ì´íŠ¸
            #     pass
            
            logger.info("[PATENT-BIBLIO] âœ… ì„œì§€ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"[PATENT-BIBLIO] ì„œì§€ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ (non-fatal): {e}")
    
    def _extract_application_number(self, text: str) -> Optional[str]:
        """
        ì¶œì›ë²ˆí˜¸ ì¶”ì¶œ
        
        í•œêµ­ íŠ¹í—ˆ ì¶œì›ë²ˆí˜¸ í˜•ì‹:
        - 10-2023-0012345
        - 10-2023-12345
        - KR 10-2023-0012345
        """
        import re
        pattern = r'(?:ì¶œì›ë²ˆí˜¸|Application\s+No\.?)\s*[:ï¼š]\s*((?:KR\s+)?10-\d{4}-\d+)'
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        return None
    
    # ğŸ”œ í–¥í›„ ì¶”ê°€ ë©”ì„œë“œë“¤ (placeholder)
    
    # def _parse_patent_claims(self, claims_section: PatentSection) -> List[Dict]:
    #     """
    #     ì²­êµ¬í•­ íŒŒì‹± (ë…ë¦½í•­/ì¢…ì†í•­ êµ¬ë¶„)
    #     """
    #     pass
    
    # def _extract_technical_field(self, text: str) -> Optional[str]:
    #     """
    #     ê¸°ìˆ ë¶„ì•¼ ì¶”ì¶œ
    #     """
    #     pass
    
    # def _parse_cited_patents(self, prior_art_section: PatentSection) -> List[Dict]:
    #     """
    #     ì¸ìš© íŠ¹í—ˆ íŒŒì‹±
    #     """
    #     pass

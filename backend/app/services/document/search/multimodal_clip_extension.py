"""ë©€í‹°ëª¨ë‹¬ CLIP ê²€ìƒ‰ í™•ì¥ ëª¨ë“ˆ

MultimodalSearchServiceì— CLIP ê¸°ë°˜ ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€
"""

# ì´ íŒŒì¼ì˜ ë‚´ìš©ì„ multimodal_search_service.pyì˜ MultimodalSearchService í´ë˜ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤

async def search_multimodal_clip(
    self,
    query: str,
    session: AsyncSession,
    query_type: str = "text",  # "text" or "image"
    top_k: int = 10,
    container_ids: Optional[List[str]] = None,
    file_ids: Optional[List[int]] = None,
    similarity_threshold: float = 0.3,
    modality_filter: Optional[str] = None  # "text", "image", None (ëª¨ë‘)
) -> List[Dict[str, Any]]:
    """CLIP ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (í…ìŠ¤íŠ¸) ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
        session: DB ì„¸ì…˜
        query_type: "text" (í…ìŠ¤íŠ¸ ì¿¼ë¦¬) ë˜ëŠ” "image" (ì´ë¯¸ì§€ ì¿¼ë¦¬)
        top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        container_ids: í•„í„°ë§í•  ì»¨í…Œì´ë„ˆ ID ëª©ë¡
        file_ids: í•„í„°ë§í•  íŒŒì¼ ID ëª©ë¡
        similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
        modality_filter: ê²€ìƒ‰í•  ëª¨ë‹¬ë¦¬í‹° í•„í„° (text/image/None)
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if not image_embedding_service or not image_embedding_service.use_azure_clip:
        logger.warning("CLIP ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return await self.search_similar_chunks(
            query, session, top_k, container_ids, file_ids, similarity_threshold
        )
    
    try:
        # 1. ì¿¼ë¦¬ CLIP ì„ë² ë”© ìƒì„±
        if query_type == "text":
            query_embedding = await image_embedding_service.generate_text_embedding(query)
        else:  # image
            query_embedding = await image_embedding_service.generate_image_embedding(image_path=query)
        
        if not query_embedding:
            logger.warning("ì¿¼ë¦¬ CLIP ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return []
        
        # 2. ì°¨ì› ì¡°ì • (512d)
        clip_dim = 512
        if len(query_embedding) < clip_dim:
            query_embedding = query_embedding + [0.0] * (clip_dim - len(query_embedding))
        elif len(query_embedding) > clip_dim:
            query_embedding = query_embedding[:clip_dim]
        
        # 3. pgvector ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (í”„ë¡œë°”ì´ë”ë³„ ë²¡í„° ì»¬ëŸ¼ ì‚¬ìš©)
        vector_literal = "[" + ",".join(map(str, query_embedding)) + "]"
        
        # ğŸ”· í”„ë¡œë°”ì´ë”ë³„ ë²¡í„° ì»¬ëŸ¼ ì„ íƒ
        from app.core.config import settings
        provider = settings.get_current_embedding_provider()
        
        if provider == 'bedrock':
            # AWS Bedrock: TwelveLabs Marengo (512d)
            vector_column = "de.aws_marengo_vector_512"
            vector_not_null = f"{vector_column} IS NOT NULL"
        else:
            # Azure OpenAI: CLIP (512d)
            vector_column = "COALESCE(de.azure_clip_vector, de.clip_vector)"
            vector_not_null = "(de.azure_clip_vector IS NOT NULL OR de.clip_vector IS NOT NULL)"
        
        # ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ (í”„ë¡œë°”ì´ë”ë³„ ë²¡í„° ì»¬ëŸ¼ ì‚¬ìš©)
        base_query = f"""
        SELECT 
            de.embedding_id,
            dc.chunk_id,
            dc.file_bss_info_sno,
            dc.chunk_index,
            dc.content_text,
            dc.token_count,
            dc.modality,
            fbf.file_lgc_nm as file_name,
            fbf.knowledge_container_id,
            {vector_column} <=> '{vector_literal}'::vector(512) as distance
        FROM doc_embedding de
        JOIN doc_chunk dc ON de.chunk_id = dc.chunk_id
        JOIN tb_file_bss_info fbf ON dc.file_bss_info_sno = fbf.file_bss_info_sno
        WHERE fbf.del_yn = 'N'
          AND {vector_not_null}
        """
        
        # í•„í„° ì¡°ê±´ ì¶”ê°€
        conditions = []
        
        if modality_filter:
            conditions.append(f"dc.modality = '{modality_filter}'")
        
        if container_ids:
            container_filter = "'" + "','".join(container_ids) + "'"
            conditions.append(f"fbf.knowledge_container_id IN ({container_filter})")
        
        if file_ids:
            file_filter = ",".join(map(str, file_ids))
            conditions.append(f"dc.file_bss_info_sno IN ({file_filter})")
        
        if conditions:
            base_query += " AND " + " AND ".join(conditions)
        
        # ì •ë ¬ ë° ì œí•œ
        base_query += f"""
        ORDER BY distance ASC
        LIMIT {top_k}
        """
        
        # 4. ì¿¼ë¦¬ ì‹¤í–‰
        result = await session.execute(text(base_query))
        rows = result.fetchall()
        
        # 5. ê²°ê³¼ í¬ë§·íŒ…
        search_results = []
        for row in rows:
            similarity_score = 1.0 - float(row.distance)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            
            if similarity_score < similarity_threshold:
                continue
            
            search_results.append({
                "chunk_id": row.chunk_id,
                "embedding_id": row.embedding_id,
                "file_id": row.file_bss_info_sno,
                "chunk_index": row.chunk_index,
                "content": row.content_text,
                "token_count": row.token_count,
                "modality": row.modality,
                "file_name": row.file_name,
                "container_id": row.knowledge_container_id,
                "similarity_score": similarity_score,
                "distance": float(row.distance),
                "search_type": "clip_multimodal"
            })
        
        logger.info(f"CLIP ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼ (ì„ê³„ê°’: {similarity_threshold})")
        return search_results
        
    except Exception as e:
        logger.error(f"CLIP ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

async def search_hybrid(
    self,
    query_text: str,
    session: AsyncSession,
    top_k: int = 20,
    container_ids: Optional[List[str]] = None,
    file_ids: Optional[List[int]] = None,
    text_weight: float = 0.6,
    clip_weight: float = 0.4,
    similarity_threshold: float = 0.3
) -> List[Dict[str, Any]]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ + CLIP)
    
    Args:
        query_text: ê²€ìƒ‰ ì¿¼ë¦¬
        session: DB ì„¸ì…˜
        top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        container_ids: í•„í„°ë§í•  ì»¨í…Œì´ë„ˆ ID ëª©ë¡
        file_ids: í•„í„°ë§í•  íŒŒì¼ ID ëª©ë¡
        text_weight: í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.6)
        clip_weight: CLIP ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.4)
        similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ê¸°ë°˜ ì •ë ¬)
    """
    try:
        # 1. í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text_results = await self.search_similar_chunks(
            query_text, session, top_k * 2, 
            container_ids, file_ids, 0.0  # ì„ê³„ê°’ ì—†ì´ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
        )
        
        # 2. CLIP ê²€ìƒ‰
        clip_results = []
        if image_embedding_service and image_embedding_service.use_azure_clip:
            clip_results = await self.search_multimodal_clip(
                query_text, session, "text", top_k * 2,
                container_ids, file_ids, 0.0, None
            )
        
        # 3. ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
        chunk_scores: Dict[int, Dict[str, Any]] = {}
        
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        for result in text_results:
            chunk_id = result["chunk_id"]
            text_score = result["similarity_score"] * text_weight
            chunk_scores[chunk_id] = {
                **result,
                "text_score": text_score,
                "clip_score": 0.0,
                "hybrid_score": text_score
            }
        
        # CLIP ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        for result in clip_results:
            chunk_id = result["chunk_id"]
            clip_score = result["similarity_score"] * clip_weight
            
            if chunk_id in chunk_scores:
                # ê¸°ì¡´ ê²°ê³¼ì— CLIP ì ìˆ˜ ì¶”ê°€
                chunk_scores[chunk_id]["clip_score"] = clip_score
                chunk_scores[chunk_id]["hybrid_score"] += clip_score
            else:
                # CLIP ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
                chunk_scores[chunk_id] = {
                    **result,
                    "text_score": 0.0,
                    "clip_score": clip_score,
                    "hybrid_score": clip_score
                }
        
        # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬
        hybrid_results = sorted(
            chunk_scores.values(),
            key=lambda x: x["hybrid_score"],
            reverse=True
        )
        
        # 5. ì„ê³„ê°’ í•„í„°ë§ ë° top_k ì œí•œ
        final_results = [
            r for r in hybrid_results 
            if r["hybrid_score"] >= similarity_threshold
        ][:top_k]
        
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼ (text: {len(text_results)}, clip: {len(clip_results)})")
        return final_results
        
    except Exception as e:
        logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

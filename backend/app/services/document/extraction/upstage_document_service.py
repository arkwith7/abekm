"""
Upstage Document Parse API ì„œë¹„ìŠ¤

í•œêµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ì— ìµœì í™”ëœ Upstage Document Parse API í†µí•©
- Layout Analysis (ë ˆì´ì•„ì›ƒ ë¶„ì„)
- Table Detection & Extraction (í…Œì´ë¸” ì¶”ì¶œ)
- Figure Detection (ì´ë¯¸ì§€ ì¶”ì¶œ)
- OCR (í•œêµ­ì–´ ìµœì í™”)
- Azure Document Intelligence ëŒ€ì•ˆ

API Documentation: https://console.upstage.ai/docs
"""

import asyncio
import json
import logging
import time
from collections import defaultdict
from typing import Dict, List, Optional, Any

import requests
from pathlib import Path

from app.core.config import settings

logger = logging.getLogger(__name__)


class UpstageResult:
    """Upstage Document Parse ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        success: bool = True,
        text: str = "",
        markdown: str = "",  # ğŸ†• ë§ˆí¬ë‹¤ìš´ ì¶”ê°€
        html: str = "",      # ğŸ†• HTML ì¶”ê°€
        pages: Optional[List[Dict[str, Any]]] = None,
        tables: Optional[List[Dict[str, Any]]] = None,
        figures: Optional[List[Dict[str, Any]]] = None,
        elements: Optional[List[Dict[str, Any]]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None,
        extraction_method: str = "upstage_document_parse"
    ):
        self.success = success
        self.text = text
        self.markdown = markdown  # ğŸ†•
        self.html = html          # ğŸ†•
        self.pages = pages or []
        self.tables = tables or []
        self.figures = figures or []
        self.elements = elements or []
        self.metadata = metadata or {}
        self.error = error
        self.extraction_method = extraction_method


class UpstageDocumentService:
    """Upstage Document Parse API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.api_key = settings.upstage_api_key
        self.api_endpoint = settings.upstage_api_endpoint
        self.max_pages = settings.upstage_max_pages
        self.timeout_seconds = settings.upstage_timeout_seconds
        self.retry_max_attempts = settings.upstage_retry_max_attempts
        self.model = settings.upstage_model
        self.ocr_mode = settings.upstage_ocr_mode
        self.base64_categories = settings.upstage_base64_categories or []
        self.merge_multipage_tables = settings.upstage_merge_multipage_tables
        self.use_async_api = settings.upstage_use_async_api
        self.async_poll_interval = settings.upstage_async_poll_interval_seconds
        self.async_timeout_seconds = settings.upstage_async_timeout_seconds
        self.async_endpoint = settings.upstage_async_api_endpoint or self._infer_async_endpoint(self.api_endpoint)
        self.status_endpoint = settings.upstage_async_status_endpoint or self._infer_status_endpoint(self.api_endpoint)
        
        # ì´ˆê¸°í™” ë¡œê·¸ (ë””ë²„ê¹…ìš©)
        logger.info(f"[UPSTAGE] UpstageDocumentService ì´ˆê¸°í™”")
        logger.info(f"[UPSTAGE] API Endpoint: {self.api_endpoint}")
        logger.info(f"[UPSTAGE] API Key ì„¤ì • ì—¬ë¶€: {bool(self.api_key)}")
        logger.info(f"[UPSTAGE] Max Pages: {self.max_pages}")
        logger.info(f"[UPSTAGE] Timeout: {self.timeout_seconds}s")
        logger.info(f"[UPSTAGE] Retry Attempts: {self.retry_max_attempts}")
        logger.info(f"[UPSTAGE] Model Alias: {self.model}")
        logger.info(f"[UPSTAGE] OCR Mode: {self.ocr_mode or 'auto'}")
        if self.base64_categories:
            logger.info(f"[UPSTAGE] Base64 Encoding Targets: {self.base64_categories}")
        logger.info(f"[UPSTAGE] Merge Multipage Tables: {self.merge_multipage_tables}")
        logger.info(
            f"[UPSTAGE] Async API Enabled: {self.use_async_api and self._supports_async_api()}"
        )
        
        if not self.api_key:
            logger.error("[UPSTAGE] âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. UPSTAGE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            logger.info(f"[UPSTAGE] âœ… API í‚¤ ì„¤ì • ì™„ë£Œ (ê¸¸ì´: {len(self.api_key)}ì)")

    def _infer_async_endpoint(self, endpoint: Optional[str]) -> Optional[str]:
        if not endpoint:
            return None
        base = endpoint.rstrip('/')
        if base.endswith("/document-digitization"):
            return f"{base}/async"
        return None

    def _infer_status_endpoint(self, endpoint: Optional[str]) -> Optional[str]:
        if not endpoint:
            return None
        base = endpoint.rstrip('/')
        if base.endswith("/document-digitization"):
            return f"{base}/requests"
        return None

    def _supports_async_api(self) -> bool:
        return bool(self.async_endpoint and self.status_endpoint)

    def _build_request_payload(self) -> Dict[str, str]:
        payload: Dict[str, str] = {}
        if self.model:
            payload["model"] = self.model
        if self.ocr_mode:
            payload["ocr"] = self.ocr_mode
        # ğŸ†• ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ìš”ì²­ (ì„¹ì…˜ êµ¬ì¡° ë³´ì¡´)
        payload["output_formats"] = "html,markdown,text"
        if self.base64_categories:
            try:
                payload["base64_encoding"] = json.dumps(self.base64_categories)
            except Exception:
                payload["base64_encoding"] = str(self.base64_categories)
        if self.merge_multipage_tables is not None:
            payload["merge_multipage_tables"] = str(self.merge_multipage_tables).lower()
        return payload
    
    async def parse_document(self, file_path: str) -> UpstageResult:
        """
        ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸, í…Œì´ë¸”, ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            UpstageResult: ì¶”ì¶œ ê²°ê³¼
        """
        if not self.api_key:
            logger.error("[UPSTAGE] âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return UpstageResult(
                success=False,
                error="Upstage API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
        
        if not Path(file_path).exists():
            logger.error(f"[UPSTAGE] âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return UpstageResult(
                success=False,
                error=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"
            )
        
        file_size = Path(file_path).stat().st_size
        logger.info(f"[UPSTAGE] ğŸš€ ë¬¸ì„œ ë¶„ì„ ì‹œì‘")
        logger.info(f"[UPSTAGE]    ğŸ“„ íŒŒì¼: {Path(file_path).name}")
        logger.info(f"[UPSTAGE]    ğŸ“Š í¬ê¸°: {file_size / 1024:.2f} KB")
        logger.info(f"[UPSTAGE]    ğŸ”§ ì„¤ì •: max_pages={self.max_pages}, timeout={self.timeout_seconds}s, retry={self.retry_max_attempts}")
        
        start_time = time.time()
        
        try:
            # API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            result = await self._call_api_with_retry(file_path)
            
            elapsed = time.time() - start_time
            
            if result.success:
                logger.info(f"[UPSTAGE] âœ… ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
                logger.info(f"[UPSTAGE]    ğŸ“Š í†µê³„:")
                logger.info(f"[UPSTAGE]       - í˜ì´ì§€ ìˆ˜: {result.metadata.get('page_count', len(result.pages))}")
                logger.info(f"[UPSTAGE]       - í…Œì´ë¸” ìˆ˜: {result.metadata.get('table_count', len(result.tables))}")
                logger.info(f"[UPSTAGE]       - ì´ë¯¸ì§€ ìˆ˜: {result.metadata.get('figure_count', len(result.figures))}")
                logger.info(f"[UPSTAGE]       - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result.text)} ë¬¸ì")
                logger.info(f"[UPSTAGE]       - ëª¨ë¸: {result.metadata.get('model', 'unknown')}")
            else:
                logger.error(f"[UPSTAGE] âŒ ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {elapsed:.2f}ì´ˆ, error={result.error}")
            
            return result
            
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"[UPSTAGE] âŒ ë¬¸ì„œ ë¶„ì„ ì˜ˆì™¸ ë°œìƒ: {elapsed:.2f}ì´ˆ", exc_info=True)
            logger.error(f"[UPSTAGE]    ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            return UpstageResult(
                success=False,
                error=str(e)
            )
    
    async def _call_api_with_retry(self, file_path: str) -> UpstageResult:
        """ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ API í˜¸ì¶œ"""
        
        last_error = None
        retry_reasons = []
        
        for attempt in range(1, self.retry_max_attempts + 1):
            try:
                if attempt == 1:
                    logger.info(f"[UPSTAGE] ğŸ”„ API í˜¸ì¶œ ì‹œë„ {attempt}/{self.retry_max_attempts}")
                else:
                    logger.warning(f"[UPSTAGE] ğŸ”„ ì¬ì‹œë„ {attempt}/{self.retry_max_attempts} (ì´ì „ ì‹¤íŒ¨: {last_error})")
                
                call_start = time.time()
                
                # ë¹„ë™ê¸° HTTP ìš”ì²­ì„ ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰ (requests ì‚¬ìš©)
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None,
                    self._call_api_sync,
                    file_path
                )
                
                call_elapsed = time.time() - call_start
                
                if result.success:
                    logger.info(f"[UPSTAGE] âœ… API í˜¸ì¶œ ì„±ê³µ: {call_elapsed:.2f}ì´ˆ (ì‹œë„ {attempt}/{self.retry_max_attempts})")
                    return result
                else:
                    last_error = result.error
                    retry_reasons.append(f"Attempt {attempt}: {last_error}")
                    logger.warning(f"[UPSTAGE] âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨: {call_elapsed:.2f}ì´ˆ, error={last_error}")
                    
                    # ğŸ†• 413 ì˜¤ë¥˜ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨ (ì¬ì‹œë„í•´ë„ í•´ê²° ë¶ˆê°€ëŠ¥)
                    if '413' in str(last_error) or 'too large' in str(last_error).lower():
                        logger.error(f"[UPSTAGE] ğŸš« íŒŒì¼ í¬ê¸° ì œí•œ ì´ˆê³¼ (HTTP 413) - ì¬ì‹œë„ ì¤‘ë‹¨")
                        return result
                    
            except Exception as e:
                last_error = str(e)
                retry_reasons.append(f"Attempt {attempt}: {type(e).__name__}: {str(e)}")
                logger.warning(f"[UPSTAGE] âš ï¸ API í˜¸ì¶œ ì˜ˆì™¸: ì‹œë„ {attempt}, error={e}")
                
                # ğŸ†• 413 ì˜¤ë¥˜ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                if '413' in str(e):
                    logger.error(f"[UPSTAGE] ğŸš« íŒŒì¼ í¬ê¸° ì œí•œ ì´ˆê³¼ (HTTP 413) - ì¬ì‹œë„ ì¤‘ë‹¨")
                    return UpstageResult(success=False, error=str(e))
            
            # ì¬ì‹œë„ ì „ ëŒ€ê¸° (ë°±ì˜¤í”„)
            if attempt < self.retry_max_attempts:
                wait_time = attempt * 2  # 2ì´ˆ, 4ì´ˆ, 6ì´ˆ...
                logger.info(f"[UPSTAGE] â³ {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                await asyncio.sleep(wait_time)
        
        logger.error(f"[UPSTAGE] âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ({self.retry_max_attempts}íšŒ)")
        logger.error(f"[UPSTAGE]    ì¬ì‹œë„ íˆìŠ¤í† ë¦¬: {retry_reasons}")
        
        return UpstageResult(
            success=False,
            error=f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ({self.retry_max_attempts}íšŒ): {last_error}"
        )
    
    def _call_api_sync(self, file_path: str) -> UpstageResult:
        """ë™ê¸°/ë¹„ë™ê¸° API í˜¸ì¶œ ì§„ì…ì """
        if self.use_async_api and self._supports_async_api():
            logger.info("[UPSTAGE] ğŸŒ€ Async Document Digitization API ì‚¬ìš©")
            return self._call_async_document_parse(file_path)
        return self._call_sync_document_parse(file_path)

    def _call_sync_document_parse(self, file_path: str) -> UpstageResult:
        """ë™ê¸° ë°©ì‹ API í˜¸ì¶œ (requests ì‚¬ìš©)"""
        
        headers = {
            "Authorization": f"Bearer {self.api_key[:20]}...",  # API í‚¤ ì¼ë¶€ë§Œ ë¡œê¹…
        }
        
        file_name = Path(file_path).name
        file_size = Path(file_path).stat().st_size
        
        logger.debug(f"[UPSTAGE] ğŸ“¤ HTTP POST ìš”ì²­ ì¤€ë¹„")
        logger.debug(f"[UPSTAGE]    Endpoint: {self.api_endpoint}")
        logger.debug(f"[UPSTAGE]    File: {file_name} ({file_size / 1024:.2f} KB)")
        
        try:
            request_start = time.time()
            
            with open(file_path, "rb") as f:
                files = {
                    "document": (file_name, f, "application/pdf")
                }
                
                logger.debug(f"[UPSTAGE] ğŸ“¡ HTTP ìš”ì²­ ì „ì†¡ ì¤‘... (timeout={self.timeout_seconds}s)")
                
                response = requests.post(
                    self.api_endpoint,
                    headers={"Authorization": f"Bearer {self.api_key}"},  # ì‹¤ì œ ìš”ì²­ì—ëŠ” ì „ì²´ í‚¤ ì‚¬ìš©
                    data=self._build_request_payload(),
                    files=files,
                    timeout=self.timeout_seconds
                )
            
            request_elapsed = time.time() - request_start
            logger.info(f"[UPSTAGE] ğŸ“¥ HTTP ì‘ë‹µ ìˆ˜ì‹ : {response.status_code} ({request_elapsed:.2f}ì´ˆ)")
            
            # HTTP ì˜¤ë¥˜ ì²´í¬
            response.raise_for_status()
            
            # JSON ì‘ë‹µ íŒŒì‹±
            response_size = len(response.content)
            logger.debug(f"[UPSTAGE] ğŸ“Š ì‘ë‹µ í¬ê¸°: {response_size / 1024:.2f} KB")
            
            data = response.json()
            logger.debug(f"[UPSTAGE] ğŸ” JSON íŒŒì‹± ì™„ë£Œ, ì‘ë‹µ íŒŒì‹± ì‹œì‘...")
            
            # ê²°ê³¼ ë³€í™˜
            return self._parse_response(data)
            
        except requests.exceptions.Timeout:
            logger.error(f"[UPSTAGE] â±ï¸ API ìš”ì²­ íƒ€ì„ì•„ì›ƒ: {self.timeout_seconds}ì´ˆ ì´ˆê³¼")
            return UpstageResult(
                success=False,
                error=f"API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ({self.timeout_seconds}ì´ˆ ì´ˆê³¼)"
            )
        except requests.exceptions.HTTPError as e:
            error_text = e.response.text[:500] if e.response else "No response"
            logger.error(f"[UPSTAGE] ğŸš« HTTP ì˜¤ë¥˜: {e.response.status_code}")
            logger.error(f"[UPSTAGE]    ì‘ë‹µ: {error_text}")
            return UpstageResult(
                success=False,
                error=f"HTTP ì˜¤ë¥˜: {e.response.status_code} - {error_text}"
            )
        except Exception as e:
            logger.error(f"[UPSTAGE] âŒ API í˜¸ì¶œ ì˜ˆì™¸: {type(e).__name__}: {str(e)}")
            return UpstageResult(
                success=False,
                error=f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"
            )

    def _call_async_document_parse(self, file_path: str) -> UpstageResult:
        if not self._supports_async_api():
            logger.warning("[UPSTAGE] âš ï¸ Async API ì •ë³´ê°€ ì—†ì–´ ë™ê¸° APIë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            return self._call_sync_document_parse(file_path)

        file_name = Path(file_path).name
        logger.info(f"[UPSTAGE] ğŸ“¨ Async ìš”ì²­ ì „ì†¡: endpoint={self.async_endpoint}, file={file_name}")

        try:
            with open(file_path, "rb") as f:
                files = {"document": (file_name, f, "application/pdf")}
                response = requests.post(
                    self.async_endpoint,
                    headers={"Authorization": f"Bearer {self.api_key}"},
                    data=self._build_request_payload(),
                    files=files,
                    timeout=self.timeout_seconds
                )

            response.raise_for_status()
            submission = response.json()
            request_id = submission.get("request_id") or submission.get("id")
            if not request_id:
                logger.error(f"[UPSTAGE] âŒ Async ì‘ë‹µì— request_idê°€ ì—†ìŠµë‹ˆë‹¤: {submission}")
                return UpstageResult(success=False, error="Async ì‘ë‹µì— request_idê°€ ì—†ìŠµë‹ˆë‹¤")

            logger.info(f"[UPSTAGE] ğŸ†” Async request_id={request_id}")
            detail = self._poll_async_request(request_id)
            merged_payload = self._collect_async_batches(detail)
            # detail ë©”íƒ€ë°ì´í„° ë³´ê°•
            merged_payload.setdefault("model", detail.get("model"))
            merged_payload.setdefault("usage", {"pages": detail.get("total_pages")})
            merged_payload.setdefault("api", detail.get("api"))
            return self._parse_response(merged_payload)

        except requests.exceptions.Timeout:
            logger.error("[UPSTAGE] â±ï¸ Async API ìš”ì²­ íƒ€ì„ì•„ì›ƒ")
            return UpstageResult(success=False, error="Async API ìš”ì²­ íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            logger.error(f"[UPSTAGE] âŒ Async API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}", exc_info=True)
            return UpstageResult(success=False, error=f"Async API ì‹¤íŒ¨: {str(e)}")

    def _poll_async_request(self, request_id: str) -> Dict[str, Any]:
        status_url = f"{self.status_endpoint.rstrip('/')}/{request_id}"
        deadline = time.time() + self.async_timeout_seconds
        logger.info(f"[UPSTAGE] â³ Async ìƒíƒœ ì¡°íšŒ ì‹œì‘ (timeout={self.async_timeout_seconds}s)")

        while time.time() < deadline:
            resp = requests.get(
                status_url,
                headers={"Authorization": f"Bearer {self.api_key}"},
                timeout=self.timeout_seconds
            )
            resp.raise_for_status()
            detail = resp.json()
            status = (detail.get("status") or "").lower()
            logger.info(f"[UPSTAGE] ğŸ“¡ Async status={status} completed_pages={detail.get('completed_pages')} / {detail.get('total_pages')}")

            if status == "completed":
                return detail
            if status in {"failed", "error"}:
                failure_message = detail.get("failure_message") or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                raise RuntimeError(f"Async ì‘ì—… ì‹¤íŒ¨: {failure_message}")

            time.sleep(self.async_poll_interval)

        raise TimeoutError("Async ì‘ì—…ì´ ì§€ì •ëœ ì‹œê°„ ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    def _collect_async_batches(self, request_detail: Dict[str, Any]) -> Dict[str, Any]:
        batches = request_detail.get("batches") or []
        if not batches:
            raise RuntimeError("Async ì‘ë‹µì— batch ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")

        payloads: List[Dict[str, Any]] = []
        for batch in batches:
            if (batch.get("status") or "").lower() != "completed":
                logger.warning(f"[UPSTAGE] âš ï¸ batch {batch.get('id')} ìƒíƒœ={batch.get('status')} - ê±´ë„ˆëœ€")
                continue
            download_url = batch.get("download_url")
            if not download_url:
                logger.warning(f"[UPSTAGE] âš ï¸ batch {batch.get('id')} ë‹¤ìš´ë¡œë“œ URL ì—†ìŒ")
                continue
            logger.info(f"[UPSTAGE] ğŸ“¥ batch {batch.get('id')} ë‹¤ìš´ë¡œë“œ")
            resp = requests.get(download_url, timeout=self.timeout_seconds)
            resp.raise_for_status()
            payloads.append(resp.json())

        if not payloads:
            raise RuntimeError("ë‹¤ìš´ë¡œë“œí•œ batch ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")

        if len(payloads) == 1:
            return payloads[0]
        return self._merge_batch_payloads(payloads)

    def _merge_batch_payloads(self, payloads: List[Dict[str, Any]]) -> Dict[str, Any]:
        merged = payloads[0]
        merged_content = merged.setdefault("content", {})
        for batch in payloads[1:]:
            content = batch.get("content") or {}
            for key in ("pages", "tables", "figures", "elements"):
                if key in content:
                    merged_content.setdefault(key, [])
                    merged_content[key].extend(content.get(key) or [])
            for key in ("text", "html", "markdown"):
                value = content.get(key)
                if value:
                    existing = merged_content.get(key, "")
                    merged_content[key] = f"{existing}\n{value}".strip() if existing else value
        return merged

    def _build_pages(self, content: Dict[str, Any], elements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        pages: List[Dict[str, Any]] = []
        page_entries = content.get("pages") if isinstance(content, dict) else None
        if isinstance(page_entries, list) and page_entries:
            logger.info(f"[UPSTAGE] ğŸ“„ í˜ì´ì§€ ë°ì´í„° {len(page_entries)}ê±´ íŒŒì‹±")
            for page in page_entries:
                if not isinstance(page, dict):
                    continue
                text_value = page.get("text") or page.get("html") or page.get("content") or ""
                pages.append({
                    "page_number": page.get("page") or page.get("page_number") or 0,
                    "text": text_value,
                    "width": page.get("width", 0),
                    "height": page.get("height", 0)
                })
            return pages

        # elements ê¸°ë°˜ ì¬êµ¬ì„±
        text_by_page: Dict[int, List[str]] = defaultdict(list)
        for elem in elements:
            page_num = int(elem.get("page") or 0)
            elem_text = elem.get("text")
            if elem_text:
                text_by_page[page_num].append(elem_text)

        for page_num in sorted(text_by_page.keys()):
            combined_text = "\n".join(text_by_page[page_num]).strip()
            pages.append({
                "page_number": page_num,
                "text": combined_text,
                "width": 0,
                "height": 0
            })
        return pages

    def _compose_text_from_pages(self, pages: List[Dict[str, Any]]) -> str:
        segments = [p.get("text", "").strip() for p in pages if p.get("text")]
        return "\n\n".join(segments).strip()

    def _compose_text_from_elements(self, elements: List[Dict[str, Any]]) -> str:
        segments = [elem.get("text", "").strip() for elem in elements if elem.get("text")]
        return "\n".join(segments).strip()

    def _normalize_elements(self, raw_elements: Optional[List[Any]]) -> List[Dict[str, Any]]:
        normalized: List[Dict[str, Any]] = []
        if not isinstance(raw_elements, list):
            return normalized
        for elem in raw_elements:
            if not isinstance(elem, dict):
                continue
            normalized.append({
                "id": elem.get("id"),
                "category": elem.get("category") or elem.get("type"),
                "page": elem.get("page") or elem.get("page_number") or elem.get("pageIndex") or 0,
                "text": self._resolve_content_field(elem, "text"),
                "markdown": self._resolve_content_field(elem, "markdown"),
                "html": self._resolve_content_field(elem, "html"),
                "coordinates": elem.get("coordinates") or elem.get("bbox"),
                "base64_encoding": elem.get("base64_encoding"),
                "confidence": elem.get("confidence")
            })
        return normalized

    def _resolve_content_field(self, elem: Dict[str, Any], field: str) -> str:
        value = elem.get(field)
        if isinstance(value, str):
            return value
        content_obj = elem.get("content")
        if isinstance(content_obj, dict):
            inner_val = content_obj.get(field)
            if isinstance(inner_val, str):
                return inner_val
        return ""

    def _extract_tables(self, content: Dict[str, Any], elements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        tables: List[Dict[str, Any]] = []
        
        # ğŸ¯ ìš°ì„ ìˆœìœ„: elementsì—ì„œ ì¶”ì¶œ (í˜ì´ì§€ ì •ë³´ í¬í•¨)
        table_categories = {"table", "table_continued", "table_header", "table_body"}
        elements_tables = []
        for elem in elements:
            category = (elem.get("category") or "").lower()
            if category in table_categories:
                elements_tables.append({
                    "table_index": len(elements_tables),
                    "page": elem.get("page", 0),
                    "bbox": elem.get("coordinates", []),
                    "html": elem.get("html", ""),
                    "markdown": elem.get("markdown", ""),
                    "text": elem.get("text", ""),
                    "element_id": elem.get("id"),
                    "base64": elem.get("base64_encoding")
                })
        
        # elementsì—ì„œ í…Œì´ë¸”ì„ ì°¾ì•˜ìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if elements_tables:
            return elements_tables
        
        # Fallback: content["tables"] ì‚¬ìš© (í˜ì´ì§€ ì •ë³´ ì—†ì„ ìˆ˜ ìˆìŒ)
        table_entries = content.get("tables") if isinstance(content, dict) else None
        if isinstance(table_entries, list):
            for idx, table in enumerate(table_entries):
                if not isinstance(table, dict):
                    continue
                tables.append({
                    "table_index": idx,
                    "page": table.get("page", 0),
                    "bbox": table.get("bbox", []),
                    "html": table.get("html", ""),
                    "markdown": table.get("markdown", ""),
                    "text": table.get("text")
                })
        return tables

    def _extract_figures(self, content: Dict[str, Any], elements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        figures: List[Dict[str, Any]] = []
        
        # ğŸ¯ ìš°ì„ ìˆœìœ„: elementsì—ì„œ ì¶”ì¶œ (í˜ì´ì§€ ì •ë³´ í¬í•¨)
        figure_categories = {"figure", "chart", "image", "diagram"}
        elements_figures = []
        for elem in elements:
            category = (elem.get("category") or "").lower()
            if category in figure_categories:
                elements_figures.append({
                    "figure_index": len(elements_figures),
                    "page": elem.get("page", 0),
                    "bbox": elem.get("coordinates", []),
                    "caption": elem.get("text", ""),
                    "image": None,
                    "base64": elem.get("base64_encoding"),
                    "element_id": elem.get("id")
                })
        
        # elementsì—ì„œ figureë¥¼ ì°¾ì•˜ìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if elements_figures:
            return elements_figures
        
        # Fallback: content["figures"] ì‚¬ìš© (í˜ì´ì§€ ì •ë³´ ì—†ì„ ìˆ˜ ìˆìŒ)
        figure_entries = content.get("figures") if isinstance(content, dict) else None
        if isinstance(figure_entries, list):
            for idx, figure in enumerate(figure_entries):
                if not isinstance(figure, dict):
                    continue
                figures.append({
                    "figure_index": idx,
                    "page": figure.get("page", 0),
                    "bbox": figure.get("bbox", []),
                    "caption": figure.get("caption", ""),
                    "image": figure.get("image"),
                    "base64": figure.get("base64_encoding")
                })
        return figures
    
    def _parse_response(self, data: Dict[str, Any]) -> UpstageResult:
        """Upstage API ì‘ë‹µì„ ë‚´ë¶€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ PDF, ì¼ë°˜ PDF ëª¨ë‘ ì§€ì›)"""
        
        try:
            logger.info(f"[UPSTAGE] ğŸ“‹ ì „ì²´ ì‘ë‹µ í‚¤: {list(data.keys())}")
            content = data.get("content") or data.get("data") or data.get("result") or {}
            if not isinstance(content, dict):
                logger.warning(f"[UPSTAGE] âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ content íƒ€ì…: {type(content)}")
                content = {}

            logger.info(f"[UPSTAGE] ğŸ“‹ content í‚¤: {list(content.keys())}")

            document_html = content.get("html")
            document_markdown = content.get("markdown")
            document_text = (content.get("text") or "").strip()

            raw_elements = content.get("elements") or data.get("elements")
            normalized_elements = self._normalize_elements(raw_elements)
            logger.info(f"[UPSTAGE] ğŸ“ ìš”ì†Œ ìˆ˜: {len(normalized_elements)}")

            pages = self._build_pages(content, normalized_elements)
            full_text = document_text or self._compose_text_from_pages(pages)
            if not full_text:
                full_text = self._compose_text_from_elements(normalized_elements)

            tables = self._extract_tables(content, normalized_elements)
            figures = self._extract_figures(content, normalized_elements)

            usage = data.get("usage", {})
            # ğŸ¯ í˜ì´ì§€ ìˆ˜: usage['pages'] ìš°ì„ , ì—†ìœ¼ë©´ pages ë¦¬ìŠ¤íŠ¸ ê¸¸ì´
            page_count = usage.get("pages", len(pages)) if usage else len(pages)
            
            metadata = {
                "model": data.get("model", "unknown"),
                "usage": usage,
                "page_count": page_count,
                "table_count": len(tables),
                "figure_count": len(figures),
                "api_version": data.get("api", "unknown"),
                "html": document_html or "",      # ğŸ†• í‚¤ ì´ë¦„ ë‹¨ìˆœí™”
                "markdown": document_markdown or "",  # ğŸ†• í‚¤ ì´ë¦„ ë‹¨ìˆœí™”
                "element_count": len(normalized_elements)
            }

            if len(full_text) < 10 and not pages and not tables and not figures:
                logger.warning("[UPSTAGE] âš ï¸ ì¶”ì¶œëœ ì •ë³´ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                logger.warning(f"[UPSTAGE]    ì‘ë‹µ ìƒ˜í”Œ: {str(data)[:300]}...")

            logger.info("[UPSTAGE] âœ… ì‘ë‹µ íŒŒì‹± ì™„ë£Œ")
            logger.info(f"[UPSTAGE]    ğŸ“Š ìµœì¢… í†µê³„: í˜ì´ì§€={page_count}, í…Œì´ë¸”={len(tables)}, Figure={len(figures)}, í…ìŠ¤íŠ¸={len(full_text)}ì")
            if usage:
                logger.info(f"[UPSTAGE]       - Usage: {usage}")

            return UpstageResult(
                success=True,
                text=full_text.strip(),
                markdown=document_markdown or "",  # ğŸ†• ë§ˆí¬ë‹¤ìš´ ì „ë‹¬
                html=document_html or "",          # ğŸ†• HTML ì „ë‹¬
                pages=pages,
                tables=tables,
                figures=figures,
                elements=normalized_elements,
                metadata=metadata
            )
            
        except Exception as e:
            logger.error(f"[UPSTAGE] âŒ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {type(e).__name__}: {str(e)}", exc_info=True)
            logger.error(f"[UPSTAGE]    ì‘ë‹µ ë°ì´í„° ìƒ˜í”Œ: {str(data)[:500]}...")
            return UpstageResult(
                success=False,
                error=f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}"
            )
    
    async def analyze_pdf(self, file_path: str) -> UpstageResult:
        """
        PDF ë¬¸ì„œ ë¶„ì„ (Azure DI analyze_pdfì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
        
        Azure Document Intelligence Serviceì˜ analyze_pdf ë©”ì„œë“œì™€
        ì™„ì „íˆ ë™ì¼í•œ ì‹œê·¸ë‹ˆì²˜ë¥¼ ì œê³µí•˜ì—¬ text_extractor_service.pyì—ì„œ
        íˆ¬ëª…í•˜ê²Œ êµì²´ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            UpstageResult: ì¶”ì¶œ ê²°ê³¼ (DocumentIntelligenceResultì™€ í˜¸í™˜)
        """
        logger.info(f"[UPSTAGE] ğŸ”„ analyze_pdf í˜¸ì¶œë¨ (Azure DI í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)")
        return await self.parse_document(file_path)
    
    def create_internal_extraction_result(self, upstage_result: UpstageResult) -> Dict[str, Any]:
        """
        Upstage ê²°ê³¼ë¥¼ ë‚´ë¶€ extraction result í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        (text_extractor_service.pyì™€ í˜¸í™˜)
        
        Azure DIì˜ create_internal_extraction_resultì™€ ë™ì¼í•œ í˜•ì‹ ë°˜í™˜
        """
        
        logger.debug(f"[UPSTAGE] ğŸ”§ ë‚´ë¶€ extraction result í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        
        if not upstage_result.success:
            logger.warning(f"[UPSTAGE] âš ï¸ ì‹¤íŒ¨í•œ ê²°ê³¼ë¥¼ ë³€í™˜: {upstage_result.error}")
            return {
                "text": "",
                "metadata": {},
                "success": False,
                "error": upstage_result.error,
                "text_length": 0,
                "extraction_method": "upstage_document_parse"
            }
        
        logger.info(f"[UPSTAGE] âœ… ì„±ê³µí•œ ê²°ê³¼ë¥¼ ë³€í™˜:")
        logger.info(f"[UPSTAGE]    - í˜ì´ì§€: {len(upstage_result.pages)}")
        logger.info(f"[UPSTAGE]    - í…Œì´ë¸”: {len(upstage_result.tables)}")
        logger.info(f"[UPSTAGE]    - Figure: {len(upstage_result.figures)}")
        logger.info(f"[UPSTAGE]    - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(upstage_result.text)}")
        
        return {
            "text": upstage_result.text,
            "metadata": {
                "provider": "upstage",  # ğŸ¯ Provider ì •ë³´ ì¶”ê°€ (multimodal_document_serviceì—ì„œ ì‚¬ìš©)
                "page_count": len(upstage_result.pages),
                "table_count": len(upstage_result.tables),
                "figure_count": len(upstage_result.figures),
                "extraction_method": "upstage_document_parse",
                "upstage_model": upstage_result.metadata.get("model", "unknown"),
                "pages": upstage_result.pages,
                "tables": upstage_result.tables,
                "figures": upstage_result.figures,
                "elements": upstage_result.elements
            },
            "success": True,
            "error": None,
            "text_length": len(upstage_result.text),
            "extraction_method": "upstage_document_parse"
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# ğŸ¯ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ëª¨ë“ˆ import ì‹œ ìë™ ì´ˆê¸°í™”)
logger.info("[UPSTAGE] ğŸ”· upstage_document_service ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì‘")
upstage_document_service = UpstageDocumentService()
logger.info("[UPSTAGE] ğŸ”· upstage_document_service ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")

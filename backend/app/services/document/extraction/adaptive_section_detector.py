"""
ì ì‘í˜• ì„¹ì…˜ ê°ì§€ ì„œë¹„ìŠ¤ (Adaptive Section Detection)
=======================================================

ê¸°ì¡´ íŒ¨í„´ ë§¤ì¹­ ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ 2ë‹¨ê³„ ì ‘ê·¼:
1. êµ¬ì¡° ê°ì§€: ë…¼ë¬¸ì˜ ëª¨ë“  ì„¹ì…˜ í—¤ë”ë¥¼ í˜•ì‹ ê¸°ë°˜ìœ¼ë¡œ ê°ì§€ (ë‚´ìš© ëˆ„ë½ ë°©ì§€)
2. ì˜ë¯¸ ë§¤í•‘: ê°ì§€ëœ í—¤ë”ë¥¼ í‘œì¤€ ì„¹ì…˜ê³¼ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë§¤í•‘

ì¥ì :
- ëª¨ë“  ì„¹ì…˜ í—¤ë” ê°ì§€ â†’ ë‚´ìš© ëˆ„ë½ ì—†ìŒ
- ë¹„í‘œì¤€ í—¤ë”ë„ ì²˜ë¦¬ ê°€ëŠ¥ ("Methodology" â†’ "methods")
- ë¯¸ë¶„ë¥˜ ì„¹ì…˜ë„ ë³´ì¡´ (type="other")
"""
import re
import logging
from typing import List, Dict, Optional, Tuple, Set
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)


class AdaptiveSectionDetector:
    """ì ì‘í˜• ì„¹ì…˜ ê°ì§€ê¸° - ë…¼ë¬¸ êµ¬ì¡°ë¥¼ ë¨¼ì € ê°ì§€í•˜ê³  ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜"""

    # í‘œì¤€ ì„¹ì…˜ íƒ€ì…ê³¼ ê´€ë ¨ í‚¤ì›Œë“œ (ì˜ë¯¸ ë§¤í•‘ìš©)
    STANDARD_SECTIONS = {
        "abstract": [
            "abstract", "summary", "executive summary", "synopsis", "overview"
        ],
        "introduction": [
            "introduction", "background", "overview", "motivation", "preliminaries",
            "preamble", "prologue"
        ],
        "methods": [
            "methods", "methodology", "materials", "experimental", "approach",
            "procedure", "design", "setup", "implementation", "materials and methods",
            "experimental design", "experimental setup", "research methods"
        ],
        "results": [
            "results", "findings", "observations", "outcomes", "data",
            "experimental results", "empirical results"
        ],
        "discussion": [
            "discussion", "analysis", "interpretation", "implications",
            "results and discussion", "discussion and analysis"
        ],
        "conclusion": [
            "conclusion", "conclusions", "summary", "closing", "final remarks",
            "future work", "concluding remarks", "summary and conclusion"
        ],
        "references": [
            "references", "bibliography", "works cited", "literature cited",
            "citations", "bibliographic references"
        ],
        "acknowledgments": [
            "acknowledgments", "acknowledgements", "acknowledgment", "acknowledgement",
            "thanks", "credits"
        ],
        "appendix": [
            "appendix", "appendices", "supplementary", "additional", "supplemental"
        ]
    }

    # ì„¹ì…˜ í—¤ë” ê°ì§€ë¥¼ ìœ„í•œ íŒ¨í„´
    HEADER_PATTERNS = [
        # 1. ë²ˆí˜¸ íŒ¨í„´: "1. Introduction", "1.1 Data Collection"
        re.compile(r"^\s*(\d+(\.\d+)*\.?\s+)([A-Z][^\n]{2,80})$", re.MULTILINE),
        
        # 2. ëŒ€ë¬¸ì ì „ì²´: "ABSTRACT", "INTRODUCTION"
        re.compile(r"^\s*([A-Z][A-Z\s]{2,80})$", re.MULTILINE),
        
        # 3. Title Case: "Research Methodology", "Data Analysis"
        re.compile(r"^\s*([A-Z][a-z]+(\s+[A-Z][a-z]+){0,8})$", re.MULTILINE),
        
        # 4. ë¡œë§ˆ ìˆ«ì: "I. Introduction", "II. Methods"
        re.compile(r"^\s*([IVXLCDM]+\.\s+)([A-Z][^\n]{2,80})$", re.MULTILINE),
    ]

    def __init__(self):
        """ì´ˆê¸°í™”"""
        # í‘œì¤€ ì„¹ì…˜ í‚¤ì›Œë“œë¥¼ ì†Œë¬¸ìë¡œ ì •ê·œí™”
        self.standard_keywords: Dict[str, Set[str]] = {
            section_type: set(kw.lower() for kw in keywords)
            for section_type, keywords in self.STANDARD_SECTIONS.items()
        }
        
        # ì„¹ì…˜ ìˆœì„œ (ë…¼ë¬¸ ì¼ë°˜ì  êµ¬ì¡°)
        self.section_order = [
            "abstract", "introduction", "methods", "results",
            "discussion", "conclusion", "acknowledgments", "references", "appendix"
        ]
        
        logger.info("[ADAPTIVE-SECTION] AdaptiveSectionDetector ì´ˆê¸°í™” ì™„ë£Œ")

    def detect_sections(
        self, full_text: str, pages: Optional[List[Dict]] = None
    ) -> List[Dict]:
        """
        ì ì‘í˜• ì„¹ì…˜ ê°ì§€: Azure DI role ìš°ì„  â†’ íŒ¨í„´ ë§¤ì¹­ í´ë°± â†’ ì˜ë¯¸ ë§¤í•‘
        
        Args:
            full_text: ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸
            pages: í˜ì´ì§€ ì •ë³´ (ì„ íƒ, Azure DIì˜ section_headers í¬í•¨ ê°€ëŠ¥)
        
        Returns:
            ì„¹ì…˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸: [{
                "type": str,              # í‘œì¤€ ì„¹ì…˜ íƒ€ì… ë˜ëŠ” "other"
                "original_title": str,    # ì›ë³¸ í—¤ë” í…ìŠ¤íŠ¸
                "normalized_title": str,  # ì •ê·œí™”ëœ í—¤ë”
                "mapped_type": str,       # ë§¤í•‘ëœ í‘œì¤€ íƒ€ì… (ìˆìœ¼ë©´)
                "confidence": float,      # ë§¤í•‘ ì‹ ë¢°ë„ (0~1)
                "detection_source": str,  # "azure_di_role" | "pattern_match"
                "start_pos": int,
                "end_pos": int,
                "page_start": int,
                "page_end": int,
                "word_count": int,
            }]
        """
        if not full_text or not full_text.strip():
            logger.warning("[ADAPTIVE-SECTION] í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ")
            return []

        # ğŸ¯ 1ë‹¨ê³„: Azure DIì˜ role ê¸°ë°˜ ì„¹ì…˜ í—¤ë” ì¶”ì¶œ (ìš°ì„ )
        if pages:
            logger.debug(f"[ADAPTIVE-SECTION] í˜ì´ì§€ ë°ì´í„° ì œê³µë¨ - {len(pages)}í˜ì´ì§€")
            # ë””ë²„ê¹…: ì²« í˜ì´ì§€ì˜ section_headers í™•ì¸
            if pages:
                first_page = pages[0]
                section_headers_count = len(first_page.get('section_headers', []))
                paragraphs_count = len(first_page.get('paragraphs', []))
                logger.debug(f"[ADAPTIVE-SECTION] ì²« í˜ì´ì§€ - section_headers: {section_headers_count}, paragraphs: {paragraphs_count}")
        
        azure_headers = self._extract_azure_di_headers(pages) if pages else []
        
        if azure_headers:
            logger.info(f"[ADAPTIVE-SECTION] ğŸ¯ Azure DI role ê¸°ë°˜ í—¤ë” {len(azure_headers)}ê°œ ê°ì§€")
            for i, h in enumerate(azure_headers[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ ë¡œê¹…
                logger.debug(f"  {i}. {h['text'][:50]} (page={h.get('page_no')}, pos={h['start_pos']})")
            all_headers = azure_headers
        else:
            logger.info("[ADAPTIVE-SECTION] Azure DI role ì •ë³´ ì—†ìŒ, íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í´ë°±")
            # 2ë‹¨ê³„: íŒ¨í„´ ê¸°ë°˜ í—¤ë” ê°ì§€ (í´ë°±)
            all_headers = self._detect_all_headers(full_text)
        
        if not all_headers:
            logger.warning("[ADAPTIVE-SECTION] í—¤ë”ë¥¼ ì°¾ì§€ ëª»í•¨")
            return []

        logger.info(f"[ADAPTIVE-SECTION] {len(all_headers)}ê°œ í—¤ë” ê°ì§€ë¨")

        # 2ë‹¨ê³„: ê° í—¤ë”ë¥¼ í‘œì¤€ ì„¹ì…˜ìœ¼ë¡œ ë§¤í•‘
        sections = []
        page_boundaries = self._build_page_boundaries(full_text, pages) if pages else []

        for i, header in enumerate(all_headers):
            # ì˜ë¯¸ ë§¤í•‘ (í™•ì • ë§¤í•‘ + ê°€ì¥ ê°€ê¹Œìš´ ì„¹ì…˜)
            mapped_type, confidence, closest_type, closest_score = self._map_to_standard(header["text"])
            
            # ë‹¤ìŒ í—¤ë”ê¹Œì§€ë¥¼ ì„¹ì…˜ ë²”ìœ„ë¡œ ì„¤ì •
            if i + 1 < len(all_headers):
                end_pos = all_headers[i + 1]["start_pos"]
            else:
                end_pos = len(full_text)
            
            # í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
            page_start = self._find_page_number(header["start_pos"], page_boundaries)
            page_end = self._find_page_number(end_pos - 1, page_boundaries) or page_start
            
            # ì„¹ì…˜ í…ìŠ¤íŠ¸ ë° ë‹¨ì–´ ìˆ˜
            section_text = full_text[header["start_pos"]:end_pos]
            word_count = len(section_text.split())
            
            # ê°ì§€ ì†ŒìŠ¤ (Azure DI role vs íŒ¨í„´ ë§¤ì¹­)
            detection_source = header.get("detection_source", "pattern_match")
            
            section_info = {
                "type": mapped_type or "other",
                "original_title": header["text"],
                "normalized_title": self._normalize_header(header["text"]),
                "mapped_type": mapped_type,
                "confidence": confidence,
                "closest_standard_section": closest_type,  # ê°€ì¥ ê°€ê¹Œìš´ í‘œì¤€ ì„¹ì…˜
                "closest_similarity": closest_score,        # ìœ ì‚¬ë„ ì ìˆ˜
                "detection_source": detection_source,  # ğŸ¯ Azure DI role | pattern_match
                "index": i,  # ì„¹ì…˜ ìˆœì„œ ì¸ë±ìŠ¤ (ìˆœì„œ ë³´ì¡´ìš©)
                "start_pos": header["start_pos"],
                "end_pos": end_pos,
                "page_start": page_start or 1,
                "page_end": page_end or page_start or 1,
                "word_count": word_count,
            }
            sections.append(section_info)
            
            # ë¡œê¹…: "other"ì¸ ê²½ìš° ê°€ì¥ ê°€ê¹Œìš´ ì„¹ì…˜ í‘œì‹œ
            source_emoji = "ğŸ¯" if detection_source == "azure_di_role" else "ğŸ”"
            if mapped_type:
                log_msg = f"[ADAPTIVE-SECTION] {source_emoji} '{header['text']}' â†’ {mapped_type} (ì‹ ë¢°ë„: {confidence:.2f})"
            else:
                log_msg = (
                    f"[ADAPTIVE-SECTION] {source_emoji} '{header['text']}' â†’ other "
                    f"(ê°€ì¥ ê°€ê¹Œìš´: {closest_type}, ìœ ì‚¬ë„: {closest_score:.2f})"
                )
            logger.debug(log_msg)

        # 3ë‹¨ê³„: ê°ì§€ëœ ì„¹ì…˜ íƒ€ì… ì§‘ê³„
        detected_types = [s["type"] for s in sections if s["type"] != "other"]
        other_sections = [s for s in sections if s["type"] == "other"]
        
        logger.info(
            f"[ADAPTIVE-SECTION] ë§¤í•‘ ì™„ë£Œ - ì´ {len(sections)}ê°œ ì„¹ì…˜, "
            f"í‘œì¤€ ë§¤í•‘: {len(detected_types)}ê°œ, ê¸°íƒ€: {len(other_sections)}ê°œ"
        )
        logger.info(f"[ADAPTIVE-SECTION] ê°ì§€ëœ í‘œì¤€ ì„¹ì…˜: {', '.join(set(detected_types))}")
        
        # "other" ì„¹ì…˜ì˜ ê°€ì¥ ê°€ê¹Œìš´ í‘œì¤€ ì„¹ì…˜ ë¶„í¬ ë¡œê¹…
        if other_sections:
            closest_distribution = {}
            for s in other_sections:
                closest = s.get("closest_standard_section", "unknown")
                closest_distribution[closest] = closest_distribution.get(closest, 0) + 1
            
            logger.info(
                f"[ADAPTIVE-SECTION] 'other' ì„¹ì…˜ì˜ ê·¼ì ‘ ë¶„í¬: "
                f"{', '.join([f'{k}({v})' for k, v in closest_distribution.items()])}"
            )

        return sections

    def _extract_azure_di_headers(self, pages: List[Dict]) -> List[Dict]:
        """
        Azure DIì˜ role ê¸°ë°˜ ì„¹ì…˜ í—¤ë” ì¶”ì¶œ
        
        Args:
            pages: Azure DIê°€ ë°˜í™˜í•œ í˜ì´ì§€ ì •ë³´ (section_headers í¬í•¨)
        
        Returns:
            [{"text": str, "start_pos": int, "detection_source": "azure_di_role", "page_no": int}, ...]
        """
        headers = []
        current_text_pos = 0
        
        total_section_headers = 0
        for page in pages:
            section_headers = page.get('section_headers', [])
            total_section_headers += len(section_headers)
        
        logger.debug(f"[ADAPTIVE-SECTION][AZURE-DI] ì „ì²´ {len(pages)}í˜ì´ì§€ì—ì„œ {total_section_headers}ê°œ section_headers ë°œê²¬")
        
        for page in pages:
            page_no = page.get('page_no', 1)
            page_text = page.get('text', '')
            
            # Azure DIì˜ section_headers ì¶”ì¶œ
            section_headers = page.get('section_headers', [])
            
            if section_headers:
                logger.debug(f"[ADAPTIVE-SECTION][AZURE-DI] í˜ì´ì§€ {page_no}: {len(section_headers)}ê°œ í—¤ë”")
            
            for header_info in section_headers:
                header_text = header_info.get('content', '').strip()
                if not header_text:
                    continue
                
                role = header_info.get('role', 'unknown')
                logger.debug(f"[ADAPTIVE-SECTION][AZURE-DI] í—¤ë” ë°œê²¬ - page={page_no}, role={role}, text='{header_text[:50]}'")
                
                # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í—¤ë” ìœ„ì¹˜ ì°¾ê¸°
                search_start = current_text_pos
                header_pos = page_text.find(header_text, search_start - current_text_pos if search_start >= current_text_pos else 0)
                
                if header_pos == -1:
                    # í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ëª» ì°¾ìœ¼ë©´ ëŒ€ëµì ì¸ ìœ„ì¹˜ ì‚¬ìš©
                    header_pos = current_text_pos
                else:
                    header_pos = current_text_pos + header_pos
                
                headers.append({
                    'text': header_text,
                    'start_pos': header_pos,
                    'detection_source': 'azure_di_role',
                    'page_no': page_no,
                    'role': header_info.get('role', 'sectionHeading'),
                    'confidence': header_info.get('confidence', 1.0)
                })
                
                logger.debug(
                    f"[AZURE-DI-HEADER] page={page_no}, role={header_info.get('role')}, "
                    f"text='{header_text[:50]}', pos={header_pos}"
                )
            
            # ë‹¤ìŒ í˜ì´ì§€ë¥¼ ìœ„í•œ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
            current_text_pos += len(page_text) + 2  # "\n\n" êµ¬ë¶„ì ê³ ë ¤
        
        # ìœ„ì¹˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        headers.sort(key=lambda x: x['start_pos'])
        
        logger.info(f"[ADAPTIVE-SECTION] Azure DI role ê¸°ë°˜ í—¤ë” {len(headers)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
        return headers

    def _detect_all_headers(self, full_text: str) -> List[Dict]:
        """
        ë…¼ë¬¸ì˜ ëª¨ë“  ì„¹ì…˜ í—¤ë” ê°ì§€ (í˜•ì‹ ê¸°ë°˜)
        
        Returns:
            [{"text": str, "start_pos": int, "pattern_type": str}, ...]
        """
        headers = []
        seen_positions = set()
        
        for pattern_idx, pattern in enumerate(self.HEADER_PATTERNS):
            for match in pattern.finditer(full_text):
                start_pos = match.start()
                
                # ì¤‘ë³µ ìœ„ì¹˜ ì œê±° (ì—¬ëŸ¬ íŒ¨í„´ì— ë§¤ì¹­ë  ìˆ˜ ìˆìŒ)
                if start_pos in seen_positions:
                    continue
                
                # í—¤ë” í…ìŠ¤íŠ¸ ì¶”ì¶œ
                header_text = match.group(0).strip()
                
                # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ í—¤ë” ì œì™¸
                if len(header_text) < 3 or len(header_text) > 100:
                    continue
                
                # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
                if header_text.replace(".", "").replace(" ", "").isdigit():
                    continue

                normalized_text = self._normalize_header(header_text)
                normalized_tokens = [t for t in normalized_text.split() if t]
                alpha_chars = sum(1 for ch in normalized_text if ch.isalpha())

                # ìµœì†Œ ê¸€ì/í† í° ìˆ˜ í•„í„°: í‘œ/ì§€í‘œ ë‹¨ì¼ í† í°(ì˜ˆ: CLB, DMA) ì œê±°
                if alpha_chars < 4:
                    continue
                if not normalized_tokens:
                    continue
                if len(normalized_tokens) == 1 and len(normalized_tokens[0]) < 4:
                    continue
                
                headers.append({
                    "text": header_text,
                    "start_pos": start_pos,
                    "pattern_type": f"pattern_{pattern_idx}",
                    "detection_source": "pattern_match"  # ğŸ” íŒ¨í„´ ë§¤ì¹­ í‘œì‹œ
                })
                seen_positions.add(start_pos)
        
        # ìœ„ì¹˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        headers.sort(key=lambda x: x["start_pos"])
        
        return headers

    def _normalize_header(self, header: str) -> str:
        """
        í—¤ë” í…ìŠ¤íŠ¸ ì •ê·œí™” (ë¹„êµìš©)
        
        - ì†Œë¬¸ì ë³€í™˜
        - ë²ˆí˜¸ ì œê±°
        - íŠ¹ìˆ˜ë¬¸ì ì œê±°
        - ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        """
        # ë²ˆí˜¸ íŒ¨í„´ ì œê±°: "1.", "1.1", "I.", "II." ë“±
        normalized = re.sub(r"^[\divxlcdm]+\.?\s*", "", header, flags=re.IGNORECASE)
        
        # ì†Œë¬¸ì ë³€í™˜
        normalized = normalized.lower()
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ê³µë°± ìœ ì§€)
        normalized = re.sub(r"[^\w\s]", "", normalized)
        
        # ì—°ì† ê³µë°± ì œê±°
        normalized = re.sub(r"\s+", " ", normalized).strip()
        
        return normalized

    def _map_to_standard(self, header: str) -> Tuple[Optional[str], float, Optional[str], float]:
        """
        í—¤ë”ë¥¼ í‘œì¤€ ì„¹ì…˜ìœ¼ë¡œ ë§¤í•‘ (í‚¤ì›Œë“œ ë§¤ì¹­ + ìœ ì‚¬ë„)
        
        Returns:
            (mapped_type, confidence, closest_type, closest_score)
            - mapped_type: í‘œì¤€ ì„¹ì…˜ íƒ€ì… (ì‹ ë¢°ë„ 0.6 ì´ìƒ) ë˜ëŠ” None
            - confidence: ë§¤í•‘ ì‹ ë¢°ë„ (0~1)
            - closest_type: ê°€ì¥ ê°€ê¹Œìš´ í‘œì¤€ ì„¹ì…˜ (ì‹ ë¢°ë„ ë¬´ê´€)
            - closest_score: ê°€ì¥ ê°€ê¹Œìš´ ì„¹ì…˜ì˜ ìœ ì‚¬ë„ (0~1)
        """
        normalized = self._normalize_header(header)
        
        best_match_type = None
        best_score = 0.0
        closest_type = None
        closest_score = 0.0
        
        def _length_ratio(a: str, b: str) -> float:
            shorter = min(len(a), len(b))
            longer = max(len(a), len(b)) or 1
            return shorter / longer

        # 1ì°¨: í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­
        for section_type, keywords in self.standard_keywords.items():
            if normalized in keywords:
                return section_type, 1.0, section_type, 1.0  # ì™„ë²½í•œ ë§¤ì¹­
            
            # ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œê°€ í—¤ë”ì— í¬í•¨)
            for keyword in keywords:
                keyword_norm = keyword.lower()
                if keyword_norm in normalized and len(keyword_norm) >= 4:
                    score = _length_ratio(normalized, keyword_norm)
                    if score > best_score:
                        best_score = score
                        best_match_type = section_type
                    if score > closest_score:
                        closest_score = score
                        closest_type = section_type
                elif normalized in keyword_norm and len(normalized) >= 4:
                    score = _length_ratio(normalized, keyword_norm)
                    if score > best_score:
                        best_score = score
                        best_match_type = section_type
                    if score > closest_score:
                        closest_score = score
                        closest_type = section_type
        
        # 2ì°¨: ë¬¸ìì—´ ìœ ì‚¬ë„ (Fuzzy Matching)
        if best_score < 0.8:  # ì •í™• ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ìœ ì‚¬ë„ ê²€ì‚¬
            for section_type, keywords in self.standard_keywords.items():
                for keyword in keywords:
                    similarity = SequenceMatcher(None, normalized, keyword).ratio()
                    if similarity > best_score:
                        best_score = similarity
                        best_match_type = section_type
                    if similarity > closest_score:
                        closest_score = similarity
                        closest_type = section_type
        
        # ë§¤í•‘ ê²°ì •: ì‹ ë¢°ë„ 0.6 ì´ìƒë§Œ í™•ì • ë§¤í•‘
        if best_score >= 0.6:
            return best_match_type, best_score, best_match_type, best_score
        
        # ì‹ ë¢°ë„ ë¯¸ë‹¬ì´ì§€ë§Œ ê°€ì¥ ê°€ê¹Œìš´ ì„¹ì…˜ ì •ë³´ëŠ” ë°˜í™˜
        return None, 0.0, closest_type, closest_score

    def _build_page_boundaries(
        self, full_text: str, pages: List[Dict]
    ) -> List[Tuple[int, int, int]]:
        """
        í˜ì´ì§€ ê²½ê³„ ê³„ì‚° (í˜ì´ì§€ ë²ˆí˜¸ ë§¤í•‘ìš©)
        
        Returns:
            [(start_pos, end_pos, page_no), ...]
        """
        boundaries = []
        current_pos = 0

        for page in pages:
            page_no = page.get("page_no", 1)
            page_text = page.get("text", "")
            
            # í˜ì´ì§€ ë§ˆì»¤ê°€ ìˆëŠ”ì§€ í™•ì¸
            page_marker = f"\n[í˜ì´ì§€ {page_no}]\n"
            marker_pos = full_text.find(page_marker, current_pos)
            
            if marker_pos >= 0:
                start_pos = marker_pos
                end_pos = start_pos + len(page_marker) + len(page_text)
            else:
                # ë§ˆì»¤ ì—†ìœ¼ë©´ ìˆœì°¨ì ìœ¼ë¡œ ë°°ì¹˜
                end_pos = current_pos + len(page_text)
                start_pos = current_pos
            
            boundaries.append((start_pos, end_pos, page_no))
            current_pos = end_pos

        return boundaries

    def _find_page_number(
        self, pos: int, page_boundaries: List[Tuple[int, int, int]]
    ) -> Optional[int]:
        """ì£¼ì–´ì§„ ìœ„ì¹˜ì˜ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°"""
        for start, end, page_no in page_boundaries:
            if start <= pos < end:
                return page_no
        return None

    def get_section_summary(self, sections: List[Dict]) -> Dict:
        """
        ì„¹ì…˜ ê°ì§€ ê²°ê³¼ ìš”ì•½ í†µê³„
        
        Returns:
            {
                "total_sections": int,
                "mapped_sections": int,
                "unmapped_sections": int,
                "sections_found": List[str],
                "other_sections": List[str],
                "abstract_words": int,
                "references_start_page": int,
                "azure_di_role_used": bool,  # ğŸ¯ Azure DI role ì‚¬ìš© ì—¬ë¶€
                "azure_di_sections": int,    # ğŸ¯ Azure DIë¡œ ê°ì§€ëœ ì„¹ì…˜ ìˆ˜
            }
        """
        if not sections:
            return {
                "total_sections": 0,
                "mapped_sections": 0,
                "unmapped_sections": 0,
                "sections_found": [],
                "other_sections": [],
                "abstract_words": 0,
                "references_start_page": None,
                "azure_di_role_used": False,
                "azure_di_sections": 0,
            }

        mapped_sections = [s for s in sections if s["type"] != "other"]
        unmapped_sections = [s for s in sections if s["type"] == "other"]
        
        sections_found = [s["type"] for s in mapped_sections]
        other_sections = [s["original_title"] for s in unmapped_sections]
        
        # ğŸ¯ Azure DI role ì‚¬ìš© ì—¬ë¶€ ì§‘ê³„
        azure_di_sections = [s for s in sections if s.get("detection_source") == "azure_di_role"]
        azure_di_role_used = len(azure_di_sections) > 0
        
        # "other" ì„¹ì…˜ì˜ ê°€ì¥ ê°€ê¹Œìš´ í‘œì¤€ ì„¹ì…˜ ì§‘ê³„
        other_sections_with_proximity = []
        for s in unmapped_sections:
            other_sections_with_proximity.append({
                "title": s["original_title"],
                "closest_section": s.get("closest_standard_section"),
                "similarity": s.get("closest_similarity", 0.0),
                "index": s.get("index", 0),
                "detection_source": s.get("detection_source", "pattern_match")
            })
        
        abstract_words = next(
            (s["word_count"] for s in sections if s["type"] == "abstract"), 0
        )
        references_start_page = next(
            (s["page_start"] for s in sections if s["type"] == "references"), None
        )

        return {
            "total_sections": len(sections),
            "mapped_sections": len(mapped_sections),
            "unmapped_sections": len(unmapped_sections),
            "sections_found": sections_found,
            "other_sections": other_sections,
            "other_sections_proximity": other_sections_with_proximity,  # ê·¼ì ‘ ì •ë³´ ì¶”ê°€
            "abstract_words": abstract_words,
            "references_start_page": references_start_page,
            "azure_di_role_used": azure_di_role_used,  # ğŸ¯ Azure DI role ì‚¬ìš©
            "azure_di_sections": len(azure_di_sections),  # ğŸ¯ Azure DI ì„¹ì…˜ ìˆ˜
        }

"""
Azure Document Intelligence (DI) ì„œë¹„ìŠ¤

SDK 4.x ë§ˆì´ê·¸ë ˆì´ì…˜ (2025-03-26 GA):
 - azure-ai-documentintelligence (4.x) ì‚¬ìš©
 - DocumentIntelligenceClientë¡œ ë³€ê²½ (ê¸°ì¡´ DocumentAnalysisClient ëŒ€ì²´)
 - DocumentAnalysisFeature â†’ DocumentContentElement ë¡œ ë³€ê²½
 - FIGURES ê¸°ëŠ¥ ì§€ì› (Layout v4.0+)
 - prebuilt-document (ë˜ëŠ” ì„¤ì •ëœ ê¸°ë³¸ ëª¨ë¸) 1ì°¨ í˜¸ì¶œë¡œ í˜ì´ì§€/ë¼ì¸ ì¶”ì¶œ
 - í•„ìš” ì‹œ prebuilt-layout ì¬í˜¸ì¶œí•˜ì—¬ í…Œì´ë¸” êµ¬ì¡° ë° Figure ì¶”ì¶œ (2-pass)
 - ì»¬ëŸ¼(ë‹¤ë‹¨) ë¬¸ì„œ ì •ë ¬: ì¢Œí‘œ ê¸°ë°˜ ê²½ëŸ‰ 1D k-means íœ´ë¦¬ìŠ¤í‹± ì ìš©
 - í…Œì´ë¸” ì…€ ê·¸ë¦¬ë“œ ì¬êµ¬ì„± ë° í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì •
 - ì„œë¹„ìŠ¤ ê²°ê³¼ë¥¼ ê¸°ì¡´ í†µí•© íŒŒì´í”„ë¼ì¸ì—ì„œ ì†Œë¹„ ê°€ëŠ¥í•œ êµ¬ì¡°(DocumentIntelligenceResult)ë¡œ ìœ ì§€
 - 404/ëª¨ë¸ ë¯¸ì§€ì›/ì¼ì‹œ ì˜¤ë¥˜ ì¬ì‹œë„ ë¡œì§ ë‹¨ìˆœí™”

ì£¼ì˜:
 - settings.azure_document_intelligence_default_model ì´ prebuilt-read ì¸ ê²½ìš°
     ê°€ê¸‰ì  prebuilt-document ë¡œ ìë™ í´ë°± ì‹œë„ í›„ ì‹¤íŒ¨ ì‹œ ì…ë ¥ê°’ ì‚¬ìš©
 - API ë²„ì „ 2024-11-30 ì´ìƒ ê¶Œì¥ (FIGURES ì§€ì›)
 - analyze_pdf() ì™¸ë¶€ ì‹œê·¸ë‹ˆì²˜ëŠ” ìœ ì§€ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)
"""

import asyncio
import logging
import time
from typing import Dict, List, Optional, Any, Tuple

from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError, ClientAuthenticationError
from azure.core.polling import LROPoller

from app.core.config import settings

logger = logging.getLogger(__name__)

# Azure Document Intelligence 4.x SDK (GA)
try:
    from azure.ai.documentintelligence import DocumentIntelligenceClient  # type: ignore
    from azure.ai.documentintelligence.models import DocumentAnalysisFeature, AnalyzeOutputOption  # type: ignore
    SDK_VERSION = "4.x"
    logger.info("[AZURE-DI] azure-ai-documentintelligence 4.x SDK ë¡œë“œ ì„±ê³µ")
except Exception as e4x:
    logger.warning(f"[AZURE-DI] azure-ai-documentintelligence 4.x ë¡œë“œ ì‹¤íŒ¨: {e4x}")
    # Fallback to 3.3.x (FormRecognizer)
    try:
        from azure.ai.formrecognizer import DocumentAnalysisClient as DocumentIntelligenceClient  # type: ignore
        DocumentAnalysisFeature = None  # type: ignore
        AnalyzeOutputOption = None  # type: ignore
        SDK_VERSION = "3.3.x"
        logger.warning("[AZURE-DI] Fallback to azure-ai-formrecognizer 3.3.x (FIGURES ë¯¸ì§€ì›)")
    except Exception as e3x:
        logger.error(f"[AZURE-DI] ëª¨ë“  SDK ë¡œë“œ ì‹¤íŒ¨: {e3x}")
        DocumentIntelligenceClient = None  # type: ignore
        DocumentAnalysisFeature = None  # type: ignore
        AnalyzeOutputOption = None  # type: ignore
        SDK_VERSION = "none"


class DocumentIntelligenceResult:
    """Document Intelligence ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        success: bool = True,
        text: str = "",
        pages: Optional[List[Dict[str, Any]]] = None,
        tables: Optional[List[Dict[str, Any]]] = None,
        figures: Optional[List[Dict[str, Any]]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None,
        extraction_method: str = "azure_document_intelligence"
    ):
        self.success = success
        self.text = text
        self.pages = pages or []
        self.tables = tables or []
        self.figures = figures or []
        self.metadata = metadata or {}
        self.error = error
        self.extraction_method = extraction_method


class AzureDocumentIntelligenceService:
    """Azure Document Intelligence API í´ë¼ì´ì–¸íŠ¸ ë˜í¼"""
    
    def __init__(self):
        # Azure SDK HTTP ë¡œê¹… ìµœì†Œí™” (Document Intelligence í˜¸ì¶œ ì‹œ request/response ë¡œê·¸ ì–µì œ)
        logging.getLogger("azure.core.pipeline.policies.http_logging_policy").setLevel(logging.WARNING)

        self.endpoint = settings.azure_document_intelligence_endpoint
        self.api_key = settings.azure_document_intelligence_api_key
        self.api_version = settings.azure_document_intelligence_api_version
        self.default_model = settings.azure_document_intelligence_default_model
        self.layout_model = settings.azure_document_intelligence_layout_model
        self.max_pages = settings.azure_document_intelligence_max_pages
        self.timeout_seconds = settings.azure_document_intelligence_timeout_seconds
        self.retry_max_attempts = settings.azure_document_intelligence_retry_max_attempts
        self.confidence_threshold = settings.azure_document_intelligence_confidence_threshold

        self.enabled_features = self._resolve_enabled_features()
        self.enabled_outputs = self._resolve_enabled_outputs()

        # ë‚´ë¶€ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        # NOTE: ì¼ë¶€ í™˜ê²½ì—ì„œ í´ë˜ìŠ¤ ë‚´ë¶€ PEP526 ì£¼ì„ ì²˜ë¦¬ ë¬¸ì œê°€ ë˜ì–´ ë‹¨ìˆœ í• ë‹¹ ì‚¬ìš©
        self._client = None  # type: ignore
        self._init_client()
    
    def _init_client(self):
        """Document Intelligence 4.x í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if not self.endpoint:
            logger.warning("Azure Document Intelligence endpoint ë¯¸ì„¤ì •")
            return
        if DocumentIntelligenceClient is None:
            logger.error("azure-ai-documentintelligence íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        try:
            if self.api_key:
                credential = AzureKeyCredential(self.api_key)
            else:
                raise RuntimeError("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env í™•ì¸")
            
            # 4.x SDKëŠ” api_versionì„ ì´ˆê¸°í™” ì‹œ ì „ë‹¬í•  ìˆ˜ ì—†ìŒ (ìš”ì²­ ì‹œ ì„¤ì •)
            self._client = DocumentIntelligenceClient(
                endpoint=self.endpoint.rstrip('/'), 
                credential=credential
            )
            logger.info(f"[AZURE-DI] Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: SDK={SDK_VERSION}, endpoint={self.endpoint}")
        except Exception as e:
            logger.error(f"í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._client = None

    def _resolve_enabled_features(self) -> List[Any]:
        """DI 4.xì—ì„œ ê¸°ë³¸ í™œì„±í™”í•  ë¶„ì„ feature ëª©ë¡ êµ¬ì„±"""
        if DocumentAnalysisFeature is None:
            return []

        features: List[Any] = []
        for feature_name in ("STYLE_FONT", "LANGUAGES", "FORMULAS"):
            feature_value = getattr(DocumentAnalysisFeature, feature_name, None)
            if feature_value:
                features.append(feature_value)

        if features:
            logger.info("[AZURE-DI] í™œì„±í™”ëœ features=%s", [getattr(f, "value", str(f)) for f in features])
        return features

    def _resolve_enabled_outputs(self) -> List[Any]:
        """DI 4.x analyze output ì˜µì…˜ êµ¬ì„± (FIGURES ë“±)"""
        if 'AnalyzeOutputOption' not in globals() or AnalyzeOutputOption is None:
            return []

        outputs: List[Any] = []
        option_figures = getattr(AnalyzeOutputOption, "FIGURES", None)
        if option_figures:
            outputs.append(option_figures)

        if outputs:
            logger.info("[AZURE-DI] í™œì„±í™”ëœ output ì˜µì…˜=%s", [getattr(o, "value", str(o)) for o in outputs])
        return outputs
    
    def is_available(self) -> bool:
        """Document Intelligence ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return bool(self._client and settings.use_azure_document_intelligence_pdf and self.endpoint)
    
    async def analyze_pdf(
        self,
        file_path: str,
        model: Optional[str] = None,
        pages: Optional[List[int]] = None,
    ) -> DocumentIntelligenceResult:
        """
        PDF íŒŒì¼ì„ Azure Document Intelligenceë¡œ ë¶„ì„
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: prebuilt-layout)
            pages: ë¶„ì„í•  í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            DocumentIntelligenceResult: ë¶„ì„ ê²°ê³¼
        """
        if not self.is_available():
            return DocumentIntelligenceResult(
                success=False,
                error="Azure Document Intelligence ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                extraction_method="azure_document_intelligence_unavailable"
            )
        
        # í˜ì´ì§€ ìˆ˜ ì œí•œ ê²€ì‚¬
        if await self._check_page_limit(file_path):
            return DocumentIntelligenceResult(
                success=False,
                error=f"PDF í˜ì´ì§€ ìˆ˜ê°€ ì œí•œ({self.max_pages})ì„ ì´ˆê³¼í•©ë‹ˆë‹¤.",
                extraction_method="azure_document_intelligence_page_limit_exceeded"
            )
        
        # SDK 4.x ëª¨ë¸ ì„ íƒ: í•™ìˆ ë…¼ë¬¸ ì²˜ë¦¬ëŠ” prebuilt-layout 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„
        # prebuilt-layout: í…ìŠ¤íŠ¸ + ë¬¸ë‹¨ + í‘œ + ê·¸ë¦¼ + ì„¹ì…˜ í—¤ë” ëª¨ë‘ í¬í•¨
        # ì£¼ì˜: SDK 4.xì—ì„œëŠ” prebuilt-document ëª¨ë¸ì´ ì œê±°ë˜ì—ˆìŒ
        configured_default = self.default_model or "prebuilt-layout"
        
        # SDK 4.xì—ì„œëŠ” prebuilt-documentê°€ ì—†ìœ¼ë¯€ë¡œ prebuilt-layoutìœ¼ë¡œ ë³€ê²½
        if configured_default == "prebuilt-document":
            primary_model = "prebuilt-layout"
            logger.info("[AZURE-DI] SDK 4.x: prebuilt-document â†’ prebuilt-layout ìë™ ë³€í™˜")
        elif configured_default == "prebuilt-read":
            # prebuilt-readëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ë¯€ë¡œ layoutìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
            primary_model = "prebuilt-layout"
            logger.info("[AZURE-DI] SDK 4.x: prebuilt-read â†’ prebuilt-layout ì—…ê·¸ë ˆì´ë“œ (í‘œ/ê·¸ë¦¼ í¬í•¨)")
        else:
            primary_model = configured_default
            
        if model:  # í˜¸ì¶œìê°€ ëª…ì‹œí•œ ê²½ìš°
            if model == "prebuilt-document":
                primary_model = "prebuilt-layout"
                logger.info("[AZURE-DI] SDK 4.x: prebuilt-document â†’ prebuilt-layout ìë™ ë³€í™˜ (caller override)")
            elif model == "prebuilt-read":
                primary_model = "prebuilt-layout"
                logger.info("[AZURE-DI] SDK 4.x: prebuilt-read â†’ prebuilt-layout ì—…ê·¸ë ˆì´ë“œ (caller override)")
            else:
                primary_model = model

        # SDK 4.xì—ì„œëŠ” layout í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ë©´ ë˜ë¯€ë¡œ layout_modelì€ ì‚¬ìš© ì•ˆ í•¨
        layout_model = None  # prebuilt-layout 1íšŒ í˜¸ì¶œë¡œ ì¶©ë¶„
        start_time = time.time()
        perf_start_total = time.perf_counter()

        logger.info(f"Azure DI ë¶„ì„ ì‹œì‘: {file_path}, ëª¨ë¸(read)={primary_model} layout={layout_model}")

        # pdfplumber í•œ ë²ˆë§Œ ì—´ê¸° (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€ - ì„±ëŠ¥ ìµœì í™”)
        pdf_doc = None
        try:
            import pdfplumber  # type: ignore
            pdf_doc = pdfplumber.open(file_path)
            logger.debug(f"[PERF] pdfplumber.open() ì™„ë£Œ: {len(pdf_doc.pages)} í˜ì´ì§€")
        except Exception as e:
            logger.warning(f"[PERF] pdfplumber.open() ì‹¤íŒ¨: {e} - fallback ê¸°ëŠ¥ ì¼ë¶€ ì œí•œ")

        try:
            # íŒŒì¼ ì½ê¸°
            with open(file_path, 'rb') as file:
                file_content = file.read()

            # ë³‘ë ¬ ì²˜ë¦¬ ë° ìºì‹± ì„¤ì •ê°’ (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì•ˆì „í•œ ê¸°ë³¸ê°’)
            di_parallel_enabled: bool = bool(getattr(settings, "di_parallel_enabled", False))
            di_page_group_size: int = int(getattr(settings, "di_page_group_size", 3) or 3)
            di_max_concurrency: int = int(getattr(settings, "di_max_concurrency", 3) or 3)
            di_cache_enabled: bool = bool(getattr(settings, "di_cache_enabled", False))
            two_col_reorder_enabled: bool = bool(getattr(settings, "di_two_column_reorder_enabled", True))

            # íŒŒì¼ í•´ì‹œ ê¸°ë°˜ ê°„ë‹¨ ìºì‹œ í‚¤ (ë¡œì»¬ ì„ì‹œ ë””ìŠ¤í¬ ìºì‹œ)
            import hashlib, os, json as _json
            file_hash = hashlib.sha1(file_content).hexdigest()  # nosec - ìºì‹œ í‚¤ ìš©ë„
            cache_root = f"/tmp/di_cache/{file_hash}"
            if di_cache_enabled:
                os.makedirs(cache_root, exist_ok=True)

            # ë³‘ë ¬ í˜ì´ì§€ ê·¸ë£¹ ì²˜ë¦¬ ê²½ë¡œ (pages ì¸ìê°€ Noneì¼ ë•Œë§Œ ì „ì²´ ë¬¸ì„œ ëŒ€ìƒìœ¼ë¡œ ì ìš©)
            total_pages = 0
            if pages is None and di_parallel_enabled:
                # ì´ í˜ì´ì§€ ìˆ˜ íŒŒì•… (ì´ë¯¸ ì—´ë¦° pdf_doc ì¬ì‚¬ìš©)
                total_pages = len(pdf_doc.pages) if pdf_doc else 0
                if total_pages <= 0:
                    logger.warning("PDF í˜ì´ì§€ ìˆ˜ í™•ì¸ ì‹¤íŒ¨, ë‹¨ì¼ í˜¸ì¶œë¡œ í´ë°±")
                    di_parallel_enabled = False

            if pages is None and di_parallel_enabled and (total_pages or 0) > di_page_group_size:
                logger.info(
                    f"[AZURE-DI] ë³‘ë ¬ í˜ì´ì§€ ê·¸ë£¹ ì²˜ë¦¬ ì‹œì‘: total_pages={total_pages}, group_size={di_page_group_size}, max_concurrency={di_max_concurrency}"
                )

                # ê·¸ë£¹ ë¶„í•  (1-indexed í˜ì´ì§€)
                page_numbers = list(range(1, total_pages + 1))
                groups: List[List[int]] = [
                    page_numbers[i : i + di_page_group_size] for i in range(0, len(page_numbers), di_page_group_size)
                ]

                # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œí•œ
                import asyncio as _asyncio
                semaphore = _asyncio.Semaphore(di_max_concurrency)

                async def analyze_group(g: List[int]):
                    async with semaphore:
                        # ìºì‹œ í™•ì¸
                        cache_path = os.path.join(cache_root, f"group_{g[0]}_{g[-1]}.json") if di_cache_enabled else None
                        if di_cache_enabled and cache_path and os.path.exists(cache_path):
                            try:
                                with open(cache_path, "r", encoding="utf-8") as cf:
                                    cached = _json.load(cf)
                                return self._result_from_serialized(cached)
                            except Exception:
                                pass
                        # í˜¸ì¶œ
                        rr = await self._analyze_with_retry(file_content, primary_model, pages=g)
                        if rr and rr.success:
                            # ë ˆì´ì•„ì›ƒ(í‘œ/ê·¸ë¦¼) ì¶”ê°€ íŒ¨ìŠ¤
                            lr = await self._analyze_layout_with_retry(file_content, layout_model, pages=g)
                            if lr and lr.success:
                                rr.tables = lr.tables or rr.tables
                                rr.figures = lr.figures or []
                                rr.metadata.update({
                                    "layout_model_used": layout_model,
                                    "table_count": len(rr.tables or []),
                                    "figure_count": len(rr.figures or []),
                                })
                                self._merge_figures_into_pages(rr)
                            # ìºì‹œ ì €ì¥
                            if di_cache_enabled and cache_path:
                                try:
                                    with open(cache_path, "w", encoding="utf-8") as cf:
                                        _json.dump(self._serialize_result(rr), cf, ensure_ascii=False)
                                except Exception:
                                    pass
                        return rr

                tasks = [analyze_group(g) for g in groups]
                group_results: List[DocumentIntelligenceResult] = list(
                    await _asyncio.gather(*tasks, return_exceptions=False)
                )

                # ë³‘í•©
                read_result = self._merge_group_results(group_results)
                if not read_result.success:
                    return read_result

                # íƒ€ì´ë° ê¸°ë¡
                group_read_secs = sum((gr.metadata or {}).get("timing", {}).get("read_seconds", 0) for gr in group_results if gr)
                group_layout_secs = sum((gr.metadata or {}).get("timing", {}).get("layout_seconds", 0) for gr in group_results if gr)
                read_result.metadata.setdefault("timing", {})
                read_result.metadata["timing"]["read_seconds"] = group_read_secs
                if group_layout_secs:
                    read_result.metadata["timing"]["layout_seconds"] = group_layout_secs
            else:
                # ë‹¨ì¼ í˜¸ì¶œ ê²½ë¡œ: SDK 4.x prebuilt-layoutì€ 1íšŒ í˜¸ì¶œë¡œ ëª¨ë“  ê²ƒ ì¶”ì¶œ
                perf_start = time.perf_counter()
                read_result = await self._analyze_with_retry(file_content, primary_model, pages=pages)
                elapsed = time.perf_counter() - perf_start
                logger.info(f"[AZURE-DI][TIMER] '{primary_model}' completed in {elapsed:.2f}s")
                
                if not read_result.success:
                    return read_result
                    
                if read_result.metadata is None:
                    read_result.metadata = {}
                read_result.metadata.setdefault("timing", {})
                read_result.metadata["timing"]["analysis_seconds"] = elapsed
                
                # SDK 4.x prebuilt-layoutì€ í…ìŠ¤íŠ¸+í‘œ+ê·¸ë¦¼ì„ í•œ ë²ˆì— ë°˜í™˜
                logger.info(f"[AZURE-DI][RESULT] SDK 4.x {primary_model} - "
                           f"pages: {len(read_result.pages)}, "
                           f"tables: {len(read_result.tables)}, "
                           f"figures: {len(read_result.figures)}")

            # pdfplumber fallback for figures (SDK 4.xì—ì„œë„ figuresê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
            if not read_result.figures:
                logger.info("[FIGURE-FALLBACK] Azure DI figures ì—†ìŒ â†’ pdfplumber fallback ì‹œë„")
                # ì´ë¯¸ ì—´ë¦° pdf_doc ì¬ì‚¬ìš© (ì¤‘ë³µ open ë°©ì§€)
                fallback_figures = self._extract_figures_with_pdfplumber_doc(pdf_doc) if pdf_doc else []
                if fallback_figures:
                    logger.info(f"[FIGURE-FALLBACK] âœ… {len(fallback_figures)}ê°œ figureë¥¼ pdfplumberë¡œ ì¶”ì¶œ")
                    read_result.figures = fallback_figures
                    read_result.metadata.update({
                        "figure_count": len(read_result.figures),
                        "figure_extraction_fallback": "pdfplumber_images"
                    })
                    self._merge_figures_into_pages(read_result)
                else:
                    logger.warning("[FIGURE-FALLBACK] âŒ pdfplumber fallbackë„ figure ì¶”ì¶œ ì‹¤íŒ¨")
            else:
                logger.info(f"[AZURE-DI][FIGURES] âœ… Azure DIë¡œ {len(read_result.figures)}ê°œ figure ì¶”ì¶œ ì™„ë£Œ")

            result = read_result

            # 2ì—´(dual-column) ë ˆì´ì•„ì›ƒ ì¬êµ¬ì„±: í˜ì´ì§€ í…ìŠ¤íŠ¸ ìˆœì„œ ë³´ì • (ì˜µì…˜)
            if two_col_reorder_enabled and pdf_doc:
                try:
                    # ì´ë¯¸ ì—´ë¦° pdf_doc ì¬ì‚¬ìš© (ì¤‘ë³µ open ë°©ì§€)
                    self._reorder_two_column_pages_doc(pdf_doc, result)
                except Exception as _e2:  # pragma: no cover - ì‹¤íŒ¨í•´ë„ ì¹˜ëª…ì  ì•„ë‹˜
                    logger.debug(f"dual-column ì¬êµ¬ì„± ì‹¤íŒ¨(ë¬´ì‹œ): {_e2}")

            # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
            processing_time = time.time() - start_time
            result.metadata.update({
                'di_processing_time_seconds': round(processing_time, 2),
                'di_model_used': primary_model,
                'di_api_version': self.api_version,
                'di_endpoint': self.endpoint,
                'di_layout_model_used': layout_model
            })
            total_elapsed = time.perf_counter() - perf_start_total
            result.metadata.setdefault("timing", {})
            result.metadata["timing"]["total_seconds"] = total_elapsed
            logger.info(f"[AZURE-DI][TIMER] total analyze_pdf completed in {total_elapsed:.2f}s")

            logger.info(f"Azure DI ë¶„ì„ ì™„ë£Œ: {processing_time:.2f}ì´ˆ, ì„±ê³µ: {result.success}")
            return result
            
        except Exception as e:
            logger.error(f"Azure DI ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return DocumentIntelligenceResult(
                success=False,
                error=f"Azure Document Intelligence ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                extraction_method="azure_document_intelligence_error"
            )
        finally:
            # pdfplumber ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ì¤‘ë³µ open ë°©ì§€ë¥¼ ìœ„í•´ í•œ ë²ˆë§Œ ì—´ê³  ì‚¬ìš© í›„ ë‹«ê¸°)
            if pdf_doc:
                try:
                    pdf_doc.close()
                    logger.debug("[PERF] pdfplumber.close() ì™„ë£Œ")
                except Exception:
                    pass
    
    async def _analyze_with_retry(self, file_content: bytes, model: str, pages: Optional[List[int]] = None) -> DocumentIntelligenceResult:
        """í…ìŠ¤íŠ¸/ë ˆì´ì•„ì›ƒ(ë¼ì¸ ì¤‘ì‹¬) ë¶„ì„ ì¬ì‹œë„ (SDK 4.x í˜¸í™˜)"""
        last_error: Optional[str] = None
        for attempt in range(1, self.retry_max_attempts + 1):
            try:
                if not self._client:
                    raise RuntimeError("í´ë¼ì´ì–¸íŠ¸ ë¯¸ì´ˆê¸°í™”")
                logger.info(f"[read pass] ëª¨ë¸={model} SDK={SDK_VERSION} ì‹œë„ {attempt}/{self.retry_max_attempts}")
                
                if SDK_VERSION == "4.x":
                    # 4.x SDK: begin_analyze_document(model_id, body)
                    from io import BytesIO
                    body_params = {}
                    if pages:
                        body_params["pages"] = self._pages_to_range(pages)
                    if self.enabled_features:
                        body_params["features"] = list(self.enabled_features)
                    if self.enabled_outputs:
                        body_params["output"] = list(self.enabled_outputs)
                    
                    poller = self._client.begin_analyze_document(
                        model_id=model,
                        body=BytesIO(file_content),
                        **body_params
                    )
                else:
                    # 3.3.x Fallback
                    kwargs: Dict[str, Any] = {"logging_enable": False}
                    if pages:
                        kwargs["pages"] = self._pages_to_range(pages)
                    poller = self._client.begin_analyze_document(
                        model,
                        document=file_content,
                        **kwargs,
                    )
                
                analyze_result = await asyncio.wait_for(self._poll_result(poller), timeout=self.timeout_seconds)
                return self._convert_read_result(analyze_result, model)
            except ClientAuthenticationError as e:
                return DocumentIntelligenceResult(success=False, error=f"ì¸ì¦ ì‹¤íŒ¨: {e}", extraction_method="azure_document_intelligence_auth_error")
            except asyncio.TimeoutError:
                last_error = f"timeout {self.timeout_seconds}s"
                logger.warning(last_error)
            except HttpResponseError as e:
                if e.status_code in [429, 502, 503, 504]:
                    last_error = f"HTTP {e.status_code}: {e.message}"
                    if attempt < self.retry_max_attempts:
                        await asyncio.sleep(min(2 ** attempt, 30))
                else:
                    return DocumentIntelligenceResult(success=False, error=f"HTTP {e.status_code}: {e.message}", extraction_method="azure_document_intelligence_http_error")
            except Exception as e:  # pragma: no cover
                last_error = str(e)
                logger.warning(f"[read pass] ì˜ˆì™¸ ë°œìƒ: {e}")
                if attempt < self.retry_max_attempts:
                    await asyncio.sleep(min(2 ** attempt, 30))
        return DocumentIntelligenceResult(success=False, error=f"read pass ì‹¤íŒ¨: {last_error}", extraction_method="azure_document_intelligence_retry_exhausted")

    async def _analyze_layout_with_retry(self, file_content: bytes, layout_model: str, pages: Optional[List[int]] = None) -> Optional[DocumentIntelligenceResult]:
        """í‘œ/ê·¸ë¦¼ ì¶”ì¶œ ì¬ì‹œë„ (SDK 4.x FIGURES ì§€ì›)"""
        last_error: Optional[str] = None
        for attempt in range(1, self.retry_max_attempts + 1):
            try:
                if not self._client:
                    raise RuntimeError("í´ë¼ì´ì–¸íŠ¸ ë¯¸ì´ˆê¸°í™”")
                logger.info(f"[layout pass] ëª¨ë¸={layout_model} SDK={SDK_VERSION} ì‹œë„ {attempt}/{self.retry_max_attempts}")
                
                if SDK_VERSION == "4.x":
                    # 4.x SDK: FIGURES feature ì§€ì›
                    from io import BytesIO
                    body_params = {}
                    if pages:
                        body_params["pages"] = self._pages_to_range(pages)
                    
                    if self.enabled_features:
                        body_params["features"] = list(self.enabled_features)
                    if self.enabled_outputs:
                        body_params["output"] = list(self.enabled_outputs)
                    
                    poller = self._client.begin_analyze_document(
                        model_id=layout_model,
                        body=BytesIO(file_content),
                        **body_params
                    )
                else:
                    # 3.3.x Fallback (FIGURES ë¯¸ì§€ì›)
                    logger.warning("[AZURE-DI][FIGURES] âš ï¸ SDK 3.3.x - FIGURES ë¯¸ì§€ì›, API ê¸°ë³¸ ë™ì‘ë§Œ ì‚¬ìš©")
                    kwargs_l: Dict[str, Any] = {"logging_enable": False}
                    if pages:
                        kwargs_l["pages"] = self._pages_to_range(pages)
                    poller = self._client.begin_analyze_document(
                        layout_model,
                        document=file_content,
                        **kwargs_l,
                    )
                
                analyze_result = await asyncio.wait_for(self._poll_result(poller), timeout=self.timeout_seconds)
                return self._convert_layout_result(analyze_result, layout_model)
            except (ClientAuthenticationError, asyncio.TimeoutError, HttpResponseError, Exception) as e:  # noqa: E722
                last_error = str(e)
                logger.warning(f"[layout pass] ì˜ˆì™¸ ë°œìƒ: {e}")
                if attempt < self.retry_max_attempts:
                    await asyncio.sleep(min(2 ** attempt, 20))
        logger.warning(f"layout pass ì‹¤íŒ¨: {last_error}")
        return None

    async def _get_page_count(self, file_path: str) -> int:
        """PDF ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ë°˜í™˜ (pdfplumber ê²½ëŸ‰ í˜¸ì¶œ)"""
        try:
            import pdfplumber  # type: ignore
            with pdfplumber.open(file_path) as pdf:
                return len(pdf.pages)
        except Exception:
            return 0

    def _merge_group_results(self, results: List["DocumentIntelligenceResult"]) -> "DocumentIntelligenceResult":
        """ì—¬ëŸ¬ ê·¸ë£¹ ê²°ê³¼ë¥¼ í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ë³‘í•©"""
        merged = DocumentIntelligenceResult(success=True, text="", pages=[], tables=[], figures=[], metadata={})
        for r in results:
            if not r or not r.success:
                return DocumentIntelligenceResult(success=False, error=getattr(r, "error", "group analyze failed"))
            merged.pages.extend(r.pages or [])
            merged.tables.extend(r.tables or [])
            merged.figures.extend(r.figures or [])
        # í˜ì´ì§€ ë²ˆí˜¸ ê¸°ì¤€ ì •ë ¬ ë° ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        try:
            merged.pages.sort(key=lambda p: p.get("page_no", 0))
        except Exception:
            pass
        merged.text = "".join([
            (f"\n[í˜ì´ì§€ {p.get('page_no')}]\n" + (p.get("text", "") or "")) for p in (merged.pages or [])
        ])
        return merged

    def _reorder_two_column_pages_doc(self, pdf: Any, result: "DocumentIntelligenceResult") -> None:
        """
        2ì—´ ë ˆì´ì•„ì›ƒ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ìˆœì„œë¥¼ ì¬êµ¬ì„± (ì´ë¯¸ ì—´ë¦° pdfplumber ê°ì²´ ì¬ì‚¬ìš©)
        
        Args:
            pdf: ì´ë¯¸ ì—´ë¦° pdfplumber.PDF ê°ì²´
            result: ìˆ˜ì •í•  DocumentIntelligenceResult
        """
        if not result.pages:
            return

        # í˜ì´ì§€ë³„ë¡œ pdfplumberì—ì„œ ë‹¨ì–´ ì¢Œí‘œë¥¼ ê°€ì ¸ì™€ 2ì—´ ì—¬ë¶€ íŒë‹¨ ë° ì¬êµ¬ì„±
        new_pages = []
        for pg in result.pages:
            pno = int(pg.get("page_no", 0) or 0)
            if pno <= 0 or pno > len(pdf.pages):
                new_pages.append(pg)
                continue
            page = pdf.pages[pno - 1]
            words = page.extract_words(x_tolerance=2, y_tolerance=2, keep_blank_chars=False) or []
            if not words:
                new_pages.append(pg)
                continue
            W = float(page.width or 0)
            # ì¤‘ì•™ ë°´ë“œ ë¹„ì›€ ì—¬ë¶€ì™€ ì¢Œìš° ë¶„í¬ë¡œ 2ì—´ íŒë‹¨
            centers = [ (float(w.get("x0",0))+float(w.get("x1",0)))/2.0 for w in words ]
            left = [w for w,cx in zip(words, centers) if cx < W*0.47]
            right = [w for w,cx in zip(words, centers) if cx > W*0.53]
            mid_band = [w for w,cx in zip(words, centers) if W*0.47 <= cx <= W*0.53]
            is_dual = len(left) > 0 and len(right) > 0 and len(mid_band) < max(3, int(0.02*len(words)))
            if not is_dual:
                new_pages.append(pg)
                continue
            # ì¢Œ/ìš° ì»¬ëŸ¼ ë‚´ì—ì„œ y, x ì •ë ¬
            def sort_key(w):
                return (float(w.get("top",0)), float(w.get("x0",0)))
            left_sorted = sorted(left, key=sort_key)
            right_sorted = sorted(right, key=sort_key)
            left_text = self._join_words(left_sorted)
            right_text = self._join_words(right_sorted)
            combined_text = (left_text + "\n" + right_text).strip()
            new_pg = dict(pg)
            # ë©”íƒ€ì— dual column í”Œë˜ê·¸ ê¸°ë¡
            meta = dict(new_pg.get("metadata", {}))
            meta.update({"dual_column": True})
            new_pg["metadata"] = meta
            new_pg["text"] = combined_text if combined_text else pg.get("text", "")
            new_pages.append(new_pg)

        result.pages = new_pages
        # result.text ì¬êµ¬ì„±
        result.text = "".join([
            (f"\n[í˜ì´ì§€ {p.get('page_no')}]\n" + (p.get("text", "") or "")) for p in (result.pages or [])
        ])
        
        # INFO ë¡œê·¸: 2ì—´ ì¬êµ¬ì„± ì ìš© í˜ì´ì§€ ìˆ˜ í‘œì‹œ
        dual_count = sum(1 for pg in new_pages if pg.get("metadata", {}).get("dual_column"))
        if dual_count > 0:
            logger.info(f"[AZURE-DI] 2ì—´ ë ˆì´ì•„ì›ƒ ì¬êµ¬ì„± ì ìš©: {dual_count}/{len(new_pages)} í˜ì´ì§€")

    def _reorder_two_column_pages(self, file_path: str, result: "DocumentIntelligenceResult") -> None:
        """
        [DEPRECATED] 2ì—´ ë ˆì´ì•„ì›ƒ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ìˆœì„œë¥¼ ì¬êµ¬ì„± (ë ˆê±°ì‹œ)
        
        ì´ í•¨ìˆ˜ëŠ” ì´ì œ _reorder_two_column_pages_doc()ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        pdfplumberë¥¼ ë§¤ë²ˆ ì—¬ëŠ” ëŒ€ì‹  ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.
        """
        try:
            import pdfplumber  # type: ignore
        except Exception:
            logger.debug("pdfplumber ë¯¸ì„¤ì¹˜ - dual-column ì¬êµ¬ì„± ìƒëµ")
            return

        if not result.pages:
            return

        # í˜ì´ì§€ë³„ë¡œ pdfplumberì—ì„œ ë‹¨ì–´ ì¢Œí‘œë¥¼ ê°€ì ¸ì™€ 2ì—´ ì—¬ë¶€ íŒë‹¨ ë° ì¬êµ¬ì„±
        new_pages = []
        with pdfplumber.open(file_path) as pdf:
            for pg in result.pages:
                pno = int(pg.get("page_no", 0) or 0)
                if pno <= 0 or pno > len(pdf.pages):
                    new_pages.append(pg)
                    continue
                page = pdf.pages[pno - 1]
                words = page.extract_words(x_tolerance=2, y_tolerance=2, keep_blank_chars=False) or []
                if not words:
                    new_pages.append(pg)
                    continue
                W = float(page.width or 0)
                # ì¤‘ì•™ ë°´ë“œ ë¹„ì›€ ì—¬ë¶€ì™€ ì¢Œìš° ë¶„í¬ë¡œ 2ì—´ íŒë‹¨
                centers = [ (float(w.get("x0",0))+float(w.get("x1",0)))/2.0 for w in words ]
                left = [w for w,cx in zip(words, centers) if cx < W*0.47]
                right = [w for w,cx in zip(words, centers) if cx > W*0.53]
                mid_band = [w for w,cx in zip(words, centers) if W*0.47 <= cx <= W*0.53]
                is_dual = len(left) > 0 and len(right) > 0 and len(mid_band) < max(3, int(0.02*len(words)))
                if not is_dual:
                    new_pages.append(pg)
                    continue
                # ì¢Œ/ìš° ì»¬ëŸ¼ ë‚´ì—ì„œ y, x ì •ë ¬
                def sort_key(w):
                    return (float(w.get("top",0)), float(w.get("x0",0)))
                left_sorted = sorted(left, key=sort_key)
                right_sorted = sorted(right, key=sort_key)
                left_text = self._join_words(left_sorted)
                right_text = self._join_words(right_sorted)
                combined_text = (left_text + "\n" + right_text).strip()
                new_pg = dict(pg)
                # ë©”íƒ€ì— dual column í”Œë˜ê·¸ ê¸°ë¡
                meta = dict(new_pg.get("metadata", {}))
                meta.update({"dual_column": True})
                new_pg["metadata"] = meta
                new_pg["text"] = combined_text if combined_text else pg.get("text", "")
                new_pages.append(new_pg)

        result.pages = new_pages
        # result.text ì¬êµ¬ì„±
        result.text = "".join([
            (f"\n[í˜ì´ì§€ {p.get('page_no')}]\n" + (p.get("text", "") or "")) for p in (result.pages or [])
        ])
        
        # INFO ë¡œê·¸: 2ì—´ ì¬êµ¬ì„± ì ìš© í˜ì´ì§€ ìˆ˜ í‘œì‹œ
        dual_count = sum(1 for pg in new_pages if pg.get("metadata", {}).get("dual_column"))
        if dual_count > 0:
            logger.info(f"[AZURE-DI] 2ì—´ ë ˆì´ì•„ì›ƒ ì¬êµ¬ì„± ì ìš©: {dual_count}/{len(new_pages)} í˜ì´ì§€")

    def _join_words(self, words: List[Dict[str, Any]]) -> str:
        # ê°„ë‹¨í•œ ì¤„ ë°”ê¿ˆ íŒíŠ¸: y ì°¨ì´ê°€ í´ ë•Œ ì¤„ë°”ê¿ˆ
        if not words:
            return ""
        lines: List[List[Dict[str, Any]]] = []
        current: List[Dict[str, Any]] = []
        last_top = None
        for w in words:
            top = float(w.get("top", 0))
            if last_top is None or abs(top - last_top) <= 3.0:
                current.append(w)
                last_top = top if last_top is None else (last_top*0.7 + top*0.3)
            else:
                lines.append(current)
                current = [w]
                last_top = top
        if current:
            lines.append(current)
        # ê° ì¤„ì—ì„œ x0 ìˆœ ì •ë ¬ í›„ í…ìŠ¤íŠ¸ ê²°í•©
        def line_text(ws: List[Dict[str, Any]]) -> str:
            ws_sorted = sorted(ws, key=lambda w: float(w.get("x0", 0)))
            return " ".join([w.get("text", "") for w in ws_sorted]).strip()
        return "\n".join([line_text(ln) for ln in lines]).strip()

    def _pages_to_range(self, pages: List[int]) -> str:
        """í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ Azure DI pages ë§¤ê°œë³€ìˆ˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜ˆ: "1-3,5,7-9")"""
        if not pages:
            return ""
        ps = sorted(set(int(p) for p in pages if p and p > 0))
        ranges: List[str] = []
        start = ps[0]
        prev = ps[0]
        for p in ps[1:]:
            if p == prev + 1:
                prev = p
                continue
            if start == prev:
                ranges.append(str(start))
            else:
                ranges.append(f"{start}-{prev}")
            start = prev = p
        if start == prev:
            ranges.append(str(start))
        else:
            ranges.append(f"{start}-{prev}")
        return ",".join(ranges)

    def _serialize_result(self, r: "DocumentIntelligenceResult") -> Dict[str, Any]:
        return {
            "success": r.success,
            "text": r.text,
            "pages": r.pages,
            "tables": r.tables,
            "figures": r.figures,
            "metadata": r.metadata,
            "error": r.error,
            "extraction_method": r.extraction_method,
        }

    def _result_from_serialized(self, d: Dict[str, Any]) -> "DocumentIntelligenceResult":
        return DocumentIntelligenceResult(
            success=bool(d.get("success", True)),
            text=d.get("text", "") or "",
            pages=d.get("pages") or [],
            tables=d.get("tables") or [],
            figures=d.get("figures") or [],
            metadata=d.get("metadata") or {},
            error=d.get("error"),
            extraction_method=d.get("extraction_method", "azure_document_intelligence"),
        )
    
    async def _poll_result(self, poller: LROPoller) -> Any:
        """ë¹„ë™ê¸° í´ë§ ê²°ê³¼ ëŒ€ê¸°"""
        while not poller.done():
            await asyncio.sleep(1)  # 1ì´ˆë§ˆë‹¤ ìƒíƒœ í™•ì¸
        return poller.result()
    
    # ------------------ ë³€í™˜ ìœ í‹¸ (Read Pass) ------------------
    def _convert_read_result(self, analyze_result: Any, model: str) -> DocumentIntelligenceResult:
        try:
            pages = getattr(analyze_result, "pages", []) or []
            logger.debug(f"[AZURE-DI][READ] analyze_result.pages - count={len(pages)}")
            
            # ğŸ¯ SDK 4.x: analyze_result.paragraphsë¥¼ í˜ì´ì§€ë³„ë¡œ ë¶„ë°°
            doc_paragraphs = list(getattr(analyze_result, "paragraphs", []) or [])
            logger.info(f"[AZURE-DI][READ] analyze_result.paragraphs - count={len(doc_paragraphs)}")
            
            # í˜ì´ì§€ë³„ paragraphs ë§¤í•‘ (bounding_regionsë¡œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ)
            paragraphs_by_page: Dict[int, List[Any]] = {}
            for para in doc_paragraphs:
                bounding_regions = getattr(para, 'bounding_regions', [])
                if bounding_regions:
                    page_no = getattr(bounding_regions[0], 'page_number', 1)
                    if page_no not in paragraphs_by_page:
                        paragraphs_by_page[page_no] = []
                    paragraphs_by_page[page_no].append(para)
            
            logger.info(f"[AZURE-DI][READ] paragraphsë¥¼ {len(paragraphs_by_page)}ê°œ í˜ì´ì§€ë¡œ ë¶„ë°° ì™„ë£Œ")
            
            all_pages: List[Dict[str, Any]] = []
            full_text_parts: List[str] = []
            total_section_headers = 0
            
            for idx, page in enumerate(pages, 1):
                try:
                    # í•´ë‹¹ í˜ì´ì§€ì˜ paragraphs ì „ë‹¬
                    page_paragraphs = paragraphs_by_page.get(idx, [])
                    page_dict = self._process_page(page, idx, page_paragraphs)
                    all_pages.append(page_dict)
                    total_section_headers += len(page_dict.get('section_headers', []))
                    if page_dict.get("text"):
                        full_text_parts.append(f"\n\n=== í˜ì´ì§€ {idx} ===\n" + page_dict["text"])
                except Exception as page_err:
                    logger.error(f"[AZURE-DI][READ] í˜ì´ì§€ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {page_err}", exc_info=True)
                    # í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    continue
            
            full_text = "\n".join(full_text_parts).strip()
            
            logger.info(f"[AZURE-DI][READ] âœ… {len(all_pages)}í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ - ì „ì²´ section_headers: {total_section_headers}ê°œ")
            
            # ğŸ¯ SDK 4.x prebuilt-layout: tablesì™€ figuresë„ í•¨ê»˜ ì¶”ì¶œ
            tables: List[Dict[str, Any]] = []
            tables_raw = getattr(analyze_result, "tables", []) or []
            if tables_raw:
                logger.info(f"[AZURE-DI][READ] analyze_resultì—ì„œ {len(tables_raw)}ê°œ table ë°œê²¬")
                tables = [self._process_table(table, idx) for idx, table in enumerate(tables_raw)]
            
            figures: List[Dict[str, Any]] = []
            doc_figs = getattr(analyze_result, "figures", None)
            if doc_figs:
                logger.info(f"[AZURE-DI][READ] analyze_resultì—ì„œ {len(doc_figs)}ê°œ figure ë°œê²¬ (ë¬¸ì„œ ë ˆë²¨)")
                for idx, fig in enumerate(doc_figs, 1):
                    figures.append(self._process_figure(fig, idx))
            
            # ë¬¸ì„œ ë ˆë²¨ figuresê°€ ì—†ìœ¼ë©´ í˜ì´ì§€ë³„ figures í™•ì¸
            if not figures and hasattr(analyze_result, "pages"):
                logger.info(f"[AZURE-DI][READ] í˜ì´ì§€ë³„ figures í™•ì¸ ì¤‘ (ì´ {len(analyze_result.pages)}í˜ì´ì§€)")
                for page in analyze_result.pages:
                    page_figures = getattr(page, "figures", None)
                    if page_figures:
                        logger.info(f"[AZURE-DI][READ] í˜ì´ì§€ {getattr(page, 'page_number', '?')}ì—ì„œ {len(page_figures)}ê°œ figure ë°œê²¬")
                        for fig in page_figures:
                            figures.append(self._process_figure(fig, len(figures) + 1))
            
            metadata = {
                "page_count": len(all_pages),
                "table_count": len(tables),
                "figure_count": len(figures),
                "char_count": len(full_text),
                "di_model": model,
                "extraction_method": "azure_document_intelligence",
            }
            
            logger.info(f"[AZURE-DI][READ] âœ… ì¶”ì¶œ ì™„ë£Œ - tables: {len(tables)}, figures: {len(figures)}")
            
            return DocumentIntelligenceResult(success=True, text=full_text, pages=all_pages, tables=tables, figures=figures, metadata=metadata)
        except Exception as e:  # pragma: no cover
            logger.error(f"[AZURE-DI][READ] read ë³€í™˜ ì‹¤íŒ¨: {e}", exc_info=True)
            return DocumentIntelligenceResult(success=False, error=f"read ë³€í™˜ ì‹¤íŒ¨: {e}")

    # ------------------ ë³€í™˜ ìœ í‹¸ (Layout/Table Pass) ------------------
    def _convert_layout_result(self, analyze_result: Any, model: str) -> DocumentIntelligenceResult:
        try:
            tables_raw = getattr(analyze_result, "tables", []) or []
            tables = [self._process_table(table, idx) for idx, table in enumerate(tables_raw)]

            figures: List[Dict[str, Any]] = []
            doc_figs = getattr(analyze_result, "figures", None)
            logger.info(f"[AZURE-DI][FIGURES] ë¬¸ì„œ ë ˆë²¨ figures ì†ì„±: {type(doc_figs)}, count={len(doc_figs) if doc_figs else 0}")
            if doc_figs:
                for idx, fig in enumerate(doc_figs, 1):
                    figures.append(self._process_figure(fig, idx))

            if not figures and hasattr(analyze_result, "pages"):
                logger.info(f"[AZURE-DI][FIGURES] ë¬¸ì„œ ë ˆë²¨ figures ì—†ìŒ â†’ í˜ì´ì§€ë³„ figures í™•ì¸ ì¤‘ (ì´ {len(analyze_result.pages)}í˜ì´ì§€)")
                for page in analyze_result.pages:
                    page_figures = getattr(page, "figures", None)
                    if not page_figures:
                        continue
                    logger.info(f"[AZURE-DI][FIGURES] í˜ì´ì§€ {getattr(page, 'page_number', '?')}ì—ì„œ {len(page_figures)}ê°œ figure ë°œê²¬")
                    for fig in page_figures:
                        figures.append(self._process_figure(fig, len(figures) + 1))

            metadata = {
                "layout_model": model,
                "layout_tables": len(tables),
                "layout_figures": len(figures)
            }

            return DocumentIntelligenceResult(
                success=True,
                text="",
                tables=tables,
                figures=figures,
                pages=[],
                metadata=metadata
            )
        except Exception as e:  # pragma: no cover
            return DocumentIntelligenceResult(success=False, error=f"layout ë³€í™˜ ì‹¤íŒ¨: {e}")
    
    # ------------------ í˜ì´ì§€ ì²˜ë¦¬ & ì»¬ëŸ¼ ì •ë ¬ ------------------
    def _process_page(self, page: Any, page_no: int, page_paragraphs: List[Any] = None) -> Dict[str, Any]:
        """
        í˜ì´ì§€ ë‹¨ìœ„ ì²˜ë¦¬
        
        Args:
            page: Azure DI page ê°ì²´
            page_no: í˜ì´ì§€ ë²ˆí˜¸ (1-based)
            page_paragraphs: í•´ë‹¹ í˜ì´ì§€ì˜ paragraphs ë¦¬ìŠ¤íŠ¸ (SDK 4.xëŠ” ë¬¸ì„œ ë ˆë²¨ì—ì„œ ì „ë‹¬)
        """
        width = float(getattr(page, 'width', 0) or 0)
        height = float(getattr(page, 'height', 0) or 0)
        lines = list(getattr(page, 'lines', []) or [])
        processed_lines: List[Dict[str, Any]] = []
        raw_line_objs: List[Any] = []
        for ln in lines:
            content = getattr(ln, 'content', getattr(ln, 'text', '')) or ''
            if not content.strip():
                continue
            processed_lines.append({
                'content': content,
                'bbox': self._extract_bbox(ln),
                'confidence': getattr(ln, 'confidence', 1.0)
            })
            raw_line_objs.append(ln)

        # ğŸ¯ Azure DIì˜ paragraphsì™€ role ì •ë³´ ì¶”ì¶œ (ì„¹ì…˜ ê°ì§€ í™œìš©)
        # SDK 4.x: paragraphsëŠ” ë¬¸ì„œ ë ˆë²¨ì—ì„œ í˜ì´ì§€ë³„ë¡œ ë¶„ë°°ë¨
        paragraphs = page_paragraphs if page_paragraphs is not None else []
        section_headers: List[Dict[str, Any]] = []
        paragraph_blocks: List[Dict[str, Any]] = []
        
        logger.debug(f"[AZURE-DI][PAGE-{page_no}] paragraphs ì „ë‹¬ ë°›ìŒ - count={len(paragraphs)}")
        
        for para in paragraphs:
            content = getattr(para, 'content', '') or ''
            if not content.strip():
                continue
            
            role = getattr(para, 'role', None)
            bbox = self._extract_bbox(para)
            
            para_info = {
                'content': content,
                'role': role,
                'bbox': bbox,
                'confidence': getattr(para, 'confidence', 1.0)
            }
            
            # role ê¸°ë°˜ ì„¹ì…˜ í—¤ë” ì‹ë³„
            if role and 'heading' in role.lower():
                section_headers.append(para_info)
                logger.debug(f"[AZURE-DI][ROLE] ì„¹ì…˜ í—¤ë” ê°ì§€ - page={page_no}, role={role}, content='{content[:50]}'")
            else:
                paragraph_blocks.append(para_info)
        
        if section_headers:
            logger.info(f"[AZURE-DI][PAGE-{page_no}] âœ… {len(section_headers)}ê°œ ì„¹ì…˜ í—¤ë” ê°ì§€ (role ê¸°ë°˜)")
        else:
            logger.debug(f"[AZURE-DI][PAGE-{page_no}] ì„¹ì…˜ í—¤ë” ì—†ìŒ (paragraphs={len(paragraph_blocks)})")


        # ì»¬ëŸ¼ ë¶„í•  (ê°„ë‹¨: 2ì—´ ì‹œë„ í›„ ë‹¤ë‹¨)
        groups = self._split_into_n_columns(raw_line_objs, width, height, max_cols=2, min_lines_per_col=3)
        if len(groups) <= 1:
            groups = [raw_line_objs]
        merged_text_parts: List[str] = []
        for g in groups:
            # ê° ê·¸ë£¹ ë‚´ ë¼ì¸ y,x ì •ë ¬
            g_sorted = sorted(g, key=lambda obj: self._line_left_top_norm(obj, width, height))
            merged_text_parts.append('\n'.join([getattr(obj, 'content', getattr(obj, 'text', '')) for obj in g_sorted]).strip())
        merged_text = '\n\n'.join([p for p in merged_text_parts if p]).strip()

        page_result = {
            'page_no': page_no,
            'text': merged_text,
            'width': width,
            'height': height,
            'lines': processed_lines,
            'paragraphs': paragraph_blocks,  # ğŸ¯ ì¼ë°˜ ë¬¸ë‹¨ ì •ë³´
            'section_headers': section_headers,  # ğŸ¯ role ê¸°ë°˜ ì„¹ì…˜ í—¤ë”
            'figures': [],  # í–¥í›„ í•„ìš” ì‹œ í™•ì¥
            'tables': [],
            'images_metadata': [],
            'columns_detected': len(groups)
        }
        
        logger.debug(f"[AZURE-DI][PAGE-{page_no}] ë°˜í™˜ - text_len={len(merged_text)}, lines={len(processed_lines)}, "
                    f"section_headers={len(section_headers)}, paragraphs={len(paragraph_blocks)}")
        
        return page_result
    
    def _process_table(self, table: Any, table_idx: int) -> Dict[str, Any]:
        cells = getattr(table, 'cells', []) or []
        processed_cells: List[Dict[str, Any]] = []
        for c in cells:
            if getattr(c, 'confidence', 1.0) < self.confidence_threshold:
                continue
            processed_cells.append({
                'row_index': getattr(c, 'row_index', 0),
                'column_index': getattr(c, 'column_index', 0),
                'row_span': getattr(c, 'row_span', 1),
                'column_span': getattr(c, 'column_span', 1),
                'content': getattr(c, 'content', getattr(c, 'text', '')) or '',
                'confidence': getattr(c, 'confidence', 1.0),
                'bbox': self._extract_bbox(c)
            })
        return {
            'table_index': table_idx + 1,
            'row_count': getattr(table, 'row_count', 0),
            'column_count': getattr(table, 'column_count', 0),
            'bbox': self._extract_bbox(table),
            'cells': processed_cells
        }

    def _process_figure(self, figure: Any, figure_idx: int) -> Dict[str, Any]:
        bbox = self._extract_bbox(figure)
        page_no = None
        if hasattr(figure, 'bounding_regions') and figure.bounding_regions:
            region = figure.bounding_regions[0]
            page_no = getattr(region, 'page_number', None) or getattr(region, 'page', None)
        
        # Caption ì²˜ë¦¬: DocumentCaption ê°ì²´ì¸ ê²½ìš° .contentë¡œ ë¬¸ìì—´ ì¶”ì¶œ
        caption_obj = getattr(figure, 'caption', None)
        if caption_obj:
            # DocumentCaption ê°ì²´ì¸ ê²½ìš° .content ì†ì„± ì‚¬ìš©
            caption_text = getattr(caption_obj, 'content', str(caption_obj)) if hasattr(caption_obj, 'content') else str(caption_obj)
        else:
            caption_text = ''
        
        return {
            'figure_index': figure_idx,
            'page_no': page_no,
            'bbox': bbox,
            'confidence': getattr(figure, 'confidence', None),
            'caption': caption_text,  # ë¬¸ìì—´ë¡œ ì €ì¥
            'extraction_source': 'azure_document_intelligence'
        }

    # ------------------ ì¢Œí‘œ/ì»¬ëŸ¼ ìœ í‹¸ (ë…¸íŠ¸ë¶ íœ´ë¦¬ìŠ¤í‹± ì´ì‹) ------------------
    def _line_left_top_norm(self, line: Any, page_w: float, page_h: float) -> Tuple[float, float]:
        """ë¼ì¸ ê°ì²´ì—ì„œ ì •ê·œí™”ëœ ì¢Œìƒë‹¨ ì¢Œí‘œ ì¶”ì¶œ (SDK 3.x & 4.x í˜¸í™˜)"""
        poly = getattr(line, 'polygon', None) or getattr(line, 'bounding_polygon', None)
        if poly:
            xs = []
            ys = []
            
            # SDK 4.x í™•ì¸: flat array [x1, y1, x2, y2, ...] ë˜ëŠ” Point ê°ì²´ ë¦¬ìŠ¤íŠ¸
            if poly and len(poly) > 0:
                first_elem = poly[0]
                
                # Case 1: SDK 4.x flat array (ìˆ«ì ë¦¬ìŠ¤íŠ¸)
                if isinstance(first_elem, (int, float)):
                    # [x1, y1, x2, y2, x3, y3, x4, y4] í˜•ì‹
                    for i in range(0, len(poly), 2):
                        if i + 1 < len(poly):
                            xs.append(float(poly[i]))
                            ys.append(float(poly[i + 1]))
                
                # Case 2: Point ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” [x, y] ë¦¬ìŠ¤íŠ¸
                else:
                    for p in poly:
                        try:
                            # SDK 4.x: Point ê°ì²´ (p.x, p.y)
                            if hasattr(p, 'x') and hasattr(p, 'y'):
                                xs.append(float(p.x))
                                ys.append(float(p.y))
                            # SDK 3.x ë˜ëŠ” ë¦¬ìŠ¤íŠ¸: [x, y]
                            elif isinstance(p, (list, tuple)) and len(p) >= 2:
                                xs.append(float(p[0]))
                                ys.append(float(p[1]))
                        except (TypeError, ValueError, IndexError) as e:
                            logger.debug(f"[AZURE-DI][NORM] polygon point íŒŒì‹± ì‹¤íŒ¨: {e}, type={type(p)}")
                            continue
            
            if xs and ys:
                left = (min(xs) / (page_w or 1.0))
                top = (min(ys) / (page_h or 1.0))
            else:
                left, top = 0.0, 0.0
        else:
            left, top = 0.0, 0.0
        return left, top

    def _kmeans_1d(self, values: List[float], k: int, iters: int = 15) -> List[int]:
        if k <= 1 or len(values) <= k:
            return [0 for _ in values]
        sv = sorted(set(values))
        if len(sv) < k:
            k = len(sv)
        centers = [sv[max(0, min(len(sv)-1, round((i+0.5)*len(sv)/k)-1))] for i in range(k)]
        assign = [0]*len(values)
        for _ in range(iters):
            for i, v in enumerate(values):
                ci = min(range(k), key=lambda j: abs(v - centers[j]))
                assign[i] = ci
            new_centers = centers[:]
            for j in range(k):
                grp = [values[i] for i, a in enumerate(assign) if a == j]
                if grp:
                    new_centers[j] = sum(grp)/len(grp)
            if all(abs(new_centers[j]-centers[j]) < 1e-4 for j in range(k)):
                break
            centers = new_centers
        return assign

    def _split_into_n_columns(self, lines: List[Any], page_w: float, page_h: float, max_cols: int, min_lines_per_col: int):
        if not lines:
            return [lines]
        xs = [self._line_left_top_norm(l, page_w, page_h)[0] for l in lines]
        best = [lines]
        for k in range(2, max_cols+1):
            assign = self._kmeans_1d(xs, k)
            groups = [[] for _ in range(k)]
            for l, a in zip(lines, assign):
                groups[a].append(l)
            groups = [g for g in groups if len(g) >= min_lines_per_col]
            if len(groups) <= 1:
                continue
            def left_mean(g):
                vals = [self._line_left_top_norm(x, page_w, page_h)[0] for x in g]
                return sum(vals)/len(vals)
            groups.sort(key=left_mean)
            best = groups
        return best
    
    def _extract_bbox(self, element: Any) -> List[List[float]]:
        """ìš”ì†Œì—ì„œ bounding box ì¶”ì¶œ (SDK 3.x & 4.x í˜¸í™˜)"""
        # SDK 4.x: bounding_regions[].polygon
        if hasattr(element, 'bounding_regions') and element.bounding_regions:
            try:
                region = element.bounding_regions[0]
                if hasattr(region, 'polygon') and region.polygon:
                    poly = region.polygon
                    polygon = []
                    
                    # SDK 4.x í™•ì¸: flat array ë˜ëŠ” Point ê°ì²´ ë¦¬ìŠ¤íŠ¸
                    if poly and len(poly) > 0:
                        first_elem = poly[0]
                        
                        # Case 1: SDK 4.x flat array [x1, y1, x2, y2, ...]
                        if isinstance(first_elem, (int, float)):
                            for i in range(0, len(poly), 2):
                                if i + 1 < len(poly):
                                    polygon.append([float(poly[i]), float(poly[i + 1])])
                        
                        # Case 2: Point ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” [x, y] ë¦¬ìŠ¤íŠ¸
                        else:
                            for i, point in enumerate(poly):
                                try:
                                    if hasattr(point, 'x') and hasattr(point, 'y'):
                                        # Point ê°ì²´
                                        polygon.append([float(point.x), float(point.y)])
                                    elif isinstance(point, (list, tuple)) and len(point) >= 2:
                                        # [x, y] í˜•ì‹
                                        polygon.append([float(point[0]), float(point[1])])
                                except (TypeError, ValueError, IndexError, AttributeError) as e:
                                    logger.debug(f"[AZURE-DI][BBOX] polygon point[{i}] ë³€í™˜ ì‹¤íŒ¨: {e}")
                                    continue
                    
                    if polygon:
                        return polygon
                    else:
                        logger.debug(f"[AZURE-DI][BBOX] polygon íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ (region.polygon ê¸¸ì´: {len(poly)})")
            except Exception as e:
                logger.debug(f"[AZURE-DI][BBOX] bounding_regions íŒŒì‹± ì‹¤íŒ¨: {e}, element_type={type(element).__name__}")
        
        # SDK 3.x í˜¸í™˜: bounding_box ì†ì„±
        if hasattr(element, 'bounding_box') and element.bounding_box:
            try:
                bbox = element.bounding_box
                return [[bbox.x, bbox.y], [bbox.x + bbox.width, bbox.y], 
                       [bbox.x + bbox.width, bbox.y + bbox.height], [bbox.x, bbox.y + bbox.height]]
            except Exception as e:
                logger.warning(f"[AZURE-DI][BBOX] bounding_box íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return []  # ë¹ˆ bounding box

    def _merge_figures_into_pages(self, di_result: DocumentIntelligenceResult) -> None:
        """layout passì—ì„œ ì¶”ì¶œí•œ figureë¥¼ í˜ì´ì§€ ë©”íƒ€ì— ë°˜ì˜"""
        if not di_result.figures or not di_result.pages:
            return
        figures_by_page: Dict[int, List[Dict[str, Any]]] = {}
        for fig in di_result.figures:
            page_no = fig.get('page_no')
            if page_no is None:
                continue
            figures_by_page.setdefault(page_no, []).append(fig)

        for page in di_result.pages:
            page_no = page.get('page_no')
            if not page_no or page_no not in figures_by_page:
                continue
            page.setdefault('figures', [])
            page.setdefault('images_metadata', [])
            existing_images = page['images_metadata']
            start_idx = len(existing_images) + 1
            for idx, fig in enumerate(figures_by_page[page_no], start=start_idx):
                page['figures'].append(fig)
                page['images_metadata'].append({
                    'image_index': idx,
                    'page_no': page_no,
                    'bbox': fig.get('bbox', []),
                    'width': 0,
                    'height': 0,
                    'extraction_source': fig.get('extraction_source', 'azure_document_intelligence')
                })

    def _extract_figures_with_pdfplumber_doc(self, pdf: Any) -> List[Dict[str, Any]]:
        """
        pdfplumber ì´ë¯¸ì§€ë¥¼ í™œìš©í•œ ë³´ì¡° figure ì¶”ì¶œ (ì´ë¯¸ ì—´ë¦° ê°ì²´ ì¬ì‚¬ìš©)
        
        Args:
            pdf: ì´ë¯¸ ì—´ë¦° pdfplumber.PDF ê°ì²´
            
        Returns:
            ì¶”ì¶œëœ figure ëª©ë¡
        """
        figures: List[Dict[str, Any]] = []
        try:
            logger.info(f"[FIGURE-FALLBACK] pdfplumberë¡œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘ (ì¬ì‚¬ìš©ëœ PDF ê°ì²´)")
            for page_idx, page in enumerate(pdf.pages, start=1):
                page_images = page.images or []
                logger.debug(f"[FIGURE-FALLBACK] í˜ì´ì§€ {page_idx}: {len(page_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
                # Dedup and size filter within page
                seen: List[Tuple[float, float, float, float]] = []
                per_page_count = 0
                max_per_page = 50  # prevent runaway counts
                min_area = max(1000.0, (page.width or 0) * (page.height or 0) * 0.002)  # skip tiny marks
                for img_idx, img in enumerate(page_images, start=1):
                    x0, y0, x1, y1 = img.get('x0'), img.get('y0'), img.get('x1'), img.get('y1')
                    if any(v is None for v in (x0, y0, x1, y1)):
                        logger.debug(f"[FIGURE-FALLBACK] í˜ì´ì§€ {page_idx} ì´ë¯¸ì§€ {img_idx}: bbox ë¶ˆì™„ì „ â†’ ìŠ¤í‚µ")
                        continue
                    try:
                        fx0, fy0, fx1, fy1 = float(x0), float(y0), float(x1), float(y1)
                    except Exception:
                        # ì¢Œí‘œ ìºìŠ¤íŒ… ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
                        continue
                    w = fx1 - fx0
                    h = fy1 - fy0
                    if w <= 2 or h <= 2 or (w * h) < min_area:
                        # ignore too small images (likely artifacts)
                        continue
                    # dedup by IoU with seen bboxes
                    cand = (fx0, fy0, fx1, fy1)
                    is_dup = False
                    for (sx0, sy0, sx1, sy1) in seen:
                        ix0, iy0 = max(cand[0], sx0), max(cand[1], sy0)
                        ix1, iy1 = min(cand[2], sx1), min(cand[3], sy1)
                        inter = max(0.0, ix1 - ix0) * max(0.0, iy1 - iy0)
                        area_c = (cand[2] - cand[0]) * (cand[3] - cand[1])
                        area_s = (sx1 - sx0) * (sy1 - sy0)
                        union = area_c + area_s - inter if (area_c + area_s - inter) > 0 else 1.0
                        iou = inter / union
                        if iou > 0.9:
                            is_dup = True
                            break
                    if is_dup:
                        continue
                    seen.append(cand)
                    bbox = [[x0, y0], [x1, y0], [x1, y1], [x0, y1]]
                    figures.append({
                        'figure_index': len(figures) + 1,
                        'page_no': page_idx,
                        'bbox': bbox,
                        'confidence': None,
                        'caption': '',
                        'extraction_source': 'pdfplumber_image'
                    })
                    per_page_count += 1
                    if per_page_count >= max_per_page:
                        break
            logger.info(f"[FIGURE-FALLBACK] pdfplumber ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(figures)}ê°œ")
        except Exception as e:
            logger.error(f"[FIGURE-FALLBACK] pdfplumber ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return figures

    def _extract_figures_with_pdfplumber(self, file_path: str) -> List[Dict[str, Any]]:
        """
        [DEPRECATED] pdfplumber ì´ë¯¸ì§€ë¥¼ í™œìš©í•œ ë³´ì¡° figure ì¶”ì¶œ (ë ˆê±°ì‹œ)
        
        ì´ í•¨ìˆ˜ëŠ” ì´ì œ _extract_figures_with_pdfplumber_doc()ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        pdfplumberë¥¼ ë§¤ë²ˆ ì—¬ëŠ” ëŒ€ì‹  ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.
        """
        try:
            import pdfplumber
        except Exception:
            logger.warning("[FIGURE-FALLBACK] pdfplumber ë¯¸ì„¤ì¹˜ë¡œ figure fallback ë¶ˆê°€")
            return []

        figures: List[Dict[str, Any]] = []
        try:
            logger.info(f"[FIGURE-FALLBACK] pdfplumberë¡œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘: {file_path}")
            with pdfplumber.open(file_path) as pdf:
                for page_idx, page in enumerate(pdf.pages, start=1):
                    page_images = page.images or []
                    logger.debug(f"[FIGURE-FALLBACK] í˜ì´ì§€ {page_idx}: {len(page_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
                    # Dedup and size filter within page
                    seen: List[Tuple[float, float, float, float]] = []
                    per_page_count = 0
                    max_per_page = 50  # prevent runaway counts
                    min_area = max(1000.0, (page.width or 0) * (page.height or 0) * 0.002)  # skip tiny marks
                    for img_idx, img in enumerate(page_images, start=1):
                        x0, y0, x1, y1 = img.get('x0'), img.get('y0'), img.get('x1'), img.get('y1')
                        if any(v is None for v in (x0, y0, x1, y1)):
                            logger.debug(f"[FIGURE-FALLBACK] í˜ì´ì§€ {page_idx} ì´ë¯¸ì§€ {img_idx}: bbox ë¶ˆì™„ì „ â†’ ìŠ¤í‚µ")
                            continue
                        try:
                            fx0, fy0, fx1, fy1 = float(x0), float(y0), float(x1), float(y1)
                        except Exception:
                            # ì¢Œí‘œ ìºìŠ¤íŒ… ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
                            continue
                        w = fx1 - fx0
                        h = fy1 - fy0
                        if w <= 2 or h <= 2 or (w * h) < min_area:
                            # ignore too small images (likely artifacts)
                            continue
                        # dedup by IoU with seen bboxes
                        cand = (fx0, fy0, fx1, fy1)
                        is_dup = False
                        for (sx0, sy0, sx1, sy1) in seen:
                            ix0, iy0 = max(cand[0], sx0), max(cand[1], sy0)
                            ix1, iy1 = min(cand[2], sx1), min(cand[3], sy1)
                            inter = max(0.0, ix1 - ix0) * max(0.0, iy1 - iy0)
                            area_c = (cand[2] - cand[0]) * (cand[3] - cand[1])
                            area_s = (sx1 - sx0) * (sy1 - sy0)
                            union = area_c + area_s - inter if (area_c + area_s - inter) > 0 else 1.0
                            iou = inter / union
                            if iou > 0.9:
                                is_dup = True
                                break
                        if is_dup:
                            continue
                        seen.append(cand)
                        bbox = [[x0, y0], [x1, y0], [x1, y1], [x0, y1]]
                        figures.append({
                            'figure_index': len(figures) + 1,
                            'page_no': page_idx,
                            'bbox': bbox,
                            'confidence': None,
                            'caption': '',
                            'extraction_source': 'pdfplumber_image'
                        })
                        per_page_count += 1
                        if per_page_count >= max_per_page:
                            logger.debug(f"[FIGURE-FALLBACK] í˜ì´ì§€ {page_idx} ìµœëŒ€ {max_per_page}ê°œ ë„ë‹¬, ì´í›„ ìŠ¤í‚µ")
                            break
            logger.info(f"[FIGURE-FALLBACK] âœ… pdfplumberë¡œ {len(figures)}ê°œ figure ì¶”ì¶œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"[FIGURE-FALLBACK] âŒ pdfplumber figure ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

        return figures
    
    async def _check_page_limit(self, file_path: str) -> bool:
        """PDF í˜ì´ì§€ ìˆ˜ê°€ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            import PyPDF2
            with open(file_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                page_count = len(reader.pages)
                
                if page_count > self.max_pages:
                    logger.warning(f"PDF í˜ì´ì§€ ìˆ˜ ì´ˆê³¼: {page_count} > {self.max_pages}")
                    return True
                    
                logger.info(f"PDF í˜ì´ì§€ ìˆ˜ í™•ì¸: {page_count} (ì œí•œ: {self.max_pages})")
                return False
                
        except Exception as e:
            logger.warning(f"PDF í˜ì´ì§€ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False  # í™•ì¸ ì‹¤íŒ¨ì‹œ ì§„í–‰
    
    def create_internal_extraction_result(self, di_result: DocumentIntelligenceResult) -> Dict[str, Any]:
        """Document Intelligence ê²°ê³¼ë¥¼ ê¸°ì¡´ ì¶”ì¶œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not di_result.success:
            return {
                'success': False,
                'error': di_result.error,
                'text': '',
                'metadata': {
                    'extraction_method': di_result.extraction_method,
                    'extraction_note': di_result.error
                }
            }
        
        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        result = {
            'success': True,
            'text': di_result.text,
            'metadata': {
                'provider': 'azure_di',  # ğŸ¯ Provider ì •ë³´ ì¶”ê°€ (multimodal_document_serviceì—ì„œ ì‚¬ìš©)
                **di_result.metadata,
                'pages': di_result.pages,
                'tables': di_result.tables,
                'figures': di_result.figures,  # âœ… figures ì¶”ê°€
                'images_metadata': []
            }
        }
        
        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° í†µí•©
        for page in di_result.pages:
            if 'images_metadata' in page:
                result['metadata']['images_metadata'].extend(page['images_metadata'])
        
        return result


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
azure_document_intelligence_service = AzureDocumentIntelligenceService()
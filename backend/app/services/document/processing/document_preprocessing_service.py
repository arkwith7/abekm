"""
ğŸ“„ ë¬¸ì„œ ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤
===================

ë¬¸ì„œ ì—…ë¡œë“œ í›„ ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ê¹Œì§€ì˜ ì „ì²˜ë¦¬ ë‹´ë‹¹
1. í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF, DOCX, etc.)
2. í…ìŠ¤íŠ¸ ì •ì œ ë° êµ¬ì¡°í™”
3. ì˜ë¯¸ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
4. ì²­í¬ ë©”íƒ€ë°ì´í„° ìƒì„±
"""

import re
import hashlib
import logging
import json
from typing import Dict, List, Any, Optional
from pathlib import Path
import tiktoken

from app.services.document.extraction.text_extractor_service import text_extractor_service
from app.core.config import settings
try:
    from app.services.core.azure_blob_service import get_azure_blob_service
except Exception:  # pragma: no cover
    get_azure_blob_service = None  # type: ignore
try:
    from app.utils.storage_paths import (
        build_intermediate_page_key,
        build_intermediate_extraction_summary_key,
    )
except Exception:  # pragma: no cover
    build_intermediate_page_key = None  # type: ignore
    build_intermediate_extraction_summary_key = None  # type: ignore

logger = logging.getLogger(__name__)

class DocumentPreprocessingService:
    """ë¬¸ì„œ ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤ - ì—…ë¡œë“œë¶€í„° ì²­í‚¹ê¹Œì§€"""
    
    def __init__(self):
        # AWS Bedrock Titan Embeddings V2 í† í° ì œí•œì— ë§ì¶˜ ì„¤ì •
        self.max_tokens_per_chunk = 6000  # ì•ˆì „ ë§ˆì§„ í¬í•¨ (8192 ì œí•œ)
        self.min_tokens_per_chunk = 200  # ìµœì†Œ ì²­í¬ í¬ê¸° (í† í°) - ì˜ë¯¸ìˆëŠ” ë§¥ë½ ë³´ì¥
        self.target_tokens_per_chunk = 3000  # ëª©í‘œ ì²­í¬ í¬ê¸° - ê· í˜• ì¡íŒ í¬ê¸°
        self.overlap_tokens = 300  # ê²¹ì¹¨ í† í° ìˆ˜ (ì•½ 10%)
        self.tokenizer = tiktoken.get_encoding("cl100k_base")  # ì •í™•í•œ í† í° ê³„ì‚°
        
        # kss (Korean Sentence Splitter) ì´ˆê¸°í™”
        try:
            import kss
            self.kss = kss
            self.use_kss = True
            logger.info("í•œêµ­ì–´ ë¬¸ì¥ ë¶„í• ê¸° (kss) ë¡œë“œ ì„±ê³µ")
        except ImportError:
            self.kss = None
            self.use_kss = False
            logger.warning("kss ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - í´ë°± ë¬¸ì¥ ë¶„í•  ì‚¬ìš©")
        
        logger.info(f"ë¬¸ì„œ ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” - ìµœëŒ€: {self.max_tokens_per_chunk}, ëª©í‘œ: {self.target_tokens_per_chunk}, ìµœì†Œ: {self.min_tokens_per_chunk}, ê²¹ì¹¨: {self.overlap_tokens}")
    
    async def preprocess_document(
        self,
        file_path: str,
        file_extension: str,
        container_id: str,
        user_emp_no: str
    ) -> Dict[str, Any]:
        """
        ë‹¨ê³„ 1) ë¬¸ì„œ ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰ (í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì •ì œ)

        ë°˜í™˜:
          { success, extracted_text, cleaned_text, extraction_metadata }
        """
        try:
            logger.info(f"[PREPROCESS] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {file_path}")
            extraction_result = await text_extractor_service.extract_text_from_file(file_path)
            if not extraction_result.get('success'):
                return {
                    'success': False,
                    'error': f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {extraction_result.get('error')}"
                }

            raw_text = extraction_result.get('text', '')
            cleaned_text = self._clean_text(raw_text)

            return {
                'success': True,
                'extracted_text': raw_text,
                'cleaned_text': cleaned_text,
                'extraction_metadata': extraction_result.get('metadata', {})
            }
        except Exception as e:
            logger.error(f"[PREPROCESS] ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def chunk_text(
        self,
        cleaned_text: str,
        *,
        file_path: Optional[str] = None,
        container_id: Optional[str] = None,
        user_emp_no: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ë‹¨ê³„ 2) ì²­í‚¹ë§Œ ìˆ˜í–‰

        ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì€ ì •ì œ í…ìŠ¤íŠ¸ë¥¼ í† í° í•œë„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹í•©ë‹ˆë‹¤.
        íŒŒì¼/ì»¨í…Œì´ë„ˆ ì •ë³´ê°€ ì£¼ì–´ì§€ë©´ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            if not cleaned_text or not cleaned_text.strip():
                return {'success': False, 'error': 'ì²­í‚¹í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.'}

            chunks = self._smart_chunk_text(cleaned_text)

            chunk_metadata: List[Dict[str, Any]] = []
            for i, chunk in enumerate(chunks):
                if file_path and container_id and user_emp_no:
                    meta = self._create_chunk_metadata(
                        chunk=chunk,
                        chunk_index=i,
                        total_chunks=len(chunks),
                        file_path=file_path,
                        container_id=container_id,
                        user_emp_no=user_emp_no
                    )
                    chunk_metadata.append(meta)

            return {
                'success': True,
                'chunks': chunks,
                'metadata': chunk_metadata,
                'total_chunks': len(chunks),
                'total_tokens': sum(len(self.tokenizer.encode(c)) for c in chunks)
            }
        except Exception as e:
            logger.error(f"[CHUNK] ì²­í‚¹ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}

    async def process_document(
        self, 
        file_path: str, 
        file_extension: str,
        container_id: str,
        user_emp_no: str
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ì „ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸ (í˜¸í™˜ ìœ ì§€)
        - 1) ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰
        - 2) ì²­í‚¹ë§Œ ìˆ˜í–‰
        """
        try:
            logger.info(f"ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹œì‘: {file_path}")

            # 1) ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰
            pre = await self.preprocess_document(
                file_path=file_path,
                file_extension=file_extension,
                container_id=container_id,
                user_emp_no=user_emp_no
            )
            if not pre.get('success'):
                return pre

            cleaned_text = pre.get('cleaned_text', '')
            if not cleaned_text or len(cleaned_text.strip()) < 10:
                logger.warning(f"ì¶”ì¶œ/ì •ì œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ: {len(cleaned_text)} ë¬¸ì")
                return {
                    'success': False,
                    'error': 'ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤',
                    'extracted_text': pre.get('extracted_text', '')
                }

            # 2) ì²­í‚¹ë§Œ ìˆ˜í–‰
            ch = self.chunk_text(
                cleaned_text,
                file_path=file_path,
                container_id=container_id,
                user_emp_no=user_emp_no
            )
            if not ch.get('success'):
                return ch

            logger.info(f"ë¬¸ì„œ ì „ì²˜ë¦¬ ì™„ë£Œ: {ch.get('total_chunks', 0)}ê°œ ì²­í¬ ìƒì„±")
            # ê¸°ì¡´ ë°˜í™˜ êµ¬ì¡° + extracted_text í¬í•¨
            result_payload = {
                'success': True,
                'chunks': ch.get('chunks', []),
                'metadata': ch.get('metadata', []),
                'total_chunks': ch.get('total_chunks', 0),
                'total_tokens': ch.get('total_tokens', 0),
                'extracted_text': pre.get('extracted_text', ''),
                'extraction_metadata': pre.get('extraction_metadata', {})
            }

            # Azure Blob intermediate ì €ì¥ (ì˜µì…˜)
            try:
                if settings.storage_backend == 'azure_blob' and get_azure_blob_service and build_intermediate_page_key:
                    azure = get_azure_blob_service()
                    # file_bss_info_snoëŠ” ì•„ì§ ëª¨ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 0 placeholder (ì¶”í›„ dual-write ì—°ê²° í›„ êµì²´ ê°€ëŠ¥)
                    file_id_placeholder = result_payload['extraction_metadata'].get('file_id') or 0
                    
                    # 1. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥
                    full_text_key = f"{container_id}/{file_id_placeholder}/full_text.txt"
                    full_text_bytes = result_payload['extracted_text'].encode('utf-8')
                    azure.upload_bytes(full_text_bytes, full_text_key, purpose='intermediate')
                    logger.info(f"[BLOB] ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥: {full_text_key} ({len(full_text_bytes)} bytes)")
                    
                    # 2. í˜ì´ì§€/ìŠ¬ë¼ì´ë“œ/ì‹œíŠ¸ ì •ë³´ê°€ extraction_metadata ì•ˆì— êµ¬ì¡°í™” ë˜ì–´ ìˆë‹¤ë©´ í˜ì´ì§€ë³„ ì €ì¥
                    pages = result_payload['extraction_metadata'].get('pages') or []
                    slides = result_payload['extraction_metadata'].get('slides') or []
                    sheets = result_payload['extraction_metadata'].get('sheets') or []
                    
                    # PDF í˜ì´ì§€ë³„ ì €ì¥
                    for page in pages:
                        pno = page.get('page_no') or page.get('page_number') or 0
                        key = build_intermediate_page_key(container_id, file_id_placeholder, int(pno))
                        azure.upload_bytes(json.dumps(page, ensure_ascii=False).encode('utf-8'), key, purpose='intermediate')
                    
                    # PPTX ìŠ¬ë¼ì´ë“œë³„ ì €ì¥
                    for slide in slides:
                        sno = slide.get('slide_no') or 0
                        key = f"{container_id}/{file_id_placeholder}/slide_{sno}.json"
                        azure.upload_bytes(json.dumps(slide, ensure_ascii=False).encode('utf-8'), key, purpose='intermediate')
                    
                    # XLSX ì‹œíŠ¸ë³„ ì €ì¥
                    for sheet in sheets:
                        sheet_no = sheet.get('sheet_no') or 0
                        sheet_name = sheet.get('sheet_name', f'sheet_{sheet_no}')
                        key = f"{container_id}/{file_id_placeholder}/sheet_{sheet_no}_{sheet_name}.json"
                        azure.upload_bytes(json.dumps(sheet, ensure_ascii=False).encode('utf-8'), key, purpose='intermediate')
                    
                    # 3. ìš”ì•½ ì •ë³´ ì €ì¥
                    if build_intermediate_extraction_summary_key:
                        summary_key = build_intermediate_extraction_summary_key(container_id, file_id_placeholder)
                        summary_doc = {
                            'page_count': len(pages),
                            'slide_count': len(slides),
                            'sheet_count': len(sheets),
                            'container_id': container_id,
                            'original_file': Path(file_path).name,
                            'total_chars': len(result_payload['extracted_text']),
                            'extraction_method': result_payload['extraction_metadata'].get('extraction_method', 'unknown')
                        }
                        azure.upload_bytes(json.dumps(summary_doc, ensure_ascii=False).encode('utf-8'), summary_key, purpose='intermediate')
                    
                    logger.info(f"[BLOB] ì¤‘ê°„ ì‚°ì¶œë¬¼ ì €ì¥ ì™„ë£Œ - í˜ì´ì§€:{len(pages)}, ìŠ¬ë¼ì´ë“œ:{len(slides)}, ì‹œíŠ¸:{len(sheets)}")
            except Exception as e_blob:
                logger.warning(f"[PREPROCESS] Intermediate Blob ì—…ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e_blob}")

            return result_payload
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ ë° ì •ê·œí™”"""
        if not text:
            return ""
        
        # ê¸°ë³¸ ì •ì œ
        cleaned = text.strip()
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        cleaned = re.sub(r'[^\w\s\.,!?;:()\[\]{}"\'-]', ' ', cleaned)
        
        # ë¬¸ë‹¨ êµ¬ë¶„ ì •ë¦¬
        cleaned = re.sub(r'\n\s*\n', '\n\n', cleaned)
        
        return cleaned.strip()
    
    def _split_into_paragraphs(self, text: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ë¥¼ ë‹¨ë½ìœ¼ë¡œ ë¶„í•  (êµ¬ì¡° ì¸ì‹ í¬í•¨)"""
        if not text or not text.strip():
            return []
        
        # ì œëª©/ë²ˆí˜¸ íŒ¨í„´ ì •ì˜
        heading_patterns = [
            r'^\s*(\d+\.|\d+\))',  # 1., 1), 2., 2) ë“±
            r'^\s*([\uac00-\ud7a3]\.|[\uac00-\ud7a3]\))',  # ê°€., ê°€), ë‚˜., ë‚˜) ë“±
            r'^\s*([IVX]+\.|[IVX]+\))',  # I., I), II., II) ë“± (ë¡œë§ˆ ìˆ«ì)
            r'^\s*(ì œ\s*\d+\s*[ì¥ì ˆí•­ê´€])',  # ì œ1ì¥, ì œ2ì ˆ, ì œ3í•­ ë“±
            r'^\s*(â– |â—|â€¢|\*|\-)',  # ë¶ˆë¦¿ ê¸°í˜¸
            r'^\s*([\[\(]\d+[\]\)])',  # [1], (1) ë“±
        ]
        
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ(\n\n)ìœ¼ë¡œ ë‹¨ë½ ë¶„í• 
        raw_paragraphs = re.split(r'\n\s*\n', text)
        
        structured_paragraphs = []
        for para in raw_paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # ì œëª©/ë²ˆí˜¸ íŒ¨í„´ í™•ì¸
            is_heading = False
            heading_type = None
            
            for i, pattern in enumerate(heading_patterns):
                if re.match(pattern, para, re.MULTILINE):
                    is_heading = True
                    heading_type = ['numbered', 'alphabetic', 'roman', 'chapter', 'bullet', 'bracketed'][i]
                    break
            
            # ì§§ì€ í…ìŠ¤íŠ¸ + ë§ˆì¹¨í‘œë¡œ ëë‚˜ë©´ ì œëª©ìœ¼ë¡œ ê°„ì£¼
            if not is_heading and len(para) < 100 and (para.endswith(':') or para.endswith('ìœ¼ë¡œ') or para.endswith('ëŠ”')):
                is_heading = True
                heading_type = 'inferred'
            
            structured_paragraphs.append({
                'text': para,
                'is_heading': is_heading,
                'heading_type': heading_type,
                'char_count': len(para),
                'token_count': len(self.tokenizer.encode(para))
            })
        
        logger.info(f"ë‹¨ë½ ë¶„í•  ì™„ë£Œ: {len(structured_paragraphs)}ê°œ (ì œëª©: {sum(1 for p in structured_paragraphs if p['is_heading'])}ê°œ)")
        return structured_paragraphs
    
    def _smart_chunk_text(self, text: str) -> List[str]:
        """ê°œì„ ëœ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ - ë‹¨ë½ ìš°ì„ , ê²¹ì¹¨, ìµœì†Œ/ìµœëŒ€ í¬ê¸°, í•œêµ­ì–´ ë¶„í•  ì§€ì›"""
        if not text:
            return []
        
        # ì „ì²´ í† í° ìˆ˜ ê³„ì‚°
        total_tokens = len(self.tokenizer.encode(text))
        logger.info(f"ì „ì²´ í…ìŠ¤íŠ¸ í† í° ìˆ˜: {total_tokens}")
        
        # ìµœì†Œ í¬ê¸° ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if total_tokens <= self.min_tokens_per_chunk:
            logger.warning(f"í…ìŠ¤íŠ¸ê°€ ìµœì†Œ í¬ê¸° ì´í•˜: {total_tokens} < {self.min_tokens_per_chunk}")
            return [text]
        
        # ëª©í‘œ í¬ê¸° ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if total_tokens <= self.target_tokens_per_chunk:
            return [text]
        
        # 1ë‹¨ê³„: ë‹¨ë½ìœ¼ë¡œ ë¶„í•  (êµ¬ì¡° ì¸ì‹)
        paragraphs = self._split_into_paragraphs(text)
        
        if not paragraphs:
            # ë‹¨ë½ ë¶„í•  ì‹¤íŒ¨ ì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ í´ë°±
            return self._chunk_by_sentences(text)
        
        # 2ë‹¨ê³„: ë‹¨ë½ ê¸°ë°˜ ì²­í‚¹
        chunks = []
        current_chunk = ""
        current_tokens = 0
        current_paragraphs = []  # í˜„ì¬ ì²­í¬ì— í¬í•¨ëœ ë‹¨ë½ë“¤
        
        for i, para_info in enumerate(paragraphs):
            para_text = para_info['text']
            para_tokens = para_info['token_count']
            is_heading = para_info['is_heading']
            
            # ë‹¨ë½ì´ ëª©í‘œ í¬ê¸°ë³´ë‹¤ í¬ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            if para_tokens > self.target_tokens_per_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk.strip() and current_tokens >= self.min_tokens_per_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                    current_tokens = 0
                    current_paragraphs = []
                
                # í° ë‹¨ë½ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                para_chunks = self._split_paragraph_to_chunks(para_text)
                chunks.extend(para_chunks)
                continue
            
            # ì œëª©ì´ ë‚˜íƒ€ë‚¬ê³  í˜„ì¬ ì²­í¬ê°€ ìˆìœ¼ë©´ ìƒˆ ì²­í¬ ì‹œì‘
            if is_heading and current_chunk.strip() and current_tokens >= self.min_tokens_per_chunk:
                chunks.append(current_chunk.strip())
                # ê²¹ì¹¨ ì²˜ë¦¬
                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + "\n\n" + para_text if overlap_text else para_text
                current_tokens = len(self.tokenizer.encode(current_chunk))
                current_paragraphs = [para_info]
                continue
            
            # ë‹¨ë½ ì¶”ê°€ ì‹œ ëª©í‘œ í¬ê¸° ì´ˆê³¼ í™•ì¸
            if current_tokens + para_tokens > self.target_tokens_per_chunk:
                # í˜„ì¬ ì²­í¬ê°€ ìµœì†Œ í¬ê¸° ì´ìƒì´ë©´ ì €ì¥
                if current_tokens >= self.min_tokens_per_chunk and current_chunk.strip():
                    chunks.append(current_chunk.strip())
                    
                    # ê²¹ì¹¨ ì²˜ë¦¬
                    overlap_text = self._get_overlap_text(current_chunk)
                    current_chunk = overlap_text + "\n\n" + para_text if overlap_text else para_text
                    current_tokens = len(self.tokenizer.encode(current_chunk))
                    current_paragraphs = [para_info]
                else:
                    # ìµœì†Œ í¬ê¸° ë¯¸ë§Œì´ë©´ ê³„ì† ì¶”ê°€
                    current_chunk += "\n\n" + para_text if current_chunk else para_text
                    current_tokens += para_tokens
                    current_paragraphs.append(para_info)
            else:
                # ë‹¨ë½ ì¶”ê°€
                current_chunk += "\n\n" + para_text if current_chunk else para_text
                current_tokens += para_tokens
                current_paragraphs.append(para_info)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            chunk_tokens = len(self.tokenizer.encode(current_chunk))
            if chunk_tokens >= self.min_tokens_per_chunk:
                chunks.append(current_chunk.strip())
            elif chunks:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì— ë³‘í•©
                chunks[-1] = chunks[-1] + "\n\n" + current_chunk.strip()
                logger.info(f"ë§ˆì§€ë§‰ ì‘ì€ ì²­í¬ë¥¼ ì´ì „ ì²­í¬ì— ë³‘í•© ({chunk_tokens} í† í°)")
            else:
                chunks.append(current_chunk.strip())  # ìœ ì¼í•œ ì²­í¬ë©´ ê·¸ëŒ€ë¡œ ì €ì¥
        
        # ì²­í¬ ê²€ì¦ ë° ì´ˆê³¼ í¬ê¸° ì²˜ë¦¬
        validated_chunks = []
        for i, chunk in enumerate(chunks):
            chunk_tokens = len(self.tokenizer.encode(chunk))
            
            if chunk_tokens > self.max_tokens_per_chunk:
                logger.warning(f"ì²­í¬ {i}ê°€ ìµœëŒ€ ì œí•œ ì´ˆê³¼: {chunk_tokens} > {self.max_tokens_per_chunk} - ë¶„í• ")
                # ë¬¸ì¥ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ë¶„í•  ì‹œë„
                sub_chunks = self._split_large_chunk(chunk)
                validated_chunks.extend(sub_chunks)
            else:
                validated_chunks.append(chunk)
        
        avg_tokens = sum(len(self.tokenizer.encode(c)) for c in validated_chunks) // len(validated_chunks) if validated_chunks else 0
        logger.info(f"ë‹¨ë½ ê¸°ë°˜ ì²­í‚¹ ì™„ë£Œ: {len(validated_chunks)}ê°œ ì²­í¬ (í‰ê·  {avg_tokens} í† í°)")
        return validated_chunks
    
    def _split_paragraph_to_chunks(self, paragraph: str) -> List[str]:
        """í° ë‹¨ë½ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²­í¬ ìƒì„±"""
        sentences = self._split_into_sentences(paragraph)
        
        if not sentences:
            return [paragraph]
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = len(self.tokenizer.encode(sentence))
            
            if current_tokens + sentence_tokens > self.target_tokens_per_chunk:
                if current_chunk.strip() and current_tokens >= self.min_tokens_per_chunk:
                    chunks.append(current_chunk.strip())
                    # ê²¹ì¹¨ ì²˜ë¦¬
                    overlap_text = self._get_overlap_text(current_chunk)
                    current_chunk = overlap_text + " " + sentence if overlap_text else sentence
                    current_tokens = len(self.tokenizer.encode(current_chunk))
                else:
                    current_chunk += " " + sentence if current_chunk else sentence
                    current_tokens += sentence_tokens
            else:
                current_chunk += " " + sentence if current_chunk else sentence
                current_tokens += sentence_tokens
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [paragraph]
    
    def _chunk_by_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ê¸°ë°˜ ì²­í‚¹ (ë‹¨ë½ ë¶„í•  ì‹¤íŒ¨ ì‹œ í´ë°±)"""
        sentences = self._split_into_sentences(text)
        
        if not sentences:
            return [text]
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = len(self.tokenizer.encode(sentence))
            
            if current_tokens + sentence_tokens > self.target_tokens_per_chunk:
                if current_chunk.strip() and current_tokens >= self.min_tokens_per_chunk:
                    chunks.append(current_chunk.strip())
                    overlap_text = self._get_overlap_text(current_chunk)
                    current_chunk = overlap_text + " " + sentence if overlap_text else sentence
                    current_tokens = len(self.tokenizer.encode(current_chunk))
                else:
                    current_chunk += " " + sentence if current_chunk else sentence
                    current_tokens += sentence_tokens
            else:
                current_chunk += " " + sentence if current_chunk else sentence
                current_tokens += sentence_tokens
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [text]
    
    def _get_overlap_text(self, chunk: str) -> str:
        """ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œ ê²¹ì¹¨ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        tokens = self.tokenizer.encode(chunk)
        if len(tokens) <= self.overlap_tokens:
            return chunk  # ì²­í¬ê°€ ê²¹ì¹¨ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ì „ì²´ ë°˜í™˜
        
        overlap_start = len(tokens) - self.overlap_tokens
        overlap_tokens_list = tokens[overlap_start:]
        overlap_text = self.tokenizer.decode(overlap_tokens_list)
        return overlap_text.strip()
    
    def _split_large_chunk(self, chunk: str) -> List[str]:
        """í° ì²­í¬ë¥¼ ë¬¸ì¥ ê²½ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ë¶„í• """
        # ë¨¼ì € ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„
        sentences = self._split_into_sentences(chunk)
        
        if len(sentences) <= 1:
            # ë¬¸ì¥ì´ í•˜ë‚˜ë¿ì´ë©´ ê°•ì œ í† í° ë¶„í• 
            return self._force_split_chunk(chunk)
        
        # ë¬¸ì¥ì„ ë¬¶ì–´ì„œ ì²­í¬ ìƒì„±
        sub_chunks = []
        current_sub = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = len(self.tokenizer.encode(sentence))
            
            if current_tokens + sentence_tokens > self.max_tokens_per_chunk:
                if current_sub.strip():
                    sub_chunks.append(current_sub.strip())
                current_sub = sentence
                current_tokens = sentence_tokens
            else:
                current_sub += " " + sentence if current_sub else sentence
                current_tokens += sentence_tokens
        
        if current_sub.strip():
            sub_chunks.append(current_sub.strip())
        
        return sub_chunks if sub_chunks else self._force_split_chunk(chunk)
    
    def _force_split_chunk(self, chunk: str) -> List[str]:
        """í† í° ì œí•œì„ ì´ˆê³¼í•œ ì²­í¬ë¥¼ ê°•ì œë¡œ ë¶„í• """
        tokens = self.tokenizer.encode(chunk)
        sub_chunks = []
        
        for i in range(0, len(tokens), self.max_tokens_per_chunk):
            sub_tokens = tokens[i:i + self.max_tokens_per_chunk]
            sub_text = self.tokenizer.decode(sub_tokens)
            if sub_text.strip():
                sub_chunks.append(sub_text.strip())
        
        return sub_chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """ê°œì„ ëœ í…ìŠ¤íŠ¸ ë¬¸ì¥ ë¶„í•  - kss ìš°ì„ , í´ë°± íŒ¨í„´ ì‚¬ìš©"""
        if not text or not text.strip():
            return []
        
        # kss ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (í•œêµ­ì–´ ìµœì í™”)
        if self.use_kss and self.kss:
            try:
                sentences = self.kss.split_sentences(text)
                if sentences:
                    # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
                    sentences = [s.strip() for s in sentences if s.strip()]
                    if sentences:
                        return sentences
            except Exception as e:
                logger.warning(f"kss ë¬¸ì¥ ë¶„í•  ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {e}")
        
        # í´ë°±: ê°œì„ ëœ ì •ê·œì‹ íŒ¨í„´
        # í•œêµ­ì–´ ì¢…ê²°ì–´ë¯¸ ê³ ë ¤
        korean_endings = r'(ë‹¤|ìš”|ê¹Œ|ë„¤|ì§€|ì•¼|ì–´|ì•„|ì£ |ã…‚ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|ã…‚ë‹ˆê¹Œ|ìŠµë‹ˆê¹Œ)\.'
        # ì˜ì–´ ë¬¸ì¥ ë (ì•½ì–´ ì œì™¸)
        english_endings = r'(?<![A-Z])[.!?]+(?=\s+[A-Zê°€-í£])'
        # ì¤‘ì¼ ë¬¸ì¥ ë
        cjk_endings = r'[ã€‚ï¼ï¼Ÿ]+'
        
        # í†µí•© íŒ¨í„´
        pattern = f'{korean_endings}|{english_endings}|{cjk_endings}'
        
        sentences = []
        parts = re.split(f'({pattern})', text)
        
        # êµ¬ë¶„ìë¥¼ í¬í•¨í•˜ì—¬ ë¬¸ì¥ ì¬êµ¬ì„±
        current_sentence = ""
        for i, part in enumerate(parts):
            if not part or not part.strip():
                continue
            current_sentence += part
            # êµ¬ë¶„ìì¸ ê²½ìš° ë¬¸ì¥ ì™„ì„±
            if re.match(pattern, part):
                if current_sentence.strip():
                    sentences.append(current_sentence.strip())
                current_sentence = ""
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì¶”ê°€
        if current_sentence.strip():
            sentences.append(current_sentence.strip())
        
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì¤„ë°”ê¿ˆ ê¸°ì¤€ ë¶„í• 
        if not sentences:
            sentences = [s.strip() for s in text.split('\n') if s.strip()]
        
        # ìµœì¢… ì•ˆì „ ì¥ì¹˜: ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
        if not sentences:
            sentences = [text.strip()]
        
        return sentences
    
    def _create_chunk_metadata(
        self, 
        chunk: str, 
        chunk_index: int, 
        total_chunks: int,
        file_path: str,
        container_id: str,
        user_emp_no: str
    ) -> Dict[str, Any]:
        """ì²­í¬ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        chunk_tokens = len(self.tokenizer.encode(chunk))
        chunk_hash = hashlib.md5(chunk.encode()).hexdigest()
        
        return {
            'chunk_index': chunk_index,
            'chunk_type': self._determine_chunk_type(chunk),
            'token_count': chunk_tokens,
            'char_count': len(chunk),
            'content_hash': chunk_hash,
            'korean_keywords': self._extract_korean_keywords(chunk),
            'container_id': container_id,
            'file_name': Path(file_path).name,
            'created_by': user_emp_no
        }
    
    def _determine_chunk_type(self, chunk: str) -> str:
        """ì²­í¬ ìœ í˜• íŒë³„"""
        if len(chunk) < 100:
            return "title"
        elif re.search(r'^[0-9]+\.', chunk):
            return "section_header"
        elif re.search(r'[ê·¸ë¦¼|í‘œ|Figure|Table]', chunk):
            return "figure_caption"
        else:
            return "content"
    
    def _extract_korean_keywords(self, chunk: str) -> List[str]:
        """í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]+', chunk)
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì„ íƒ
        word_freq = {}
        for word in korean_words:
            if len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ ì„ íƒ
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        return [word for word, freq in keywords]

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
document_preprocessing_service = DocumentPreprocessingService()

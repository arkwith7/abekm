"""
Bibliographic metadata extraction and persistence for academic papers.
Lightweight heuristics now; can be improved later.
"""
import re
import logging
from typing import Dict, Any, List, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from app.models import (
    TbAcademicDocumentMetadata,
)

logger = logging.getLogger(__name__)


DOI_PATTERN = re.compile(r"10\.\d{4,}/\S+", re.IGNORECASE)


class BibliographyService:
    def __init__(self, db: AsyncSession):
        self.db = db

    async def upsert_document_metadata(
        self,
        file_bss_info_sno: int,
        full_text: str,
        sections_summary: Optional[Dict[str, Any]] = None,
        first_page_text: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Extract minimal metadata (title, abstract, doi, year) and upsert into tb_academic_document_metadata.
        """
        logger.info(f"[BIBLIO] ğŸ“š upsert_document_metadata í˜¸ì¶œ: file_bss_info_sno={file_bss_info_sno}")
        try:
            title = self._guess_title(full_text, first_page_text)
            abstract = self._get_abstract_from_sections(sections_summary, full_text)
            doi = self._extract_doi(full_text)
            year = self._extract_year(full_text)
            
            logger.info(f"[BIBLIO] ğŸ” ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°:")
            logger.info(f"  - title: {title[:80] if title else None}...")
            logger.info(f"  - abstract: {'ìˆìŒ' if abstract else 'ì—†ìŒ'} ({len(abstract) if abstract else 0}ì)")
            logger.info(f"  - doi: {doi}")
            logger.info(f"  - year: {year}")

            # Idempotent upsert strategy
            # 1) Prefer matching by DOI (unique). If found, update fields in-place (do NOT change PK file_bss_info_sno).
            # 2) If no DOI or not found, upsert by file_bss_info_sno.
            row = None

            if doi:
                res_by_doi = await self.db.execute(
                    select(TbAcademicDocumentMetadata).where(TbAcademicDocumentMetadata.doi == doi)
                )
                row = res_by_doi.scalar_one_or_none()
                if row:
                    logger.info("[BIBLIO] ğŸ”„ ê¸°ì¡´ DOI ë ˆì½”ë“œ ë°œê²¬ â€“ ë³‘í•© ì—…ë°ì´íŠ¸ ì§„í–‰")
                    self._merge_metadata_fields(row, title=title, abstract=abstract, year=year)
                else:
                    # No row with this DOI exists; check by PK to avoid duplicate PK insert
                    res_by_pk = await self.db.execute(
                        select(TbAcademicDocumentMetadata).where(
                            TbAcademicDocumentMetadata.file_bss_info_sno == file_bss_info_sno
                        )
                    )
                    row = res_by_pk.scalar_one_or_none()
                    if row is None:
                        logger.info("[BIBLIO] ğŸ†• ì‹ ê·œ ë©”íƒ€ë°ì´í„° INSERT (PK=íŒŒì¼, DOI ì§€ì •)")
                        row = TbAcademicDocumentMetadata(
                            file_bss_info_sno=file_bss_info_sno,
                            title=title,
                            abstract=abstract,
                            doi=doi,
                            year=year,
                        )
                        self.db.add(row)
                    else:
                        logger.info("[BIBLIO] âœï¸ ê¸°ì¡´ PK ë ˆì½”ë“œì— DOI/í•„ë“œ ì—…ë°ì´íŠ¸")
                        self._merge_metadata_fields(row, title=title, abstract=abstract, year=year, doi=doi)
            else:
                # No DOI extracted â€“ fallback to file PK upsert
                res_by_pk = await self.db.execute(
                    select(TbAcademicDocumentMetadata).where(
                        TbAcademicDocumentMetadata.file_bss_info_sno == file_bss_info_sno
                    )
                )
                row = res_by_pk.scalar_one_or_none()
                if row is None:
                    logger.info("[BIBLIO] ğŸ†• DOI ì—†ìŒ â€“ ì‹ ê·œ ë©”íƒ€ë°ì´í„° INSERT (PK=íŒŒì¼)")
                    row = TbAcademicDocumentMetadata(
                        file_bss_info_sno=file_bss_info_sno,
                        title=title,
                        abstract=abstract,
                        doi=None,
                        year=year,
                    )
                    self.db.add(row)
                else:
                    logger.info("[BIBLIO] âœï¸ DOI ì—†ìŒ â€“ ê¸°ì¡´ PK ë ˆì½”ë“œ ì—…ë°ì´íŠ¸")
                    self._merge_metadata_fields(row, title=title, abstract=abstract, year=year)

            await self.db.commit()
            logger.info(f"[BIBLIO] âœ… DB ì»¤ë°‹ ì„±ê³µ: file_bss_info_sno={file_bss_info_sno}")
            return {"success": True, "file_bss_info_sno": file_bss_info_sno, "doi": doi, "year": year, "title": title}
        except Exception as e:
            await self.db.rollback()
            logger.error(f"[BIBLIO] âŒ upsert ì‹¤íŒ¨: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _merge_metadata_fields(
        self,
        row: "TbAcademicDocumentMetadata",
        *,
        title: Optional[str] = None,
        abstract: Optional[str] = None,
        year: Optional[str] = None,
        doi: Optional[str] = None,
    ) -> None:
        """Merge non-empty fields into existing row without overwriting non-empty values."""
        if title and not getattr(row, "title", None):
            setattr(row, "title", title)
        if abstract and not getattr(row, "abstract", None):
            setattr(row, "abstract", abstract)
        if year and not getattr(row, "year", None):
            setattr(row, "year", year)
        if doi and not getattr(row, "doi", None):
            setattr(row, "doi", doi)

    def _extract_doi(self, text: str) -> Optional[str]:
        if not text:
            return None
        m = DOI_PATTERN.search(text)
        return m.group(0) if m else None

    def _extract_year(self, text: str) -> Optional[str]:
        # naive: first occurrence of 20xx or 19xx
        m = re.search(r"\b(20\d{2}|19\d{2})\b", text)
        return m.group(1) if m else None

    def _guess_title(self, full_text: str, first_page_text: Optional[str]) -> Optional[str]:
        """
        í•™ìˆ ë…¼ë¬¸ ì œëª© ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
        - "[í˜ì´ì§€ N]" ê°™ì€ í—¤ë” ì œì™¸
        - ì œëª©ìœ¼ë¡œ ë³´ì´ëŠ” ì¤„ ì°¾ê¸° (ì ì ˆí•œ ê¸¸ì´, ëŒ€ë¬¸ì ì‹œì‘ ë“±)
        """
        source = first_page_text or full_text
        if not source:
            return None
        
        lines = source.splitlines()

        def looks_like_journal_info(s: str) -> bool:
            s_lower = s.lower()
            journal_keywords = [
                "journal", "issn", "volume", "vol.", "vol ", "no.", "no ", "publisher", "proceedings",
                "í•™íšŒ", "í•™íšŒì§€", "å­¸æœƒ", "å­¸æœƒèªŒ", "doi:", "kits", "society"
            ]
            if any(k in s_lower for k in journal_keywords):
                return True
            # dates and issue-like patterns
            if re.search(r"\b(19|20)\d{2}\b", s):  # year present in same line â€“ often journal header
                if re.search(r"\b(no\.|vol\.|issue|pp\.|pages)\b", s_lower):
                    return True
            # mostly digits/symbols
            letters = sum(ch.isalpha() for ch in s)
            nonletters = max(1, len(s) - letters)
            if letters / nonletters < 0.3:
                return True
            # explicit known words seen in logs
            if "itì„œë¹„ìŠ¤" in s_lower or "æœå‹™" in s:
                return True
            return False
        for i, line in enumerate(lines):
            l = line.strip()
            
            # í—¤ë” íŒ¨í„´ ì œì™¸
            if l.startswith('[í˜ì´ì§€') or l.startswith('[Page'):
                continue
            if re.match(r'^\d+\s*$', l):  # ë‹¨ìˆœ í˜ì´ì§€ ë²ˆí˜¸
                continue
            if len(l) < 10:  # ë„ˆë¬´ ì§§ì€ ì¤„ ì œì™¸
                continue
            if len(l) > 300:  # ë„ˆë¬´ ê¸´ ì¤„ ì œì™¸
                continue
            
            # ì œëª©ìœ¼ë¡œ ë³´ì´ëŠ” ì¡°ê±´
            # 1. ì²« ê¸€ìê°€ ëŒ€ë¬¸ìì´ê±°ë‚˜ í•œê¸€
            # 2. ê¸¸ì´ê°€ ì ë‹¹í•¨ (10-300ì)
            # 3. DOIë‚˜ ì €ë„ ì •ë³´ê°€ ì•„ë‹˜
            if l.lower().startswith('doi:') or l.lower().startswith('journal'):
                continue
            if re.match(r'^\d{4}å¹´|^\d{4}-\d{2}-\d{2}', l):  # ë‚ ì§œ í˜•ì‹ ì œì™¸
                continue
            if looks_like_journal_info(l):
                continue
            
            # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ê±°ë‚˜ í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° ì œëª©ìœ¼ë¡œ ê°„ì£¼
            if l[0].isupper() or any('\uAC00' <= c <= '\uD7A3' for c in l):
                return l
        
        # ëª» ì°¾ìœ¼ë©´ ì²« ë²ˆì§¸ ì ì ˆí•œ ê¸¸ì´ì˜ ì¤„ ë°˜í™˜
        for line in lines:
            l = line.strip()
            if 10 < len(l) < 300 and not l.startswith('[') and not looks_like_journal_info(l):
                return l
        
        return None

    def _get_abstract_from_sections(self, summary: Optional[Dict[str, Any]], full_text: Optional[str] = None) -> Optional[str]:
        """
        sections.json ë˜ëŠ” full_textì—ì„œ abstract ì¶”ì¶œ
        
        ì „ëµ:
        1. sections ë°°ì—´ì—ì„œ type/mapped_type='abstract' ì°¾ê¸°
        2. ì‹¤íŒ¨í•˜ë©´ full_textì—ì„œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì¶”ì¶œ
        """
        if not full_text:
            return None
        
        # ì „ëµ 1: sections ë°°ì—´ì—ì„œ ì°¾ê¸°
        if summary:
            sections = summary.get('sections', [])
            
            for sec in sections:
                sec_type = sec.get('type', '')
                mapped_type = sec.get('mapped_type', '')
                
                # abstractë¡œ ë§¤í•‘ëœ ì„¹ì…˜ ì°¾ê¸°
                if sec_type == 'abstract' or mapped_type == 'abstract':
                    if sec.get('start_pos') is not None and sec.get('end_pos') is not None:
                        start = sec['start_pos']
                        end = sec['end_pos']
                        abstract_text = full_text[start:end].strip()
                        if len(abstract_text) > 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                            return abstract_text
        
        # ì „ëµ 2: full_textì—ì„œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸°
        # "Abstract" í‚¤ì›Œë“œ ë‹¤ìŒë¶€í„° "Introduction", "Keywords", "1." ë“±ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€
        abstract_pattern = re.compile(
            r'\bAbstract\b\s*[:\n](.*?)(?=\b(?:Introduction|Keywords|Background|1\.|2\.|INTRODUCTION|METHODS|Results|DISCUSSION)\b)',
            re.IGNORECASE | re.DOTALL
        )
        
        match = abstract_pattern.search(full_text)
        if match:
            abstract_text = match.group(1).strip()
            # í—¤ë” ì •ë³´ ì œê±° (DOI, ì €ìëª… ë“±)
            # "[í˜ì´ì§€ N]" ê°™ì€ íŒ¨í„´ ì œê±°
            abstract_text = re.sub(r'\[í˜ì´ì§€ \d+\]', '', abstract_text)
            abstract_text = re.sub(r'\[Page \d+\]', '', abstract_text)
            # í‘œ/ì§€í‘œ ì¡ìŒ ì œê±°
            noise_tokens = [
                r"\bAVE\b", r"\bBCI\b", r"P-Value", r"\bDMA\b", r"\bIWB\b", r"\bOLB\b", r"\bCLB\b", r"\bRC\b",
                r"\bOE\b", r"\bWL\b", r"\bMGA\b"
            ]
            abstract_text = re.sub("|".join(noise_tokens), " ", abstract_text, flags=re.IGNORECASE)
            # ì—°ì†ëœ ê³µë°± ì •ë¦¬
            abstract_text = re.sub(r'\s+', ' ', abstract_text).strip()
            
            if len(abstract_text) > 100:  # ìµœì†Œ 100ì ì´ìƒì´ì–´ì•¼ ì‹¤ì œ ì´ˆë¡ìœ¼ë¡œ ê°„ì£¼
                return abstract_text
        
        # ì „ëµ 3: ê°„ë‹¨í•œ fallback - Abstract í‚¤ì›Œë“œ ë‹¤ìŒ 1000ì
        simple_match = re.search(r'\bAbstract\b\s*[:\n]', full_text, re.IGNORECASE)
        if simple_match:
            start_pos = simple_match.end()
            # ë‹¤ìŒ 1000ìë¥¼ ê°€ì ¸ì™€ì„œ ì •ë¦¬
            candidate = full_text[start_pos:start_pos + 1500]
            # "[í˜ì´ì§€ N]" ì´í›„ ì‹œì‘
            page_match = re.search(r'\[í˜ì´ì§€ \d+\]\s*', candidate)
            if page_match:
                candidate = candidate[page_match.end():]
            
            # Introduction ì „ê¹Œì§€ë§Œ
            intro_match = re.search(r'\b(?:Introduction|Keywords|1\.|2\.|Methods|Results|Discussion)\b', candidate, re.IGNORECASE)
            if intro_match:
                candidate = candidate[:intro_match.start()]
            
            # ì¡ìŒ ì œê±° í›„ ê³µë°± ì •ë¦¬
            candidate = re.sub("|".join([r"\bAVE\b", r"\bBCI\b", r"P-Value"]), " ", candidate, flags=re.IGNORECASE)
            candidate = re.sub(r'\s+', ' ', candidate).strip()
            if len(candidate) > 100:
                return candidate[:1000]  # ìµœëŒ€ 1000ìë¡œ ì œí•œ
        
        logger.warning(f"[BIBLIO] Abstract ì¶”ì¶œ ì‹¤íŒ¨ - ëª¨ë“  ì „ëµ ì‹œë„í–ˆìœ¼ë‚˜ ë°œê²¬ ëª»í•¨")
        return None

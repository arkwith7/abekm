"""
üé® Î©ÄÌã∞Î™®Îã¨ Î¨∏ÏÑú Ï≤òÎ¶¨ ÏÑúÎπÑÏä§
============================

ÏÉàÎ°úÏö¥ Î©ÄÌã∞Î™®Îã¨ Ïä§ÌÇ§ÎßàÎ•º ÌôúÏö©Ìïú Î¨∏ÏÑú Ï∂îÏ∂ú/Ï≤≠ÌÇπ/ÏûÑÎ≤†Îî© ÌååÏù¥ÌîÑÎùºÏù∏
- DocExtractionSession: Ï∂îÏ∂ú ÏÑ∏ÏÖò Í¥ÄÎ¶¨
- DocExtractedObject: ÌéòÏù¥ÏßÄÎ≥Ñ Í∞ùÏ≤¥ Ï∂îÏ∂ú (ÌÖçÏä§Ìä∏, Ìëú, Ïù¥ÎØ∏ÏßÄ)
- DocChunkSession: Ï≤≠ÌÇπ ÏÑ∏ÏÖò Í¥ÄÎ¶¨
- DocChunk: Ï≤≠ÌÅ¨ Ï†ÄÏû•
- DocEmbedding: ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ï†ÄÏû•
- Azure Blob Storage: Ï∂îÏ∂ú Í≤∞Í≥º Î∞è Ï≤òÎ¶¨ ÏïÑÌã∞Ìå©Ìä∏ Ï†ÄÏû•
"""

import logging
import hashlib
import json
import time
import os
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Set
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, text, type_coerce
from sqlalchemy.dialects.postgresql import INT4RANGE

from app.models.document.multimodal_models import (
    DocExtractionSession,
    DocExtractedObject,
    DocChunkSession,
    DocChunk,
    DocEmbedding
)
from app.models import TbDocumentSearchIndex
from app.models.document.file_models import TbFileBssInfo
from app.models.document.vector_models import VsDocContentsChunks
from app.services.document.extraction.text_extractor_service import text_extractor_service
from app.services.core.korean_nlp_service import korean_nlp_service
from app.core.config import settings
from app.services.document.chunking.advanced_chunker import advanced_chunk_text
from app.services.document.extraction.adaptive_section_detector import AdaptiveSectionDetector
from app.services.document.storage.search_index_store import SearchIndexStoreService

# Azure Blob Storage ÌÜµÌï©
try:
    from app.services.core.azure_blob_service import get_azure_blob_service
except ImportError:
    get_azure_blob_service = None

# Ïù¥ÎØ∏ÏßÄ ÌäπÏßï Ï∂îÏ∂ú ÏÑúÎπÑÏä§
try:
    from app.services.document.vision.image_embedding_service import (
        image_embedding_service as default_image_embedding_service,
    )
except ImportError:
    default_image_embedding_service = None

logger = logging.getLogger(__name__)

class MultimodalDocumentService:
    """Î©ÄÌã∞Î™®Îã¨ Î¨∏ÏÑú Ï≤òÎ¶¨ ÏÑúÎπÑÏä§"""
    
    def __init__(self, image_embedding_service: Optional[Any] = None):
        """ÏÑúÎπÑÏä§ Ï¥àÍ∏∞Ìôî"""
        self.search_index_service = SearchIndexStoreService()
        # Ïù∏Ïä§ÌÑ¥Ïä§ Ï£ºÏûÖÏù¥ ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ Ï†ÑÏó≠ ÏÑúÎπÑÏä§Î•º ÏÇ¨Ïö©
        self.image_embedding_service = image_embedding_service or default_image_embedding_service
        # Ï†ÅÏùëÌòï ÏÑπÏÖò Í∞êÏßÄ ÏÑúÎπÑÏä§ (Î™®Îì† Ìó§Îçî Í∞êÏßÄ + ÏùòÎØ∏ Îß§Ìïë)
        self.section_detector = AdaptiveSectionDetector()
    
    async def process_document_multimodal(
        self,
        file_path: str,
        file_bss_info_sno: int,
        container_id: str,
        user_emp_no: str,
        session: AsyncSession,
        provider: Optional[str] = None,
        model_profile: str = "default",
        processing_options: Optional[Dict[str, Any]] = None,
        document_type: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Î©ÄÌã∞Î™®Îã¨ ÌååÏù¥ÌîÑÎùºÏù∏ (Î¶¨Ìå©ÌÑ∞ Î≤ÑÏ†Ñ)

        Îã®Í≥Ñ:
        1) Ï∂îÏ∂ú ÏÑ∏ÏÖò + Í∞ùÏ≤¥ Ï†ÄÏû•
        2) Í≥†Í∏â Ï≤≠ÌÇπ(Î¨∏Îã®/ÌÜ†ÌÅ∞ Í∏∞Î∞ò)
        3) ÏûÑÎ≤†Îî© ÏÉùÏÑ± (Ï†úÎ°ú Ìå®Îî©)
        4) Í≤ÄÏÉâ Ïù∏Îç±Ïä§ Î≥¥Í∞ï
        5) ÌÜµÍ≥Ñ Î∞òÌôò
        """
        started = datetime.now()
        result: Dict[str, Any] = {
            "success": False,
            "extraction_session_id": None,
            "chunk_session_id": None,
            "objects_count": 0,
            "chunks_count": 0,
            "embeddings_count": 0,
            "stats": {},
            "error": None,
            "stages": []
        }
        stage_timers: Dict[str, float] = {}

        provider = provider or settings.get_current_llm_provider()
        processing_options = processing_options or {}
        pipeline_type = processing_options.get("pipeline_type") or provider or settings.default_llm_provider
        document_type_normalized = (document_type or processing_options.get("document_type") or "").lower()
        section_chunking_requested = processing_options.get("section_chunking_enabled", True)
        apply_section_chunking = document_type_normalized == "academic_paper" and section_chunking_requested
        section_chunking_meta: Dict[str, Any] = {
            "requested": bool(apply_section_chunking),
            "enabled": False,
            "detected_sections": [],
            "chunk_counts": {},
            "stored_to_blob": False,
        }
        precomputed_sections_info: List[Dict[str, Any]] = []
        precomputed_section_summary: Optional[Dict[str, Any]] = None
        visual_page_filter: Optional[Set[int]] = None
        section_combined_text: str = ""
        section_object_spans: List[Tuple[DocExtractedObject, int, int]] = []
        image_ids_with_binary: Set[int] = set()
        image_object_ids_seen: Set[int] = set()

        def _start_stage(name: str):
            stage_timers[name] = time.perf_counter()
            logger.info(f"[MULTIMODAL][TIMER] {name} stage started")

        def _stage(name: str, success: bool, **extra):
            elapsed = None
            if name in stage_timers:
                elapsed = time.perf_counter() - stage_timers.pop(name)
                extra.setdefault("elapsed_seconds", elapsed)
            result["stages"].append({"name": name, "success": success, **extra})
            if elapsed is not None:
                logger.info(f"[MULTIMODAL][TIMER] {name} stage completed in {elapsed:.2f}s (success={success})")
            else:
                logger.info(f"[MULTIMODAL][TIMER] {name} stage completed (success={success})")

        try:
            # -----------------------------
            # 1. Extraction
            # -----------------------------
            _start_stage("extraction_setup")
            extraction_session = DocExtractionSession(
                file_bss_info_sno=file_bss_info_sno,
                provider=provider,
                model_profile=model_profile,
                pipeline_type=pipeline_type,
                status="running",
                started_at=datetime.now()
            )
            session.add(extraction_session)
            await session.flush()
            result["extraction_session_id"] = extraction_session.extraction_session_id
            _stage("extraction_setup", True, extraction_session_id=extraction_session.extraction_session_id)
            logger.info(f"[MULTIMODAL] Extraction session started: {extraction_session.extraction_session_id}")

            _start_stage("extraction")
            extraction_result = await text_extractor_service.extract_text_from_file(file_path)
            if not extraction_result.get("success"):
                # Î™®Îç∏ ÌïÑÎìúÏóê ÏßÅÏ†ë Ìï†Îãπ Ïãú Ï†ïÏ†Å ÌÉÄÏûÖ Í≤ΩÍ≥† ÌöåÌîº ÏúÑÌï¥ setattr ÏÇ¨Ïö©
                setattr(extraction_session, "status", "failed")
                setattr(extraction_session, "error_message", extraction_result.get("error"))
                setattr(extraction_session, "completed_at", datetime.now())
                await session.commit()
                _stage("extraction", False, error=extraction_result.get("error"))
                result["error"] = extraction_result.get("error")
                return result

            # Ïã§Ï†ú ÌååÏùº Í≤ΩÎ°ú ÌôïÎ≥¥ (Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂úÏö©)
            actual_file_path = extraction_result.get("actual_file_path", file_path)
            is_temp_file = extraction_result.get("is_temp_file", False)
            
            metadata = extraction_result.get("metadata", {})
            extracted_objects: List[DocExtractedObject] = []
            
            def _add_text_obj(page_no: Optional[int], text_val: str):
                if not text_val or not text_val.strip():
                    return
                extracted_objects.append(
                    DocExtractedObject(
                        extraction_session_id=extraction_session.extraction_session_id,
                        file_bss_info_sno=file_bss_info_sno,
                        page_no=page_no,
                        object_type="TEXT_BLOCK",
                        sequence_in_page=0,
                        content_text=text_val,
                        char_count=len(text_val),
                        token_estimate=len(text_val.split()),
                        hash_sha256=hashlib.sha256(text_val.encode()).hexdigest(),
                    )
                )

            # Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Ïû¨Ï°∞Î¶Ω Ìó¨Ìçº (fallback)
            def _assemble_full_text(objs: List[DocExtractedObject]) -> str:
                try:
                    return "\n\n".join([
                        (o.content_text or "").strip()
                        for o in objs
                        if getattr(o, 'object_type', None) == 'TEXT_BLOCK' and (o.content_text or '').strip()
                    ])
                except Exception as e:
                    logger.warning(f"[MULTIMODAL] _assemble_full_text Ïã§Ìå®: {e}")
                    return ""

            # Îß§ÎãàÌéòÏä§Ìä∏ ÏóîÌä∏Î¶¨ Íµ¨ÏÑ± Ìó¨Ìçº
            def _object_to_manifest_entry(idx: int, obj: DocExtractedObject, blob_key: str) -> Dict[str, Any]:
                return {
                    "object_index": idx,
                    "object_id": getattr(obj, 'object_id', None),
                    "object_type": getattr(obj, 'object_type', None),
                    "page_no": getattr(obj, 'page_no', None),
                    "sequence_in_page": getattr(obj, 'sequence_in_page', None),
                    "blob_key": blob_key,
                    "char_count": len(getattr(obj, 'content_text', '') or ''),
                    "has_structure": bool(getattr(obj, 'structure_json', None)),
                    "bbox": getattr(obj, 'bbox', None)
                }

            # PDF
            if "pages" in metadata:
                for p in metadata["pages"]:
                    page_no = p.get("page_no")
                    page_text = p.get("text", "")
                    
                    # ÌéòÏù¥ÏßÄ bbox Í≥ÑÏÇ∞ (Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄ ÌÅ¨Í∏∞)
                    page_width = p.get("width", 0)
                    page_height = p.get("height", 0)
                    page_bbox = None
                    if page_width and page_height:
                        # Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄÎ•º TEXT_BLOCKÏùò bboxÎ°ú ÏÑ§Ï†ï
                        page_bbox = [0, 0, int(page_width * 72), int(page_height * 72)]  # inch ‚Üí points Î≥ÄÌôò
                    
                    # TEXT_BLOCK Í∞ùÏ≤¥ ÏÉùÏÑ± (bbox Ìè¨Ìï®)
                    if page_text and page_text.strip():
                        extracted_objects.append(
                            DocExtractedObject(
                                extraction_session_id=extraction_session.extraction_session_id,
                                file_bss_info_sno=file_bss_info_sno,
                                page_no=page_no,
                                object_type="TEXT_BLOCK",
                                sequence_in_page=0,
                                content_text=page_text,
                                char_count=len(page_text),
                                token_estimate=len(page_text.split()),
                                hash_sha256=hashlib.sha256(page_text.encode()).hexdigest(),
                                bbox=page_bbox
                            )
                        )
                    # Persist DI figures (if any) as FIGURE objects alongside IMAGEs
                    for fig in p.get("figures", []) or []:
                        try:
                            # Convert polygon bbox to [x0,y0,x1,y1] if possible
                            bbox_poly = fig.get("bbox") or []
                            if isinstance(bbox_poly, list) and len(bbox_poly) >= 4:
                                xs = [pt[0] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                ys = [pt[1] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                if xs and ys:
                                    _bbox = [int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))]
                                else:
                                    _bbox = [0, 0, 0, 0]
                            else:
                                _bbox = [0, 0, 0, 0]
                        except Exception:
                            _bbox = [0, 0, 0, 0]

                        caption_text = (fig.get("caption") or "").strip()
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=p.get("page_no"),
                            object_type="FIGURE",
                            sequence_in_page=fig.get("figure_index") or 0,
                            bbox=_bbox,
                            content_text=caption_text,
                            structure_json=fig
                        ))
                    # ‚ùå ÌéòÏù¥ÏßÄÎ≥Ñ tables_countÎäî ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùå (Î¨∏ÏÑú Î†àÎ≤®ÏóêÏÑú Ï≤òÎ¶¨)
                    
                    for img_meta in p.get("images_metadata", []):
                        # DOCX/Îã§Î•∏ ÌòïÏãùÏùò Í≤ΩÏö∞ binary_dataÎ•º structure_jsonÏóêÏÑú Ï†úÍ±∞
                        clean_img_meta = dict(img_meta)
                        if 'binary_data' in clean_img_meta:
                            clean_img_meta.pop('binary_data')
                        
                        # üéØ Caption Ï∂îÏ∂ú (Azure DIÏóêÏÑú Ï†úÍ≥µ)
                        caption = clean_img_meta.get('caption', '') or ''
                        if caption:
                            logger.info(f"[CAPTION] üìù Ïù¥ÎØ∏ÏßÄ Ï∫°ÏÖò Î∞úÍ≤¨ - page={p.get('page_no')}, caption={caption[:100]}")
                        
                        # bbox Î≥ÄÌôò: polygon [[x,y], [x,y], ...] ‚Üí [x0, y0, x1, y1]
                        _bbox = [0, 0, 0, 0]
                        try:
                            # Case 1: x0, y0, x1, y1 ÌòïÏãù (Í∏∞Ï°¥)
                            if 'x0' in img_meta and 'y0' in img_meta:
                                _bbox = [
                                    int(img_meta.get('x0', 0)),
                                    int(img_meta.get('y0', 0)),
                                    int(img_meta.get('x1', 0)),
                                    int(img_meta.get('y1', 0))
                                ]
                            # Case 2: bbox polygon ÌòïÏãù (Azure DI)
                            elif 'bbox' in img_meta:
                                bbox_poly = img_meta.get('bbox')
                                if isinstance(bbox_poly, list) and len(bbox_poly) >= 4:
                                    # polygon: [[x,y], [x,y], ...] ‚Üí [x0, y0, x1, y1]
                                    xs = [pt[0] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) >= 2]
                                    ys = [pt[1] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) >= 2]
                                    if xs and ys:
                                        _bbox = [int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))]
                                        logger.debug(f"[BBOX] polygon ‚Üí rect: {bbox_poly} ‚Üí {_bbox}")
                        except Exception as e:
                            logger.warning(f"[BBOX] Î≥ÄÌôò Ïã§Ìå®: {e}, img_meta={img_meta}")
                            _bbox = [0, 0, 0, 0]
                        
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=p.get("page_no"),
                            object_type="IMAGE",
                            sequence_in_page=img_meta.get("image_index"),
                            bbox=_bbox,
                            content_text=caption,  # üéØ CaptionÏùÑ content_textÏóê Ï†ÄÏû•
                            structure_json=clean_img_meta
                        ))
                
                # üéØ Î¨∏ÏÑú Î†àÎ≤® ÌÖåÏù¥Î∏î Ï≤òÎ¶¨ (Azure DI SDK 4.x)
                # Azure DIÎäî analyze_result.tablesÎ•º Î¨∏ÏÑú Î†àÎ≤®ÏóêÏÑú Ï∂îÏ∂úÌïòÎØÄÎ°ú,
                # metadata["tables"] Î∞∞Ïó¥ÏùÑ ÏàúÌöåÌïòÎ©∞ Ïã§Ï†ú TABLE Í∞ùÏ≤¥Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
                doc_tables = metadata.get("tables", [])
                if doc_tables:
                    logger.info(f"[MULTIMODAL-EXTRACT] üìä Î¨∏ÏÑú Î†àÎ≤® ÌÖåÏù¥Î∏î {len(doc_tables)}Í∞ú Ï≤òÎ¶¨ ÏãúÏûë")
                    for table in doc_tables:
                        try:
                            # bbox polygon ‚Üí rectangle Î≥ÄÌôò
                            bbox_poly = table.get("bbox") or []
                            if isinstance(bbox_poly, list) and len(bbox_poly) >= 4:
                                xs = [pt[0] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                ys = [pt[1] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                _bbox = [int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))] if xs and ys else [0, 0, 0, 0]
                            else:
                                _bbox = [0, 0, 0, 0]
                        except Exception:
                            _bbox = [0, 0, 0, 0]
                        
                        # ÌÖåÏù¥Î∏î ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú (cellsÏóêÏÑú Ï°∞Ìï©)
                        table_text = ""
                        cells = table.get("cells", [])
                        if cells:
                            # cellsÎ•º Ìñâ/Ïó¥ ÏàúÏÑúÎ°ú Ï†ïÎ†¨ÌïòÏó¨ ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
                            sorted_cells = sorted(cells, key=lambda c: (c.get("row_index", 0), c.get("column_index", 0)))
                            table_text = " | ".join([c.get("content", "") for c in sorted_cells if c.get("content", "").strip()])
                        
                        # ÌéòÏù¥ÏßÄ Î≤àÌò∏ Ï∂îÏ∂ú (Azure DI ÌÖåÏù¥Î∏îÏùÄ ÌéòÏù¥ÏßÄ Ï†ïÎ≥¥Í∞Ä ÏóÜÏùÑ Ïàò ÏûàÏùå)
                        # bounding_regionsÏóêÏÑú ÌéòÏù¥ÏßÄ Î≤àÌò∏Î•º Ï∞æÍ±∞ÎÇò, bbox Ï¢åÌëúÎ°ú ÌéòÏù¥ÏßÄ Îß§Ïπ≠
                        table_page_no = None
                        
                        # Î∞©Î≤ï 1: table ÏûêÏ≤¥Ïóê page_noÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
                        if "page_no" in table:
                            table_page_no = table.get("page_no")
                        
                        # Î∞©Î≤ï 2: bounding_regionsÏóêÏÑú Ï∂îÏ∂ú
                        elif "bounding_regions" in table and table["bounding_regions"]:
                            first_region = table["bounding_regions"][0]
                            table_page_no = first_region.get("page_number") or first_region.get("page")
                        
                        # Î∞©Î≤ï 3: bbox Ï¢åÌëúÎ°ú ÌéòÏù¥ÏßÄ Îß§Ïπ≠ (Ìè¥Î∞±)
                        if not table_page_no and _bbox != [0, 0, 0, 0]:
                            # Í∞Å ÌéòÏù¥ÏßÄÏùò bboxÏôÄ ÎπÑÍµêÌïòÏó¨ Í∞ÄÏû• ÎßéÏù¥ Í≤πÏπòÎäî ÌéòÏù¥ÏßÄ Ï∞æÍ∏∞
                            for p in metadata.get("pages", []):
                                page_width = p.get("width", 0)
                                page_height = p.get("height", 0)
                                if page_width > 0 and page_height > 0:
                                    page_bbox = [0, 0, int(page_width * 72), int(page_height * 72)]
                                    # Í∞ÑÎã®Ìïú Ìè¨Ìï® Ïó¨Î∂Ä ÌôïÏù∏
                                    if (_bbox[0] >= page_bbox[0] and _bbox[1] >= page_bbox[1] and
                                        _bbox[2] <= page_bbox[2] and _bbox[3] <= page_bbox[3]):
                                        table_page_no = p.get("page_no")
                                        break
                        
                        # ÌéòÏù¥ÏßÄ Î≤àÌò∏Î•º Ï∞æÏßÄ Î™ªÌïú Í≤ΩÏö∞ 1Î°ú ÏÑ§Ï†ï
                        if not table_page_no:
                            table_page_no = 1
                            logger.warning(f"[MULTIMODAL-EXTRACT] ‚ö†Ô∏è ÌÖåÏù¥Î∏î ÌéòÏù¥ÏßÄ Î≤àÌò∏Î•º Ï∞æÏßÄ Î™ªÌï®, Í∏∞Î≥∏Í∞í 1Î°ú ÏÑ§Ï†ï")
                        
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=table_page_no,
                            object_type="TABLE",
                            sequence_in_page=table.get("table_index") or 0,
                            bbox=_bbox,
                            content_text=table_text[:5000],  # ÏµúÎåÄ 5000ÏûêÎ°ú Ï†úÌïú
                            structure_json=table  # Ï†ÑÏ≤¥ ÌÖåÏù¥Î∏î Íµ¨Ï°∞ (cells, row_count, column_count Ìè¨Ìï®)
                        ))
                    
                    logger.info(f"[MULTIMODAL-EXTRACT] ‚úÖ Î¨∏ÏÑú Î†àÎ≤® ÌÖåÏù¥Î∏î {len(doc_tables)}Í∞ú Ï≤òÎ¶¨ ÏôÑÎ£å")
                
                # üéØ Î¨∏ÏÑú Î†àÎ≤® figures Ï≤òÎ¶¨ (Azure DI SDK 4.x)
                # Azure DIÎäî analyze_result.figuresÎ•º Î¨∏ÏÑú Î†àÎ≤®ÏóêÏÑú Ï∂îÏ∂úÌïòÍ≥†,
                # _merge_figures_into_pages()Î°ú ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÎ∞∞Î•º ÏãúÎèÑÌïòÏßÄÎßå,
                # ÌéòÏù¥ÏßÄÎ≥Ñ Î∂ÑÎ∞∞Ïóê Ïã§Ìå®Ìïú Í≤ΩÏö∞Î•º ÎåÄÎπÑÌïòÏó¨ Î¨∏ÏÑú Î†àÎ≤®ÏóêÏÑúÎèÑ Ï≤òÎ¶¨Ìï©ÎãàÎã§.
                doc_figures = metadata.get("figures", [])
                if doc_figures:
                    logger.info(f"[MULTIMODAL-EXTRACT] üìä Î¨∏ÏÑú Î†àÎ≤® figure {len(doc_figures)}Í∞ú Ï≤òÎ¶¨ ÏãúÏûë")
                    for fig in doc_figures:
                        try:
                            # bbox polygon ‚Üí rectangle Î≥ÄÌôò
                            bbox_poly = fig.get("bbox") or []
                            if isinstance(bbox_poly, list) and len(bbox_poly) >= 4:
                                xs = [pt[0] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                ys = [pt[1] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                _bbox = [int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))] if xs and ys else [0, 0, 0, 0]
                            else:
                                _bbox = [0, 0, 0, 0]
                        except Exception:
                            _bbox = [0, 0, 0, 0]
                        
                        # ÌéòÏù¥ÏßÄ Î≤àÌò∏ Ï∂îÏ∂ú (fig.page_no ÎòêÎäî bbox Ï¢åÌëú Îß§Ïπ≠)
                        fig_page_no = fig.get("page_no")
                        if not fig_page_no and _bbox != [0, 0, 0, 0]:
                            # bbox Ï¢åÌëúÎ°ú ÌéòÏù¥ÏßÄ Îß§Ïπ≠
                            for p in metadata.get("pages", []):
                                page_width = p.get("width", 0)
                                page_height = p.get("height", 0)
                                if page_width > 0 and page_height > 0:
                                    page_bbox = [0, 0, int(page_width * 72), int(page_height * 72)]
                                    if (_bbox[0] >= page_bbox[0] and _bbox[1] >= page_bbox[1] and
                                        _bbox[2] <= page_bbox[2] and _bbox[3] <= page_bbox[3]):
                                        fig_page_no = p.get("page_no")
                                        break
                        
                        if not fig_page_no:
                            fig_page_no = 1
                            logger.warning(f"[MULTIMODAL-EXTRACT] ‚ö†Ô∏è Figure ÌéòÏù¥ÏßÄ Î≤àÌò∏Î•º Ï∞æÏßÄ Î™ªÌï®, Í∏∞Î≥∏Í∞í 1Î°ú ÏÑ§Ï†ï")
                        
                        caption_text = (fig.get("caption") or "").strip()
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=fig_page_no,
                            object_type="FIGURE",
                            sequence_in_page=fig.get("figure_index") or 0,
                            bbox=_bbox,
                            content_text=caption_text,
                            structure_json=fig
                        ))
                    
                    logger.info(f"[MULTIMODAL-EXTRACT] ‚úÖ Î¨∏ÏÑú Î†àÎ≤® figure {len(doc_figures)}Í∞ú Ï≤òÎ¶¨ ÏôÑÎ£å")
            
            # PPT
            elif "slides" in metadata:
                for s in metadata["slides"]:
                    _add_text_obj(s.get("slide_no"), s.get("text", ""))
                    for idx in range(s.get("tables_count", 0)):
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=s.get("slide_no"),
                            object_type="TABLE",
                            sequence_in_page=idx + 1,
                            content_text=f"[Ìëú {idx+1}]",
                            structure_json={"table_index": idx}
                        ))
                    for idx in range(s.get("charts_count", 0)):
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=s.get("slide_no"),
                            object_type="FIGURE",
                            sequence_in_page=idx + 100,
                            content_text=f"[Ï∞®Ìä∏ {idx+1}]",
                            structure_json={"chart_index": idx}
                        ))
                    for img_meta in s.get("images_metadata", []):
                        # PPT Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÏóêÏÑúÎèÑ binary_data Ï†úÍ±∞
                        clean_img_meta = dict(img_meta)
                        if 'binary_data' in clean_img_meta:
                            clean_img_meta.pop('binary_data')
                        _bbox = [
                            int(img_meta.get('left', 0)),
                            int(img_meta.get('top', 0)),
                            int(img_meta.get('left', 0) + img_meta.get('width', 0)),
                            int(img_meta.get('top', 0) + img_meta.get('height', 0))
                        ]
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=s.get("slide_no"),
                            object_type="IMAGE",
                            sequence_in_page=img_meta.get("image_index"),
                            bbox=_bbox,
                            structure_json=clean_img_meta
                        ))
            # XLSX
            elif "sheets" in metadata:
                for sh in metadata["sheets"]:
                    text_val = sh.get("text", "")
                    if text_val.strip():
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=sh.get("sheet_no"),
                            object_type="TABLE",
                            sequence_in_page=0,
                            content_text=text_val,
                            char_count=len(text_val),
                            structure_json=sh
                        ))

            # Fallback: pages/slides/sheets Î™®Îëê ÏóÜÍ≥† textÎßå Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞(Ïòà: direct_text_read)
            if not extracted_objects and extraction_result.get("text"):
                raw_text = extraction_result.get("text") or ""
                if raw_text.strip():
                    extracted_objects.append(
                        DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=None,
                            object_type="TEXT_BLOCK",
                            sequence_in_page=0,
                            content_text=raw_text,
                            char_count=len(raw_text),
                            token_estimate=len(raw_text.split()),
                            hash_sha256=hashlib.sha256(raw_text.encode()).hexdigest(),
                        )
                    )
                    logger.info("[MULTIMODAL] Fallback single TEXT_BLOCK object created from raw text")

            # ÏÑπÏÖò Í∞êÏßÄÎ•º ÏÑ†ÌñâÌïòÏó¨ Ïù¥ÎØ∏ÏßÄ/Ìëú ÌïÑÌÑ∞ÎßÅ Î≤îÏúÑ ÌååÏïÖ (ÌïôÏà† ÎÖºÎ¨∏ ÌïúÏ†ï)
            if apply_section_chunking:
                text_objs_for_sections = [
                    obj for obj in extracted_objects
                    if getattr(obj, "object_type", None) == "TEXT_BLOCK"
                    and (getattr(obj, "content_text", "") or "").strip()
                ]
                if text_objs_for_sections:
                    separator = "\n\n"
                    combined_parts: List[str] = []
                    section_object_spans = []
                    current_pos = 0
                    for obj in text_objs_for_sections:
                        content = (getattr(obj, "content_text", "") or "").strip()
                        if not content:
                            continue
                        if combined_parts:
                            combined_parts.append(separator)
                            current_pos += len(separator)
                        start_pos = current_pos
                        combined_parts.append(content)
                        current_pos += len(content)
                        section_object_spans.append((obj, start_pos, current_pos))
                    section_combined_text = "".join(combined_parts)

                    if section_combined_text.strip():
                        # Pass DI pages so detector can leverage Azure paragraph roles
                        precomputed_sections_info = self.section_detector.detect_sections(
                            section_combined_text,
                            pages=metadata.get("pages") or None,
                        )
                        if precomputed_sections_info:
                            precomputed_section_summary = self.section_detector.get_section_summary(precomputed_sections_info)
                            section_chunking_meta["detected_sections"] = [s.get("type") for s in precomputed_sections_info]
                            if precomputed_section_summary:
                                section_chunking_meta.setdefault("summary", precomputed_section_summary)

                            visual_page_filter = self._derive_core_content_page_set(
                                precomputed_sections_info,
                                section_object_spans,
                            )

                            # Derive allowed pages; if too narrow or unavailable, widen using safe defaults
                            total_pages = len(metadata.get('pages', metadata.get('slides', metadata.get('sheets', []))))
                            if visual_page_filter:
                                counts_before = {
                                    "IMAGE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "IMAGE"),
                                    "TABLE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "TABLE"),
                                    "FIGURE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "FIGURE"),
                                }
                                filtered_objects: List[DocExtractedObject] = []
                                removed_counts = {"IMAGE": 0, "TABLE": 0, "FIGURE": 0}
                                for obj in extracted_objects:
                                    otype = getattr(obj, "object_type", None)
                                    if otype in removed_counts:
                                        page_no = getattr(obj, "page_no", None)
                                        if page_no is not None and page_no not in visual_page_filter:
                                            removed_counts[otype] += 1
                                            continue
                                    filtered_objects.append(obj)

                                if len(filtered_objects) != len(extracted_objects):
                                    extracted_objects = filtered_objects
                                    logger.info(
                                        "[MULTIMODAL] ÏÑπÏÖò Î≤îÏúÑ ÌïÑÌÑ∞ Ï†ÅÏö© - Ïù¥ÎØ∏ÏßÄ %sÍ∞ú, Ìëú %sÍ∞ú, Ï∞®Ìä∏ %sÍ∞ú Ï†úÏô∏",
                                        removed_counts["IMAGE"],
                                        removed_counts["TABLE"],
                                        removed_counts["FIGURE"],
                                    )

                                counts_after = {
                                    "IMAGE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "IMAGE"),
                                    "TABLE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "TABLE"),
                                    "FIGURE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "FIGURE"),
                                }
                                # widen if roles not used or filter too small
                                widened = False
                                reason = None
                                if (len(visual_page_filter) < 3) and total_pages:
                                    summary = section_chunking_meta.get("summary") or {}
                                    if not summary.get("azure_di_role_used"):
                                        # Use middle pages (2..N-1) as a safe default
                                        start = 2 if total_pages >= 3 else 1
                                        end = total_pages - 1 if total_pages >= 3 else total_pages
                                        widened_pages = set(range(start, end + 1))
                                        # reapply filter against widened set
                                        if widened_pages and widened_pages != visual_page_filter:
                                            visual_page_filter = widened_pages
                                            widened = True
                                            reason = "di_roles_unavailable_or_narrow"
                                            # re-run filtering with widened set
                                            filtered_objects = []
                                            removed_counts = {"IMAGE": 0, "TABLE": 0, "FIGURE": 0}
                                            for obj in extracted_objects:
                                                otype = getattr(obj, "object_type", None)
                                                if otype in removed_counts:
                                                    pno = getattr(obj, "page_no", None)
                                                    if pno is not None and pno not in visual_page_filter:
                                                        removed_counts[otype] += 1
                                                        continue
                                                filtered_objects.append(obj)
                                            extracted_objects = filtered_objects
                                            counts_after = {
                                                "IMAGE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "IMAGE"),
                                                "TABLE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "TABLE"),
                                                "FIGURE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "FIGURE"),
                                            }
                                section_chunking_meta["figure_table_filter"] = {
                                    "enabled": True,
                                    "allowed_pages": sorted(visual_page_filter),
                                    "before": counts_before,
                                    "after": counts_after,
                                    "widened": widened,
                                    "widen_reason": reason,
                                }
                            else:
                                section_chunking_meta["figure_table_filter"] = {"enabled": False, "reason": "no_core_pages"}
                        else:
                            section_chunking_meta["figure_table_filter"] = {"enabled": False, "reason": "no_sections"}
                    else:
                        section_chunking_meta["figure_table_filter"] = {"enabled": False, "reason": "empty_combined_text"}
            
            for obj in extracted_objects:
                session.add(obj)
            await session.flush()
            setattr(extraction_session, "status", "success")
            setattr(extraction_session, "completed_at", datetime.now())
            setattr(extraction_session, "page_count_detected", len(metadata.get('pages', metadata.get('slides', metadata.get('sheets', [])))))
            result["objects_count"] = len(extracted_objects)
            _stage("extraction", True, objects=len(extracted_objects))

            # -----------------------------
            # 1.5. Azure Blob Storage - Ï§ëÍ∞Ñ Í≤∞Í≥º Ï†ÄÏû•
            # -----------------------------
            performed_blob_intermediate = False
            try:
                if settings.storage_backend == 'azure_blob' and get_azure_blob_service and file_bss_info_sno:
                    _start_stage("blob_intermediate_save")
                    performed_blob_intermediate = True
                    azure_factory = get_azure_blob_service if callable(get_azure_blob_service) else None
                    if not azure_factory:
                        raise RuntimeError("Azure Blob service factory not available")
                    azure = azure_factory()
                    
                    # Ï†ÑÏ≤¥ Ï∂îÏ∂ú ÌÖçÏä§Ìä∏ Ï†ÄÏû• (intermediate Ïª®ÌÖåÏù¥ÎÑà)
                    full_text_key = f"multimodal/{file_bss_info_sno}/extraction_full_text.txt"
                    full_text_content = extraction_result.get("text", "") or ""
                    # ÌïÑÏöîÏãú fallback Ï°∞Î¶Ω
                    if not full_text_content.strip():
                        full_text_content = _assemble_full_text(extracted_objects)
                    if full_text_content.strip():
                        azure.upload_bytes(
                            full_text_content.encode('utf-8'), 
                            full_text_key, 
                            purpose='intermediate'
                        )
                        logger.info(f"[MULTIMODAL-BLOB] Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Ï†ÄÏû•: {full_text_key} (len={len(full_text_content)})")
                    else:
                        logger.info("[MULTIMODAL-BLOB] Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ ÎπÑÏñ¥ÏûàÏñ¥ Ï†ÄÏû• ÏÉùÎûµ")
                    
                    # Ï∂îÏ∂ú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû• (binary_data Ï†úÍ±∞)
                    metadata_key = f"multimodal/{file_bss_info_sno}/extraction_metadata.json"
                    
                    # metadataÏóêÏÑú binary_data Ï†úÍ±∞ (JSON ÏßÅÎ†¨Ìôî Ïò§Î•ò Î∞©ÏßÄ)  
                    clean_metadata = _clean_metadata_for_json(metadata)
                    
                    metadata_content = {
                        "extraction_session_id": extraction_session.extraction_session_id,
                        "provider": provider,
                        "pipeline_type": pipeline_type,
                        "extracted_objects_count": len(extracted_objects),
                        "pages_detected": extraction_session.page_count_detected,
                        "extraction_metadata": clean_metadata,
                        "has_full_text": bool(full_text_content.strip()),
                        "timestamp": datetime.now().isoformat()
                    }
                    azure.upload_bytes(
                        json.dumps(metadata_content, ensure_ascii=False).encode('utf-8'),
                        metadata_key,
                        purpose='intermediate'
                    )
                    logger.info(f"[MULTIMODAL-BLOB] Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•: {metadata_key}")
                    
                    # Í∞ùÏ≤¥Î≥Ñ ÏÑ∏Î∂Ä Ï†ïÎ≥¥ Ï†ÄÏû• + Îß§ÎãàÌéòÏä§Ìä∏ Íµ¨ÏÑ±
                    objects_manifest: List[Dict[str, Any]] = []
                    saved_counts = {"TEXT_BLOCK": 0, "TABLE": 0, "IMAGE": 0, "FIGURE": 0}
                    # PDF Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂úÏùÑ ÏúÑÌïú ÏÇ¨Ï†Ñ Ï§ÄÎπÑ
                    pdf_pages = None
                    pdf_doc = None
                    is_pdf = False
                    # IMAGE ÎòêÎäî FIGURE Í∞ùÏ≤¥Í∞Ä ÏûàÏúºÎ©¥ pdfplumber Ï¥àÍ∏∞Ìôî
                    has_images_or_figures = any(getattr(o, 'object_type', None) in ['IMAGE', 'FIGURE'] for o in extracted_objects)
                    if file_path.lower().endswith('.pdf') and has_images_or_figures:
                        try:
                            import pdfplumber  # type: ignore
                            # Ïã§Ï†ú ÌååÏùº Í≤ΩÎ°ú ÏÇ¨Ïö© (Azure Blob Í≤ΩÎ°úÍ∞Ä ÏïÑÎãå Î°úÏª¨ ÏûÑÏãú ÌååÏùº)
                            pdf_doc = pdfplumber.open(actual_file_path)
                            pdf_pages = pdf_doc.pages
                            is_pdf = True
                            logger.info(f"[MULTIMODAL-BLOB] PDF Ï¥àÍ∏∞Ìôî ÏôÑÎ£å - FIGURE/IMAGE Î∞îÏù¥ÎÑàÎ¶¨ Ï∂îÏ∂ú Ï§ÄÎπÑ")
                        except Exception as e:
                            logger.warning(f"[MULTIMODAL-BLOB] PDF Ïù¥ÎØ∏ÏßÄ Ï¥àÍ∏∞Ìôî Ïã§Ìå® (Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨ Ï∂îÏ∂ú ÏÉùÎûµ): {e}")
                            pdf_pages = None
                            pdf_doc = None
                    object_save_errors: List[str] = []
                    for idx, obj in enumerate(extracted_objects):
                        try:
                            blob_key = None
                            if getattr(obj, 'object_type', None) == 'TEXT_BLOCK' and (obj.content_text or '').strip():
                                blob_key = f"multimodal/{file_bss_info_sno}/objects/text_block_{idx}_{obj.page_no or 0}.txt"
                                azure.upload_bytes(
                                    (obj.content_text or '').encode('utf-8'),
                                    blob_key,
                                    purpose='intermediate'
                                )
                            elif getattr(obj, 'object_type', None) in ['TABLE', 'IMAGE', 'FIGURE']:
                                blob_key = f"multimodal/{file_bss_info_sno}/objects/{obj.object_type.lower()}_{idx}_{obj.page_no or 0}.json"

                                if getattr(obj, 'object_type', None) in ['IMAGE', 'FIGURE']:
                                    obj_id_for_tracking = getattr(obj, 'object_id', idx)
                                    image_object_ids_seen.add(obj_id_for_tracking)

                                # structure_json Ï†ÑÏ≤¥ Ïû¨Í∑Ä Ï†ïÎ¶¨ (binary_data/bytes Ï†úÍ±∞)
                                clean_structure_json = _clean_metadata_for_json(getattr(obj, 'structure_json', None))

                                obj_content = {
                                    "object_type": obj.object_type,
                                    "page_no": obj.page_no,
                                    "sequence_in_page": obj.sequence_in_page,
                                    # TABLE ÏùÄ placeholder ÌÖçÏä§Ìä∏Ïùº Ïàò ÏûàÏùå ‚Üí Íµ¨Ï°∞ Í∞úÏÑ† TODO
                                    "content_text": obj.content_text,
                                    "structure_json": clean_structure_json,
                                    "bbox": obj.bbox
                                }
                                try:
                                    azure.upload_bytes(
                                        json.dumps(obj_content, ensure_ascii=False).encode('utf-8'),
                                        blob_key,
                                        purpose='intermediate'
                                    )
                                except TypeError as te:
                                    # ÎîîÎ≤ÑÍπÖÏö© Î°úÍ∑∏: Ïñ¥Îñ§ ÌïÑÎìú ÎïåÎ¨∏Ïóê Ïã§Ìå®ÌñàÎäîÏßÄ ÌôïÏù∏
                                    logger.warning(f"[MULTIMODAL-BLOB] Í∞ùÏ≤¥ JSON ÏßÅÎ†¨Ìôî Ïã§Ìå® idx={idx}: {te}")
                                    # Í∞ïÏ†ú fallback: structure_json Ï†úÍ±∞ ÌõÑ Ï†ÄÏû•
                                    fallback_content = dict(obj_content)
                                    fallback_content.pop('structure_json', None)
                                    azure.upload_bytes(
                                        json.dumps(fallback_content, ensure_ascii=False).encode('utf-8'),
                                        blob_key,
                                        purpose='intermediate'
                                    )
                                # Ïù¥ÎØ∏ÏßÄ ÎòêÎäî FIGUREÏù∏ Í≤ΩÏö∞ Î∞îÏù¥ÎÑàÎ¶¨ Ï†ÄÏû• Î∞è ÌäπÏßï Ï∂îÏ∂ú
                                if getattr(obj, 'object_type', None) in ['IMAGE', 'FIGURE']:
                                    obj_type = getattr(obj, 'object_type', None)
                                    logger.info(f"[MULTIMODAL-BLOB] {obj_type} Í∞ùÏ≤¥ Î∞úÍ≤¨ idx={idx}, page={getattr(obj, 'page_no', None)}, obj_id={getattr(obj, 'object_id', None)}")
                                    img_bytes = None
                                    page_no_val = getattr(obj, 'page_no', None) or 1
                                    
                                    # üéØ STEP 1: Azure DIÍ∞Ä Ïù¥ÎØ∏ Î∞îÏù¥ÎÑàÎ¶¨ Îç∞Ïù¥ÌÑ∞Î•º Ï†úÍ≥µÌñàÎäîÏßÄ ÌôïÏù∏
                                    azure_binary = getattr(obj, 'binary_data', None)
                                    if azure_binary and len(azure_binary) > 0:
                                        img_bytes = azure_binary
                                        logger.info(f"[MULTIMODAL-BLOB] ‚úÖ Azure DI Î∞îÏù¥ÎÑàÎ¶¨ ÏÇ¨Ïö© - idx={idx}, size={len(img_bytes)} bytes")
                                    
                                    # STEP 2: Azure DI Î∞îÏù¥ÎÑàÎ¶¨Í∞Ä ÏóÜÏúºÎ©¥ PDFÏóêÏÑú Ï∂îÏ∂ú
                                    elif is_pdf and pdf_pages is not None:
                                        logger.info(f"[MULTIMODAL-BLOB] PDF Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÏãúÏûë idx={idx}, page={page_no_val}, is_pdf={is_pdf}, pdf_pages={len(pdf_pages) if pdf_pages else 0}")
                                        
                                        # üéØ Azure DI polygon bbox Ï∂îÏ∂ú (Ï†ïÌôïÌïú Ï¢åÌëú)
                                        structure_json = getattr(obj, 'structure_json', None)
                                        bbox_val = None
                                        
                                        if structure_json and isinstance(structure_json, dict):
                                            polygon = structure_json.get('bbox')
                                            if polygon and isinstance(polygon, list) and len(polygon) == 4:
                                                # polygon: [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                                                # bounding box Í≥ÑÏÇ∞: (min_x, min_y, max_x, max_y)
                                                x_coords = [pt[0] for pt in polygon if isinstance(pt, (list, tuple)) and len(pt) >= 2]
                                                y_coords = [pt[1] for pt in polygon if isinstance(pt, (list, tuple)) and len(pt) >= 2]
                                                
                                                if len(x_coords) == 4 and len(y_coords) == 4:
                                                    x0 = min(x_coords)
                                                    y0 = min(y_coords)
                                                    x1 = max(x_coords)
                                                    y1 = max(y_coords)
                                                    bbox_val = [x0, y0, x1, y1]
                                                    logger.info(f"[MULTIMODAL-BLOB] ‚úÖ Azure DI polygon bbox Ï∂îÏ∂ú - idx={idx}, polygon={polygon[:2]}..., bbox={bbox_val}, size={(x1-x0):.2f}x{(y1-y0):.2f}inch")
                                        
                                        # Fallback: obj.bbox ÏÇ¨Ïö© (Ï†ïÏàò Î∞òÏò¨Î¶º Î≤ÑÏ†Ñ)
                                        if not bbox_val:
                                            bbox_val = getattr(obj, 'bbox', None)
                                            logger.info(f"[MULTIMODAL-BLOB] bbox fallback (obj.bbox) - idx={idx}, bbox_val={bbox_val}, type={type(bbox_val)}, page_no_val={page_no_val}, type={type(page_no_val)}")
                                        
                                        # üéØ bboxÍ∞Ä [0,0,0,0]Ïù∏ Í≤ΩÏö∞ Í∞ôÏùÄ ÌéòÏù¥ÏßÄÏùò FIGURE bbox Ï∞æÏïÑÏÑú ÏÇ¨Ïö©
                                        if bbox_val == [0, 0, 0, 0] or (isinstance(bbox_val, (list, tuple)) and all(v == 0 for v in bbox_val)):
                                            logger.warning(f"[MULTIMODAL-BLOB] ‚ö†Ô∏è IMAGE bbox Î¨¥Ìö® (idx={idx}) ‚Üí Í∞ôÏùÄ ÌéòÏù¥ÏßÄÏùò FIGURE bbox Í≤ÄÏÉâ Ï§ë...")
                                            sequence_in_page = getattr(obj, 'sequence_in_page', None)
                                            
                                            # Í∞ôÏùÄ ÌéòÏù¥ÏßÄÏóêÏÑú sequenceÍ∞Ä ÎπÑÏä∑Ìïú FIGURE Ï∞æÍ∏∞
                                            for candidate_obj in extracted_objects:
                                                if (getattr(candidate_obj, 'object_type', None) == 'FIGURE' and
                                                    getattr(candidate_obj, 'page_no', None) == page_no_val):
                                                    
                                                    candidate_bbox = getattr(candidate_obj, 'bbox', None)
                                                    candidate_seq = getattr(candidate_obj, 'sequence_in_page', None)
                                                    
                                                    # bboxÍ∞Ä Ïú†Ìö®ÌïòÍ≥† sequenceÍ∞Ä ÎπÑÏä∑ÌïòÎ©¥ ÏÇ¨Ïö©
                                                    if (candidate_bbox and 
                                                        isinstance(candidate_bbox, (list, tuple)) and 
                                                        len(candidate_bbox) == 4 and
                                                        not all(v == 0 for v in candidate_bbox)):
                                                        
                                                        # sequenceÍ∞Ä Í∞ôÍ±∞ÎÇò ¬±1 Ï∞®Ïù¥Î©¥ Îß§Ïπ≠
                                                        if sequence_in_page is None or candidate_seq is None or abs(candidate_seq - sequence_in_page) <= 1:
                                                            bbox_val = candidate_bbox
                                                            logger.info(f"[MULTIMODAL-BLOB] ‚úÖ FIGURE bbox Ï†ÅÏö© ÏÑ±Í≥µ - FIGURE seq={candidate_seq}, IMAGE seq={sequence_in_page}, bbox={bbox_val}")
                                                            break
                                            
                                            if bbox_val == [0, 0, 0, 0] or (isinstance(bbox_val, (list, tuple)) and all(v == 0 for v in bbox_val)):
                                                logger.warning(f"[MULTIMODAL-BLOB] ‚ùå Îß§Ïπ≠ÎêòÎäî FIGURE bboxÎ•º Ï∞æÏßÄ Î™ªÌï® ‚Üí pdfplumber fallback ÏãúÎèÑ")
                                        
                                        # bbox Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
                                        is_valid_bbox = (
                                            isinstance(page_no_val, int) and 
                                            isinstance(bbox_val, (list, tuple)) and 
                                            len(bbox_val) == 4 and
                                            not all(v == 0 for v in bbox_val)  # [0,0,0,0] Ï†úÏô∏
                                        )
                                        
                                        if is_valid_bbox:
                                            logger.info(f"[MULTIMODAL-BLOB] bbox Í≤ÄÏ¶ù ÌÜµÍ≥º - idx={idx}, ÌÅ¨Î°≠ ÏãúÎèÑ ÏãúÏûë")
                                            try:
                                                page_index = page_no_val - 1
                                                if 0 <= page_index < len(pdf_pages):
                                                    page = pdf_pages[page_index]
                                                    x0, y0, x1, y1 = [float(v) for v in bbox_val]
                                                    
                                                    # üéØ Azure DI bboxÎäî inch Îã®ÏúÑ ‚Üí 150 DPIÎ°ú ÌîΩÏÖÄ Î≥ÄÌôò
                                                    # ÏµúÏÜå ÌÅ¨Í∏∞: 0.5 inch (75 ÌîΩÏÖÄ @ 150 DPI)
                                                    width_inch = x1 - x0
                                                    height_inch = y1 - y0
                                                    width_px = width_inch * 150
                                                    height_px = height_inch * 150
                                                    
                                                    logger.info(f"[MULTIMODAL-BLOB] bbox ÌÅ¨Í∏∞ - idx={idx}, inch=({width_inch:.2f}x{height_inch:.2f}), pixels=({width_px:.0f}x{height_px:.0f})")
                                                    
                                                    page_image = page.to_image(resolution=150)
                                                    
                                                    # ÏµúÏÜå ÌÅ¨Í∏∞ Í≤ÄÏ¶ù: 0.3 inch (45 ÌîΩÏÖÄ) Ïù¥ÏÉÅ
                                                    if width_inch > 0.3 and height_inch > 0.3:
                                                        import io
                                                        from PIL import Image  # type: ignore
                                                        
                                                        # üéØ Ï¢åÌëú Î≥ÄÌôò: Azure DI bbox (inch) ‚Üí pdfplumber image (ÌîΩÏÖÄ)
                                                        # PDF Í∏∞Î≥∏ Ìï¥ÏÉÅÎèÑ: 72 DPI
                                                        # to_image(resolution=150) Ïä§ÏºÄÏùº Ìå©ÌÑ∞: 150/72 = 2.083333
                                                        render_dpi = 150
                                                        pdf_dpi = 72
                                                        scale_factor = render_dpi / pdf_dpi
                                                        
                                                        # inch ‚Üí points (72 DPI) ‚Üí scaled pixels (150 DPI)
                                                        x0_px = x0 * pdf_dpi * scale_factor  # inch * 72 * 2.083 = inch * 150
                                                        y0_px = y0 * pdf_dpi * scale_factor
                                                        x1_px = x1 * pdf_dpi * scale_factor
                                                        y1_px = y1 * pdf_dpi * scale_factor
                                                        
                                                        logger.info(f"[MULTIMODAL-BLOB] Ï¢åÌëú Î≥ÄÌôò - inch=({x0:.2f},{y0:.2f},{x1:.2f},{y1:.2f}) ‚Üí pixels@150dpi=({x0_px:.1f},{y0_px:.1f},{x1_px:.1f},{y1_px:.1f}), scale={scale_factor:.3f}")
                                                        
                                                        cropped = page_image.original.crop((x0_px, y0_px, x1_px, y1_px))
                                                        buf = io.BytesIO()
                                                        cropped.save(buf, format='PNG')
                                                        buf.seek(0)
                                                        img_bytes = buf.getvalue()
                                                        logger.info(f"[MULTIMODAL-BLOB] ‚úÖ PDF Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú ÏÑ±Í≥µ (Azure DI polygon bbox) idx={idx}, page={page_no_val}, size={len(img_bytes)} bytes, dimensions={cropped.size}")
                                                    else:
                                                        logger.warning(f"[MULTIMODAL-BLOB] Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Î∂ÄÏ°± - idx={idx}, inch=({width_inch:.2f}x{height_inch:.2f}), pixels=({width_px:.0f}x{height_px:.0f})")
                                            except Exception as img_err:
                                                logger.warning(f"[MULTIMODAL-BLOB] PDF Ïù¥ÎØ∏ÏßÄ ÌÅ¨Î°≠ Ïã§Ìå® idx={idx}, page={page_no_val}, bbox={bbox_val}, error={img_err}")
                                        elif bbox_val == [0, 0, 0, 0] or (isinstance(bbox_val, (list, tuple)) and all(v == 0 for v in bbox_val)):
                                            logger.warning(f"[MULTIMODAL-BLOB] ‚ö†Ô∏è Î¨¥Ìö®Ìïú bbox Í∞êÏßÄ - idx={idx}, bbox={bbox_val} ‚Üí pdfplumber ÏßÅÏ†ë Ï∂îÏ∂ú ÏãúÎèÑ")
                                            # Fallback: pdfplumberÏùò Ïù¥ÎØ∏ÏßÄ Í∞ùÏ≤¥ ÏßÅÏ†ë Ï∂îÏ∂ú
                                            try:
                                                page_index = page_no_val - 1
                                                if 0 <= page_index < len(pdf_pages):
                                                    page = pdf_pages[page_index]
                                                    images = page.images
                                                    logger.info(f"[MULTIMODAL-BLOB] pdfplumber Í∞êÏßÄ Ïù¥ÎØ∏ÏßÄ Ïàò: {len(images)} on page {page_no_val}")
                                                    
                                                    # ÌéòÏù¥ÏßÄ ÎÇ¥ Ïù¥ÎØ∏ÏßÄ ÏãúÌÄÄÏä§ Ï∂îÏ†ï (Azure DI: 1-based ‚Üí pdfplumber: 0-based Î≥ÄÌôò)
                                                    sequence_in_page = getattr(obj, 'sequence_in_page', 1)  # Azure DI default: 1
                                                    image_index = sequence_in_page - 1  # 0-based Ïù∏Îç±Ïä§Î°ú Î≥ÄÌôò
                                                    logger.info(f"[MULTIMODAL-BLOB] Ïù¥ÎØ∏ÏßÄ Ïù∏Îç±Ïã± Î≥ÄÌôò - Azure DI sequence={sequence_in_page} ‚Üí pdfplumber index={image_index}")
                                                    
                                                    # üéØ Î™®Îì† Ïù¥ÎØ∏ÏßÄÎ•º ÌÅ¨Í∏∞ÏàúÏúºÎ°ú Ï†ïÎ†¨ÌïòÏó¨ Í∞ÄÏû• ÌÅ∞ Í≤É ÏÑ†ÌÉù
                                                    if images:
                                                        # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Í≥ÑÏÇ∞ Î∞è Ï†ïÎ†¨
                                                        sized_images = []
                                                        for i, img_obj in enumerate(images):
                                                            x0, top, x1, bottom = img_obj['x0'], img_obj['top'], img_obj['x1'], img_obj['bottom']
                                                            width = x1 - x0
                                                            height = bottom - top
                                                            area = width * height
                                                            
                                                            # ÏµúÏÜå ÌÅ¨Í∏∞ ÌïÑÌÑ∞ (50x50 Ïù¥ÏÉÅ)
                                                            if width >= 50 and height >= 50:
                                                                sized_images.append({
                                                                    'index': i,
                                                                    'obj': img_obj,
                                                                    'width': width,
                                                                    'height': height,
                                                                    'area': area,
                                                                    'x0': x0,
                                                                    'top': top,
                                                                    'x1': x1,
                                                                    'bottom': bottom
                                                                })
                                                        
                                                        # Î©¥Ï†Å Í∏∞Ï§Ä ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨
                                                        sized_images.sort(key=lambda x: x['area'], reverse=True)
                                                        
                                                        logger.info(f"[MULTIMODAL-BLOB] Ïú†Ìö® Ïù¥ÎØ∏ÏßÄ {len(sized_images)}Í∞ú (ÏµúÏÜå 50x50 Ïù¥ÏÉÅ)")
                                                        
                                                        # sequenceÏóê Ìï¥ÎãπÌïòÎäî Ïù¥ÎØ∏ÏßÄ ÎòêÎäî Í∞ÄÏû• ÌÅ∞ Ïù¥ÎØ∏ÏßÄ ÏÑ†ÌÉù
                                                        target_img = None
                                                        if 0 <= image_index < len(sized_images):
                                                            target_img = sized_images[image_index]
                                                            logger.info(f"[MULTIMODAL-BLOB] sequence={image_index} Ïù¥ÎØ∏ÏßÄ ÏÑ†ÌÉù")
                                                        elif sized_images:
                                                            target_img = sized_images[0]
                                                            logger.info(f"[MULTIMODAL-BLOB] sequence Î≤îÏúÑ Ï¥àÍ≥º, Í∞ÄÏû• ÌÅ∞ Ïù¥ÎØ∏ÏßÄ ÏÑ†ÌÉù (area={target_img['area']:.0f})")
                                                        
                                                        if target_img:
                                                            import io
                                                            from PIL import Image  # type: ignore
                                                            
                                                            page_image = page.to_image(resolution=150)
                                                            cropped = page_image.original.crop((
                                                                target_img['x0'],
                                                                target_img['top'],
                                                                target_img['x1'],
                                                                target_img['bottom']
                                                            ))
                                                            
                                                            # üîç Ïù¥ÎØ∏ÏßÄ ÌíàÏßà Í≤ÄÏ¶ù (ÎÑàÎ¨¥ Îã®ÏàúÌïú Ïù¥ÎØ∏ÏßÄ Ï†úÏô∏)
                                                            import numpy as np
                                                            img_array = np.array(cropped)
                                                            
                                                            # ÏÉâÏÉÅ Î∂ÑÏÇ∞ Í≥ÑÏÇ∞ (Îã®ÏÉâ Ïù¥ÎØ∏ÏßÄ Ï†úÏô∏)
                                                            if len(img_array.shape) >= 3:
                                                                color_variance = np.var(img_array)
                                                                unique_colors = len(np.unique(img_array.reshape(-1, img_array.shape[-1]), axis=0))
                                                                logger.info(f"[MULTIMODAL-BLOB] Ïù¥ÎØ∏ÏßÄ ÌíàÏßà - variance={color_variance:.1f}, unique_colors={unique_colors}")
                                                                
                                                                # ÎÑàÎ¨¥ Îã®ÏàúÌïú Ïù¥ÎØ∏ÏßÄ Ï†úÏô∏ (ÏàúÎ∞±ÏÉâ, ÏàúÌùëÏÉâ Îì±)
                                                                if color_variance < 10 and unique_colors < 5:
                                                                    logger.warning(f"[MULTIMODAL-BLOB] ‚ùå Îã®Ïàú Ïù¥ÎØ∏ÏßÄ Ï†úÏô∏ (variance={color_variance:.1f}, colors={unique_colors})")
                                                                    target_img = None
                                                            
                                                            if target_img:
                                                                buf = io.BytesIO()
                                                                cropped.save(buf, format='PNG')
                                                                buf.seek(0)
                                                                img_bytes = buf.getvalue()
                                                                logger.info(f"[MULTIMODAL-BLOB] ‚úÖ pdfplumber ÏßÅÏ†ë Ï∂îÏ∂ú ÏÑ±Í≥µ idx={idx}, page={page_no_val}, size={len(img_bytes)} bytes, "
                                                                          f"dimensions={target_img['width']:.0f}x{target_img['height']:.0f}, "
                                                                          f"bbox=({target_img['x0']:.1f},{target_img['top']:.1f},{target_img['x1']:.1f},{target_img['bottom']:.1f})")
                                                        else:
                                                            logger.warning(f"[MULTIMODAL-BLOB] ‚ùå Ïú†Ìö®Ìïú Ïù¥ÎØ∏ÏßÄÎ•º Ï∞æÏßÄ Î™ªÌï®")
                                                    else:
                                                        logger.warning(f"[MULTIMODAL-BLOB] pdfplumber Ïù¥ÎØ∏ÏßÄ Ïù∏Îç±Ïä§ Î≤îÏúÑ Ï¥àÍ≥º - Azure DI sequence={sequence_in_page}, pdfplumber index={image_index}, available={len(images)}")
                                            except Exception as fallback_err:
                                                logger.warning(f"[MULTIMODAL-BLOB] pdfplumber ÏßÅÏ†ë Ï∂îÏ∂ú Ïã§Ìå® idx={idx}, page={page_no_val}, error={fallback_err}")
                                                import traceback
                                                logger.debug(traceback.format_exc())
                                        else:
                                            logger.warning(f"[MULTIMODAL-BLOB] bbox Í≤ÄÏ¶ù Ïã§Ìå® - idx={idx}, page_no_val={page_no_val} (type={type(page_no_val)}), bbox_val={bbox_val} (type={type(bbox_val)}, len={len(bbox_val) if bbox_val and hasattr(bbox_val, '__len__') else 'N/A'})")
                                    
                                    # DOCX Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ (Azure DI Î∞îÏù¥ÎÑàÎ¶¨Í∞Ä ÏóÜÎäî Í≤ΩÏö∞Îßå)
                                    elif not img_bytes and file_path.lower().endswith('.docx'):
                                        try:
                                            # ÏõêÎ≥∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨ Ï∞æÍ∏∞
                                            image_index = getattr(obj, 'sequence_in_page', None)
                                            page_no_val = getattr(obj, 'page_no', 1)
                                            
                                            # pages Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÏóêÏÑú Ìï¥Îãπ Ïù¥ÎØ∏ÏßÄ Ï∞æÍ∏∞
                                            if "pages" in metadata:
                                                for page in metadata["pages"]:
                                                    if page.get("page_no") == page_no_val:
                                                        for img_meta in page.get("images_metadata", []):
                                                            if img_meta.get("image_index") == image_index and 'binary_data' in img_meta:
                                                                img_bytes = img_meta['binary_data']
                                                                logger.debug(f"[MULTIMODAL-BLOB] DOCX Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨ Î∞úÍ≤¨ idx={idx}, size={len(img_bytes)}")
                                                                break
                                        except Exception as docx_err:
                                            logger.debug(f"[MULTIMODAL-BLOB] DOCX Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ïã§Ìå® idx={idx}: {docx_err}")
                                    # PPTX Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ (Azure DI Î∞îÏù¥ÎÑàÎ¶¨Í∞Ä ÏóÜÎäî Í≤ΩÏö∞Îßå)
                                    elif not img_bytes and file_path.lower().endswith('.pptx'):
                                        try:
                                            image_index = getattr(obj, 'sequence_in_page', None)
                                            slide_no_val = getattr(obj, 'page_no', 1)
                                            if "slides" in metadata:
                                                for slide in metadata["slides"]:
                                                    if slide.get("slide_no") == slide_no_val:
                                                        for img_meta in slide.get("images_metadata", []):
                                                            if img_meta.get("image_index") == image_index and 'binary_data' in img_meta:
                                                                img_bytes = img_meta['binary_data']
                                                                logger.debug(f"[MULTIMODAL-BLOB] PPTX Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨ Î∞úÍ≤¨ idx={idx}, size={len(img_bytes)}")
                                                                break
                                        except Exception as pptx_err:
                                            logger.debug(f"[MULTIMODAL-BLOB] PPTX Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ïã§Ìå® idx={idx}: {pptx_err}")
                                    
                                    # STEP 3: Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨ Í≤ÄÏ¶ù Î∞è Ï†ÄÏû•
                                    if img_bytes:
                                        logger.info(f"[MULTIMODAL-BLOB] ‚úÖ Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨ ÏµúÏ¢Ö ÌôïÎ≥¥ idx={idx}, size={len(img_bytes)} bytes, page={page_no_val}")
                                        try:
                                            # object_idÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏùºÍ¥ÄÎêú blob ÌÇ§ ÏÉùÏÑ±
                                            obj_id = getattr(obj, 'object_id', idx)
                                            img_blob_key = f"multimodal/{file_bss_info_sno}/objects/image_{obj_id}_{page_no_val}.png"
                                            azure.upload_bytes(img_bytes, img_blob_key, purpose='intermediate')
                                            image_ids_with_binary.add(obj_id)
                                            
                                            # B/C. Extract image features (pHash, dimensions)
                                            enhanced_features = {}
                                            if self.image_embedding_service:
                                                try:
                                                    features = await self.image_embedding_service.extract_features(img_bytes)
                                                    enhanced_features = {
                                                        "phash": features.get("phash"),
                                                        "width": features.get("width"),
                                                        "height": features.get("height"),
                                                        "aspect_ratio": features.get("aspect_ratio")
                                                    }
                                                    
                                                    # D. Update database object with extracted features
                                                    setattr(obj, 'phash', features.get("phash"))
                                                    setattr(obj, 'image_width', features.get("width"))
                                                    setattr(obj, 'image_height', features.get("height"))
                                                    
                                                    # C. Save enhanced feature JSON
                                                    feature_key = f"multimodal/{file_bss_info_sno}/objects/image_{obj_id}_{page_no_val}_features.json"
                                                    azure.upload_bytes(
                                                        json.dumps(features, ensure_ascii=False, indent=2).encode('utf-8'),
                                                        feature_key,
                                                        purpose='intermediate'
                                                    )
                                                except Exception as feat_err:
                                                    logger.debug(f"[MULTIMODAL-BLOB] Ïù¥ÎØ∏ÏßÄ ÌäπÏßï Ï∂îÏ∂ú Ïã§Ìå® obj_id={obj_id}: {feat_err}")
                                            
                                            objects_manifest.append({
                                                **_object_to_manifest_entry(idx, obj, blob_key),
                                                "binary_image_key": img_blob_key,
                                                "has_binary": True,
                                                **enhanced_features
                                            })
                                            saved_counts['IMAGE'] += 1
                                            continue
                                            
                                        except Exception as save_err:
                                            logger.warning(f"[MULTIMODAL-BLOB] Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• Ïã§Ìå® obj_id={obj_id}, page={page_no_val}: {save_err}")
                                    else:
                                        # Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨Î•º ÌôïÎ≥¥ÌïòÏßÄ Î™ªÌïú Í≤ΩÏö∞
                                        obj_id = getattr(obj, 'object_id', None)
                                        has_azure_binary = getattr(obj, 'binary_data', None) is not None
                                        bbox_val = getattr(obj, 'bbox', None)
                                        logger.warning(
                                            f"[MULTIMODAL-BLOB] ‚ùå Ïù¥ÎØ∏ÏßÄ Î∞îÏù¥ÎÑàÎ¶¨ ÏóÜÏùå - "
                                            f"idx={idx}, obj_id={obj_id}, page={page_no_val}, "
                                            f"Azure_DI_binary={'ÏûàÏùå' if has_azure_binary else 'ÏóÜÏùå'}, "
                                            f"bbox={bbox_val}, "
                                            f"file_type={file_path.split('.')[-1] if file_path else 'unknown'}"
                                        )
                            if blob_key:
                                objects_manifest.append({**_object_to_manifest_entry(idx, obj, blob_key), "has_binary": False})
                                otype = getattr(obj, 'object_type', None)
                                if isinstance(otype, str) and otype in saved_counts:
                                    saved_counts[otype] += 1
                        except Exception as oe:
                            msg = f"idx={idx} type={getattr(obj,'object_type',None)} err={oe}"
                            object_save_errors.append(msg)
                            logger.warning(f"[MULTIMODAL-BLOB] Í∞ùÏ≤¥ Ï†ÄÏû• Ïò§Î•ò: {msg}")
                    # PDF Î¨∏ÏÑú Ìï∏Îì§ Îã´Í∏∞
                    try:
                        if pdf_doc:
                            pdf_doc.close()
                    except Exception:
                        pass
                    
                    # Í∞ùÏ≤¥ Îß§ÎãàÌéòÏä§Ìä∏ Ï†ÄÏû•
                    manifest_key = f"multimodal/{file_bss_info_sno}/objects_manifest.json"
                    azure.upload_bytes(
                        json.dumps(objects_manifest, ensure_ascii=False, indent=2).encode('utf-8'),
                        manifest_key,
                        purpose='intermediate'
                    )
                    logger.info(f"[MULTIMODAL-BLOB] objects_manifest Ï†ÄÏû•: {manifest_key} ({len(objects_manifest)} entries)")
                    
                    # Ensure database objects are updated with extracted features
                    await session.flush()

                    logger.info(
                        f"[MULTIMODAL-BLOB] Í∞ùÏ≤¥ Ï†ÄÏû• ÏôÑÎ£å text={saved_counts['TEXT_BLOCK']} table={saved_counts['TABLE']} "
                        f"image={saved_counts['IMAGE']} figure={saved_counts['FIGURE']} errors={len(object_save_errors)}"
                    )
                    _stage(
                        "blob_intermediate_save", True,
                        objects_saved=len(objects_manifest),
                        text_blocks=saved_counts['TEXT_BLOCK'],
                        tables=saved_counts['TABLE'],
                        images=saved_counts['IMAGE'],
                        figures=saved_counts['FIGURE'],
                        object_save_errors=object_save_errors[:5]
                    )
            except Exception as blob_err:
                logger.warning(f"[MULTIMODAL-BLOB] Ï§ëÍ∞Ñ Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå® (Î¨¥ÏãúÌïòÍ≥† Í≥ÑÏÜç): {blob_err}")
                if performed_blob_intermediate:
                    _stage("blob_intermediate_save", False, error=str(blob_err))
            finally:
                if not performed_blob_intermediate:
                    _stage("blob_intermediate_save", False, skipped=True)

            # -----------------------------
            # 2. Chunking (advanced)
            # -----------------------------
            _start_stage("chunking")
            # SQLAlchemy Ïª¨Îüº ÏÜçÏÑ± ÎåÄÏã† ÏïàÏ†ÑÌïòÍ≤å getattr ÏÇ¨Ïö©
            text_objs = [o for o in extracted_objects if getattr(o, 'object_type', None) == "TEXT_BLOCK"]
            raw_image_objs = [o for o in extracted_objects if getattr(o, 'object_type', None) in ["IMAGE", "FIGURE"]]
            if image_object_ids_seen:
                image_objs = [
                    o for o in raw_image_objs
                    if getattr(o, 'object_id', None) in image_ids_with_binary
                ]
                skipped_images = len(raw_image_objs) - len(image_objs)
                if skipped_images > 0:
                    logger.info(
                        "[MULTIMODAL] Î∞îÏù¥ÎÑàÎ¶¨ ÎàÑÎùΩÏúºÎ°ú Ïù¥ÎØ∏ÏßÄ Ï≤≠ÌÅ¨ %sÍ∞ú Ï†úÏô∏ (object_ids=%s)",
                        skipped_images,
                        [getattr(o, 'object_id', None) for o in raw_image_objs if getattr(o, 'object_id', None) not in image_ids_with_binary],
                    )
            else:
                image_objs = raw_image_objs
            table_objs = [o for o in extracted_objects if getattr(o, 'object_type', None) == "TABLE"]
            text_objs = [o for o in text_objs if (getattr(o, 'content_text', '') or '').strip()]

            chunk_params: Dict[str, Any] = {
                "min_tokens": 80,
                "target_tokens": 280,
                "max_tokens": 420,
                "overlap_tokens": 40,
            }

            sections_info: List[Dict[str, Any]] = list(precomputed_sections_info) if precomputed_sections_info else []
            section_summary: Optional[Dict[str, Any]] = precomputed_section_summary
            # ÏÑπÏÖò ÏàúÏÑú Î≥¥Ï°¥: (type, index, title) ÌäúÌîåÍ≥º Í∞ùÏ≤¥ Î¶¨Ïä§Ìä∏ Ïåç
            section_groups: List[Tuple[Optional[Tuple[str, int, str]], List[DocExtractedObject]]] = []
            object_spans: List[Tuple[DocExtractedObject, int, int]] = list(section_object_spans)

            if apply_section_chunking and text_objs:
                try:
                    separator = "\n\n"
                    if section_combined_text and section_object_spans:
                        combined_text = section_combined_text
                        object_spans_local = list(section_object_spans)
                    else:
                        combined_parts: List[str] = []
                        object_spans_local = []
                        current_pos = 0
                        for obj in text_objs:
                            content = (getattr(obj, 'content_text', '') or '').strip()
                            if not content:
                                continue
                            if combined_parts:
                                combined_parts.append(separator)
                                current_pos += len(separator)
                            start_pos = current_pos
                            combined_parts.append(content)
                            current_pos += len(content)
                            object_spans_local.append((obj, start_pos, current_pos))

                        combined_text = "".join(combined_parts)
                        section_combined_text = combined_text
                        section_object_spans = list(object_spans_local)

                    if combined_text.strip() and not sections_info:
                        sections_info = self.section_detector.detect_sections(
                            combined_text,
                            pages=metadata.get("pages") or None,
                        )
                        section_summary = self.section_detector.get_section_summary(sections_info)

                    if sections_info:
                        if not section_chunking_meta.get("detected_sections"):
                            section_chunking_meta["detected_sections"] = [s.get("type") for s in sections_info]
                        if section_summary and not section_chunking_meta.get("summary"):
                            section_chunking_meta["summary"] = section_summary
                        object_spans = list(object_spans_local)

                        # ÏÑπÏÖò ÏàúÏÑú Î≥¥Ï°¥: (type, index, original_title) ÌäúÌîåÎ°ú Îß§Ìïë
                        object_section_map: Dict[DocExtractedObject, Optional[Tuple[str, int, str]]] = {}
                        for section in sections_info:
                            s_type = section.get("type")
                            s_index = section.get("index")  # ÏàúÏÑú Ïù∏Îç±Ïä§
                            s_title = section.get("original_title")  # ÏõêÎ≥∏ Ï†úÎ™©
                            s_start = section.get("start_pos", 0)
                            s_end = section.get("end_pos", 0)
                            for obj, span_start, span_end in object_spans:
                                if span_end <= s_start or span_start >= s_end:
                                    continue
                                # (type, index, title) ÌäúÌîåÎ°ú Ï†ÄÏû•ÌïòÏó¨ ÏàúÏÑú Î≥¥Ï°¥
                                object_section_map[obj] = (s_type, s_index, s_title)

                        current_group: List[DocExtractedObject] = []
                        current_label: Optional[Tuple[str, int, str]] = None
                        for obj in text_objs:
                            content = (getattr(obj, 'content_text', '') or '').strip()
                            if not content:
                                continue
                            label = object_section_map.get(obj)
                            if current_label is None:
                                current_label = label
                                current_group.append(obj)
                                continue
                            if label != current_label:
                                if current_group:
                                    section_groups.append((current_label, current_group))
                                current_group = [obj]
                                current_label = label
                            else:
                                current_group.append(obj)
                        if current_group:
                            section_groups.append((current_label, current_group))

                        chunk_params["section_chunking"] = {
                            "detected_sections": section_chunking_meta["detected_sections"],
                            "total_detected": len(section_chunking_meta["detected_sections"]),
                        }

                        if settings.storage_backend == 'azure_blob' and get_azure_blob_service and file_bss_info_sno:
                            try:
                                azure_factory2 = get_azure_blob_service if callable(get_azure_blob_service) else None
                                if not azure_factory2:
                                    raise RuntimeError("Azure Blob service factory not available")
                                azure_sections_service = azure_factory2()
                                sections_blob_path = f"multimodal/{file_bss_info_sno}/sections.json"
                                sections_payload = {
                                    "sections": sections_info,
                                    "summary": section_summary,
                                    "detected_at": datetime.now().isoformat(),
                                }
                                azure_sections_service.upload_bytes(
                                    json.dumps(sections_payload, ensure_ascii=False, indent=2).encode("utf-8"),
                                    sections_blob_path,
                                    purpose='intermediate'
                                )
                                section_chunking_meta["stored_to_blob"] = True
                                logger.info(f"[SECTION-DETECT] ÏÑπÏÖò Ï†ïÎ≥¥ Ï†ÄÏû•: {sections_blob_path}")
                            except Exception as section_blob_err:
                                logger.warning(f"[SECTION-DETECT] ÏÑπÏÖò Ï†ïÎ≥¥ Ï†ÄÏû• Ïã§Ìå® (Í∏∞Î≥∏ Ï≤≠ÌÇπ Í≥ÑÏÜç): {section_blob_err}")
                    else:
                        logger.info("[SECTION-DETECT] ÏÑπÏÖò Í∞êÏßÄ Í≤∞Í≥ºÍ∞Ä ÏóÜÏñ¥ Í∏∞Î≥∏ Ï≤≠ÌÇπ Ï†ÅÏö©")
                except Exception as sec_err:
                    logger.warning(f"[SECTION-DETECT] ÏÑπÏÖò Í∞êÏßÄ Ï§ë ÏòàÏô∏ Î∞úÏÉù (Í∏∞Î≥∏ Ï≤≠ÌÇπÏúºÎ°ú ÏßÑÌñâ): {sec_err}")

            section_chunk_counts: Dict[str, int] = {}
            adv_chunks: List[Dict[str, Any]] = []
            if section_groups:
                section_chunking_meta["enabled"] = True
                # ÏÑπÏÖò ÏàúÏÑú Î≥¥Ï°¥: labelÏùÄ (type, index, title) ÌäúÌîå
                for label, group in section_groups:
                    iterable = (
                        (
                            (getattr(o, 'content_text', '') or ''),
                            getattr(o, 'page_no', None),
                            getattr(o, 'object_id', None) or 0
                        ) for o in group
                    )
                    section_chunks = advanced_chunk_text(iterable)
                    
                    # ÏàúÏÑú Î≥¥Ï°¥ÏùÑ ÏúÑÌïú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
                    if label:
                        section_type, section_index, section_title = label
                        for chunk_dict in section_chunks:
                            chunk_dict['section'] = section_type  # ÏÑπÏÖò ÌÉÄÏûÖ (other, methods Îì±)
                            chunk_dict['section_title'] = section_title  # ÏõêÎ≥∏ Ï†úÎ™© (Related Work Îì±)
                            chunk_dict['section_index'] = section_index  # ÏàúÏÑú Ïù∏Îç±Ïä§
                    else:
                        for chunk_dict in section_chunks:
                            chunk_dict['section'] = None
                            chunk_dict['section_title'] = None
                            chunk_dict['section_index'] = None
                    
                    adv_chunks.extend(section_chunks)
                    key = label[0] if label else "unassigned"  # section_type ÏÇ¨Ïö©
                    section_chunk_counts[key] = section_chunk_counts.get(key, 0) + len(section_chunks)
            else:
                adv_chunks = advanced_chunk_text(
                    (
                        (
                            (getattr(o, 'content_text', '') or ''),
                            getattr(o, 'page_no', None),
                            getattr(o, 'object_id', None) or 0
                        ) for o in text_objs
                    )
                )
                section_chunking_meta["enabled"] = False

            if section_chunk_counts:
                section_chunking_meta["chunk_counts"] = section_chunk_counts
            
            chunk_session = DocChunkSession(
                file_bss_info_sno=file_bss_info_sno,
                extraction_session_id=extraction_session.extraction_session_id,
                strategy_name="advanced_paragraph_token",
                params_json=chunk_params,
                status="running",
                started_at=datetime.now()
            )
            session.add(chunk_session)
            await session.flush()
            result["chunk_session_id"] = chunk_session.chunk_session_id

            doc_chunks: List[DocChunk] = []
            for idx, cdict in enumerate(adv_chunks):
                # ÏÑπÏÖò ÏàúÏÑú Ï†ïÎ≥¥Î•º Î°úÍ∑∏Î°ú Í∏∞Î°ù (ÎîîÎ≤ÑÍπÖÏö©)
                if cdict.get('section_index') is not None:
                    logger.debug(
                        f"[SECTION-ORDER] Ï≤≠ÌÅ¨ {idx}: section_index={cdict['section_index']}, "
                        f"type={cdict.get('section')}, title={cdict.get('section_title')}"
                    )
                
                # page_range ÏÉùÏÑ±: page_numbersÏóêÏÑú ÏµúÏÜå/ÏµúÎåÄ ÌéòÏù¥ÏßÄ Ï∂îÏ∂ú
                page_range_value = None
                page_numbers = cdict.get('page_numbers', [])
                if page_numbers:
                    min_page = min(page_numbers)
                    max_page = max(page_numbers)
                    # PostgreSQL int4range: [lower, upper) ÌòïÏãù
                    # SQLAlchemy type_coerceÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Î¨∏ÏûêÏó¥ÏùÑ int4rangeÎ°ú Î≥ÄÌôò
                    page_range_str = f"[{min_page},{max_page + 1})"
                    page_range_value = type_coerce(text(f"'{page_range_str}'"), INT4RANGE)
                
                doc_chunk = DocChunk(
                    chunk_session_id=chunk_session.chunk_session_id,
                    file_bss_info_sno=file_bss_info_sno,
                    chunk_index=idx,
                    source_object_ids=cdict.get('source_object_ids', []),
                    content_text=cdict['content_text'],
                    token_count=cdict['token_count'],
                    modality="text",
                    section_heading=cdict.get('section'),  # ÏÑπÏÖò ÌÉÄÏûÖ (other, methods Îì±)
                    page_range=page_range_value  # ÌéòÏù¥ÏßÄ Î≤îÏúÑ Ï∂îÍ∞Ä
                )
                session.add(doc_chunk)
                doc_chunks.append(doc_chunk)
            
            # Ïù¥ÎØ∏ÏßÄ Ï≤≠ÌÅ¨ ÏÉùÏÑ± (Í∞Å Ïù¥ÎØ∏ÏßÄÎ•º ÎèÖÎ¶ΩÏ†ÅÏù∏ Ï≤≠ÌÅ¨Î°ú)
            image_chunk_start_idx = len(doc_chunks)
            for img_idx, img_obj in enumerate(image_objs):
                object_id = getattr(img_obj, 'object_id', None)
                page_no = getattr(img_obj, 'page_no', None)
                
                # Ïù¥ÎØ∏ÏßÄ Ï∫°ÏÖò/ÏÑ§Î™Ö Ï∂îÏ∂ú (ÏûàÏúºÎ©¥)
                img_text = getattr(img_obj, 'content_text', '') or f"Image on page {page_no}"
                
                # page_range ÏÉùÏÑ±: PostgreSQL int4range ÌÉÄÏûÖ '[start, end)' ÌòïÏãù
                page_range_value = None
                if page_no is not None:
                    # SQLAlchemy type_coerceÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Î¨∏ÏûêÏó¥ÏùÑ int4rangeÎ°ú Î≥ÄÌôò
                    page_range_str = f"[{page_no},{page_no + 1})"
                    page_range_value = type_coerce(text(f"'{page_range_str}'"), INT4RANGE)
                    logger.info(f"[IMAGE_CHUNK_CREATE] object_id={object_id}, page_no={page_no}, page_range={page_range_str}")
                
                # blob_key ÏÉùÏÑ±: Blob Storage ÌååÏùº Í≤ΩÎ°ú
                blob_key_value = None
                if object_id and page_no is not None:
                    blob_key_value = f"multimodal/{file_bss_info_sno}/objects/image_{object_id}_{page_no}.png"
                    logger.info(f"[IMAGE_CHUNK_CREATE] blob_key={blob_key_value}")
                
                img_chunk = DocChunk(
                    chunk_session_id=chunk_session.chunk_session_id,
                    file_bss_info_sno=file_bss_info_sno,
                    chunk_index=image_chunk_start_idx + img_idx,
                    source_object_ids=[object_id] if object_id else [],
                    content_text=img_text,
                    token_count=0,  # Ïù¥ÎØ∏ÏßÄÎäî ÌÜ†ÌÅ∞ Ïπ¥Ïö¥Ìä∏ 0
                    modality="image",
                    page_range=page_range_value,  # ÌéòÏù¥ÏßÄ Î≤îÏúÑ
                    blob_key=blob_key_value  # Blob Storage ÌååÏùº Í≤ΩÎ°ú
                )
                session.add(img_chunk)
                doc_chunks.append(img_chunk)
            
            # Ìëú Ï≤≠ÌÅ¨ ÏÉùÏÑ± (Í∞Å ÌëúÎ•º ÎèÖÎ¶ΩÏ†ÅÏù∏ Ï≤≠ÌÅ¨Î°ú)
            table_chunk_start_idx = len(doc_chunks)
            for tbl_idx, tbl_obj in enumerate(table_objs):
                object_id = getattr(tbl_obj, 'object_id', None)
                page_no = getattr(tbl_obj, 'page_no', None)
                
                # ÌëúÎ•º ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò (ÎßàÌÅ¨Îã§Ïö¥ ÎòêÎäî CSV ÌòïÏãù)
                table_text = _serialize_table_to_text(tbl_obj)
                
                # ÌÜ†ÌÅ∞ Ïàò Ï∂îÏ†ï (Í≥µÎ∞± Í∏∞Ï§Ä Îã®Ïàú Í≥ÑÏÇ∞)
                token_count = len(table_text.split()) if table_text else 0
                
                # page_range ÏÉùÏÑ±: PostgreSQL int4range ÌÉÄÏûÖ
                page_range_value = None
                if page_no is not None:
                    # SQLAlchemy type_coerceÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Î¨∏ÏûêÏó¥ÏùÑ int4rangeÎ°ú Î≥ÄÌôò
                    page_range_str = f"[{page_no},{page_no + 1})"
                    page_range_value = type_coerce(text(f"'{page_range_str}'"), INT4RANGE)
                
                # blob_key ÏÉùÏÑ±: Blob Storage ÌååÏùº Í≤ΩÎ°ú (ÌÖåÏù¥Î∏îÏùÄ JSON)
                blob_key_value = None
                if object_id and page_no is not None:
                    blob_key_value = f"multimodal/{file_bss_info_sno}/objects/table_{object_id}_{page_no}.json"
                
                tbl_chunk = DocChunk(
                    chunk_session_id=chunk_session.chunk_session_id,
                    file_bss_info_sno=file_bss_info_sno,
                    chunk_index=table_chunk_start_idx + tbl_idx,
                    source_object_ids=[object_id] if object_id else [],
                    content_text=table_text,
                    token_count=token_count,
                    modality="table",
                    page_range=page_range_value,  # ÌéòÏù¥ÏßÄ Î≤îÏúÑ
                    blob_key=blob_key_value  # Blob Storage ÌååÏùº Í≤ΩÎ°ú
                )
                session.add(tbl_chunk)
                doc_chunks.append(tbl_chunk)
            
            await session.flush()
            setattr(chunk_session, "status", "success")
            setattr(chunk_session, "completed_at", datetime.now())
            setattr(chunk_session, "chunk_count", len(doc_chunks))
            result["chunks_count"] = len(doc_chunks)
            logger.info(f"[MULTIMODAL] Ï≤≠ÌÇπ ÏôÑÎ£å - ÌÖçÏä§Ìä∏: {len(adv_chunks)}Í∞ú, Ïù¥ÎØ∏ÏßÄ: {len(image_objs)}Í∞ú, Ìëú: {len(table_objs)}Í∞ú, Ï†ÑÏ≤¥: {len(doc_chunks)}Í∞ú")
            if section_chunking_meta.get("enabled"):
                logger.info(f"[SECTION-DETECT] ÏÑπÏÖò Í∏∞Î∞ò Ï≤≠ÌÇπ Ï†ÅÏö© - chunk_counts={section_chunking_meta.get('chunk_counts')}")

            chunk_stage_payload = {
                "chunks": len(doc_chunks),
                "text_chunks": len(adv_chunks),
                "image_chunks": len(image_objs),
                "table_chunks": len(table_objs),
            }
            if section_chunking_meta.get("requested"):
                chunk_stage_payload["section_chunking"] = section_chunking_meta
            _stage("chunking", True, **chunk_stage_payload)

            # -----------------------------
            # 2.4. vs_doc_contents_chunks ÌÖåÏù¥Î∏îÏóê Ï≤≠ÌÅ¨ Ï†ÄÏû• (RAG Í∏∞Îä• ÏßÄÏõê)
            # -----------------------------
            try:
                logger.info(f"[MULTIMODAL][RAG] vs_doc_contents_chunks Ï†ÄÏû• ÏãúÏûë - {len(doc_chunks)}Í∞ú Ï≤≠ÌÅ¨")
                
                for chunk in doc_chunks:
                    # Ï≤≠ÌÅ¨ ÌÖçÏä§Ìä∏ Î∞è Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
                    chunk_text = getattr(chunk, 'content_text', '') or ''
                    chunk_size = len(chunk_text)
                    chunk_idx = getattr(chunk, 'chunk_index', 0)
                    chunk_modality = getattr(chunk, 'modality', 'text')  # Ïã§Ï†ú modality ÏÇ¨Ïö©
                    
                    # ÌéòÏù¥ÏßÄ Î≤àÌò∏ Ï∂îÏ†ï (source_object_idsÏóêÏÑú Ï∂îÏ∂ú)
                    page_number = None
                    source_object_ids = getattr(chunk, 'source_object_ids', [])
                    if source_object_ids and extracted_objects:
                        # Ï≤´ Î≤àÏß∏ ÏÜåÏä§ Í∞ùÏ≤¥Ïùò ÌéòÏù¥ÏßÄ Î≤àÌò∏ ÏÇ¨Ïö©
                        for obj in extracted_objects:
                            if getattr(obj, 'object_id', None) == source_object_ids[0]:
                                page_number = getattr(obj, 'page_number', None)
                                break
                    
                    # VsDocContentsChunks Î†àÏΩîÎìú ÏÉùÏÑ±
                    vs_chunk = VsDocContentsChunks(
                        file_bss_info_sno=file_bss_info_sno,
                        chunk_index=chunk_idx,
                        chunk_text=chunk_text,
                        chunk_size=chunk_size,
                        chunk_embedding=None,  # ÏûÑÎ≤†Îî©ÏùÄ ÎÇòÏ§ëÏóê ÏóÖÎç∞Ïù¥Ìä∏
                        page_number=page_number,
                        section_title=None,  # TODO: ÏÑπÏÖò Ï†úÎ™© Ï∂îÏ∂ú Î°úÏßÅ Ï∂îÍ∞Ä
                        keywords=None,  # TODO: Ï≤≠ÌÅ¨Î≥Ñ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú Ï∂îÍ∞Ä
                        named_entities=None,  # TODO: Ï≤≠ÌÅ¨Î≥Ñ Í∞úÏ≤¥Î™Ö Ï∂îÏ∂ú Ï∂îÍ∞Ä
                        knowledge_container_id=container_id,
                        metadata_json=json.dumps({
                            'chunk_id': getattr(chunk, 'chunk_id', None),
                            'token_count': getattr(chunk, 'token_count', 0),
                            'modality': chunk_modality,  # Ïã§Ï†ú modality Î∞òÏòÅ (text/image/table)
                            'source_object_ids': source_object_ids,
                            'chunk_session_id': chunk_session.chunk_session_id
                        }, ensure_ascii=False),
                        created_by=user_emp_no,
                        del_yn='N'
                    )
                    session.add(vs_chunk)
                
                await session.flush()
                logger.info(f"[MULTIMODAL][RAG] ‚úÖ vs_doc_contents_chunks Ï†ÄÏû• ÏôÑÎ£å - {len(doc_chunks)}Í∞ú")
                
            except Exception as vs_err:
                logger.error(f"[MULTIMODAL][RAG] ‚ùå vs_doc_contents_chunks Ï†ÄÏû• Ïã§Ìå®: {vs_err}")
                # Ïã§Ìå®Ìï¥ÎèÑ Í≥ÑÏÜç ÏßÑÌñâ (Í≤ÄÏÉâ Ïù∏Îç±Ïä§Îäî Î≥ÑÎèÑ)

            # -----------------------------
            # 2.5. Azure Blob Storage - Ï≤≠ÌÇπ Í≤∞Í≥º Ï†ÄÏû• (derived)
            # -----------------------------
            performed_blob_derived = False
            try:
                if settings.storage_backend == 'azure_blob' and get_azure_blob_service and file_bss_info_sno:
                    _start_stage("blob_derived_save")
                    performed_blob_derived = True
                    azure_factory3 = get_azure_blob_service if callable(get_azure_blob_service) else None
                    if not azure_factory3:
                        raise RuntimeError("Azure Blob service factory not available")
                    azure = azure_factory3()
                    
                    # Ï≤≠ÌÇπ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
                    chunk_metadata_key = f"multimodal/{file_bss_info_sno}/chunking_metadata.json"
                    chunk_metadata = {
                        "chunk_session_id": chunk_session.chunk_session_id,
                        "strategy_name": "advanced_paragraph_token",
                        "chunk_count": len(doc_chunks),
                        "params": {
                            "min_tokens": 80,
                            "target_tokens": 280,
                            "max_tokens": 420,
                            "overlap_tokens": 40
                        },
                        "timestamp": datetime.now().isoformat()
                    }
                    azure.upload_bytes(
                        json.dumps(chunk_metadata, ensure_ascii=False).encode('utf-8'),
                        chunk_metadata_key,
                        purpose='derived'
                    )
                    
                    # Í∞úÎ≥Ñ Ï≤≠ÌÅ¨ Ï†ÄÏû•
                    chunk_manifest = []
                    for idx, chunk in enumerate(doc_chunks):
                        chunk_modality = getattr(chunk, 'modality', 'text')
                        chunk_key = f"multimodal/{file_bss_info_sno}/chunks/chunk_{idx:04d}_{chunk_modality}.json"
                        chunk_content = {
                            "chunk_id": getattr(chunk, 'chunk_id', None),
                            "chunk_index": idx,
                            "content_text": getattr(chunk, 'content_text', ''),
                            "token_count": getattr(chunk, 'token_count', 0),
                            "modality": chunk_modality,
                            "source_object_ids": getattr(chunk, 'source_object_ids', [])
                        }
                        azure.upload_bytes(
                            json.dumps(chunk_content, ensure_ascii=False).encode('utf-8'),
                            chunk_key,
                            purpose='derived'
                        )
                        chunk_manifest.append({
                            "chunk_index": idx,
                            "key": chunk_key,
                            "modality": chunk_modality,
                            "char_count": len(chunk_content["content_text"]),
                            "token_count": chunk_content["token_count"]
                        })
                    
                    # Ï≤≠ÌÅ¨ Îß§ÎãàÌéòÏä§Ìä∏ Ï†ÄÏû•
                    manifest_key = f"multimodal/{file_bss_info_sno}/chunks_manifest.json"
                    azure.upload_bytes(
                        json.dumps(chunk_manifest, ensure_ascii=False).encode('utf-8'),
                        manifest_key,
                        purpose='derived'
                    )
                    
                    logger.info(f"[MULTIMODAL-BLOB] {len(doc_chunks)}Í∞ú Ï≤≠ÌÅ¨ Î∞è Îß§ÎãàÌéòÏä§Ìä∏ Ï†ÄÏû• ÏôÑÎ£å")
                    _stage("blob_derived_save", True, chunks_saved=len(doc_chunks))
                    
            except Exception as blob_err:
                logger.warning(f"[MULTIMODAL-BLOB] Ï≤≠ÌÇπ Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå® (Î¨¥ÏãúÌïòÍ≥† Í≥ÑÏÜç): {blob_err}")
                if performed_blob_derived:
                    _stage("blob_derived_save", False, error=str(blob_err))
            finally:
                if not performed_blob_derived:
                    _stage("blob_derived_save", False, skipped=True)

            # -----------------------------
            # 3. Embeddings (ÌÖçÏä§Ìä∏ + CLIP Î©ÄÌã∞Î™®Îã¨)
            # -----------------------------
            _start_stage("embedding")
            current_embedding_model = settings.get_current_embedding_model()
            max_dim = settings.vector_dimension
            clip_dim = 512  # CLIP ÏûÑÎ≤†Îî© Ï∞®Ïõê
            embed_success = 0
            clip_embed_success = 0
            chunk_embeddings = {}  # chunk_index -> vector Îß§Ìïë
            
            # üöÄ Î∞∞Ïπò ÏûÑÎ≤†Îî© ÏµúÏ†ÅÌôî: ÌÖçÏä§Ìä∏ Ï≤≠ÌÅ¨ + Ïù¥ÎØ∏ÏßÄ Ï∫°ÏÖòÏùÑ Ìïú Î≤àÏóê Ï≤òÎ¶¨
            text_chunks_list = []
            text_chunk_indices = []
            for idx, ch in enumerate(doc_chunks):
                modality = getattr(ch, 'modality', 'text')
                content = (getattr(ch, 'content_text', '') or '').strip()
                
                # ÌÖçÏä§Ìä∏ Ï≤≠ÌÅ¨ ÎòêÎäî ÏùòÎØ∏ ÏûàÎäî Ï∫°ÏÖòÏù¥ ÏûàÎäî Ïù¥ÎØ∏ÏßÄ Ï≤≠ÌÅ¨ Ìè¨Ìï®
                if modality == 'text' and content:
                    text_chunks_list.append(content)
                    text_chunk_indices.append(idx)
                elif modality == 'image' and content:
                    # Ïù¥ÎØ∏ÏßÄ Ï∫°ÏÖòÎèÑ ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© ÏÉùÏÑ± (ÏùºÎ∞ò Í≤ÄÏÉâÏóêÏÑúÎèÑ Ï∞æÏùÑ Ïàò ÏûàÎèÑÎ°ù)
                    text_chunks_list.append(content)
                    text_chunk_indices.append(idx)
                    logger.debug(f"[MULTIMODAL][IMAGE-CAPTION] Ïù¥ÎØ∏ÏßÄ Ï∫°ÏÖò Î∞∞Ïπò Ï∂îÍ∞Ä: idx={idx}, caption='{content[:60]}'")
            
            # Î∞∞Ïπò ÏûÑÎ≤†Îî© ÏÉùÏÑ± (Ìïú Î≤àÏùò API Ìò∏Ï∂úÎ°ú Ïó¨Îü¨ ÌÖçÏä§Ìä∏ Ï≤òÎ¶¨)
            text_embeddings_batch = []
            if text_chunks_list:
                logger.info(f"[MULTIMODAL][BATCH-EMB] ÌÖçÏä§Ìä∏ Î∞∞Ïπò ÏûÑÎ≤†Îî© ÏãúÏûë: {len(text_chunks_list)}Í∞ú")
                try:
                    from app.services.core.embedding_service import EmbeddingService
                    emb_service = EmbeddingService()
                    text_embeddings_batch = await emb_service.get_embeddings_batch(text_chunks_list, batch_size=100)
                    logger.info(f"[MULTIMODAL][BATCH-EMB] ‚úÖ ÌÖçÏä§Ìä∏ Î∞∞Ïπò ÏûÑÎ≤†Îî© ÏôÑÎ£å: {len(text_embeddings_batch)}Í∞ú")
                except Exception as batch_err:
                    logger.error(f"[MULTIMODAL][BATCH-EMB] ‚ùå Î∞∞Ïπò ÏûÑÎ≤†Îî© Ïã§Ìå® (Ìè¥Î∞± Ï≤òÎ¶¨): {batch_err}")
                    # Ïã§Ìå® Ïãú Í∞úÎ≥Ñ Ï≤òÎ¶¨Î°ú Ìè¥Î∞±
                    text_embeddings_batch = []
            
            # Î∞∞Ïπò Í≤∞Í≥ºÎ•º Ï≤≠ÌÅ¨ Ïù∏Îç±Ïä§Ïóê Îß§Ìïë
            text_embedding_map = {}
            for idx, vec in zip(text_chunk_indices, text_embeddings_batch):
                text_embedding_map[idx] = vec
            
            # Ï≤≠ÌÅ¨Î≥ÑÎ°ú ÏûÑÎ≤†Îî© Ï†ÄÏû• (Î∞∞Ïπò Í≤∞Í≥º + CLIP Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî©)
            for idx, ch in enumerate(doc_chunks):
                try:
                    modality = getattr(ch, 'modality', 'text')
                    vec = None
                    clip_vec = None
                    
                    # ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî©: Î∞∞Ïπò Í≤∞Í≥ºÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞ (ÌÖçÏä§Ìä∏ Ï≤≠ÌÅ¨ + Ïù¥ÎØ∏ÏßÄ Ï∫°ÏÖò)
                    if idx in text_embedding_map:
                        vec = text_embedding_map[idx]
                        if modality == 'image':
                            logger.info(f"[MULTIMODAL][IMAGE-CAPTION] ‚úÖ Ïù¥ÎØ∏ÏßÄ Ï∫°ÏÖò ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© Ï†ÅÏö©: chunk={ch.chunk_id}")
                        else:
                            logger.debug(f"[MULTIMODAL][BATCH-EMB] ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© Îß§Ìïë: chunk={ch.chunk_id}, idx={idx}")
                    elif modality == 'text':
                        # TEXT Ï≤≠ÌÅ¨Í∞Ä Î∞∞ÏπòÏóêÏÑú ÎàÑÎùΩÎêú Í≤ΩÏö∞ÏóêÎßå Í∞úÎ≥Ñ ÏÉùÏÑ± (Ìè¥Î∞±)
                        content = getattr(ch, 'content_text', '') or ''
                        if content.strip():
                            vec = await korean_nlp_service.generate_korean_embedding(content)
                            logger.warning(f"[MULTIMODAL][BATCH-EMB] Î∞∞Ïπò ÎàÑÎùΩ - Í∞úÎ≥Ñ ÏûÑÎ≤†Îî© ÏÉùÏÑ±: chunk={ch.chunk_id}")
                    
                    # Ïù¥ÎØ∏ÏßÄ Ï≤≠ÌÅ¨Ïù∏ Í≤ΩÏö∞ CLIP ÏûÑÎ≤†Îî© ÏÉùÏÑ± (Í∞úÎ≥Ñ Ï≤òÎ¶¨ Ïú†ÏßÄ)
                    if modality == 'image' and self.image_embedding_service:
                        try:
                            # Ïù¥ÎØ∏ÏßÄ Í∞ùÏ≤¥ Ï°∞Ìöå
                            source_object_ids = getattr(ch, 'source_object_ids', [])
                            if source_object_ids:
                                # Ï≤´ Î≤àÏß∏ Ïù¥ÎØ∏ÏßÄ Í∞ùÏ≤¥ÏóêÏÑú CLIP ÏûÑÎ≤†Îî© ÏÉùÏÑ±
                                img_obj_result = await session.execute(
                                    select(DocExtractedObject)
                                    .where(DocExtractedObject.object_id == source_object_ids[0])
                                )
                                img_obj = img_obj_result.scalar_one_or_none()
                                
                                if img_obj and settings.storage_backend == 'azure_blob':
                                    # Azure BlobÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú
                                    azure_factory4 = get_azure_blob_service if callable(get_azure_blob_service) else None
                                    if not azure_factory4:
                                        raise RuntimeError("Azure Blob service factory not available")
                                    azure = azure_factory4()
                                    page_no_val = getattr(img_obj, 'page_no', 0) or 0
                                    img_blob_key = f"multimodal/{file_bss_info_sno}/objects/image_{img_obj.object_id}_{page_no_val}.png"
                                    img_bytes = azure.download_blob_to_bytes(img_blob_key, purpose='intermediate')
                                    
                                    if img_bytes:
                                        # CLIP Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî© ÏÉùÏÑ±
                                        clip_vec = await self.image_embedding_service.generate_image_embedding(
                                            image_bytes=img_bytes
                                        )
                                        if clip_vec:
                                            clip_embed_success += 1
                                            logger.info(f"[MULTIMODAL][CLIP] ‚úÖ Ïù¥ÎØ∏ÏßÄ CLIP ÏûÑÎ≤†Îî© ÏÉùÏÑ±: chunk={ch.chunk_id}, dim={len(clip_vec)}")
                        except Exception as clip_err:
                            logger.warning(f"[MULTIMODAL][CLIP] CLIP ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ïã§Ìå® chunk={ch.chunk_id}: {clip_err}")
                    
                    # ÌÖçÏä§Ìä∏ Ï≤≠ÌÅ¨Ïùò Í≤ΩÏö∞ÏóêÎèÑ CLIP ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© ÏÉùÏÑ± Í∞ÄÎä• (ÏÑ†ÌÉùÏ†Å)
                    elif modality == 'text' and self.image_embedding_service and getattr(settings, 'enable_text_clip_embedding', False):
                        try:
                            content_text = getattr(ch, 'content_text', '') or ''
                            if content_text.strip():
                                clip_vec = await self.image_embedding_service.generate_text_embedding(content_text)
                                if clip_vec:
                                    clip_embed_success += 1
                                    logger.info(f"[MULTIMODAL][CLIP] ‚úÖ ÌÖçÏä§Ìä∏ CLIP ÏûÑÎ≤†Îî© ÏÉùÏÑ±: chunk={ch.chunk_id}, dim={len(clip_vec)}")
                        except Exception as clip_err:
                            logger.warning(f"[MULTIMODAL][CLIP] ÌÖçÏä§Ìä∏ CLIP ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ïã§Ìå® chunk={ch.chunk_id}: {clip_err}")
                    
                    # ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ï†ÄÏû• (ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© ÎòêÎäî CLIP ÏûÑÎ≤†Îî© Ï§ë ÌïòÎÇòÎùºÎèÑ ÏûàÏúºÎ©¥ Ï†ÄÏû•)
                    if vec or clip_vec:
                        # ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞ Ìå®Îî© (ÏûàÎäî Í≤ΩÏö∞Îßå)
                        if vec:
                            if len(vec) < max_dim:
                                vec = vec + [0.0] * (max_dim - len(vec))
                            elif len(vec) > max_dim:
                                vec = vec[:max_dim]
                        
                        # CLIP Î≤°ÌÑ∞ Ìå®Îî© (ÏûàÎäî Í≤ΩÏö∞Îßå)
                        if clip_vec:
                            if len(clip_vec) < clip_dim:
                                clip_vec = clip_vec + [0.0] * (clip_dim - len(clip_vec))
                            elif len(clip_vec) > clip_dim:
                                clip_vec = clip_vec[:clip_dim]
                        
                        # Î≤§ÎçîÎ≥Ñ Î≤°ÌÑ∞ Ïª¨Îüº Ìï†Îãπ
                        provider = None
                        azure_vec_1536 = None
                        azure_vec_3072 = None
                        azure_clip_vec = None
                        aws_vec_1024 = None
                        aws_vec_256 = None
                        
                        if vec:
                            if max_dim == 1536:
                                provider = 'azure'
                                azure_vec_1536 = vec
                            elif max_dim == 3072:
                                provider = 'azure'
                                azure_vec_3072 = vec
                            elif max_dim == 1024:
                                provider = 'aws'
                                aws_vec_1024 = vec
                            elif max_dim == 256:
                                provider = 'aws'
                                aws_vec_256 = vec
                        
                        if clip_vec:
                            azure_clip_vec = clip_vec  # CLIPÏùÄ Azure Ï†ÑÏö©
                            if not provider:
                                provider = 'azure'
                        
                        emb = DocEmbedding(
                            chunk_id=ch.chunk_id,
                            file_bss_info_sno=file_bss_info_sno,
                            provider=provider,
                            model_name=current_embedding_model,
                            modality=modality,
                            dimension=max_dim,
                            azure_vector_1536=azure_vec_1536,
                            azure_vector_3072=azure_vec_3072,
                            azure_clip_vector=azure_clip_vec,
                            aws_vector_1024=aws_vec_1024,
                            aws_vector_256=aws_vec_256,
                            vector=vec,  # Î†àÍ±∞Ïãú Ìò∏Ìôò
                            clip_vector=clip_vec  # Î†àÍ±∞Ïãú Ìò∏Ìôò
                        )
                        session.add(emb)
                        embed_success += 1
                        
                        # Ï≤≠ÌÅ¨ Ïù∏Îç±Ïä§ Îß§Ìïë Ï†ÄÏû• (vs_doc_contents_chunks ÏóÖÎç∞Ïù¥Ìä∏Ïö©, ÌÖçÏä§Ìä∏ Î≤°ÌÑ∞Í∞Ä ÏûàÎäî Í≤ΩÏö∞Îßå)
                        if vec:
                            chunk_idx = getattr(ch, 'chunk_index', None)
                            if chunk_idx is not None:
                                chunk_embeddings[chunk_idx] = vec
                            
                except Exception as ee:
                    logger.warning(f"[MULTIMODAL] Embedding Ïã§Ìå® chunk={ch.chunk_id}: {ee}")
            await session.flush()
            result["embeddings_count"] = embed_success
            result["clip_embeddings_count"] = clip_embed_success
            _stage("embedding", True, embeddings=embed_success, clip_embeddings=clip_embed_success)

            # -----------------------------
            # 3.1. vs_doc_contents_chunks ÏûÑÎ≤†Îî© ÏóÖÎç∞Ïù¥Ìä∏ (RAG Í∏∞Îä• ÏßÄÏõê)
            # -----------------------------
            try:
                logger.info(f"[MULTIMODAL][RAG] vs_doc_contents_chunks ÏûÑÎ≤†Îî© ÏóÖÎç∞Ïù¥Ìä∏ ÏãúÏûë - {len(chunk_embeddings)}Í∞ú")
                
                # vs_doc_contents_chunks Î†àÏΩîÎìú Ï°∞Ìöå Î∞è ÏóÖÎç∞Ïù¥Ìä∏
                from sqlalchemy import update
                
                for chunk_idx, vec in chunk_embeddings.items():
                    stmt = (
                        update(VsDocContentsChunks)
                        .where(VsDocContentsChunks.file_bss_info_sno == file_bss_info_sno)
                        .where(VsDocContentsChunks.chunk_index == chunk_idx)
                        .values(chunk_embedding=vec)
                    )
                    await session.execute(stmt)
                
                await session.flush()
                logger.info(f"[MULTIMODAL][RAG] ‚úÖ vs_doc_contents_chunks ÏûÑÎ≤†Îî© ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å - {len(chunk_embeddings)}Í∞ú")
                
            except Exception as emb_err:
                logger.error(f"[MULTIMODAL][RAG] ‚ùå vs_doc_contents_chunks ÏûÑÎ≤†Îî© ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {emb_err}")
                # Ïã§Ìå®Ìï¥ÎèÑ Í≥ÑÏÜç ÏßÑÌñâ

            # -----------------------------
            # 3.5. Azure Blob Storage - ÏûÑÎ≤†Îî© Í≤∞Í≥º Ï†ÄÏû• (derived)
            # -----------------------------
            performed_blob_embedding = False
            try:
                if settings.storage_backend == 'azure_blob' and get_azure_blob_service and file_bss_info_sno:
                    _start_stage("blob_embedding_save")
                    performed_blob_embedding = True
                    azure_factory5 = get_azure_blob_service if callable(get_azure_blob_service) else None
                    if not azure_factory5:
                        raise RuntimeError("Azure Blob service factory not available")
                    azure = azure_factory5()
                    
                    # ÏûÑÎ≤†Îî© Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
                    embedding_metadata_key = f"multimodal/{file_bss_info_sno}/embedding_metadata.json"
                    embedding_metadata = {
                        "model_name": current_embedding_model,
                        "vector_dimension": max_dim,
                        "embeddings_generated": embed_success,
                        "total_chunks": len(doc_chunks),
                        "timestamp": datetime.now().isoformat()
                    }
                    azure.upload_bytes(
                        json.dumps(embedding_metadata, ensure_ascii=False).encode('utf-8'),
                        embedding_metadata_key,
                        purpose='derived'
                    )
                    
                    logger.info(f"[MULTIMODAL-BLOB] ÏûÑÎ≤†Îî© Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû• ÏôÑÎ£å - {embed_success}/{len(doc_chunks)} ÏûÑÎ≤†Îî©")
                    _stage("blob_embedding_save", True, embeddings_saved=embed_success)
                    
            except Exception as blob_err:
                logger.warning(f"[MULTIMODAL-BLOB] ÏûÑÎ≤†Îî© Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå® (Î¨¥ÏãúÌïòÍ≥† Í≥ÑÏÜç): {blob_err}")
                if performed_blob_embedding:
                    _stage("blob_embedding_save", False, error=str(blob_err))
            finally:
                if not performed_blob_embedding:
                    _stage("blob_embedding_save", False, skipped=True)

            # -----------------------------
            # 4. Search Index Creation (ÌÜµÌï© Í≤ÄÏÉâ Ïù∏Îç±Ïä§ ÏÉùÏÑ±)
            # -----------------------------
            _start_stage("search_index_creation")
            try:
                # 4.1. ÌååÏùº Ï†ïÎ≥¥ Ï°∞Ìöå
                stmt_file = select(TbFileBssInfo).where(TbFileBssInfo.file_bss_info_sno == file_bss_info_sno)
                file_result = await session.execute(stmt_file)
                file_info = file_result.scalar_one_or_none()
                
                if not file_info:
                    logger.warning(f"[MULTIMODAL] ÌååÏùº Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏùå: {file_bss_info_sno}")
                    _stage("search_index_creation", False, error="File info not found")
                else:
                    # 4.2. Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ ÏàòÏßë (Î™®Îì† Ï≤≠ÌÅ¨ ÌÜµÌï©)
                    full_text_parts = []
                    image_count = 0
                    table_count = 0
                    
                    for chunk in doc_chunks:
                        content = getattr(chunk, 'content_text', '') or ''
                        if content:
                            full_text_parts.append(content)
                    
                    # Ï∂îÏ∂úÎêú Í∞ùÏ≤¥ÏóêÏÑú Ïù¥ÎØ∏ÏßÄ/ÌÖåÏù¥Î∏î Í∞úÏàò ÌôïÏù∏
                    for obj in extracted_objects:
                        obj_type = getattr(obj, 'object_type', '')
                        if obj_type == 'IMAGE':
                            image_count += 1
                        elif obj_type == 'TABLE':
                            table_count += 1
                    
                    full_content = '\n\n'.join(full_text_parts)
                    
                    logger.info(f"[MULTIMODAL] Í≤ÄÏÉâ Ïù∏Îç±Ïä§ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏôÑÎ£å - "
                               f"ÌÖçÏä§Ìä∏ Ï≤≠ÌÅ¨: {len(doc_chunks)}Í∞ú, "
                               f"Ïù¥ÎØ∏ÏßÄ: {image_count}Í∞ú, "
                               f"ÌÖåÏù¥Î∏î: {table_count}Í∞ú, "
                               f"Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: {len(full_content)}")
                    
                    # 4.3. NLP Î∂ÑÏÑù (Ï†ÑÏ≤¥ Î¨∏ÏÑú Î†àÎ≤®)
                    # textsearch_koÍ∞Ä ÏûêÎèôÏúºÎ°ú ÌòïÌÉúÏÜå Î∂ÑÏÑùÏùÑ ÌïòÎØÄÎ°ú, Í∞ÑÏÜåÌôîÎêú Î∂ÑÏÑùÎßå ÏàòÌñâ
                    logger.info(f"[MULTIMODAL] Í≤ÄÏÉâ Ïù∏Îç±Ïä§Î•º ÏúÑÌïú ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ - ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: {len(full_content)}")
                    nlp_result = await korean_nlp_service.analyze_text_for_search(full_content[:10000])  # Ï≤òÏùå 10,000ÏûêÎßå Î∂ÑÏÑù
                    
                    # 4.4. Í≤ÄÏÉâ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± (textsearch_koÍ∞Ä tsvector ÏÉùÏÑ± Ïãú ÏûêÎèô Ï≤òÎ¶¨)
                    search_metadata = {
                        'keywords': nlp_result.get('keywords', [])[:30],  # Îπà Î¶¨Ïä§Ìä∏ (textsearch_koÍ∞Ä ÏûêÎèô Ï≤òÎ¶¨)
                        'proper_nouns': nlp_result.get('proper_nouns', [])[:30],  # Îπà Î¶¨Ïä§Ìä∏
                        'corp_names': nlp_result.get('entities', {}).get('ORG', [])[:20] if isinstance(nlp_result.get('entities'), dict) else [],
                        'main_topics': nlp_result.get('keywords', [])[:10],  # Îπà Î¶¨Ïä§Ìä∏
                    }
                    
                    logger.info(f"[MULTIMODAL] ÌÖçÏä§Ìä∏ Î∂ÑÏÑù ÏôÑÎ£å - textsearch_koÍ∞Ä tsvector ÏÉùÏÑ± Ïãú ÏûêÎèôÏúºÎ°ú ÌòïÌÉúÏÜå Î∂ÑÏÑù ÏàòÌñâ")
                    
                    # 4.5. Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥ ÏàòÏßë (Î©ÄÌã∞Î™®Îã¨ Í≤ÄÏÉâÏö©)
                    image_metadata = []
                    for obj in extracted_objects:
                        if getattr(obj, 'object_type', '') == 'IMAGE':
                            # üéØ Caption Ïö∞ÏÑ† Ï∂îÏ∂ú (content_textÏóê Ï†ÄÏû•Îê®)
                            caption = getattr(obj, 'content_text', '') or ''
                            structure_json = getattr(obj, 'structure_json', {}) or {}
                            
                            # Fallback: structure_jsonÏóêÏÑúÎèÑ ÌôïÏù∏
                            if not caption and isinstance(structure_json, dict):
                                caption = structure_json.get('caption', '')
                            
                            img_meta = {
                                'object_id': getattr(obj, 'object_id', None),
                                'page_number': getattr(obj, 'page_no', None),
                                'caption': caption,  # üéØ Azure DIÏóêÏÑú Ï∂îÏ∂úÌïú Figure caption
                                'bounding_box': getattr(obj, 'bbox', None),
                                'has_caption': bool(caption),  # üéØ Caption Ïú†Î¨¥ ÌîåÎûòÍ∑∏
                            }
                            image_metadata.append(img_meta)
                            
                            # üéØ Caption Î∞úÍ≤¨ Ïãú Î°úÍ∑∏ Ï∂úÎ†•
                            if caption:
                                logger.info(f"[CAPTION] ‚úÖ Ïù¥ÎØ∏ÏßÄ Ï∫°ÏÖò ÏàòÏßë ÏôÑÎ£å - obj_id={img_meta['object_id']}, page={img_meta['page_number']}, caption='{caption[:80]}...'")
                    
                    captions_found = sum(1 for img in image_metadata if img.get('has_caption'))
                    logger.info(f"[MULTIMODAL] Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏôÑÎ£å - {len(image_metadata)}Í∞ú (Ï∫°ÏÖò ÏûàÏùå: {captions_found}Í∞ú)")
                    
                    # 4.6. Î¨∏ÏÑú Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ (Î©ÄÌã∞Î™®Îã¨ Í≤ÄÏÉâ ÏßÄÏõê)
                    document_data = {
                        'title': getattr(file_info, 'file_lgc_nm', 'Untitled'),
                        'file_name': getattr(file_info, 'file_lgc_nm', ''),
                        'file_type': getattr(file_info, 'file_extsn', 'unknown'),
                        'full_content': full_content,  # Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ (Ï§ëÏöî!)
                        'page_count': len(set(getattr(obj, 'page_number', 1) for obj in extracted_objects)) if extracted_objects else 1,
                        'language': 'mixed',  # ÌïúÍµ≠Ïñ¥/ÏòÅÏñ¥ ÌòºÌï©
                        'has_images': image_count > 0,
                        'has_tables': table_count > 0,
                        'image_count': image_count,
                        'table_count': table_count,
                        'images': image_metadata,  # Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ (Î©ÄÌã∞Î™®Îã¨ Í≤ÄÏÉâÏö©)
                    }
                    
                    logger.info(f"[MULTIMODAL] Î¨∏ÏÑú Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ÏôÑÎ£å - "
                               f"Ï†úÎ™©: {document_data['title']}, "
                               f"ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: {len(full_content)}, "
                               f"Ïù¥ÎØ∏ÏßÄ: {image_count}Í∞ú, "
                               f"ÌÖåÏù¥Î∏î: {table_count}Í∞ú")
                    
                    # 4.6. Í≤ÄÏÉâ Ïù∏Îç±Ïä§ ÏÉùÏÑ±
                    search_result = await self.search_index_service.store_document_for_search(
                        session=session,
                        file_bss_info_sno=file_bss_info_sno,
                        container_id=container_id,
                        document_data=document_data,
                        nlp_analysis=search_metadata,
                        user_info={'emp_no': user_emp_no} if user_emp_no else None
                    )
                    
                    if search_result.get('success'):
                        logger.info(f"[MULTIMODAL] Í≤ÄÏÉâ Ïù∏Îç±Ïä§ ÏÉùÏÑ± ÏôÑÎ£å - search_doc_id: {search_result.get('search_doc_id')}")
                        _stage("search_index_creation", True, 
                              search_doc_id=search_result.get('search_doc_id'),
                              keywords_count=len(search_metadata.get('keywords', [])),
                              content_length=len(full_content))
                    else:
                        logger.warning(f"[MULTIMODAL] Í≤ÄÏÉâ Ïù∏Îç±Ïä§ ÏÉùÏÑ± Ïã§Ìå®: {search_result.get('error')}")
                        _stage("search_index_creation", False, error=search_result.get('error'))
                        
            except Exception as idx_err:
                logger.error(f"[MULTIMODAL] Í≤ÄÏÉâ Ïù∏Îç±Ïä§ ÏÉùÏÑ± Ï§ë Ïò§Î•ò: {idx_err}")
                _stage("search_index_creation", False, error=str(idx_err))
            
            # 4.7. Í∏∞Ï°¥ Í≤ÄÏÉâ Ïù∏Îç±Ïä§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ (ÏûàÎäî Í≤ΩÏö∞)
            _start_stage("index_metadata_update")
            try:
                stmt = select(TbDocumentSearchIndex).where(TbDocumentSearchIndex.file_bss_info_sno == file_bss_info_sno)
                sr = await session.execute(stmt)
                search_index = sr.scalar_one_or_none()
                if search_index:
                    search_index.extraction_session_id = extraction_session.extraction_session_id
                    search_index.primary_chunk_session_id = chunk_session.chunk_session_id
                    search_index.last_embedding_model = current_embedding_model
                    search_index.has_table = any(o.object_type == "TABLE" for o in extracted_objects)
                    search_index.has_image = any(o.object_type == "IMAGE" for o in extracted_objects)
                    logger.info(f"[MULTIMODAL] Í≤ÄÏÉâ Ïù∏Îç±Ïä§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å")
                _stage("index_metadata_update", True)
            except Exception as meta_err:
                logger.warning(f"[MULTIMODAL] Í≤ÄÏÉâ Ïù∏Îç±Ïä§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå® (Î¨¥Ïãú): {meta_err}")
                _stage("index_metadata_update", False, error=str(meta_err))

            await session.commit()

            elapsed = (datetime.now() - started).total_seconds()
            # ÌÜµÍ≥Ñ Í≥ÑÏÇ∞ (SQLAlchemy Í∞ùÏ≤¥ ÏÜçÏÑ± ÏïàÏ†ÑÌïòÍ≤å Ï†ëÍ∑º)
            tables_count = sum(1 for o in extracted_objects if getattr(o, 'object_type', '') == "TABLE")
            images_count = sum(1 for o in extracted_objects if getattr(o, 'object_type', '') == "IMAGE")  
            figures_count = sum(1 for o in extracted_objects if getattr(o, 'object_type', '') == "FIGURE")
            
            # Ï≤≠ÌÅ¨Î≥Ñ ÌÜµÍ≥Ñ
            text_chunks_count = sum(1 for c in doc_chunks if getattr(c, 'modality', 'text') == 'text')
            image_chunks_count = sum(1 for c in doc_chunks if getattr(c, 'modality', 'text') == 'image')
            table_chunks_count = sum(1 for c in doc_chunks if getattr(c, 'modality', 'text') == 'table')
            
            total_tokens = sum(getattr(c, 'token_count', 0) or 0 for c in doc_chunks)
            avg_tokens = (total_tokens / len(doc_chunks)) if doc_chunks else 0
            
            result["stats"] = {
                "elapsed_seconds": elapsed,
                "avg_chunk_tokens": avg_tokens,
                "vector_dimension": max_dim,
                "tables": tables_count,
                "images": images_count,
                "figures": figures_count,
                "text_chunks": text_chunks_count,
                "image_chunks": image_chunks_count,
                "table_chunks": table_chunks_count,
            }
            if section_chunking_meta.get("requested"):
                result["stats"]["section_chunking_enabled"] = section_chunking_meta.get("enabled")
                result["stats"]["section_chunking_detected_sections"] = section_chunking_meta.get("detected_sections", [])
                result["section_chunking"] = section_chunking_meta
            result["success"] = True
            logger.info(f"[MULTIMODAL] Pipeline success in {elapsed:.2f}s | "
                       f"chunks={result['chunks_count']} (text={text_chunks_count}, image={image_chunks_count}, table={table_chunks_count}) "
                       f"embeddings={result['embeddings_count']}")
            return result
        except Exception as e:
            logger.error(f"[MULTIMODAL] ÌååÏù¥ÌîÑÎùºÏù∏ Ïò§Î•ò: {e}")
            await session.rollback()
            result["error"] = str(e)
            _stage("fatal", False, error=str(e))
            return result
        finally:
            # ÏûÑÏãú ÌååÏùº Ï†ïÎ¶¨
            if is_temp_file and actual_file_path and os.path.exists(actual_file_path):
                try:
                    os.remove(actual_file_path)
                    logger.debug(f"[MULTIMODAL] ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú ÏôÑÎ£å: {actual_file_path}")
                except Exception as cleanup_err:
                    logger.warning(f"[MULTIMODAL] ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {cleanup_err}")

    def _derive_core_content_page_set(
        self,
        sections: List[Dict[str, Any]],
        object_spans: List[Tuple[DocExtractedObject, int, int]],
    ) -> Optional[Set[int]]:
        """
        ÏÑπÏÖò ÏàúÏÑúÎ•º Í∏∞Î∞òÏúºÎ°ú ÌïµÏã¨ Î≥∏Î¨∏ Íµ¨Í∞ÑÏóê Ìï¥ÎãπÌïòÎäî ÌéòÏù¥ÏßÄ ÏßëÌï© Í≥ÑÏÇ∞
        
        ÌïôÏà†ÎÖºÎ¨∏Ïùò Í≤ΩÏö∞:
        - References ÏÑπÏÖò Ïù¥ÌõÑÎäî Ï†úÏô∏ (Ï†ÄÏûê ÌîÑÎ°úÌïÑ ÏÇ¨ÏßÑ Îì± ÎÖºÎ¨∏ ÎÇ¥Ïö©Í≥º Î¨¥Í¥Ä)
        - IntroductionÎ∂ÄÌÑ∞ ConclusionÍπåÏßÄÍ∞Ä ÌïµÏã¨ Î≥∏Î¨∏
        
        Í∞úÏÑ†ÏÇ¨Ìï≠:
        - bbox Í∏∞Î∞òÏúºÎ°ú Ïã§Ï†ú ÌéòÏù¥ÏßÄ Î≤àÌò∏ Ï∂îÏ∂ú
        - References ÏÑπÏÖò Í∞êÏßÄ Ïã§Ìå® Ïãú Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄÏùò 80% Ïù¥ÌõÑÎ•º ReferencesÎ°ú Í∞ÑÏ£º
        """
        if not sections or not object_spans:
            return None

        # 1. Ï†ÑÏ≤¥ Í∞ùÏ≤¥ÏóêÏÑú ÌéòÏù¥ÏßÄ Î≤àÌò∏ Ï∂îÏ∂ú (bbox Í∏∞Î∞ò)
        all_pages: Set[int] = set()
        for obj, _, _ in object_spans:
            page_no = self._extract_page_from_bbox(obj)
            if page_no and page_no > 0:
                all_pages.add(page_no)
        
        if not all_pages:
            logger.warning("[FIGURE-FILTER] ÌéòÏù¥ÏßÄ Î≤àÌò∏Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏùå")
            return None
        
        max_page = max(all_pages)
        logger.info(f"[FIGURE-FILTER] Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄ Î≤îÏúÑ: 1~{max_page}")

        # 2. References ÏÑπÏÖò Í∞êÏßÄ
        intro_idx: Optional[int] = None
        conclusion_idx: Optional[int] = None
        references_idx: Optional[int] = None
        references_start_page: Optional[int] = None
        
        for section in sections:
            idx = section.get("index")
            if idx is None:
                continue
            section_type = (section.get("type") or "").lower()
            
            if intro_idx is None and section_type == "introduction":
                intro_idx = idx
            if section_type == "conclusion":
                conclusion_idx = idx
            if section_type == "references":
                references_idx = idx
                # References ÏÑπÏÖòÏùò ÏãúÏûë ÌéòÏù¥ÏßÄ Ï†ÄÏû• (ÏÑπÏÖò Ï†ïÎ≥¥ÏóêÏÑú)
                references_start_page = section.get("start_page")

        # 3. References ÏãúÏûë ÌéòÏù¥ÏßÄ Í≤∞Ï†ï (ÏÑπÏÖò Í∞êÏßÄ Ïã§Ìå® Ïãú bbox Í∏∞Î∞ò Ï∂îÏ†ï)
        if references_idx is not None and not references_start_page:
            # ÏÑπÏÖòÏùÄ ÏûàÏßÄÎßå start_pageÍ∞Ä NoneÏù∏ Í≤ΩÏö∞ ‚Üí bboxÎ°ú Ï∂îÏ†ï
            references_start_page = self._estimate_references_page_from_objects(
                sections, object_spans, references_idx
            )
        
        if not references_start_page:
            # References ÏÑπÏÖò ÏûêÏ≤¥Í∞Ä ÏóÜÍ±∞ÎÇò Ï∂îÏ†ï Ïã§Ìå® ‚Üí Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄÏùò 80% Ïù¥ÌõÑÎ•º ReferencesÎ°ú Í∞ÑÏ£º
            references_start_page = max(1, int(max_page * 0.8))
            logger.info(f"[FIGURE-FILTER] References ÏÑπÏÖò ÎØ∏Í∞êÏßÄ ‚Üí ÌéòÏù¥ÏßÄ {references_start_page}Î∂ÄÌÑ∞Î•º ÌõÑÎ∞òÎ∂ÄÎ°ú Í∞ÑÏ£º (80% Í∏∞Ï§Ä)")
        else:
            logger.info(f"[FIGURE-FILTER] References ÏÑπÏÖò Í∞êÏßÄ - idx={references_idx}, start_page={references_start_page}")

        # 4. References Ïù¥Ï†ÑÏùò Î™®Îì† ÌéòÏù¥ÏßÄÎ•º ÌóàÏö© (ÏÑπÏÖò Î≤îÏúÑ Î¨¥Ïãú)
        # ÌïôÏà†ÎÖºÎ¨∏Ïùò TABLE/FIGUREÎäî Î≥∏Î¨∏ Ï†ÑÏ≤¥Ïóê Î∂ÑÏÇ∞ÎêòÏñ¥ ÏûàÏúºÎØÄÎ°ú
        # ÏÑπÏÖò ÌÖçÏä§Ìä∏ ÏúÑÏπò(span)ÏôÄ Îß§Ïπ≠ÌïòÏßÄ ÎßêÍ≥† Îã®ÏàúÌûà ÌéòÏù¥ÏßÄ Î≤àÌò∏ÎßåÏúºÎ°ú ÌïÑÌÑ∞ÎßÅ
        allowed_pages: Set[int] = {p for p in all_pages if p < references_start_page}
        
        # References ÏÑπÏÖò Ï†ïÎ≥¥ Î°úÍπÖ (ÎîîÎ≤ÑÍπÖÏö©)
        if intro_idx is not None and conclusion_idx is not None:
            logger.info(f"[FIGURE-FILTER] ÏÑπÏÖò Ïù∏Îç±Ïä§: Introduction({intro_idx})~Conclusion({conclusion_idx}), References({references_idx})")
        
        logger.info(f"[FIGURE-FILTER] References({references_start_page}p) Ïù¥ÌõÑ Ï†úÏô∏ ‚Üí ÌóàÏö© ÌéòÏù¥ÏßÄ: {sorted(allowed_pages)}")
        
        return allowed_pages or None
    
    def _extract_page_from_bbox(self, obj: DocExtractedObject) -> Optional[int]:
        """
        Í∞ùÏ≤¥Ïùò bbox ÎòêÎäî structure_jsonÏóêÏÑú ÌéòÏù¥ÏßÄ Î≤àÌò∏ Ï∂îÏ∂ú
        
        Ïö∞ÏÑ†ÏàúÏúÑ:
        1. obj.page_no (Ïù¥ÎØ∏ ÏÑ§Ï†ïÎêòÏñ¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©)
        2. structure_jsonÏùò bbox Ï¢åÌëúÏóêÏÑú Ï∂îÏ∂ú
        3. structure_jsonÏùò page_number ÌïÑÎìú
        """
        # 1. Í∏∞Ï°¥ page_no ÏÇ¨Ïö©
        page_no = getattr(obj, "page_no", None)
        if isinstance(page_no, int) and page_no > 0:
            return page_no
        
        # 2. structure_jsonÏóêÏÑú Ï∂îÏ∂ú
        structure_json = getattr(obj, "structure_json", None)
        if structure_json and isinstance(structure_json, dict):
            # Azure DI bbox Íµ¨Ï°∞: [{"polygon": [...], "page_number": 5}]
            bboxes = structure_json.get("bounding_regions", [])
            if bboxes and isinstance(bboxes, list) and len(bboxes) > 0:
                first_bbox = bboxes[0]
                if isinstance(first_bbox, dict):
                    bbox_page = first_bbox.get("page_number")
                    if isinstance(bbox_page, int) and bbox_page > 0:
                        return bbox_page
            
            # ÏßÅÏ†ë page_number ÌïÑÎìú
            direct_page = structure_json.get("page_number")
            if isinstance(direct_page, int) and direct_page > 0:
                return direct_page
        
        return None
    
    def _estimate_references_page_from_objects(
        self,
        sections: List[Dict[str, Any]],
        object_spans: List[Tuple[DocExtractedObject, int, int]],
        references_idx: int
    ) -> Optional[int]:
        """
        References ÏÑπÏÖòÏóê ÏÜçÌïú Í∞ùÏ≤¥Îì§Ïùò bboxÏóêÏÑú ÌéòÏù¥ÏßÄ Î≤àÌò∏ Ï∂îÏ†ï
        """
        references_section = None
        for section in sections:
            if section.get("index") == references_idx:
                references_section = section
                break
        
        if not references_section:
            return None
        
        s_start = references_section.get("start_pos", 0)
        s_end = references_section.get("end_pos", 0)
        if s_end < s_start:
            s_start, s_end = s_end, s_start
        
        ref_pages: List[int] = []
        for obj, span_start, span_end in object_spans:
            # ÏÑπÏÖò Î≤îÏúÑÏôÄ Í≤πÏπòÎäîÏßÄ ÌôïÏù∏
            if span_end <= s_start or span_start >= s_end:
                continue
            page_no = self._extract_page_from_bbox(obj)
            if page_no and page_no > 0:
                ref_pages.append(page_no)
        
        if ref_pages:
            min_page = min(ref_pages)
            logger.info(f"[FIGURE-FILTER] References ÏÑπÏÖò bbox Î∂ÑÏÑù ‚Üí ÏãúÏûë ÌéòÏù¥ÏßÄ: {min_page}")
            return min_page
        
        return None

def _serialize_table_to_text(table_obj: DocExtractedObject) -> str:
    """
    TABLE Í∞ùÏ≤¥Î•º Í≤ÄÏÉâ Í∞ÄÎä•Ìïú ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò
    
    Ïö∞ÏÑ†ÏàúÏúÑ:
    1. content_textÍ∞Ä ÏûàÏúºÎ©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
    2. structure_jsonÏóê Ìëú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÏúºÎ©¥ ÎßàÌÅ¨Îã§Ïö¥ ÌëúÎ°ú Î≥ÄÌôò
    3. ÏóÜÏúºÎ©¥ ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî ÌÖçÏä§Ìä∏ Î∞òÌôò
    """
    # 1. Í∏∞Ï°¥ content_textÍ∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
    content_text = getattr(table_obj, 'content_text', '') or ''
    if content_text and content_text.strip() and not content_text.startswith('[Ìëú '):
        return content_text.strip()
    
    # 2. structure_jsonÏóêÏÑú Ìëú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
    structure_json = getattr(table_obj, 'structure_json', None)
    if structure_json and isinstance(structure_json, dict):
        # XLSX ÏãúÌä∏ Îç∞Ïù¥ÌÑ∞ (Ï†ÑÏ≤¥ ÏãúÌä∏Í∞Ä TABLEÎ°ú Ï†ÄÏû•Îê®)
        if 'text' in structure_json and structure_json.get('text', '').strip():
            return structure_json['text'].strip()
        
        # Azure DI Ìëú Íµ¨Ï°∞ (cells, rows, columns Îì±)
        if 'cells' in structure_json or 'rows' in structure_json:
            try:
                return _convert_azure_table_to_markdown(structure_json)
            except Exception as e:
                logger.debug(f"[TABLE] Azure Ìëú Î≥ÄÌôò Ïã§Ìå®: {e}")
        
        # PDF/PPT Ìëú Ïù∏Îç±Ïä§Îßå ÏûàÎäî Í≤ΩÏö∞ (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå)
        if 'table_index' in structure_json:
            table_idx = structure_json.get('table_index', 0)
            page_no = getattr(table_obj, 'page_no', None)
            return f"[Ìëú {table_idx + 1} - ÌéòÏù¥ÏßÄ {page_no}]"
    
    # 3. ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî
    page_no = getattr(table_obj, 'page_no', None)
    seq = getattr(table_obj, 'sequence_in_page', None)
    return f"[Ìëú - ÌéòÏù¥ÏßÄ {page_no}, ÏàúÏÑú {seq}]"


def _convert_azure_table_to_markdown(structure: Dict[str, Any]) -> str:
    """
    Azure Document Intelligence Ìëú Íµ¨Ï°∞Î•º ÎßàÌÅ¨Îã§Ïö¥ ÌëúÎ°ú Î≥ÄÌôò
    
    structure ÏòàÏãú:
    {
        "rowCount": 3,
        "columnCount": 2,
        "cells": [
            {"rowIndex": 0, "columnIndex": 0, "content": "Header1"},
            {"rowIndex": 0, "columnIndex": 1, "content": "Header2"},
            ...
        ]
    }
    """
    if not structure:
        return ""
    
    # ÏÖÄ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
    cells = structure.get('cells', [])
    if not cells:
        # rows ÌòïÌÉúÎ°ú Ï†úÍ≥µÎêú Í≤ΩÏö∞
        rows = structure.get('rows', [])
        if rows:
            lines = []
            for row_data in rows:
                if isinstance(row_data, dict) and 'cells' in row_data:
                    row_cells = [str(c.get('content', '')).strip() for c in row_data['cells']]
                    lines.append(' | '.join(row_cells))
                elif isinstance(row_data, list):
                    lines.append(' | '.join(str(c).strip() for c in row_data))
            return '\n'.join(lines)
        return ""
    
    # Ìñâ/Ïó¥ ÌÅ¨Í∏∞ ÌôïÏù∏
    row_count = structure.get('rowCount', max((c.get('rowIndex', 0) for c in cells), default=0) + 1)
    col_count = structure.get('columnCount', max((c.get('columnIndex', 0) for c in cells), default=0) + 1)
    
    # 2D Î∞∞Ïó¥ Ï¥àÍ∏∞Ìôî
    table_grid = [['' for _ in range(col_count)] for _ in range(row_count)]
    
    # ÏÖÄ Îç∞Ïù¥ÌÑ∞ Ï±ÑÏö∞Í∏∞
    for cell in cells:
        row_idx = cell.get('rowIndex', 0)
        col_idx = cell.get('columnIndex', 0)
        content = str(cell.get('content', '')).strip()
        
        # Î≥ëÌï©Îêú ÏÖÄ Ï≤òÎ¶¨
        row_span = cell.get('rowSpan', 1)
        col_span = cell.get('columnSpan', 1)
        
        if 0 <= row_idx < row_count and 0 <= col_idx < col_count:
            table_grid[row_idx][col_idx] = content
            
            # Î≥ëÌï©Îêú ÏÖÄ ÏòÅÏó≠ ÌëúÏãú
            for r in range(row_idx, min(row_idx + row_span, row_count)):
                for c in range(col_idx, min(col_idx + col_span, col_count)):
                    if r != row_idx or c != col_idx:
                        table_grid[r][c] = ''  # Î≥ëÌï©Îêú ÏÖÄÏùÄ Îπà Î¨∏ÏûêÏó¥
    
    # ÎßàÌÅ¨Îã§Ïö¥ Ìëú ÏÉùÏÑ±
    lines = []
    for idx, row in enumerate(table_grid):
        lines.append('| ' + ' | '.join(row) + ' |')
        # Ï≤´ Î≤àÏß∏ Ìñâ ÌõÑ Íµ¨Î∂ÑÏÑ† Ï∂îÍ∞Ä (Ìó§ÎçîÎ°ú Í∞ÑÏ£º)
        if idx == 0 and row_count > 1:
            lines.append('| ' + ' | '.join(['---'] * col_count) + ' |')
    
    return '\n'.join(lines)


def _clean_metadata_for_json(metadata: Any) -> Any:
    """
    Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÏóêÏÑú JSON ÏßÅÎ†¨ÌôîÌï† Ïàò ÏóÜÎäî binary_data Îì±ÏùÑ Ïû¨Í∑ÄÏ†ÅÏúºÎ°ú Ï†úÍ±∞
    """
    if isinstance(metadata, dict):
        result = {}
        for key, value in metadata.items():
            if key == 'binary_data':
                # binary_dataÎäî Ï†úÍ±∞ÌïòÍ≥† ÎåÄÏã† Ï∞∏Ï°∞ Ï†ïÎ≥¥Îßå Ï†ÄÏû•
                result['binary_data_info'] = {
                    'removed': True,
                    'type': type(value).__name__,
                    'size_bytes': len(value) if isinstance(value, bytes) else None
                }
            else:
                result[key] = _clean_metadata_for_json(value)
        return result
    elif isinstance(metadata, list):
        return [_clean_metadata_for_json(item) for item in metadata]
    elif isinstance(metadata, bytes):
        # bytes ÌÉÄÏûÖÏùÄ Ï†ïÎ≥¥Îßå Ï†ÄÏû•
        return {
            'binary_data_info': {
                'removed': True,
                'type': 'bytes',
                'size_bytes': len(metadata)
            }
        }
    else:
        return metadata


# Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§
multimodal_document_service = MultimodalDocumentService()

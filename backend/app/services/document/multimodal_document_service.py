"""
ğŸ¨ ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤
============================

ìƒˆë¡œìš´ ë©€í‹°ëª¨ë‹¬ ìŠ¤í‚¤ë§ˆë¥¼ í™œìš©í•œ ë¬¸ì„œ ì¶”ì¶œ/ì²­í‚¹/ì„ë² ë”© íŒŒì´í”„ë¼ì¸
- DocExtractionSession: ì¶”ì¶œ ì„¸ì…˜ ê´€ë¦¬
- DocExtractedObject: í˜ì´ì§€ë³„ ê°ì²´ ì¶”ì¶œ (í…ìŠ¤íŠ¸, í‘œ, ì´ë¯¸ì§€)
- DocChunkSession: ì²­í‚¹ ì„¸ì…˜ ê´€ë¦¬
- DocChunk: ì²­í¬ ì €ì¥
- DocEmbedding: ì„ë² ë”© ë²¡í„° ì €ì¥
- Azure Blob Storage: ì¶”ì¶œ ê²°ê³¼ ë° ì²˜ë¦¬ ì•„í‹°íŒ©íŠ¸ ì €ì¥
"""

import logging
import hashlib
import json
import time
import os
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Set
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, text, type_coerce
from sqlalchemy.dialects.postgresql import INT4RANGE

from app.models.document.multimodal_models import (
    DocExtractionSession,
    DocExtractedObject,
    DocChunkSession,
    DocChunk,
    DocEmbedding
)
from app.models import TbDocumentSearchIndex
from app.models.document.file_models import TbFileBssInfo
from app.models.document.vector_models import VsDocContentsChunks
from app.services.document.extraction.text_extractor_service import text_extractor_service
from app.services.core.korean_nlp_service import korean_nlp_service
from app.core.config import settings
from app.services.document.chunking.advanced_chunker import advanced_chunk_text
from app.services.document.chunking.section_aware_chunker import (
    chunk_by_sections,
    filter_objects_before_references
)
from app.services.document.chunking.structure_aware_chunker import StructureAwareChunker
from app.services.document.extraction.adaptive_section_detector import AdaptiveSectionDetector
from app.services.document.storage.search_index_store import SearchIndexStoreService

# Azure Blob Storage í†µí•©
try:
    from app.services.core.azure_blob_service import get_azure_blob_service
except ImportError:
    get_azure_blob_service = None

# AWS S3 Storage (ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©)
try:
    from app.services.core.aws_service import S3Service
except ImportError:  # pragma: no cover
    S3Service = None  # type: ignore

# ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ ì„œë¹„ìŠ¤
try:
    from app.services.document.vision.image_embedding_service import (
        image_embedding_service as default_image_embedding_service,
    )
except ImportError:
    default_image_embedding_service = None

logger = logging.getLogger(__name__)

class MultimodalDocumentService:
    """ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, image_embedding_service: Optional[Any] = None):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.search_index_service = SearchIndexStoreService()
        # ì¸ìŠ¤í„´ìŠ¤ ì£¼ì…ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì „ì—­ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©
        self.image_embedding_service = image_embedding_service or default_image_embedding_service
        self._s3_service: Optional['S3Service'] = None
        # ì ì‘í˜• ì„¹ì…˜ ê°ì§€ ì„œë¹„ìŠ¤ (ëª¨ë“  í—¤ë” ê°ì§€ + ì˜ë¯¸ ë§¤í•‘)
        self.section_detector = AdaptiveSectionDetector()
    
    async def process_document_multimodal(
        self,
        file_path: str,
        file_bss_info_sno: int,
        container_id: str,
        user_emp_no: str,
        session: AsyncSession,
        provider: Optional[str] = None,
        model_profile: str = "default",
        processing_options: Optional[Dict[str, Any]] = None,
        document_type: Optional[str] = None,
    ) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ (ë¦¬íŒ©í„° ë²„ì „)

        ë‹¨ê³„:
        1) ì¶”ì¶œ ì„¸ì…˜ + ê°ì²´ ì €ì¥
        2) ê³ ê¸‰ ì²­í‚¹(ë¬¸ë‹¨/í† í° ê¸°ë°˜)
        3) ì„ë² ë”© ìƒì„± (ì œë¡œ íŒ¨ë”©)
        4) ê²€ìƒ‰ ì¸ë±ìŠ¤ ë³´ê°•
        5) í†µê³„ ë°˜í™˜
        """
        started = datetime.now()
        result: Dict[str, Any] = {
            "success": False,
            "extraction_session_id": None,
            "chunk_session_id": None,
            "objects_count": 0,
            "chunks_count": 0,
            "embeddings_count": 0,
            "stats": {},
            "error": None,
            "stages": []
        }
        stage_timers: Dict[str, float] = {}

        provider = provider or settings.get_current_llm_provider()
        processing_options = processing_options or {}
        pipeline_type = processing_options.get("pipeline_type") or provider or settings.default_llm_provider
        document_type_normalized = (document_type or processing_options.get("document_type") or "").lower()
        # ê¸°ë³¸ê°’: ëª¨ë“  ë¬¸ì„œì— êµ¬ì¡° ì¸ì‹ ì²­í‚¹ ì ìš©
        structure_aware_enabled = bool(processing_options.get("structure_aware_chunking_enabled", True))
        section_chunking_requested = processing_options.get("section_chunking_enabled", True)
        apply_section_chunking = document_type_normalized == "academic_paper" and section_chunking_requested
        
        # ğŸ“‹ ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬ ë°©ì‹ ë¡œê¹…
        if apply_section_chunking:
            logger.info(f"[PIPELINE] ğŸ“ í•™ìˆ  ë…¼ë¬¸ ì²˜ë¦¬ ëª¨ë“œ: ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹, References ì´í›„ ì œì™¸")
        else:
            logger.info(f"[PIPELINE] ğŸ“„ ì¼ë°˜ ë¬¸ì„œ ì²˜ë¦¬ ëª¨ë“œ: í† í° ê¸°ë°˜ ì²­í‚¹, ì „ì²´ ì½˜í…ì¸  í¬í•¨")
            logger.info(f"[PIPELINE]    document_type={document_type_normalized or 'not_specified'}")
        
        section_chunking_meta: Dict[str, Any] = {
            "requested": bool(apply_section_chunking),
            "enabled": False,
            "detected_sections": [],
            "chunk_counts": {},
            "stored_to_blob": False,
        }
        precomputed_sections_info: List[Dict[str, Any]] = []
        precomputed_section_summary: Optional[Dict[str, Any]] = None
        visual_page_filter: Optional[Set[int]] = None
        section_combined_text: str = ""
        section_object_spans: List[Tuple[DocExtractedObject, int, int]] = []
        image_ids_with_binary: Set[int] = set()
        image_object_ids_seen: Set[int] = set()
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™” (finally ë¸”ë¡ì—ì„œ ì‚¬ìš©)
        actual_file_path: Optional[str] = None
        is_temp_file: bool = False

        def _start_stage(name: str):
            stage_timers[name] = time.perf_counter()
            logger.info(f"[MULTIMODAL][TIMER] {name} stage started")

        def _stage(name: str, success: bool, **extra):
            elapsed = None
            if name in stage_timers:
                elapsed = time.perf_counter() - stage_timers.pop(name)
                extra.setdefault("elapsed_seconds", elapsed)
            result["stages"].append({"name": name, "success": success, **extra})
            if elapsed is not None:
                logger.info(f"[MULTIMODAL][TIMER] {name} stage completed in {elapsed:.2f}s (success={success})")
            else:
                logger.info(f"[MULTIMODAL][TIMER] {name} stage completed (success={success})")

        try:
            # -----------------------------
            # 1. Extraction
            # -----------------------------
            _start_stage("extraction_setup")
            extraction_session = DocExtractionSession(
                file_bss_info_sno=file_bss_info_sno,
                provider=provider,
                model_profile=model_profile,
                pipeline_type=pipeline_type,
                status="running",
                started_at=datetime.now()
            )
            session.add(extraction_session)
            await session.flush()
            result["extraction_session_id"] = extraction_session.extraction_session_id
            _stage("extraction_setup", True, extraction_session_id=extraction_session.extraction_session_id)
            logger.info(f"[MULTIMODAL] Extraction session started: {extraction_session.extraction_session_id}")

            _start_stage("extraction")
            extraction_result = await text_extractor_service.extract_text_from_file(file_path)
            
            # âœ… extraction ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ actual_file_pathì™€ is_temp_file í™•ë³´
            actual_file_path = extraction_result.get("actual_file_path", file_path)
            is_temp_file = extraction_result.get("is_temp_file", False)
            
            if not extraction_result.get("success"):
                # ëª¨ë¸ í•„ë“œì— ì§ì ‘ í• ë‹¹ ì‹œ ì •ì  íƒ€ì… ê²½ê³  íšŒí”¼ ìœ„í•´ setattr ì‚¬ìš©
                setattr(extraction_session, "status", "failed")
                setattr(extraction_session, "error_message", extraction_result.get("error"))
                setattr(extraction_session, "completed_at", datetime.now())
                await session.commit()
                _stage("extraction", False, error=extraction_result.get("error"))
                result["error"] = extraction_result.get("error")
                return result
            
            metadata = extraction_result.get("metadata", {})
            extracted_objects: List[DocExtractedObject] = []
            
            def _add_text_obj(page_no: Optional[int], text_val: str):
                if not text_val or not text_val.strip():
                    return
                extracted_objects.append(
                    DocExtractedObject(
                        extraction_session_id=extraction_session.extraction_session_id,
                        file_bss_info_sno=file_bss_info_sno,
                        page_no=page_no,
                        object_type="TEXT_BLOCK",
                        sequence_in_page=0,
                        content_text=text_val,
                        char_count=len(text_val),
                        token_estimate=len(text_val.split()),
                        hash_sha256=hashlib.sha256(text_val.encode()).hexdigest(),
                    )
                )

            # ì „ì²´ í…ìŠ¤íŠ¸ ì¬ì¡°ë¦½ í—¬í¼ (fallback)
            def _assemble_full_text(objs: List[DocExtractedObject]) -> str:
                try:
                    return "\n\n".join([
                        (o.content_text or "").strip()
                        for o in objs
                        if getattr(o, 'object_type', None) == 'TEXT_BLOCK' and (o.content_text or '').strip()
                    ])
                except Exception as e:
                    logger.warning(f"[MULTIMODAL] _assemble_full_text ì‹¤íŒ¨: {e}")
                    return ""

            # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—”íŠ¸ë¦¬ êµ¬ì„± í—¬í¼
            def _object_to_manifest_entry(idx: int, obj: DocExtractedObject, blob_key: str) -> Dict[str, Any]:
                return {
                    "object_index": idx,
                    "object_id": getattr(obj, 'object_id', None),
                    "object_type": getattr(obj, 'object_type', None),
                    "page_no": getattr(obj, 'page_no', None),
                    "sequence_in_page": getattr(obj, 'sequence_in_page', None),
                    "blob_key": blob_key,
                    "char_count": len(getattr(obj, 'content_text', '') or ''),
                    "has_structure": bool(getattr(obj, 'structure_json', None)),
                    "bbox": getattr(obj, 'bbox', None)
                }

            # PDF
            if "pages" in metadata:
                for p in metadata["pages"]:
                    page_no = p.get("page_no")
                    page_text = p.get("text", "")
                    
                    # í˜ì´ì§€ bbox ê³„ì‚° (ì „ì²´ í˜ì´ì§€ í¬ê¸°)
                    page_width = p.get("width", 0)
                    page_height = p.get("height", 0)
                    page_bbox = None
                    if page_width and page_height:
                        # ì „ì²´ í˜ì´ì§€ë¥¼ TEXT_BLOCKì˜ bboxë¡œ ì„¤ì •
                        page_bbox = [0, 0, int(page_width * 72), int(page_height * 72)]  # inch â†’ points ë³€í™˜
                    
                    # TEXT_BLOCK ê°ì²´ ìƒì„± (bbox í¬í•¨)
                    if page_text and page_text.strip():
                        extracted_objects.append(
                            DocExtractedObject(
                                extraction_session_id=extraction_session.extraction_session_id,
                                file_bss_info_sno=file_bss_info_sno,
                                page_no=page_no,
                                object_type="TEXT_BLOCK",
                                sequence_in_page=0,
                                content_text=page_text,
                                char_count=len(page_text),
                                token_estimate=len(page_text.split()),
                                hash_sha256=hashlib.sha256(page_text.encode()).hexdigest(),
                                bbox=page_bbox
                            )
                        )
                    # Persist DI figures (if any) as FIGURE objects alongside IMAGEs
                    for fig in p.get("figures", []) or []:
                        try:
                            # Convert polygon bbox to [x0,y0,x1,y1] if possible
                            bbox_poly = fig.get("bbox") or []
                            if isinstance(bbox_poly, list) and len(bbox_poly) >= 4:
                                xs = [pt[0] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                ys = [pt[1] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                if xs and ys:
                                    _bbox = [int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))]
                                else:
                                    _bbox = [0, 0, 0, 0]
                            else:
                                _bbox = [0, 0, 0, 0]
                        except Exception:
                            _bbox = [0, 0, 0, 0]

                        caption_text = (fig.get("caption") or "").strip()
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=p.get("page_no"),
                            object_type="FIGURE",
                            sequence_in_page=fig.get("figure_index") or 0,
                            bbox=_bbox,
                            content_text=caption_text,
                            structure_json=fig
                        ))
                    # âŒ í˜ì´ì§€ë³„ tables_countëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ë¬¸ì„œ ë ˆë²¨ì—ì„œ ì²˜ë¦¬)
                    
                    for img_meta in p.get("images_metadata", []):
                        # DOCX/ë‹¤ë¥¸ í˜•ì‹ì˜ ê²½ìš° binary_dataë¥¼ structure_jsonì—ì„œ ì œê±°
                        clean_img_meta = dict(img_meta)
                        if 'binary_data' in clean_img_meta:
                            clean_img_meta.pop('binary_data')
                        
                        # ğŸ¯ Caption ì¶”ì¶œ (Azure DIì—ì„œ ì œê³µ)
                        caption = clean_img_meta.get('caption', '') or ''
                        if caption:
                            logger.info(f"[CAPTION] ğŸ“ ì´ë¯¸ì§€ ìº¡ì…˜ ë°œê²¬ - page={p.get('page_no')}, caption={caption[:100]}")
                        
                        # bbox ë³€í™˜: polygon [[x,y], [x,y], ...] â†’ [x0, y0, x1, y1]
                        _bbox = [0, 0, 0, 0]
                        try:
                            # Case 1: x0, y0, x1, y1 í˜•ì‹ (ê¸°ì¡´)
                            if 'x0' in img_meta and 'y0' in img_meta:
                                _bbox = [
                                    int(img_meta.get('x0', 0)),
                                    int(img_meta.get('y0', 0)),
                                    int(img_meta.get('x1', 0)),
                                    int(img_meta.get('y1', 0))
                                ]
                            # Case 2: bbox polygon í˜•ì‹ (Azure DI)
                            elif 'bbox' in img_meta:
                                bbox_poly = img_meta.get('bbox')
                                if isinstance(bbox_poly, list) and len(bbox_poly) >= 4:
                                    # polygon: [[x,y], [x,y], ...] â†’ [x0, y0, x1, y1]
                                    xs = [pt[0] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) >= 2]
                                    ys = [pt[1] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) >= 2]
                                    if xs and ys:
                                        _bbox = [int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))]
                                        logger.debug(f"[BBOX] polygon â†’ rect: {bbox_poly} â†’ {_bbox}")
                        except Exception as e:
                            logger.warning(f"[BBOX] ë³€í™˜ ì‹¤íŒ¨: {e}, img_meta={img_meta}")
                            _bbox = [0, 0, 0, 0]
                        
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=p.get("page_no"),
                            object_type="IMAGE",
                            sequence_in_page=img_meta.get("image_index"),
                            bbox=_bbox,
                            content_text=caption,  # ğŸ¯ Captionì„ content_textì— ì €ì¥
                            structure_json=clean_img_meta
                        ))
                
                # ğŸ¯ ë¬¸ì„œ ë ˆë²¨ í…Œì´ë¸” ì²˜ë¦¬ (Azure DI SDK 4.x)
                # Azure DIëŠ” analyze_result.tablesë¥¼ ë¬¸ì„œ ë ˆë²¨ì—ì„œ ì¶”ì¶œí•˜ë¯€ë¡œ,
                # metadata["tables"] ë°°ì—´ì„ ìˆœíšŒí•˜ë©° ì‹¤ì œ TABLE ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                doc_tables = metadata.get("tables", [])
                if doc_tables:
                    logger.info(f"[MULTIMODAL-EXTRACT] ğŸ“Š ë¬¸ì„œ ë ˆë²¨ í…Œì´ë¸” {len(doc_tables)}ê°œ ì²˜ë¦¬ ì‹œì‘")
                    for table in doc_tables:
                        try:
                            # bbox polygon â†’ rectangle ë³€í™˜
                            bbox_poly = table.get("bbox") or []
                            if isinstance(bbox_poly, list) and len(bbox_poly) >= 4:
                                xs = [pt[0] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                ys = [pt[1] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                _bbox = [int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))] if xs and ys else [0, 0, 0, 0]
                            else:
                                _bbox = [0, 0, 0, 0]
                        except Exception:
                            _bbox = [0, 0, 0, 0]
                        
                        # í…Œì´ë¸” í…ìŠ¤íŠ¸ ì¶”ì¶œ (cellsì—ì„œ ì¡°í•©)
                        table_text = ""
                        cells = table.get("cells", [])
                        if cells:
                            # cellsë¥¼ í–‰/ì—´ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            sorted_cells = sorted(cells, key=lambda c: (c.get("row_index", 0), c.get("column_index", 0)))
                            table_text = " | ".join([c.get("content", "") for c in sorted_cells if c.get("content", "").strip()])
                        
                        # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ (Providerë³„ ë™ì  ì²˜ë¦¬)
                        # - Upstage: elementsì˜ page í•„ë“œ ì§ì ‘ ì‚¬ìš©
                        # - Azure DI: bounding_regions ë˜ëŠ” bbox ì¢Œí‘œ ê¸°ë°˜ ì¶”ë¡ 
                        table_page_no = None
                        doc_processing_provider = metadata.get("provider", "").lower()
                        
                        # Upstage: page í•„ë“œ ì§ì ‘ ì‚¬ìš© (ìµœìš°ì„ )
                        if doc_processing_provider == "upstage" and "page" in table:
                            table_page_no = table.get("page")
                        
                        # Azure DI ë˜ëŠ” fallback: ë‹¤ë‹¨ê³„ ì¶”ì¶œ
                        if not table_page_no:
                            # ë°©ë²• 1: table ìì²´ì— page_noê°€ ìˆëŠ” ê²½ìš°
                            if "page_no" in table:
                                table_page_no = table.get("page_no")
                            
                            # ë°©ë²• 2: bounding_regionsì—ì„œ ì¶”ì¶œ (Azure DI)
                            elif "bounding_regions" in table and table["bounding_regions"]:
                                first_region = table["bounding_regions"][0]
                                table_page_no = first_region.get("page_number") or first_region.get("page")
                            
                            # ë°©ë²• 3: bbox ì¢Œí‘œë¡œ í˜ì´ì§€ ë§¤ì¹­ (í´ë°±)
                            elif _bbox != [0, 0, 0, 0]:
                                # ê° í˜ì´ì§€ì˜ bboxì™€ ë¹„êµí•˜ì—¬ ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” í˜ì´ì§€ ì°¾ê¸°
                                for p in metadata.get("pages", []):
                                    page_width = p.get("width", 0)
                                    page_height = p.get("height", 0)
                                    if page_width > 0 and page_height > 0:
                                        page_bbox = [0, 0, int(page_width * 72), int(page_height * 72)]
                                        # ê°„ë‹¨í•œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                                        if (_bbox[0] >= page_bbox[0] and _bbox[1] >= page_bbox[1] and
                                            _bbox[2] <= page_bbox[2] and _bbox[3] <= page_bbox[3]):
                                            table_page_no = p.get("page_no")
                                            break
                        
                        # í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° 1ë¡œ ì„¤ì •
                        if not table_page_no:
                            table_page_no = 1
                            logger.warning(f"[MULTIMODAL-EXTRACT] âš ï¸ í…Œì´ë¸” í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í•¨ (provider={doc_processing_provider}), ê¸°ë³¸ê°’ 1ë¡œ ì„¤ì •")
                        
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=table_page_no,
                            object_type="TABLE",
                            sequence_in_page=table.get("table_index") or 0,
                            bbox=_bbox,
                            content_text=table_text[:5000],  # ìµœëŒ€ 5000ìë¡œ ì œí•œ
                            structure_json=table  # ì „ì²´ í…Œì´ë¸” êµ¬ì¡° (cells, row_count, column_count í¬í•¨)
                        ))
                    
                    logger.info(f"[MULTIMODAL-EXTRACT] âœ… ë¬¸ì„œ ë ˆë²¨ í…Œì´ë¸” {len(doc_tables)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
                
                # ğŸ¯ ë¬¸ì„œ ë ˆë²¨ figures ì²˜ë¦¬ (Azure DI SDK 4.x)
                # Azure DIëŠ” analyze_result.figuresë¥¼ ë¬¸ì„œ ë ˆë²¨ì—ì„œ ì¶”ì¶œí•˜ê³ ,
                # _merge_figures_into_pages()ë¡œ í˜ì´ì§€ë³„ ë¶„ë°°ë¥¼ ì‹œë„í•˜ì§€ë§Œ,
                # í˜ì´ì§€ë³„ ë¶„ë°°ì— ì‹¤íŒ¨í•œ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ë¬¸ì„œ ë ˆë²¨ì—ì„œë„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                doc_figures = metadata.get("figures", [])
                if doc_figures:
                    logger.info(f"[MULTIMODAL-EXTRACT] ğŸ“Š ë¬¸ì„œ ë ˆë²¨ figure {len(doc_figures)}ê°œ ì²˜ë¦¬ ì‹œì‘")
                    for fig in doc_figures:
                        try:
                            # bbox polygon â†’ rectangle ë³€í™˜
                            bbox_poly = fig.get("bbox") or []
                            if isinstance(bbox_poly, list) and len(bbox_poly) >= 4:
                                xs = [pt[0] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                ys = [pt[1] for pt in bbox_poly if isinstance(pt, (list, tuple)) and len(pt) == 2]
                                _bbox = [int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))] if xs and ys else [0, 0, 0, 0]
                            else:
                                _bbox = [0, 0, 0, 0]
                        except Exception:
                            _bbox = [0, 0, 0, 0]
                        
                        # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ (Providerë³„ ë™ì  ì²˜ë¦¬)
                        doc_processing_provider = metadata.get("provider", "").lower()
                        fig_page_no = None
                        
                        # Upstage: page í•„ë“œ ì§ì ‘ ì‚¬ìš© (ìµœìš°ì„ )
                        if doc_processing_provider == "upstage" and "page" in fig:
                            fig_page_no = fig.get("page")
                        
                        # Azure DI ë˜ëŠ” fallback: page_no ë˜ëŠ” bbox ì¶”ë¡ 
                        if not fig_page_no:
                            fig_page_no = fig.get("page_no")
                        
                        if not fig_page_no and _bbox != [0, 0, 0, 0]:
                            # bbox ì¢Œí‘œë¡œ í˜ì´ì§€ ë§¤ì¹­
                            for p in metadata.get("pages", []):
                                page_width = p.get("width", 0)
                                page_height = p.get("height", 0)
                                if page_width > 0 and page_height > 0:
                                    page_bbox = [0, 0, int(page_width * 72), int(page_height * 72)]
                                    if (_bbox[0] >= page_bbox[0] and _bbox[1] >= page_bbox[1] and
                                        _bbox[2] <= page_bbox[2] and _bbox[3] <= page_bbox[3]):
                                        fig_page_no = p.get("page_no")
                                        break
                        
                        if not fig_page_no:
                            fig_page_no = 1
                            logger.warning(f"[MULTIMODAL-EXTRACT] âš ï¸ Figure í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í•¨ (provider={doc_processing_provider}), ê¸°ë³¸ê°’ 1ë¡œ ì„¤ì •")
                        
                        caption_text = (fig.get("caption") or "").strip()
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=fig_page_no,
                            object_type="FIGURE",
                            sequence_in_page=fig.get("figure_index") or 0,
                            bbox=_bbox,
                            content_text=caption_text,
                            structure_json=fig
                        ))
                    
                    logger.info(f"[MULTIMODAL-EXTRACT] âœ… ë¬¸ì„œ ë ˆë²¨ figure {len(doc_figures)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
                
                # ğŸ¯ Upstage elements ê¸°ë°˜ ê°ì²´ ì¶”ì¶œ (bboxÂ·category í™œìš©)
                # Azure DIì˜ êµ¬ì¡°í™” ìˆ˜ì¤€ê³¼ ë™ë“±í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ elements ë°°ì—´ì„ ì§ì ‘ íŒŒì‹±
                upstage_elements = metadata.get("elements", [])
                if upstage_elements:
                    logger.info(f"[MULTIMODAL-EXTRACT] ğŸ”· Upstage elements {len(upstage_elements)}ê°œ ì²˜ë¦¬ ì‹œì‘")
                    
                    # Category â†’ object_type ë§¤í•‘
                    category_map = {
                        "heading1": "TEXT_BLOCK", "heading2": "TEXT_BLOCK", "heading3": "TEXT_BLOCK",
                        "paragraph": "TEXT_BLOCK", "list": "TEXT_BLOCK", "footnote": "TEXT_BLOCK",
                        "table": "TABLE", "table_continued": "TABLE",
                        "figure": "FIGURE", "chart": "FIGURE", "image": "IMAGE", "diagram": "FIGURE",
                        "equation": "TEXT_BLOCK", "index": "TEXT_BLOCK"
                    }
                    
                    for elem in upstage_elements:
                        if not isinstance(elem, dict):
                            continue
                        
                        elem_category = (elem.get("category") or "").lower()
                        object_type = category_map.get(elem_category, "TEXT_BLOCK")
                        elem_page = elem.get("page", 1)
                        elem_text = elem.get("text", "")
                        elem_coords = elem.get("coordinates") or elem.get("bbox") or []
                        
                        # Upstage coordinatesëŠ” ìƒëŒ€ ì¢Œí‘œ [[x,y], [x,y], ...] í˜•íƒœ
                        # ì ˆëŒ€ í”½ì…€ë¡œ ë³€í™˜ (í˜ì´ì§€ í¬ê¸° ê¸°ì¤€)
                        elem_bbox = [0, 0, 0, 0]
                        if elem_coords and isinstance(elem_coords, list) and len(elem_coords) >= 4:
                            try:
                                # í˜ì´ì§€ í¬ê¸° ì¡°íšŒ
                                page_width = 612  # ê¸°ë³¸ Letter í¬ê¸° (points)
                                page_height = 792
                                for p in metadata.get("pages", []):
                                    if p.get("page_number") == elem_page:
                                        page_width = p.get("width", 612) * 72  # inch â†’ points
                                        page_height = p.get("height", 792) * 72
                                        break
                                
                                # ìƒëŒ€ ì¢Œí‘œ â†’ ì ˆëŒ€ í”½ì…€
                                xs = [pt["x"] if isinstance(pt, dict) else pt[0] for pt in elem_coords if pt]
                                ys = [pt["y"] if isinstance(pt, dict) else pt[1] for pt in elem_coords if pt]
                                if xs and ys:
                                    elem_bbox = [
                                        int(min(xs) * page_width),
                                        int(min(ys) * page_height),
                                        int(max(xs) * page_width),
                                        int(max(ys) * page_height)
                                    ]
                            except Exception as e:
                                logger.warning(f"[MULTIMODAL-EXTRACT] bbox ë³€í™˜ ì‹¤íŒ¨: {e}")
                        
                        # base64 ì¸ì½”ë”©ì´ ìˆìœ¼ë©´ structure_jsonì— í¬í•¨
                        structure_data = {
                            "category": elem_category,
                            "element_id": elem.get("id"),
                            "markdown": elem.get("markdown"),
                            "html": elem.get("html")
                        }
                        if elem.get("base64_encoding"):
                            structure_data["base64_encoding"] = elem.get("base64_encoding")
                        
                        # ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ doc_tables/doc_figuresì—ì„œ ì²˜ë¦¬ëœ ê°ì²´ëŠ” ê±´ë„ˆë›°ê¸°
                        # (element_id ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ëŠ” ë³µì¡í•˜ë¯€ë¡œ, í˜ì´ì§€Â·íƒ€ì…Â·í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ê°„ë‹¨íˆ í•„í„°)
                        skip = False
                        for existing in extracted_objects:
                            if (existing.page_no == elem_page and 
                                existing.object_type == object_type and
                                (existing.content_text or "").strip() == elem_text.strip()):
                                skip = True
                                break
                        
                        if not skip and elem_text.strip():
                            extracted_objects.append(DocExtractedObject(
                                extraction_session_id=extraction_session.extraction_session_id,
                                file_bss_info_sno=file_bss_info_sno,
                                page_no=elem_page,
                                object_type=object_type,
                                sequence_in_page=len([o for o in extracted_objects if o.page_no == elem_page]),
                                bbox=elem_bbox if elem_bbox != [0, 0, 0, 0] else None,
                                content_text=elem_text[:5000],
                                structure_json=structure_data,
                                char_count=len(elem_text),
                                token_estimate=len(elem_text.split()),
                                hash_sha256=hashlib.sha256(elem_text.encode()).hexdigest()
                            ))
                    
                    logger.info(f"[MULTIMODAL-EXTRACT] âœ… Upstage elements ì²˜ë¦¬ ì™„ë£Œ (ì¶”ê°€ ê°ì²´: {len([o for o in extracted_objects if o.extraction_session_id == extraction_session.extraction_session_id])}ê°œ)")
            
            # PPT
            elif "slides" in metadata:
                for s in metadata["slides"]:
                    _add_text_obj(s.get("slide_no"), s.get("text", ""))
                    for idx in range(s.get("tables_count", 0)):
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=s.get("slide_no"),
                            object_type="TABLE",
                            sequence_in_page=idx + 1,
                            content_text=f"[í‘œ {idx+1}]",
                            structure_json={"table_index": idx}
                        ))
                    for idx in range(s.get("charts_count", 0)):
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=s.get("slide_no"),
                            object_type="FIGURE",
                            sequence_in_page=idx + 100,
                            content_text=f"[ì°¨íŠ¸ {idx+1}]",
                            structure_json={"chart_index": idx}
                        ))
                    for img_meta in s.get("images_metadata", []):
                        # PPT ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ì—ì„œë„ binary_data ì œê±°
                        clean_img_meta = dict(img_meta)
                        if 'binary_data' in clean_img_meta:
                            clean_img_meta.pop('binary_data')
                        _bbox = [
                            int(img_meta.get('left', 0)),
                            int(img_meta.get('top', 0)),
                            int(img_meta.get('left', 0) + img_meta.get('width', 0)),
                            int(img_meta.get('top', 0) + img_meta.get('height', 0))
                        ]
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=s.get("slide_no"),
                            object_type="IMAGE",
                            sequence_in_page=img_meta.get("image_index"),
                            bbox=_bbox,
                            structure_json=clean_img_meta
                        ))
            # XLSX
            elif "sheets" in metadata:
                for sh in metadata["sheets"]:
                    text_val = sh.get("text", "")
                    if text_val.strip():
                        extracted_objects.append(DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=sh.get("sheet_no"),
                            object_type="TABLE",
                            sequence_in_page=0,
                            content_text=text_val,
                            char_count=len(text_val),
                            structure_json=sh
                        ))

            # Fallback: pages/slides/sheets ëª¨ë‘ ì—†ê³  textë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš°(ì˜ˆ: direct_text_read)
            if not extracted_objects and extraction_result.get("text"):
                raw_text = extraction_result.get("text") or ""
                if raw_text.strip():
                    extracted_objects.append(
                        DocExtractedObject(
                            extraction_session_id=extraction_session.extraction_session_id,
                            file_bss_info_sno=file_bss_info_sno,
                            page_no=None,
                            object_type="TEXT_BLOCK",
                            sequence_in_page=0,
                            content_text=raw_text,
                            char_count=len(raw_text),
                            token_estimate=len(raw_text.split()),
                            hash_sha256=hashlib.sha256(raw_text.encode()).hexdigest(),
                        )
                    )
                    logger.info("[MULTIMODAL] Fallback single TEXT_BLOCK object created from raw text")

            # ì„¹ì…˜ ê°ì§€ë¥¼ ì„ í–‰í•˜ì—¬ ì´ë¯¸ì§€/í‘œ í•„í„°ë§ ë²”ìœ„ íŒŒì•… (í•™ìˆ  ë…¼ë¬¸ í•œì •)
            if apply_section_chunking:
                text_objs_for_sections = [
                    obj for obj in extracted_objects
                    if getattr(obj, "object_type", None) == "TEXT_BLOCK"
                    and (getattr(obj, "content_text", "") or "").strip()
                ]
                if text_objs_for_sections:
                    separator = "\n\n"
                    combined_parts: List[str] = []
                    section_object_spans = []
                    current_pos = 0
                    for obj in text_objs_for_sections:
                        content = (getattr(obj, "content_text", "") or "").strip()
                        if not content:
                            continue
                        if combined_parts:
                            combined_parts.append(separator)
                            current_pos += len(separator)
                        start_pos = current_pos
                        combined_parts.append(content)
                        current_pos += len(content)
                        section_object_spans.append((obj, start_pos, current_pos))
                    section_combined_text = "".join(combined_parts)

                    if section_combined_text.strip():
                        # Pass DI pages so detector can leverage Azure paragraph roles
                        # Pass Upstage elements for HTML-based section detection
                        precomputed_sections_info = self.section_detector.detect_sections(
                            section_combined_text,
                            pages=metadata.get("pages") or None,
                            markdown_text=metadata.get("markdown") or None,  # ğŸ†• ë§ˆí¬ë‹¤ìš´ ì „ë‹¬
                            elements=metadata.get("elements") or None,  # ğŸ†• Upstage elements ì „ë‹¬
                        )
                        if precomputed_sections_info:
                            precomputed_section_summary = self.section_detector.get_section_summary(precomputed_sections_info)
                            section_chunking_meta["detected_sections"] = [s.get("type") for s in precomputed_sections_info]
                            if precomputed_section_summary:
                                section_chunking_meta.setdefault("summary", precomputed_section_summary)

                            visual_page_filter = self._derive_core_content_page_set(
                                precomputed_sections_info,
                                section_object_spans,
                            )

                            # Derive allowed pages; if too narrow or unavailable, widen using safe defaults
                            total_pages = len(metadata.get('pages', metadata.get('slides', metadata.get('sheets', []))))
                            if visual_page_filter:
                                counts_before = {
                                    "IMAGE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "IMAGE"),
                                    "TABLE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "TABLE"),
                                    "FIGURE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "FIGURE"),
                                }
                                filtered_objects: List[DocExtractedObject] = []
                                removed_counts = {"IMAGE": 0, "TABLE": 0, "FIGURE": 0}
                                for obj in extracted_objects:
                                    otype = getattr(obj, "object_type", None)
                                    if otype in removed_counts:
                                        page_no = getattr(obj, "page_no", None)
                                        if page_no is not None and page_no not in visual_page_filter:
                                            removed_counts[otype] += 1
                                            continue
                                    filtered_objects.append(obj)

                                if len(filtered_objects) != len(extracted_objects):
                                    extracted_objects = filtered_objects
                                    logger.info(
                                        "[MULTIMODAL] ì„¹ì…˜ ë²”ìœ„ í•„í„° ì ìš© - ì´ë¯¸ì§€ %sê°œ, í‘œ %sê°œ, ì°¨íŠ¸ %sê°œ ì œì™¸",
                                        removed_counts["IMAGE"],
                                        removed_counts["TABLE"],
                                        removed_counts["FIGURE"],
                                    )

                                counts_after = {
                                    "IMAGE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "IMAGE"),
                                    "TABLE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "TABLE"),
                                    "FIGURE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "FIGURE"),
                                }
                                # widen if roles not used or filter too small
                                widened = False
                                reason = None
                                if (len(visual_page_filter) < 3) and total_pages:
                                    summary = section_chunking_meta.get("summary") or {}
                                    if not summary.get("azure_di_role_used"):
                                        # Use middle pages (2..N-1) as a safe default
                                        start = 2 if total_pages >= 3 else 1
                                        end = total_pages - 1 if total_pages >= 3 else total_pages
                                        widened_pages = set(range(start, end + 1))
                                        # reapply filter against widened set
                                        if widened_pages and widened_pages != visual_page_filter:
                                            visual_page_filter = widened_pages
                                            widened = True
                                            reason = "di_roles_unavailable_or_narrow"
                                            # re-run filtering with widened set
                                            filtered_objects = []
                                            removed_counts = {"IMAGE": 0, "TABLE": 0, "FIGURE": 0}
                                            for obj in extracted_objects:
                                                otype = getattr(obj, "object_type", None)
                                                if otype in removed_counts:
                                                    pno = getattr(obj, "page_no", None)
                                                    if pno is not None and pno not in visual_page_filter:
                                                        removed_counts[otype] += 1
                                                        continue
                                                filtered_objects.append(obj)
                                            extracted_objects = filtered_objects
                                            counts_after = {
                                                "IMAGE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "IMAGE"),
                                                "TABLE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "TABLE"),
                                                "FIGURE": sum(1 for obj in extracted_objects if getattr(obj, "object_type", None) == "FIGURE"),
                                            }
                                section_chunking_meta["figure_table_filter"] = {
                                    "enabled": True,
                                    "allowed_pages": sorted(visual_page_filter),
                                    "before": counts_before,
                                    "after": counts_after,
                                    "widened": widened,
                                    "widen_reason": reason,
                                }
                            else:
                                section_chunking_meta["figure_table_filter"] = {"enabled": False, "reason": "no_core_pages"}
                        else:
                            section_chunking_meta["figure_table_filter"] = {"enabled": False, "reason": "no_sections"}
                    else:
                        section_chunking_meta["figure_table_filter"] = {"enabled": False, "reason": "empty_combined_text"}
            
            for obj in extracted_objects:
                session.add(obj)
            await session.flush()
            setattr(extraction_session, "status", "success")
            setattr(extraction_session, "completed_at", datetime.now())
            setattr(extraction_session, "page_count_detected", len(metadata.get('pages', metadata.get('slides', metadata.get('sheets', [])))))
            result["objects_count"] = len(extracted_objects)
            _stage("extraction", True, objects=len(extracted_objects))

            # -----------------------------
            # 1.5. Blob Storage - ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (Azure Blob / S3)
            # -----------------------------
            performed_blob_intermediate = False
            try:
                if settings.storage_backend in ['azure_blob', 's3'] and file_bss_info_sno:
                    _start_stage("blob_intermediate_save")
                    performed_blob_intermediate = True
                    
                    if settings.storage_backend == 'azure_blob':
                        azure_factory = get_azure_blob_service if callable(get_azure_blob_service) else None
                        if not azure_factory:
                            raise RuntimeError("Azure Blob service factory not available")
                        storage = azure_factory()
                    else:  # s3
                        storage = self._get_s3_service()
                        if not storage:
                            raise RuntimeError("S3 service not available")
                    
                    # ì „ì²´ ì¶”ì¶œ í…ìŠ¤íŠ¸ ì €ì¥ (intermediate ì»¨í…Œì´ë„ˆ)
                    full_text_key = f"multimodal/{file_bss_info_sno}/extraction_full_text.txt"
                    full_text_content = extraction_result.get("text", "") or ""
                    # í•„ìš”ì‹œ fallback ì¡°ë¦½
                    if not full_text_content.strip():
                        full_text_content = _assemble_full_text(extracted_objects)
                    if full_text_content.strip():
                        storage.upload_bytes(
                            full_text_content.encode('utf-8'), 
                            full_text_key, 
                            purpose='intermediate'
                        )
                        logger.info(f"[MULTIMODAL-BLOB] ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥: {full_text_key} (len={len(full_text_content)})")
                    else:
                        logger.info("[MULTIMODAL-BLOB] ì „ì²´ í…ìŠ¤íŠ¸ ë¹„ì–´ìˆì–´ ì €ì¥ ìƒëµ")
                    
                    # Markdown ì €ì¥ (í•™ìˆ  ë…¼ë¬¸ ì„¹ì…˜ êµ¬ì¡° ë³´ì¡´)
                    markdown_content = extraction_result.get("markdown", "") or metadata.get("markdown", "")
                    if markdown_content and markdown_content.strip():
                        markdown_key = f"multimodal/{file_bss_info_sno}/extraction_full_text.md"
                        storage.upload_bytes(
                            markdown_content.encode('utf-8'),
                            markdown_key,
                            purpose='intermediate'
                        )
                        logger.info(f"[MULTIMODAL-BLOB] Markdown ì €ì¥: {markdown_key} (len={len(markdown_content)})")
                    
                    # ì¶”ì¶œ ë©”íƒ€ë°ì´í„° ì €ì¥ (binary_data ì œê±°)
                    metadata_key = f"multimodal/{file_bss_info_sno}/extraction_metadata.json"
                    
                    # metadataì—ì„œ binary_data ì œê±° (JSON ì§ë ¬í™” ì˜¤ë¥˜ ë°©ì§€)  
                    clean_metadata = _clean_metadata_for_json(metadata)
                    
                    metadata_content = {
                        "extraction_session_id": extraction_session.extraction_session_id,
                        "provider": provider,
                        "pipeline_type": pipeline_type,
                        "extracted_objects_count": len(extracted_objects),
                        "pages_detected": extraction_session.page_count_detected,
                        "extraction_metadata": clean_metadata,
                        "has_full_text": bool(full_text_content.strip()),
                        "timestamp": datetime.now().isoformat()
                    }
                    storage.upload_bytes(
                        json.dumps(metadata_content, ensure_ascii=False).encode('utf-8'),
                        metadata_key,
                        purpose='intermediate'
                    )
                    logger.info(f"[MULTIMODAL-BLOB] ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_key}")
                    
                    # ê°ì²´ë³„ ì„¸ë¶€ ì •ë³´ ì €ì¥ + ë§¤ë‹ˆí˜ìŠ¤íŠ¸ êµ¬ì„±
                    objects_manifest: List[Dict[str, Any]] = []
                    saved_counts = {"TEXT_BLOCK": 0, "TABLE": 0, "IMAGE": 0, "FIGURE": 0}
                    # ğŸ¯ Provider ì •ë³´ ì¶”ì¶œ (Azure DI vs Upstage ë¶„ê¸° ì²˜ë¦¬ìš©)
                    doc_processing_provider = metadata.get("provider", "").lower()
                    logger.info(f"[MULTIMODAL-BLOB] ë¬¸ì„œ ì²˜ë¦¬ Provider: {doc_processing_provider}")
                    
                    # PDF ì´ë¯¸ì§€ ì¶”ì¶œì„ ìœ„í•œ ì‚¬ì „ ì¤€ë¹„
                    pdf_pages = None
                    pdf_doc = None
                    is_pdf = False
                    # IMAGE ë˜ëŠ” FIGURE ê°ì²´ê°€ ìˆìœ¼ë©´ pdfplumber ì´ˆê¸°í™”
                    has_images_or_figures = any(getattr(o, 'object_type', None) in ['IMAGE', 'FIGURE'] for o in extracted_objects)
                    if file_path.lower().endswith('.pdf') and has_images_or_figures:
                        try:
                            import pdfplumber  # type: ignore
                            # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì‚¬ìš© (Azure Blob ê²½ë¡œê°€ ì•„ë‹Œ ë¡œì»¬ ì„ì‹œ íŒŒì¼)
                            pdf_doc = pdfplumber.open(actual_file_path)
                            pdf_pages = pdf_doc.pages
                            is_pdf = True
                            logger.info(f"[MULTIMODAL-BLOB] PDF ì´ˆê¸°í™” ì™„ë£Œ - FIGURE/IMAGE ë°”ì´ë„ˆë¦¬ ì¶”ì¶œ ì¤€ë¹„")
                        except Exception as e:
                            logger.warning(f"[MULTIMODAL-BLOB] PDF ì´ë¯¸ì§€ ì´ˆê¸°í™” ì‹¤íŒ¨ (ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ì¶”ì¶œ ìƒëµ): {e}")
                            pdf_pages = None
                            pdf_doc = None
                    object_save_errors: List[str] = []
                    for idx, obj in enumerate(extracted_objects):
                        try:
                            blob_key = None
                            if getattr(obj, 'object_type', None) == 'TEXT_BLOCK' and (obj.content_text or '').strip():
                                blob_key = f"multimodal/{file_bss_info_sno}/objects/text_block_{idx}_{obj.page_no or 0}.txt"
                                storage.upload_bytes(
                                    (obj.content_text or '').encode('utf-8'),
                                    blob_key,
                                    purpose='intermediate'
                                )
                            elif getattr(obj, 'object_type', None) in ['TABLE', 'IMAGE', 'FIGURE']:
                                blob_key = f"multimodal/{file_bss_info_sno}/objects/{obj.object_type.lower()}_{idx}_{obj.page_no or 0}.json"

                                if getattr(obj, 'object_type', None) in ['IMAGE', 'FIGURE']:
                                    obj_id_for_tracking = getattr(obj, 'object_id', idx)
                                    image_object_ids_seen.add(obj_id_for_tracking)

                                # structure_json ì „ì²´ ì¬ê·€ ì •ë¦¬ (binary_data/bytes ì œê±°)
                                clean_structure_json = _clean_metadata_for_json(getattr(obj, 'structure_json', None))

                                obj_content = {
                                    "object_type": obj.object_type,
                                    "page_no": obj.page_no,
                                    "sequence_in_page": obj.sequence_in_page,
                                    # TABLE ì€ placeholder í…ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ â†’ êµ¬ì¡° ê°œì„  TODO
                                    "content_text": obj.content_text,
                                    "structure_json": clean_structure_json,
                                    "bbox": obj.bbox
                                }
                                try:
                                    storage.upload_bytes(
                                        json.dumps(obj_content, ensure_ascii=False).encode('utf-8'),
                                        blob_key,
                                        purpose='intermediate'
                                    )
                                except TypeError as te:
                                    # ë””ë²„ê¹…ìš© ë¡œê·¸: ì–´ë–¤ í•„ë“œ ë•Œë¬¸ì— ì‹¤íŒ¨í–ˆëŠ”ì§€ í™•ì¸
                                    logger.warning(f"[MULTIMODAL-BLOB] ê°ì²´ JSON ì§ë ¬í™” ì‹¤íŒ¨ idx={idx}: {te}")
                                    # ê°•ì œ fallback: structure_json ì œê±° í›„ ì €ì¥
                                    fallback_content = dict(obj_content)
                                    fallback_content.pop('structure_json', None)
                                    storage.upload_bytes(
                                        json.dumps(fallback_content, ensure_ascii=False).encode('utf-8'),
                                        blob_key,
                                        purpose='intermediate'
                                    )
                                # ì´ë¯¸ì§€ ë˜ëŠ” FIGUREì¸ ê²½ìš° ë°”ì´ë„ˆë¦¬ ì €ì¥ ë° íŠ¹ì§• ì¶”ì¶œ
                                if getattr(obj, 'object_type', None) in ['IMAGE', 'FIGURE']:
                                    obj_type = getattr(obj, 'object_type', None)
                                    logger.info(f"[MULTIMODAL-BLOB] {obj_type} ê°ì²´ ë°œê²¬ idx={idx}, page={getattr(obj, 'page_no', None)}, obj_id={getattr(obj, 'object_id', None)}")
                                    img_bytes = None
                                    page_no_val = getattr(obj, 'page_no', None) or 1
                                    
                                    # ğŸ¯ STEP 1: Upstage structure_json.base64_encoding ìš°ì„  í™•ì¸ (Upstage ì „ìš©)
                                    if doc_processing_provider == "upstage":
                                        structure_json = getattr(obj, 'structure_json', None)
                                        logger.info(f"[MULTIMODAL-BLOB] STEP 1 (Upstage) - idx={idx}, structure_json type={type(structure_json).__name__}, exists={structure_json is not None}")
                                        
                                        if structure_json and isinstance(structure_json, dict):
                                            base64_data = structure_json.get('base64_encoding') or structure_json.get('base64') or structure_json.get('image')
                                            logger.info(f"[MULTIMODAL-BLOB] STEP 1 dict í™•ì¸ - idx={idx}, base64_encoding={('base64_encoding' in structure_json)}, base64={('base64' in structure_json)}, image={('image' in structure_json)}, data_len={len(base64_data) if base64_data else 0}")
                                            if base64_data:
                                                try:
                                                    import base64
                                                    img_bytes = base64.b64decode(base64_data)
                                                    logger.info(f"[MULTIMODAL-BLOB] âœ… Upstage base64 ë””ì½”ë“œ ì„±ê³µ - idx={idx}, size={len(img_bytes)} bytes, source=structure_json")
                                                except Exception as b64_err:
                                                    logger.warning(f"[MULTIMODAL-BLOB] base64 ë””ì½”ë“œ ì‹¤íŒ¨ idx={idx}: {b64_err}")
                                                    img_bytes = None
                                            else:
                                                logger.info(f"[MULTIMODAL-BLOB] STEP 1 - base64 ë°ì´í„° ì—†ìŒ, structure_json keys: {list(structure_json.keys())[:5]}")
                                        else:
                                            logger.warning(f"[MULTIMODAL-BLOB] STEP 1 ìŠ¤í‚µ - structure_jsonì´ dictê°€ ì•„ë‹˜ (type={type(structure_json).__name__})")
                                    else:
                                        logger.info(f"[MULTIMODAL-BLOB] STEP 1 ìŠ¤í‚µ - Providerê°€ Upstage ì•„ë‹˜ (provider={doc_processing_provider})")
                                    
                                    # STEP 2: Azure DI binary_data ì†ì„± ì²´í¬ (Azure DI ì „ìš©)
                                    if not img_bytes and doc_processing_provider == "azure_di":
                                        azure_binary = getattr(obj, 'binary_data', None)
                                        if azure_binary and len(azure_binary) > 0:
                                            img_bytes = azure_binary
                                            logger.info(f"[MULTIMODAL-BLOB] âœ… Azure DI binary_data ì‚¬ìš© - idx={idx}, size={len(img_bytes)} bytes")
                                        else:
                                            logger.info(f"[MULTIMODAL-BLOB] STEP 2 (Azure DI) - binary_data ì—†ìŒ")
                                    
                                    # STEP 3: PDFì—ì„œ bbox ê¸°ë°˜ í¬ë¡­ ì¶”ì¶œ (Providerë³„ ë¡œì§ ë¶„ê¸°)
                                    if not img_bytes and is_pdf and pdf_pages is not None:
                                        logger.info(f"[MULTIMODAL-BLOB] STEP 3 PDF í¬ë¡­ ì‹œì‘ - idx={idx}, page={page_no_val}, provider={doc_processing_provider}")
                                        
                                        # ğŸ¯ Providerë³„ bbox ì¶”ì¶œ ë¡œì§
                                        structure_json = getattr(obj, 'structure_json', None)
                                        bbox_val = None
                                        
                                        # Azure DI: polygon bbox ì¶”ì¶œ (inch ë‹¨ìœ„, ì •í™•)
                                        if doc_processing_provider == "azure_di" and structure_json and isinstance(structure_json, dict):
                                            polygon = structure_json.get('bbox')
                                            if polygon and isinstance(polygon, list) and len(polygon) == 4:
                                                # polygon: [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                                                # bounding box ê³„ì‚°: (min_x, min_y, max_x, max_y)
                                                x_coords = [pt[0] for pt in polygon if isinstance(pt, (list, tuple)) and len(pt) >= 2]
                                                y_coords = [pt[1] for pt in polygon if isinstance(pt, (list, tuple)) and len(pt) >= 2]
                                                
                                                if len(x_coords) == 4 and len(y_coords) == 4:
                                                    x0 = min(x_coords)
                                                    y0 = min(y_coords)
                                                    x1 = max(x_coords)
                                                    y1 = max(y_coords)
                                                    bbox_val = [x0, y0, x1, y1]
                                                    logger.info(f"[MULTIMODAL-BLOB] âœ… Azure DI polygon bbox ì¶”ì¶œ - idx={idx}, polygon={polygon[:2]}..., bbox={bbox_val}, size={(x1-x0):.2f}x{(y1-y0):.2f}inch")
                                        
                                        # Upstage: bboxëŠ” ë³´í†µ [0,0,0,0]ì´ë¯€ë¡œ obj.bbox ì²´í¬ë§Œ
                                        elif doc_processing_provider == "upstage":
                                            logger.info(f"[MULTIMODAL-BLOB] Upstage bbox ì²´í¬ (ë³´í†µ ë¬´íš¨) - idx={idx}")
                                        
                                        # Fallback: obj.bbox ì‚¬ìš© (Azure DIë§Œ ìœ íš¨)
                                        if not bbox_val and doc_processing_provider == "azure_di":
                                            bbox_val = getattr(obj, 'bbox', None)
                                            logger.info(f"[MULTIMODAL-BLOB] Azure DI bbox fallback (obj.bbox) - idx={idx}, bbox_val={bbox_val}, type={type(bbox_val)}, page_no_val={page_no_val}")
                                        elif not bbox_val:
                                            logger.info(f"[MULTIMODAL-BLOB] âš ï¸ bbox ì—†ìŒ (provider={doc_processing_provider}) - idx={idx}")
                                        
                                        # ğŸ¯ bboxê°€ [0,0,0,0]ì¸ ê²½ìš° ê°™ì€ í˜ì´ì§€ì˜ FIGURE bbox ì°¾ì•„ì„œ ì‚¬ìš©
                                        if bbox_val == [0, 0, 0, 0] or (isinstance(bbox_val, (list, tuple)) and all(v == 0 for v in bbox_val)):
                                            logger.warning(f"[MULTIMODAL-BLOB] âš ï¸ IMAGE bbox ë¬´íš¨ (idx={idx}) â†’ ê°™ì€ í˜ì´ì§€ì˜ FIGURE bbox ê²€ìƒ‰ ì¤‘...")
                                            sequence_in_page = getattr(obj, 'sequence_in_page', None)
                                            
                                            # ê°™ì€ í˜ì´ì§€ì—ì„œ sequenceê°€ ë¹„ìŠ·í•œ FIGURE ì°¾ê¸°
                                            for candidate_obj in extracted_objects:
                                                if (getattr(candidate_obj, 'object_type', None) == 'FIGURE' and
                                                    getattr(candidate_obj, 'page_no', None) == page_no_val):
                                                    
                                                    candidate_bbox = getattr(candidate_obj, 'bbox', None)
                                                    candidate_seq = getattr(candidate_obj, 'sequence_in_page', None)
                                                    
                                                    # bboxê°€ ìœ íš¨í•˜ê³  sequenceê°€ ë¹„ìŠ·í•˜ë©´ ì‚¬ìš©
                                                    if (candidate_bbox and 
                                                        isinstance(candidate_bbox, (list, tuple)) and 
                                                        len(candidate_bbox) == 4 and
                                                        not all(v == 0 for v in candidate_bbox)):
                                                        
                                                        # sequenceê°€ ê°™ê±°ë‚˜ Â±1 ì°¨ì´ë©´ ë§¤ì¹­
                                                        if sequence_in_page is None or candidate_seq is None or abs(candidate_seq - sequence_in_page) <= 1:
                                                            bbox_val = candidate_bbox
                                                            logger.info(f"[MULTIMODAL-BLOB] âœ… FIGURE bbox ì ìš© ì„±ê³µ - FIGURE seq={candidate_seq}, IMAGE seq={sequence_in_page}, bbox={bbox_val}")
                                                            break
                                            
                                            if bbox_val == [0, 0, 0, 0] or (isinstance(bbox_val, (list, tuple)) and all(v == 0 for v in bbox_val)):
                                                logger.warning(f"[MULTIMODAL-BLOB] âŒ ë§¤ì¹­ë˜ëŠ” FIGURE bboxë¥¼ ì°¾ì§€ ëª»í•¨ â†’ pdfplumber fallback ì‹œë„")
                                        
                                        # bbox ìœ íš¨ì„± ê²€ì¦
                                        is_valid_bbox = (
                                            isinstance(page_no_val, int) and 
                                            isinstance(bbox_val, (list, tuple)) and 
                                            len(bbox_val) == 4 and
                                            not all(v == 0 for v in bbox_val)  # [0,0,0,0] ì œì™¸
                                        )
                                        
                                        if is_valid_bbox:
                                            logger.info(f"[MULTIMODAL-BLOB] bbox ê²€ì¦ í†µê³¼ - idx={idx}, í¬ë¡­ ì‹œë„ ì‹œì‘")
                                            try:
                                                page_index = page_no_val - 1
                                                if 0 <= page_index < len(pdf_pages):
                                                    page = pdf_pages[page_index]
                                                    x0, y0, x1, y1 = [float(v) for v in bbox_val]
                                                    
                                                    # ğŸ¯ Azure DI bboxëŠ” inch ë‹¨ìœ„ â†’ 150 DPIë¡œ í”½ì…€ ë³€í™˜
                                                    # ìµœì†Œ í¬ê¸°: 0.5 inch (75 í”½ì…€ @ 150 DPI)
                                                    width_inch = x1 - x0
                                                    height_inch = y1 - y0
                                                    width_px = width_inch * 150
                                                    height_px = height_inch * 150
                                                    
                                                    logger.info(f"[MULTIMODAL-BLOB] bbox í¬ê¸° - idx={idx}, inch=({width_inch:.2f}x{height_inch:.2f}), pixels=({width_px:.0f}x{height_px:.0f})")
                                                    
                                                    page_image = page.to_image(resolution=150)
                                                    
                                                    # ìµœì†Œ í¬ê¸° ê²€ì¦: 0.3 inch (45 í”½ì…€) ì´ìƒ
                                                    if width_inch > 0.3 and height_inch > 0.3:
                                                        import io
                                                        from PIL import Image  # type: ignore
                                                        
                                                        # ğŸ¯ ì¢Œí‘œ ë³€í™˜: Azure DI bbox (inch) â†’ pdfplumber image (í”½ì…€)
                                                        # PDF ê¸°ë³¸ í•´ìƒë„: 72 DPI
                                                        # to_image(resolution=150) ìŠ¤ì¼€ì¼ íŒ©í„°: 150/72 = 2.083333
                                                        render_dpi = 150
                                                        pdf_dpi = 72
                                                        scale_factor = render_dpi / pdf_dpi
                                                        
                                                        # inch â†’ points (72 DPI) â†’ scaled pixels (150 DPI)
                                                        x0_px = x0 * pdf_dpi * scale_factor  # inch * 72 * 2.083 = inch * 150
                                                        y0_px = y0 * pdf_dpi * scale_factor
                                                        x1_px = x1 * pdf_dpi * scale_factor
                                                        y1_px = y1 * pdf_dpi * scale_factor
                                                        
                                                        logger.info(f"[MULTIMODAL-BLOB] ì¢Œí‘œ ë³€í™˜ - inch=({x0:.2f},{y0:.2f},{x1:.2f},{y1:.2f}) â†’ pixels@150dpi=({x0_px:.1f},{y0_px:.1f},{x1_px:.1f},{y1_px:.1f}), scale={scale_factor:.3f}")
                                                        
                                                        cropped = page_image.original.crop((x0_px, y0_px, x1_px, y1_px))
                                                        buf = io.BytesIO()
                                                        cropped.save(buf, format='PNG')
                                                        buf.seek(0)
                                                        img_bytes = buf.getvalue()
                                                        logger.info(f"[MULTIMODAL-BLOB] âœ… PDF ì´ë¯¸ì§€ ì¶”ì¶œ ì„±ê³µ (Azure DI polygon bbox) idx={idx}, page={page_no_val}, size={len(img_bytes)} bytes, dimensions={cropped.size}")
                                                    else:
                                                        logger.warning(f"[MULTIMODAL-BLOB] ì´ë¯¸ì§€ í¬ê¸° ë¶€ì¡± - idx={idx}, inch=({width_inch:.2f}x{height_inch:.2f}), pixels=({width_px:.0f}x{height_px:.0f})")
                                            except Exception as img_err:
                                                logger.warning(f"[MULTIMODAL-BLOB] PDF ì´ë¯¸ì§€ í¬ë¡­ ì‹¤íŒ¨ idx={idx}, page={page_no_val}, bbox={bbox_val}, error={img_err}")
                                        elif bbox_val == [0, 0, 0, 0] or (isinstance(bbox_val, (list, tuple)) and all(v == 0 for v in bbox_val)):
                                            logger.warning(f"[MULTIMODAL-BLOB] âš ï¸ ë¬´íš¨í•œ bbox ê°ì§€ - idx={idx}, bbox={bbox_val} â†’ pdfplumber ì§ì ‘ ì¶”ì¶œ ì‹œë„")
                                            # Fallback: pdfplumberì˜ ì´ë¯¸ì§€ ê°ì²´ ì§ì ‘ ì¶”ì¶œ
                                            try:
                                                page_index = page_no_val - 1
                                                if 0 <= page_index < len(pdf_pages):
                                                    page = pdf_pages[page_index]
                                                    images = page.images
                                                    logger.info(f"[MULTIMODAL-BLOB] pdfplumber ê°ì§€ ì´ë¯¸ì§€ ìˆ˜: {len(images)} on page {page_no_val}")
                                                    
                                                    # í˜ì´ì§€ ë‚´ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ì¶”ì •
                                                    sequence_in_page = getattr(obj, 'sequence_in_page', 1)  # Azure DI default: 1
                                                    # Upstage APIê°€ 0-basedë¥¼ ë°˜í™˜í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, 1-basedë©´ ë³€í™˜
                                                    image_index = sequence_in_page if sequence_in_page == 0 else sequence_in_page - 1
                                                    logger.info(f"[MULTIMODAL-BLOB] ì´ë¯¸ì§€ ì¸ë±ì‹± ë³€í™˜ - Azure DI sequence={sequence_in_page} â†’ pdfplumber index={image_index}")
                                                    
                                                    # ğŸ¯ ì‹¤ì œ ì´ë¯¸ì§€ë§Œ í•„í„°ë§ (í…ìŠ¤íŠ¸ ë¸”ë¡ ì œì™¸)
                                                    if images:
                                                        # ì´ë¯¸ì§€ í¬ê¸° ê³„ì‚° ë° í•„í„°ë§
                                                        sized_images = []
                                                        for i, img_obj in enumerate(images):
                                                            x0, top, x1, bottom = img_obj['x0'], img_obj['top'], img_obj['x1'], img_obj['bottom']
                                                            width = x1 - x0
                                                            height = bottom - top
                                                            area = width * height
                                                            
                                                            # ì‹¤ì œ ì„ë² ë””ë“œ ì´ë¯¸ì§€ í•„í„° (stream ì†ì„± í™•ì¸)
                                                            has_stream = 'stream' in img_obj
                                                            img_filter = img_obj.get('filter', '')
                                                            # JPEG, JPEG2000, TIFF ë“± ë˜ìŠ¤í„° ì´ë¯¸ì§€ í¬ë§·ë§Œ ì„ íƒ
                                                            is_raster_image = img_filter in ['DCTDecode', 'JPXDecode', 'CCITTFaxDecode', 'FlateDecode']
                                                            
                                                            # ìµœì†Œ í¬ê¸° í•„í„° (50x50 ì´ìƒ) + ì‹¤ì œ ì´ë¯¸ì§€ë§Œ
                                                            if width >= 50 and height >= 50 and has_stream and is_raster_image:
                                                                sized_images.append({
                                                                    'index': i,
                                                                    'obj': img_obj,
                                                                    'width': width,
                                                                    'height': height,
                                                                    'area': area,
                                                                    'x0': x0,
                                                                    'top': top,
                                                                    'x1': x1,
                                                                    'bottom': bottom,
                                                                    'filter': img_filter
                                                                })
                                                        
                                                        # ë©´ì  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                                                        sized_images.sort(key=lambda x: x['area'], reverse=True)
                                                        
                                                        logger.info(f"[MULTIMODAL-BLOB] ìœ íš¨ ë˜ìŠ¤í„° ì´ë¯¸ì§€ {len(sized_images)}ê°œ (ìµœì†Œ 50x50, stream í•„í„°: DCTDecode/JPXDecode/CCITTFaxDecode/FlateDecode)")
                                                        if sized_images:
                                                            for img_info in sized_images:
                                                                logger.debug(f"  - index={img_info['index']}, size={img_info['width']:.0f}x{img_info['height']:.0f}, area={img_info['area']:.0f}, filter={img_info['filter']}")
                                                        
                                                        # sequenceì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ ë˜ëŠ” ê°€ì¥ í° ì´ë¯¸ì§€ ì„ íƒ
                                                        target_img = None
                                                        if 0 <= image_index < len(sized_images):
                                                            target_img = sized_images[image_index]
                                                            logger.info(f"[MULTIMODAL-BLOB] âœ… sequence={image_index} ì´ë¯¸ì§€ ì„ íƒ - size={target_img['width']:.0f}x{target_img['height']:.0f}, filter={target_img['filter']}")
                                                        elif sized_images:
                                                            target_img = sized_images[0]
                                                            logger.info(f"[MULTIMODAL-BLOB] âš ï¸ sequence ë²”ìœ„ ì´ˆê³¼ (index={image_index}), ê°€ì¥ í° ë˜ìŠ¤í„° ì´ë¯¸ì§€ ì„ íƒ - size={target_img['width']:.0f}x{target_img['height']:.0f}, area={target_img['area']:.0f}, filter={target_img['filter']}")
                                                        
                                                        if target_img:
                                                            import io
                                                            from PIL import Image  # type: ignore
                                                            
                                                            page_image = page.to_image(resolution=150)
                                                            cropped = page_image.original.crop((
                                                                target_img['x0'],
                                                                target_img['top'],
                                                                target_img['x1'],
                                                                target_img['bottom']
                                                            ))
                                                            
                                                            # ğŸ” ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦ (ë„ˆë¬´ ë‹¨ìˆœí•œ ì´ë¯¸ì§€ ì œì™¸)
                                                            import numpy as np
                                                            img_array = np.array(cropped)
                                                            
                                                            # ìƒ‰ìƒ ë¶„ì‚° ê³„ì‚° (ë‹¨ìƒ‰ ì´ë¯¸ì§€ ì œì™¸)
                                                            if len(img_array.shape) >= 3:
                                                                color_variance = np.var(img_array)
                                                                unique_colors = len(np.unique(img_array.reshape(-1, img_array.shape[-1]), axis=0))
                                                                logger.info(f"[MULTIMODAL-BLOB] ì´ë¯¸ì§€ í’ˆì§ˆ - variance={color_variance:.1f}, unique_colors={unique_colors}")
                                                                
                                                                # ë„ˆë¬´ ë‹¨ìˆœí•œ ì´ë¯¸ì§€ ì œì™¸ (ìˆœë°±ìƒ‰, ìˆœí‘ìƒ‰ ë“±)
                                                                if color_variance < 10 and unique_colors < 5:
                                                                    logger.warning(f"[MULTIMODAL-BLOB] âŒ ë‹¨ìˆœ ì´ë¯¸ì§€ ì œì™¸ (variance={color_variance:.1f}, colors={unique_colors})")
                                                                    target_img = None
                                                            
                                                            if target_img:
                                                                buf = io.BytesIO()
                                                                cropped.save(buf, format='PNG')
                                                                buf.seek(0)
                                                                img_bytes = buf.getvalue()
                                                                logger.info(f"[MULTIMODAL-BLOB] âœ… pdfplumber ì§ì ‘ ì¶”ì¶œ ì„±ê³µ idx={idx}, page={page_no_val}, size={len(img_bytes)} bytes, "
                                                                          f"dimensions={target_img['width']:.0f}x{target_img['height']:.0f}, "
                                                                          f"bbox=({target_img['x0']:.1f},{target_img['top']:.1f},{target_img['x1']:.1f},{target_img['bottom']:.1f})")
                                                        else:
                                                            logger.warning(f"[MULTIMODAL-BLOB] âŒ ìœ íš¨í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í•¨")
                                                    else:
                                                        logger.warning(f"[MULTIMODAL-BLOB] pdfplumber ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼ - Azure DI sequence={sequence_in_page}, pdfplumber index={image_index}, available={len(images)}")
                                            except Exception as fallback_err:
                                                logger.warning(f"[MULTIMODAL-BLOB] pdfplumber ì§ì ‘ ì¶”ì¶œ ì‹¤íŒ¨ idx={idx}, page={page_no_val}, error={fallback_err}")
                                                import traceback
                                                logger.debug(traceback.format_exc())
                                        else:
                                            logger.warning(f"[MULTIMODAL-BLOB] bbox ê²€ì¦ ì‹¤íŒ¨ - idx={idx}, page_no_val={page_no_val} (type={type(page_no_val)}), bbox_val={bbox_val} (type={type(bbox_val)}, len={len(bbox_val) if bbox_val and hasattr(bbox_val, '__len__') else 'N/A'})")
                                    
                                    # DOCX ì´ë¯¸ì§€ ì²˜ë¦¬ (Azure DI ë°”ì´ë„ˆë¦¬ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
                                    elif not img_bytes and file_path.lower().endswith('.docx'):
                                        try:
                                            # ì›ë³¸ ë©”íƒ€ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ì°¾ê¸°
                                            image_index = getattr(obj, 'sequence_in_page', None)
                                            page_no_val = getattr(obj, 'page_no', 1)
                                            
                                            # pages ë©”íƒ€ë°ì´í„°ì—ì„œ í•´ë‹¹ ì´ë¯¸ì§€ ì°¾ê¸°
                                            if "pages" in metadata:
                                                for page in metadata["pages"]:
                                                    if page.get("page_no") == page_no_val:
                                                        for img_meta in page.get("images_metadata", []):
                                                            if img_meta.get("image_index") == image_index and 'binary_data' in img_meta:
                                                                img_bytes = img_meta['binary_data']
                                                                logger.debug(f"[MULTIMODAL-BLOB] DOCX ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ë°œê²¬ idx={idx}, size={len(img_bytes)}")
                                                                break
                                        except Exception as docx_err:
                                            logger.debug(f"[MULTIMODAL-BLOB] DOCX ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ idx={idx}: {docx_err}")
                                    # PPTX ì´ë¯¸ì§€ ì²˜ë¦¬ (Azure DI ë°”ì´ë„ˆë¦¬ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
                                    elif not img_bytes and file_path.lower().endswith('.pptx'):
                                        try:
                                            image_index = getattr(obj, 'sequence_in_page', None)
                                            slide_no_val = getattr(obj, 'page_no', 1)
                                            if "slides" in metadata:
                                                for slide in metadata["slides"]:
                                                    if slide.get("slide_no") == slide_no_val:
                                                        for img_meta in slide.get("images_metadata", []):
                                                            if img_meta.get("image_index") == image_index and 'binary_data' in img_meta:
                                                                img_bytes = img_meta['binary_data']
                                                                logger.debug(f"[MULTIMODAL-BLOB] PPTX ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ë°œê²¬ idx={idx}, size={len(img_bytes)}")
                                                                break
                                        except Exception as pptx_err:
                                            logger.debug(f"[MULTIMODAL-BLOB] PPTX ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ idx={idx}: {pptx_err}")
                                    
                                    # STEP 3: ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ê²€ì¦ ë° ì €ì¥
                                    if img_bytes:
                                        logger.info(f"[MULTIMODAL-BLOB] âœ… ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ ìµœì¢… í™•ë³´ idx={idx}, size={len(img_bytes)} bytes, page={page_no_val}")
                                        try:
                                            # object_idë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ëœ blob í‚¤ ìƒì„±
                                            obj_id = getattr(obj, 'object_id', idx)
                                            img_blob_key = f"multimodal/{file_bss_info_sno}/objects/image_{obj_id}_{page_no_val}.png"
                                            storage.upload_bytes(img_bytes, img_blob_key, purpose='intermediate')
                                            image_ids_with_binary.add(obj_id)
                                            
                                            # B/C. Extract image features (pHash, dimensions)
                                            enhanced_features = {}
                                            if self.image_embedding_service:
                                                try:
                                                    features = await self.image_embedding_service.extract_features(img_bytes)
                                                    enhanced_features = {
                                                        "phash": features.get("phash"),
                                                        "width": features.get("width"),
                                                        "height": features.get("height"),
                                                        "aspect_ratio": features.get("aspect_ratio")
                                                    }
                                                    
                                                    # D. Update database object with extracted features
                                                    setattr(obj, 'phash', features.get("phash"))
                                                    setattr(obj, 'image_width', features.get("width"))
                                                    setattr(obj, 'image_height', features.get("height"))
                                                    
                                                    # C. Save enhanced feature JSON
                                                    feature_key = f"multimodal/{file_bss_info_sno}/objects/image_{obj_id}_{page_no_val}_features.json"
                                                    storage.upload_bytes(
                                                        json.dumps(features, ensure_ascii=False, indent=2).encode('utf-8'),
                                                        feature_key,
                                                        purpose='intermediate'
                                                    )
                                                except Exception as feat_err:
                                                    logger.debug(f"[MULTIMODAL-BLOB] ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨ obj_id={obj_id}: {feat_err}")
                                            
                                            objects_manifest.append({
                                                **_object_to_manifest_entry(idx, obj, blob_key),
                                                "binary_image_key": img_blob_key,
                                                "has_binary": True,
                                                **enhanced_features
                                            })
                                            # ğŸ†• Providerë³„ ì¹´ìš´íŠ¸: Azure DI=IMAGE, Upstage=FIGURE
                                            if obj_type == 'FIGURE':
                                                saved_counts['FIGURE'] += 1
                                            else:
                                                saved_counts['IMAGE'] += 1
                                            continue
                                            
                                        except Exception as save_err:
                                            logger.warning(f"[MULTIMODAL-BLOB] ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨ obj_id={obj_id}, page={page_no_val}: {save_err}")
                                    else:
                                        # ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ë¥¼ í™•ë³´í•˜ì§€ ëª»í•œ ê²½ìš°
                                        obj_id = getattr(obj, 'object_id', None)
                                        has_azure_binary = getattr(obj, 'binary_data', None) is not None
                                        has_upstage_base64 = False
                                        structure_json = getattr(obj, 'structure_json', None)
                                        if isinstance(structure_json, dict):
                                            has_upstage_base64 = bool(structure_json.get('base64_encoding') or structure_json.get('base64') or structure_json.get('image'))
                                        bbox_val = getattr(obj, 'bbox', None)
                                        logger.warning(
                                            f"[MULTIMODAL-BLOB] âŒ {obj_type} ë°”ì´ë„ˆë¦¬ ì—†ìŒ - "
                                            f"idx={idx}, obj_id={obj_id}, page={page_no_val}, provider={doc_processing_provider}, "
                                            f"Azure_DI_binary={'ìˆìŒ' if has_azure_binary else 'ì—†ìŒ'}, "
                                            f"Upstage_base64={'ìˆìŒ' if has_upstage_base64 else 'ì—†ìŒ'}, "
                                            f"bbox={bbox_val}, "
                                            f"file_type={file_path.split('.')[-1] if file_path else 'unknown'}"
                                        )
                            if blob_key:
                                objects_manifest.append({**_object_to_manifest_entry(idx, obj, blob_key), "has_binary": False})
                                otype = getattr(obj, 'object_type', None)
                                if isinstance(otype, str) and otype in saved_counts:
                                    saved_counts[otype] += 1
                        except Exception as oe:
                            msg = f"idx={idx} type={getattr(obj,'object_type',None)} err={oe}"
                            object_save_errors.append(msg)
                            logger.warning(f"[MULTIMODAL-BLOB] ê°ì²´ ì €ì¥ ì˜¤ë¥˜: {msg}")
                    # PDF ë¬¸ì„œ í•¸ë“¤ ë‹«ê¸°
                    try:
                        if pdf_doc:
                            pdf_doc.close()
                    except Exception:
                        pass
                    
                    # ê°ì²´ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥
                    manifest_key = f"multimodal/{file_bss_info_sno}/objects_manifest.json"
                    storage.upload_bytes(
                        json.dumps(objects_manifest, ensure_ascii=False, indent=2).encode('utf-8'),
                        manifest_key,
                        purpose='intermediate'
                    )
                    logger.info(f"[MULTIMODAL-BLOB] objects_manifest ì €ì¥: {manifest_key} ({len(objects_manifest)} entries)")
                    
                    # Ensure database objects are updated with extracted features
                    await session.flush()

                    # ğŸ†• Providerë³„ ì´ë¯¸ì§€ ê°ì²´ êµ¬ë¶„: Azure DI=IMAGE, Upstage=FIGURE
                    total_visual_objects = saved_counts['IMAGE'] + saved_counts['FIGURE']
                    logger.info(
                        f"[MULTIMODAL-BLOB] ê°ì²´ ì €ì¥ ì™„ë£Œ (Provider={doc_processing_provider}) - "
                        f"text={saved_counts['TEXT_BLOCK']} table={saved_counts['TABLE']} "
                        f"visual={total_visual_objects} (IMAGE={saved_counts['IMAGE']}, FIGURE={saved_counts['FIGURE']}) "
                        f"errors={len(object_save_errors)}"
                    )
                    _stage(
                        "blob_intermediate_save", True,
                        objects_saved=len(objects_manifest),
                        text_blocks=saved_counts['TEXT_BLOCK'],
                        tables=saved_counts['TABLE'],
                        images=saved_counts['IMAGE'],
                        figures=saved_counts['FIGURE'],
                        object_save_errors=object_save_errors[:5]
                    )
            except Exception as blob_err:
                logger.warning(f"[MULTIMODAL-BLOB] ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {blob_err}")
                if performed_blob_intermediate:
                    _stage("blob_intermediate_save", False, error=str(blob_err))
            finally:
                if not performed_blob_intermediate:
                    _stage("blob_intermediate_save", False, skipped=True)

            # -----------------------------
            # 2. Chunking (advanced)
            # -----------------------------
            _start_stage("chunking")
            # SQLAlchemy ì»¬ëŸ¼ ì†ì„± ëŒ€ì‹  ì•ˆì „í•˜ê²Œ getattr ì‚¬ìš©
            text_objs = [o for o in extracted_objects if getattr(o, 'object_type', None) == "TEXT_BLOCK"]
            # ğŸ†• Providerë³„ ì´ë¯¸ì§€ íƒ€ì…: Azure DI=IMAGE, Upstage=FIGURE
            raw_image_objs = [o for o in extracted_objects if getattr(o, 'object_type', None) in ["IMAGE", "FIGURE"]]
            
            # ì´ë¯¸ì§€ íƒ€ì…ë³„ ì¹´ìš´íŠ¸ (ë””ë²„ê¹…ìš©)
            image_type_counts = {}
            for o in raw_image_objs:
                otype = getattr(o, 'object_type', None)
                image_type_counts[otype] = image_type_counts.get(otype, 0) + 1
            logger.info(f"[CHUNKING] Provider={doc_processing_provider}, ì¶”ì¶œëœ ì´ë¯¸ì§€ ê°ì²´: {image_type_counts}, ë°”ì´ë„ˆë¦¬ ìˆëŠ” object_ids: {len(image_ids_with_binary)}ê°œ")
            
            if image_object_ids_seen:
                image_objs = [
                    o for o in raw_image_objs
                    if getattr(o, 'object_id', None) in image_ids_with_binary
                ]
                skipped_images = len(raw_image_objs) - len(image_objs)
                if skipped_images > 0:
                    skipped_obj_info = [
                        f"{getattr(o, 'object_type', 'unknown')}#{getattr(o, 'object_id', None)}"
                        for o in raw_image_objs 
                        if getattr(o, 'object_id', None) not in image_ids_with_binary
                    ]
                    logger.warning(
                        f"[MULTIMODAL] âš ï¸ ë°”ì´ë„ˆë¦¬ ëˆ„ë½ìœ¼ë¡œ ì´ë¯¸ì§€ ì²­í¬ {skipped_images}ê°œ ì œì™¸ "
                        f"(Provider={doc_processing_provider}, ì œì™¸ëœ ê°ì²´={skipped_obj_info})"
                    )
            else:
                image_objs = raw_image_objs
                logger.info(f"[MULTIMODAL] image_object_ids_seen=False, ëª¨ë“  ì´ë¯¸ì§€ ê°ì²´ í¬í•¨: {len(raw_image_objs)}ê°œ")
            table_objs = [o for o in extracted_objects if getattr(o, 'object_type', None) == "TABLE"]
            text_objs = [o for o in text_objs if (getattr(o, 'content_text', '') or '').strip()]

            def _normalize_structure_elements() -> List[Dict[str, Any]]:
                """Provider ê²°ê³¼/í…ìŠ¤íŠ¸ ê°ì²´ì—ì„œ êµ¬ì¡° ìš”ì†Œ(elements) ìŠ¤íŠ¸ë¦¼ì„ ìƒì„±í•œë‹¤.

                ëª©í‘œ: ì œëª©/ì„¹ì…˜/ì„œë¸Œì„¹ì…˜/ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ëœ element ëª©ë¡ì„ ë§Œë“¤ê³ ,
                ì´ë¥¼ StructureAwareChunkerì— ë„£ì–´ ê³„ì¸µì  ì²­í‚¹ì„ ìˆ˜í–‰.
                """
                elements: List[Dict[str, Any]] = []

                # Map page_no -> TEXT_BLOCK object_id (best-effort). This keeps source_object_ids bigint[]-safe.
                page_to_text_object_id: Dict[int, int] = {}
                for obj in text_objs:
                    pno = getattr(obj, 'page_no', None)
                    oid = getattr(obj, 'object_id', None)
                    if isinstance(pno, int) and isinstance(oid, int) and pno not in page_to_text_object_id:
                        page_to_text_object_id[pno] = oid

                # 1) Upstage: metadata['elements']ê°€ ìˆìœ¼ë©´ ìµœìš°ì„  ì‚¬ìš©
                raw_elements = metadata.get('elements')
                if isinstance(raw_elements, list) and raw_elements:
                    for idx, elem in enumerate(raw_elements):
                        if not isinstance(elem, dict):
                            continue
                        cat = (elem.get('category') or elem.get('type') or '').lower()
                        text_val = (elem.get('content') or elem.get('text') or '').strip()
                        page_no = elem.get('page')
                        if not text_val and cat not in ['table', 'figure', 'image', 'chart']:
                            continue
                        source_oid = None
                        if isinstance(page_no, int):
                            source_oid = page_to_text_object_id.get(page_no)
                        elements.append({
                            'id': source_oid or 0,
                            'category': cat,
                            'text': text_val,
                            'page': page_no if isinstance(page_no, int) else None,
                        })
                    if elements:
                        return elements

                # 2) Azure DI: pages[*].paragraphs(role í¬í•¨) ê¸°ë°˜
                pages = metadata.get('pages')
                if isinstance(pages, list) and pages:
                    para_found = False
                    for p in pages:
                        if not isinstance(p, dict):
                            continue
                        page_no = p.get('page_no')
                        paras = p.get('paragraphs')
                        if not isinstance(paras, list) or not paras:
                            continue
                        para_found = True
                        source_oid = page_to_text_object_id.get(page_no) if isinstance(page_no, int) else None
                        for para_idx, para in enumerate(paras):
                            if not isinstance(para, dict):
                                continue
                            content = (para.get('content') or para.get('text') or '').strip()
                            if not content:
                                continue
                            role = (para.get('role') or '').lower()
                            # role â†’ category ë§¤í•‘ (ì‚¬ì „ ì •ì˜ ì„¹ì…˜ëª…ì´ ì•„ë‹ˆë¼, ë¬¸ì„œ ë ˆì´ì•„ì›ƒ role ê¸°ë°˜)
                            if role in ('title',):
                                cat = 'title'
                            elif 'heading' in role or role in ('sectionheading', 'section_heading'):
                                # AzureëŠ” ì„¸ë¶€ ë ˆë²¨ì„ í•­ìƒ ì£¼ì§€ ì•Šìœ¼ë¯€ë¡œ heading2ë¡œ í†µì¼(ê³„ì¸µì€ heuristicsë¡œ ë³´ê°• ê°€ëŠ¥)
                                cat = 'heading2'
                            elif role in ('pageheader', 'header'):
                                cat = 'header'
                            elif role in ('pagefooter', 'footer'):
                                cat = 'footer'
                            elif role in ('listitem', 'list_item'):
                                cat = 'list'
                            else:
                                cat = 'paragraph'

                            elements.append({
                                'id': source_oid or 0,
                                'category': cat,
                                'text': content,
                                'page': page_no if isinstance(page_no, int) else None,
                            })
                    if para_found and elements:
                        return elements

                # 3) Fallback: í˜„ì¬ TEXT_BLOCKì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í•´ + ë²ˆí˜¸/í˜•ì‹ ê¸°ë°˜ í—¤ë” ì¶”ì •
                import re
                heading_pat = re.compile(r"^\s*(?:\d{1,2}(?:\.\d{1,2}){0,6}|[IVX]{1,6})[\).\-\s]+\S+", re.IGNORECASE)

                def looks_like_heading(line: str) -> bool:
                    s = line.strip()
                    if not s:
                        return False
                    # ë„ˆë¬´ ê¸´ ì¤„ì€ í—¤ë” ê°€ëŠ¥ì„±ì´ ë‚®ìŒ
                    if len(s) > 120:
                        return False
                    # ìˆ«ì/ë¡œë§ˆìˆ«ì ê¸°ë°˜ í—¤ë”
                    if heading_pat.match(s):
                        return True
                    # ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ì§§ì€ êµ¬ë¬¸
                    if s.endswith(':') and len(s) <= 80:
                        return True
                    # ALL CAPS(ì˜ë¬¸) ì§§ì€ ì¤„
                    if len(s) <= 60 and s.isupper() and any(c.isalpha() for c in s):
                        return True
                    return False

                for obj in text_objs:
                    raw = (getattr(obj, 'content_text', '') or '').strip()
                    if not raw:
                        continue
                    page_no = getattr(obj, 'page_no', None)
                    obj_id = getattr(obj, 'object_id', None)
                    # í•œ í˜ì´ì§€ í…ìŠ¤íŠ¸ ë¸”ë¡ì´ë©´ ë¼ì¸ìœ¼ë¡œ ìª¼ê°œì„œ í—¤ë” ê°ì§€
                    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
                    # ë¼ì¸ì´ ê±°ì˜ ì—†ìœ¼ë©´ ë¬¸ë‹¨ìœ¼ë¡œ ì²˜ë¦¬
                    if len(lines) <= 2:
                        elements.append({'id': int(obj_id or 0), 'category': 'paragraph', 'text': raw, 'page': page_no})
                        continue
                    buffer: List[str] = []
                    for ln in lines:
                        if looks_like_heading(ln):
                            if buffer:
                                elements.append({'id': int(obj_id or 0), 'category': 'paragraph', 'text': "\n".join(buffer).strip(), 'page': page_no})
                                buffer = []
                            elements.append({'id': int(obj_id or 0), 'category': 'heading2', 'text': ln, 'page': page_no})
                        else:
                            buffer.append(ln)
                    if buffer:
                        elements.append({'id': int(obj_id or 0), 'category': 'paragraph', 'text': "\n".join(buffer).strip(), 'page': page_no})

                return elements
            
            # ğŸ†• References ì´ì „ ê°ì²´ë§Œ í•„í„°ë§ (í•™ìˆ  ë…¼ë¬¸ ì²˜ë¦¬)
            references_page = None
            if apply_section_chunking and precomputed_sections_info:
                try:
                    original_text_count = len(text_objs)
                    original_image_count = len(image_objs)
                    original_table_count = len(table_objs)
                    
                    text_objs, references_page = filter_objects_before_references(
                        precomputed_sections_info, text_objs
                    )
                    image_objs, _ = filter_objects_before_references(
                        precomputed_sections_info, image_objs
                    )
                    table_objs, _ = filter_objects_before_references(
                        precomputed_sections_info, table_objs
                    )
                    
                    if references_page:
                        logger.info(
                            f"[REFERENCES-FILTER] References ì´í›„ ê°ì²´ ì œì™¸ (pageâ‰¥{references_page}): "
                            f"í…ìŠ¤íŠ¸ {original_text_count}â†’{len(text_objs)}, "
                            f"ì´ë¯¸ì§€ {original_image_count}â†’{len(image_objs)}, "
                            f"í…Œì´ë¸” {original_table_count}â†’{len(table_objs)}"
                        )
                except Exception as filter_err:
                    logger.warning(f"[REFERENCES-FILTER] í•„í„°ë§ ì‹¤íŒ¨ (ëª¨ë“  ê°ì²´ í¬í•¨): {filter_err}")

            chunk_params: Dict[str, Any] = {
                "min_tokens": 80,
                "target_tokens": 280,
                "max_tokens": 420,
                "overlap_tokens": 40,
            }

            sections_info: List[Dict[str, Any]] = list(precomputed_sections_info) if precomputed_sections_info else []
            section_summary: Optional[Dict[str, Any]] = precomputed_section_summary
            # ì„¹ì…˜ ìˆœì„œ ë³´ì¡´: (type, index, title) íŠœí”Œê³¼ ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìŒ
            section_groups: List[Tuple[Optional[Tuple[str, int, str]], List[DocExtractedObject]]] = []
            object_spans: List[Tuple[DocExtractedObject, int, int]] = list(section_object_spans)

            if apply_section_chunking and text_objs:
                try:
                    separator = "\n\n"
                    if section_combined_text and section_object_spans:
                        combined_text = section_combined_text
                        object_spans_local = list(section_object_spans)
                    else:
                        combined_parts: List[str] = []
                        object_spans_local = []
                        current_pos = 0
                        for obj in text_objs:
                            content = (getattr(obj, 'content_text', '') or '').strip()
                            if not content:
                                continue
                            if combined_parts:
                                combined_parts.append(separator)
                                current_pos += len(separator)
                            start_pos = current_pos
                            combined_parts.append(content)
                            current_pos += len(content)
                            object_spans_local.append((obj, start_pos, current_pos))

                        combined_text = "".join(combined_parts)
                        section_combined_text = combined_text
                        section_object_spans = list(object_spans_local)

                    if combined_text.strip() and not sections_info:
                        sections_info = self.section_detector.detect_sections(
                            combined_text,
                            pages=metadata.get("pages") or None,
                            markdown_text=metadata.get("markdown") or None,  # ğŸ†• ë§ˆí¬ë‹¤ìš´ ì „ë‹¬
                            elements=metadata.get("elements") or None,  # ğŸ†• Upstage elements ì „ë‹¬
                        )
                        section_summary = self.section_detector.get_section_summary(sections_info)

                    if sections_info:
                        if not section_chunking_meta.get("detected_sections"):
                            section_chunking_meta["detected_sections"] = [s.get("type") for s in sections_info]
                        if section_summary and not section_chunking_meta.get("summary"):
                            section_chunking_meta["summary"] = section_summary
                        object_spans = list(object_spans_local)

                        # ì„¹ì…˜ ìˆœì„œ ë³´ì¡´: (type, index, original_title) íŠœí”Œë¡œ ë§¤í•‘
                        object_section_map: Dict[DocExtractedObject, Optional[Tuple[str, int, str]]] = {}
                        for section in sections_info:
                            s_type = section.get("type")
                            s_index = section.get("index")  # ìˆœì„œ ì¸ë±ìŠ¤
                            s_title = section.get("original_title")  # ì›ë³¸ ì œëª©
                            s_start = section.get("start_pos", 0)
                            s_end = section.get("end_pos", 0)
                            for obj, span_start, span_end in object_spans:
                                if span_end <= s_start or span_start >= s_end:
                                    continue
                                # (type, index, title) íŠœí”Œë¡œ ì €ì¥í•˜ì—¬ ìˆœì„œ ë³´ì¡´
                                object_section_map[obj] = (s_type, s_index, s_title)

                        current_group: List[DocExtractedObject] = []
                        current_label: Optional[Tuple[str, int, str]] = None
                        for obj in text_objs:
                            content = (getattr(obj, 'content_text', '') or '').strip()
                            if not content:
                                continue
                            label = object_section_map.get(obj)
                            if current_label is None:
                                current_label = label
                                current_group.append(obj)
                                continue
                            if label != current_label:
                                if current_group:
                                    section_groups.append((current_label, current_group))
                                current_group = [obj]
                                current_label = label
                            else:
                                current_group.append(obj)
                        if current_group:
                            section_groups.append((current_label, current_group))

                        chunk_params["section_chunking"] = {
                            "detected_sections": section_chunking_meta["detected_sections"],
                            "total_detected": len(section_chunking_meta["detected_sections"]),
                        }

                        if settings.storage_backend == 'azure_blob' and get_azure_blob_service and file_bss_info_sno:
                            try:
                                azure_factory2 = get_azure_blob_service if callable(get_azure_blob_service) else None
                                if not azure_factory2:
                                    raise RuntimeError("Azure Blob service factory not available")
                                azure_sections_service = azure_factory2()
                                sections_blob_path = f"multimodal/{file_bss_info_sno}/sections.json"
                                sections_payload = {
                                    "sections": sections_info,
                                    "summary": section_summary,
                                    "detected_at": datetime.now().isoformat(),
                                }
                                azure_sections_service.upload_bytes(
                                    json.dumps(sections_payload, ensure_ascii=False, indent=2).encode("utf-8"),
                                    sections_blob_path,
                                    purpose='intermediate'
                                )
                                section_chunking_meta["stored_to_blob"] = True
                                logger.info(f"[SECTION-DETECT] ì„¹ì…˜ ì •ë³´ ì €ì¥: {sections_blob_path}")
                            except Exception as section_blob_err:
                                logger.warning(f"[SECTION-DETECT] ì„¹ì…˜ ì •ë³´ ì €ì¥ ì‹¤íŒ¨ (ê¸°ë³¸ ì²­í‚¹ ê³„ì†): {section_blob_err}")
                    else:
                        logger.info("[SECTION-DETECT] ì„¹ì…˜ ê°ì§€ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ ì²­í‚¹ ì ìš©")
                except Exception as sec_err:
                    logger.warning(f"[SECTION-DETECT] ì„¹ì…˜ ê°ì§€ ì¤‘ ì˜ˆì™¸ ë°œìƒ (ê¸°ë³¸ ì²­í‚¹ìœ¼ë¡œ ì§„í–‰): {sec_err}")

            section_chunk_counts: Dict[str, int] = {}
            adv_chunks: List[Dict[str, Any]] = []

            # âœ… 2-A) êµ¬ì¡° ì¸ì‹ ì²­í‚¹ (ê¸°ë³¸)
            if structure_aware_enabled:
                try:
                    structural_elements = _normalize_structure_elements()
                    if structural_elements:
                        sa = StructureAwareChunker(
                            chunk_size=int(chunk_params.get('max_tokens', 420)),
                            chunk_overlap=int(chunk_params.get('overlap_tokens', 40)),
                            min_chunk_size=int(chunk_params.get('min_tokens', 80)),
                            emit_header_chunks=bool(processing_options.get('structure_aware_emit_header_chunks', False)),
                            # ì´ë¯¸ì§€ëŠ”/í‘œëŠ” ê¸°ì¡´ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ë³„ë„ ì²­í¬ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì¤‘ë³µ ë°©ì§€
                            include_visual_chunks=False,
                        )
                        adv_chunks = sa.chunk_elements(structural_elements)
                        if adv_chunks:
                            section_chunking_meta["enabled"] = True
                            section_chunking_meta["method"] = "structure_aware"
                            chunk_params["structure_aware"] = {
                                "enabled": True,
                                "source": "elements|paragraphs|heuristic",
                            }
                            logger.info(f"[CHUNKING] ğŸ§© êµ¬ì¡° ì¸ì‹ ì²­í‚¹ ì™„ë£Œ: {len(adv_chunks)}ê°œ ì²­í¬")
                    else:
                        logger.info("[CHUNKING] êµ¬ì¡° ìš”ì†Œë¥¼ ë§Œë“¤ì§€ ëª»í•´ ê¸°ì¡´ ì²­í‚¹ìœ¼ë¡œ í´ë°±")
                except Exception as sa_err:
                    logger.warning(f"[CHUNKING] êµ¬ì¡° ì¸ì‹ ì²­í‚¹ ì‹¤íŒ¨, ê¸°ì¡´ ì²­í‚¹ìœ¼ë¡œ í´ë°±: {sa_err}")
                    adv_chunks = []
            
            # ğŸ†• ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ ì‹œë„ (ë§ˆí¬ë‹¤ìš´ì´ ìˆê³  ì„¹ì…˜ì´ ê°ì§€ëœ ê²½ìš°)
            markdown_text = metadata.get("markdown") or None
            use_section_chunking = (
                apply_section_chunking 
                and sections_info 
                and markdown_text 
                and len(sections_info) > 0
            )
            
            if not adv_chunks and use_section_chunking:
                logger.info(f"[CHUNKING] ğŸ¯ ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ ì‚¬ìš© ({len(sections_info)}ê°œ ì„¹ì…˜)")
                section_chunking_meta["enabled"] = True
                section_chunking_meta["method"] = "section_aware"
                
                # ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ ìˆ˜í–‰
                try:
                    adv_chunks = chunk_by_sections(
                        sections=sections_info,
                        full_text=combined_text or section_combined_text,
                        min_tokens=chunk_params.get("min_tokens", 80),
                        target_tokens=chunk_params.get("target_tokens", 280),
                        max_tokens=chunk_params.get("max_tokens", 420),
                        overlap_tokens=chunk_params.get("overlap_tokens", 40),
                    )
                    
                    # ì„¹ì…˜ë³„ ì²­í¬ ìˆ˜ ì§‘ê³„
                    for chunk_dict in adv_chunks:
                        section_type = chunk_dict.get('section_type', 'unknown')
                        section_chunk_counts[section_type] = section_chunk_counts.get(section_type, 0) + 1
                        # í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ í•„ë“œëª… ì¶”ê°€
                        chunk_dict['section'] = section_type
                        chunk_dict['section_index'] = chunk_dict.get('chunk_index', 0)
                    
                    logger.info(f"[CHUNKING] âœ… ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ ì™„ë£Œ: {len(adv_chunks)}ê°œ ì²­í¬")
                    logger.info(f"[CHUNKING]    ì„¹ì…˜ë³„ ì²­í¬ ìˆ˜: {section_chunk_counts}")
                    
                except Exception as section_chunk_err:
                    logger.warning(f"[CHUNKING] âš ï¸ ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ ì‹¤íŒ¨, ê¸°ë³¸ ì²­í‚¹ìœ¼ë¡œ í´ë°±: {section_chunk_err}")
                    use_section_chunking = False
            
            # ğŸ”„ ê¸°ì¡´ ì„¹ì…˜ ê·¸ë£¹ ê¸°ë°˜ ì²­í‚¹ (í´ë°±)
            if not adv_chunks and (not use_section_chunking) and section_groups:
                logger.info(f"[CHUNKING] ì„¹ì…˜ ê·¸ë£¹ ê¸°ë°˜ ì²­í‚¹ ì‚¬ìš©")
                section_chunking_meta["enabled"] = True
                section_chunking_meta["method"] = "section_groups"
                
                # ì„¹ì…˜ ìˆœì„œ ë³´ì¡´: labelì€ (type, index, title) íŠœí”Œ
                for label, group in section_groups:
                    iterable = (
                        (
                            (getattr(o, 'content_text', '') or ''),
                            getattr(o, 'page_no', None),
                            getattr(o, 'object_id', None) or 0
                        ) for o in group
                    )
                    section_chunks = advanced_chunk_text(iterable)
                    
                    # ìˆœì„œ ë³´ì¡´ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    if label:
                        section_type, section_index, section_title = label
                        for chunk_dict in section_chunks:
                            chunk_dict['section'] = section_type  # ì„¹ì…˜ íƒ€ì… (other, methods ë“±)
                            chunk_dict['section_title'] = section_title  # ì›ë³¸ ì œëª© (Related Work ë“±)
                            chunk_dict['section_index'] = section_index  # ìˆœì„œ ì¸ë±ìŠ¤
                    else:
                        for chunk_dict in section_chunks:
                            chunk_dict['section'] = None
                            chunk_dict['section_title'] = None
                            chunk_dict['section_index'] = None
                    
                    adv_chunks.extend(section_chunks)
                    key = label[0] if label else "unassigned"  # section_type ì‚¬ìš©
                    section_chunk_counts[key] = section_chunk_counts.get(key, 0) + len(section_chunks)
            
            # ğŸ“ ê¸°ë³¸ í† í° ê¸°ë°˜ ì²­í‚¹ (ìµœì¢… í´ë°±)
            if not adv_chunks:
                logger.info(f"[CHUNKING] ê¸°ë³¸ í† í° ê¸°ë°˜ ì²­í‚¹ ì‚¬ìš©")
                adv_chunks = advanced_chunk_text(
                    (
                        (
                            (getattr(o, 'content_text', '') or ''),
                            getattr(o, 'page_no', None),
                            getattr(o, 'object_id', None) or 0
                        ) for o in text_objs
                    )
                )
                section_chunking_meta["enabled"] = False
                section_chunking_meta["method"] = "token_based"

            if section_chunk_counts:
                section_chunking_meta["chunk_counts"] = section_chunk_counts
            
            chunk_session = DocChunkSession(
                file_bss_info_sno=file_bss_info_sno,
                extraction_session_id=extraction_session.extraction_session_id,
                strategy_name=(
                    "structure_aware" if (structure_aware_enabled and section_chunking_meta.get("method") == "structure_aware")
                    else "advanced_paragraph_token"
                ),
                params_json=chunk_params,
                status="running",
                started_at=datetime.now()
            )
            session.add(chunk_session)
            await session.flush()
            result["chunk_session_id"] = chunk_session.chunk_session_id

            doc_chunks: List[DocChunk] = []
            for idx, cdict in enumerate(adv_chunks):
                # ì„¹ì…˜ ìˆœì„œ ì •ë³´ë¥¼ ë¡œê·¸ë¡œ ê¸°ë¡ (ë””ë²„ê¹…ìš©)
                if cdict.get('section_index') is not None:
                    logger.debug(
                        f"[SECTION-ORDER] ì²­í¬ {idx}: section_index={cdict['section_index']}, "
                        f"type={cdict.get('section')}, title={cdict.get('section_title')}"
                    )
                
                # page_range ìƒì„±: page_numbersì—ì„œ ìµœì†Œ/ìµœëŒ€ í˜ì´ì§€ ì¶”ì¶œ
                page_range_value = None
                page_numbers = cdict.get('page_numbers', [])
                if page_numbers:
                    min_page = min(page_numbers)
                    max_page = max(page_numbers)
                    # PostgreSQL int4range: [lower, upper) í˜•ì‹
                    # SQLAlchemy type_coerceë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ìì—´ì„ int4rangeë¡œ ë³€í™˜
                    page_range_str = f"[{min_page},{max_page + 1})"
                    page_range_value = type_coerce(text(f"'{page_range_str}'"), INT4RANGE)
                
                doc_chunk = DocChunk(
                    chunk_session_id=chunk_session.chunk_session_id,
                    file_bss_info_sno=file_bss_info_sno,
                    chunk_index=idx,
                    source_object_ids=cdict.get('source_object_ids', []),
                    content_text=cdict['content_text'],
                    token_count=cdict['token_count'],
                    modality="text",
                    # êµ¬ì¡° ì¸ì‹ ì²­í‚¹ì´ë©´ section_pathë¥¼ ìš°ì„  ì €ì¥, ì—†ìœ¼ë©´ ê¸°ì¡´ section
                    section_heading=cdict.get('section_path') or cdict.get('section'),
                    page_range=page_range_value  # í˜ì´ì§€ ë²”ìœ„ ì¶”ê°€
                )
                session.add(doc_chunk)
                doc_chunks.append(doc_chunk)
            
            # ì´ë¯¸ì§€ ì²­í¬ ìƒì„± (ê° ì´ë¯¸ì§€ë¥¼ ë…ë¦½ì ì¸ ì²­í¬ë¡œ)
            image_chunk_start_idx = len(doc_chunks)
            for img_idx, img_obj in enumerate(image_objs):
                object_id = getattr(img_obj, 'object_id', None)
                page_no = getattr(img_obj, 'page_no', None)
                
                # ì´ë¯¸ì§€ ìº¡ì…˜/ì„¤ëª… ì¶”ì¶œ (ìˆìœ¼ë©´)
                img_text = getattr(img_obj, 'content_text', '') or f"Image on page {page_no}"
                
                # page_range ìƒì„±: PostgreSQL int4range íƒ€ì… '[start, end)' í˜•ì‹
                page_range_value = None
                if page_no is not None:
                    # SQLAlchemy type_coerceë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ìì—´ì„ int4rangeë¡œ ë³€í™˜
                    page_range_str = f"[{page_no},{page_no + 1})"
                    page_range_value = type_coerce(text(f"'{page_range_str}'"), INT4RANGE)
                    logger.info(f"[IMAGE_CHUNK_CREATE] object_id={object_id}, page_no={page_no}, page_range={page_range_str}")
                
                # blob_key ìƒì„±: Blob Storage íŒŒì¼ ê²½ë¡œ
                blob_key_value = None
                if object_id and page_no is not None:
                    blob_key_value = f"multimodal/{file_bss_info_sno}/objects/image_{object_id}_{page_no}.png"
                    logger.info(f"[IMAGE_CHUNK_CREATE] blob_key={blob_key_value}")
                
                img_chunk = DocChunk(
                    chunk_session_id=chunk_session.chunk_session_id,
                    file_bss_info_sno=file_bss_info_sno,
                    chunk_index=image_chunk_start_idx + img_idx,
                    source_object_ids=[object_id] if object_id else [],
                    content_text=img_text,
                    token_count=0,  # ì´ë¯¸ì§€ëŠ” í† í° ì¹´ìš´íŠ¸ 0
                    modality="image",
                    page_range=page_range_value,  # í˜ì´ì§€ ë²”ìœ„
                    blob_key=blob_key_value  # Blob Storage íŒŒì¼ ê²½ë¡œ
                )
                session.add(img_chunk)
                doc_chunks.append(img_chunk)
            
            # í‘œ ì²­í¬ ìƒì„± (ê° í‘œë¥¼ ë…ë¦½ì ì¸ ì²­í¬ë¡œ)
            table_chunk_start_idx = len(doc_chunks)
            for tbl_idx, tbl_obj in enumerate(table_objs):
                object_id = getattr(tbl_obj, 'object_id', None)
                page_no = getattr(tbl_obj, 'page_no', None)
                
                # í‘œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë§ˆí¬ë‹¤ìš´ ë˜ëŠ” CSV í˜•ì‹)
                table_text = _serialize_table_to_text(tbl_obj)
                
                # í† í° ìˆ˜ ì¶”ì • (ê³µë°± ê¸°ì¤€ ë‹¨ìˆœ ê³„ì‚°)
                token_count = len(table_text.split()) if table_text else 0
                
                # page_range ìƒì„±: PostgreSQL int4range íƒ€ì…
                page_range_value = None
                if page_no is not None:
                    # SQLAlchemy type_coerceë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ìì—´ì„ int4rangeë¡œ ë³€í™˜
                    page_range_str = f"[{page_no},{page_no + 1})"
                    page_range_value = type_coerce(text(f"'{page_range_str}'"), INT4RANGE)
                
                # blob_key ìƒì„±: Blob Storage íŒŒì¼ ê²½ë¡œ (í…Œì´ë¸”ì€ JSON)
                blob_key_value = None
                if object_id and page_no is not None:
                    blob_key_value = f"multimodal/{file_bss_info_sno}/objects/table_{object_id}_{page_no}.json"
                
                tbl_chunk = DocChunk(
                    chunk_session_id=chunk_session.chunk_session_id,
                    file_bss_info_sno=file_bss_info_sno,
                    chunk_index=table_chunk_start_idx + tbl_idx,
                    source_object_ids=[object_id] if object_id else [],
                    content_text=table_text,
                    token_count=token_count,
                    modality="table",
                    page_range=page_range_value,  # í˜ì´ì§€ ë²”ìœ„
                    blob_key=blob_key_value  # Blob Storage íŒŒì¼ ê²½ë¡œ
                )
                session.add(tbl_chunk)
                doc_chunks.append(tbl_chunk)
            
            await session.flush()
            setattr(chunk_session, "status", "success")
            setattr(chunk_session, "completed_at", datetime.now())
            setattr(chunk_session, "chunk_count", len(doc_chunks))
            result["chunks_count"] = len(doc_chunks)
            logger.info(f"[MULTIMODAL] ì²­í‚¹ ì™„ë£Œ - í…ìŠ¤íŠ¸: {len(adv_chunks)}ê°œ, ì´ë¯¸ì§€: {len(image_objs)}ê°œ, í‘œ: {len(table_objs)}ê°œ, ì „ì²´: {len(doc_chunks)}ê°œ")
            if section_chunking_meta.get("enabled"):
                logger.info(f"[SECTION-DETECT] ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ ì ìš© - chunk_counts={section_chunking_meta.get('chunk_counts')}")

            chunk_stage_payload = {
                "chunks": len(doc_chunks),
                "text_chunks": len(adv_chunks),
                "image_chunks": len(image_objs),
                "table_chunks": len(table_objs),
            }
            if section_chunking_meta.get("requested"):
                chunk_stage_payload["section_chunking"] = section_chunking_meta
            _stage("chunking", True, **chunk_stage_payload)

            # -----------------------------
            # 2.4. vs_doc_contents_chunks í…Œì´ë¸”ì— ì²­í¬ ì €ì¥ (RAG ê¸°ëŠ¥ ì§€ì›)
            # -----------------------------
            try:
                logger.info(f"[MULTIMODAL][RAG] vs_doc_contents_chunks ì €ì¥ ì‹œì‘ - {len(doc_chunks)}ê°œ ì²­í¬")
                
                for chunk in doc_chunks:
                    # ì²­í¬ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„°
                    chunk_text = getattr(chunk, 'content_text', '') or ''
                    chunk_size = len(chunk_text)
                    chunk_idx = getattr(chunk, 'chunk_index', 0)
                    chunk_modality = getattr(chunk, 'modality', 'text')  # ì‹¤ì œ modality ì‚¬ìš©
                    
                    # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì • (source_object_idsì—ì„œ ì¶”ì¶œ)
                    page_number = None
                    source_object_ids = getattr(chunk, 'source_object_ids', [])
                    if source_object_ids and extracted_objects:
                        # ì²« ë²ˆì§¸ ì†ŒìŠ¤ ê°ì²´ì˜ í˜ì´ì§€ ë²ˆí˜¸ ì‚¬ìš©
                        for obj in extracted_objects:
                            if getattr(obj, 'object_id', None) == source_object_ids[0]:
                                page_number = getattr(obj, 'page_number', None)
                                break
                    
                    # VsDocContentsChunks ë ˆì½”ë“œ ìƒì„±
                    vs_chunk = VsDocContentsChunks(
                        file_bss_info_sno=file_bss_info_sno,
                        chunk_index=chunk_idx,
                        chunk_text=chunk_text,
                        chunk_size=chunk_size,
                        chunk_embedding=None,  # ì„ë² ë”©ì€ ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                        page_number=page_number,
                        section_title=None,  # TODO: ì„¹ì…˜ ì œëª© ì¶”ì¶œ ë¡œì§ ì¶”ê°€
                        keywords=None,  # TODO: ì²­í¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ì¶”ê°€
                        named_entities=None,  # TODO: ì²­í¬ë³„ ê°œì²´ëª… ì¶”ì¶œ ì¶”ê°€
                        knowledge_container_id=container_id,
                        metadata_json=json.dumps({
                            'chunk_id': getattr(chunk, 'chunk_id', None),
                            'token_count': getattr(chunk, 'token_count', 0),
                            'modality': chunk_modality,  # ì‹¤ì œ modality ë°˜ì˜ (text/image/table)
                            'source_object_ids': source_object_ids,
                            'chunk_session_id': chunk_session.chunk_session_id
                        }, ensure_ascii=False),
                        created_by=user_emp_no,
                        del_yn='N'
                    )
                    session.add(vs_chunk)
                
                await session.flush()
                logger.info(f"[MULTIMODAL][RAG] âœ… vs_doc_contents_chunks ì €ì¥ ì™„ë£Œ - {len(doc_chunks)}ê°œ")
                
            except Exception as vs_err:
                logger.error(f"[MULTIMODAL][RAG] âŒ vs_doc_contents_chunks ì €ì¥ ì‹¤íŒ¨: {vs_err}")
                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ê²€ìƒ‰ ì¸ë±ìŠ¤ëŠ” ë³„ë„)

            # -----------------------------
            # 2.5. Blob Storage - ì²­í‚¹ ê²°ê³¼ ì €ì¥ (Azure Blob / S3)
            # -----------------------------
            performed_blob_derived = False
            try:
                if settings.storage_backend in ['azure_blob', 's3'] and file_bss_info_sno:
                    _start_stage("blob_derived_save")
                    performed_blob_derived = True
                    
                    if settings.storage_backend == 'azure_blob':
                        azure_factory3 = get_azure_blob_service if callable(get_azure_blob_service) else None
                        if not azure_factory3:
                            raise RuntimeError("Azure Blob service factory not available")
                        storage = azure_factory3()
                    else:  # s3
                        storage = self._get_s3_service()
                        if not storage:
                            raise RuntimeError("S3 service not available")
                    
                    # ì²­í‚¹ ë©”íƒ€ë°ì´í„° ì €ì¥
                    chunk_metadata_key = f"multimodal/{file_bss_info_sno}/chunking_metadata.json"
                    chunk_metadata = {
                        "chunk_session_id": chunk_session.chunk_session_id,
                        "strategy_name": "advanced_paragraph_token",
                        "chunk_count": len(doc_chunks),
                        "params": {
                            "min_tokens": 80,
                            "target_tokens": 280,
                            "max_tokens": 420,
                            "overlap_tokens": 40
                        },
                        "timestamp": datetime.now().isoformat()
                    }
                    storage.upload_bytes(
                        json.dumps(chunk_metadata, ensure_ascii=False).encode('utf-8'),
                        chunk_metadata_key,
                        purpose='derived'
                    )
                    
                    # ê°œë³„ ì²­í¬ ì €ì¥
                    chunk_manifest = []
                    for idx, chunk in enumerate(doc_chunks):
                        chunk_modality = getattr(chunk, 'modality', 'text')
                        chunk_key = f"multimodal/{file_bss_info_sno}/chunks/chunk_{idx:04d}_{chunk_modality}.json"
                        chunk_content = {
                            "chunk_id": getattr(chunk, 'chunk_id', None),
                            "chunk_index": idx,
                            "content_text": getattr(chunk, 'content_text', ''),
                            "token_count": getattr(chunk, 'token_count', 0),
                            "modality": chunk_modality,
                            "source_object_ids": getattr(chunk, 'source_object_ids', [])
                        }
                        storage.upload_bytes(
                            json.dumps(chunk_content, ensure_ascii=False).encode('utf-8'),
                            chunk_key,
                            purpose='derived'
                        )
                        chunk_manifest.append({
                            "chunk_index": idx,
                            "key": chunk_key,
                            "modality": chunk_modality,
                            "char_count": len(chunk_content["content_text"]),
                            "token_count": chunk_content["token_count"]
                        })
                    
                    # ì²­í¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥
                    manifest_key = f"multimodal/{file_bss_info_sno}/chunks_manifest.json"
                    storage.upload_bytes(
                        json.dumps(chunk_manifest, ensure_ascii=False).encode('utf-8'),
                        manifest_key,
                        purpose='derived'
                    )
                    
                    logger.info(f"[MULTIMODAL-BLOB] {len(doc_chunks)}ê°œ ì²­í¬ ë° ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ")
                    _stage("blob_derived_save", True, chunks_saved=len(doc_chunks))
                    
            except Exception as blob_err:
                logger.warning(f"[MULTIMODAL-BLOB] ì²­í‚¹ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {blob_err}")
                if performed_blob_derived:
                    _stage("blob_derived_save", False, error=str(blob_err))
            finally:
                if not performed_blob_derived:
                    _stage("blob_derived_save", False, skipped=True)

            # -----------------------------
            # 3. Embeddings (í…ìŠ¤íŠ¸ + CLIP ë©€í‹°ëª¨ë‹¬)
            # -----------------------------
            _start_stage("embedding")
            current_embedding_model = settings.get_current_embedding_model()
            max_dim = settings.vector_dimension
            clip_dim = 512  # CLIP ì„ë² ë”© ì°¨ì›
            embed_success = 0
            clip_embed_success = 0
            chunk_embeddings = {}  # chunk_index -> vector ë§¤í•‘
            
            # ğŸš€ ë°°ì¹˜ ì„ë² ë”© ìµœì í™”: í…ìŠ¤íŠ¸ ì²­í¬ + ì´ë¯¸ì§€ ìº¡ì…˜ì„ í•œ ë²ˆì— ì²˜ë¦¬
            text_chunks_list = []
            text_chunk_indices = []
            for idx, ch in enumerate(doc_chunks):
                modality = getattr(ch, 'modality', 'text')
                content = (getattr(ch, 'content_text', '') or '').strip()
                
                # í…ìŠ¤íŠ¸ ì²­í¬ ë˜ëŠ” ì˜ë¯¸ ìˆëŠ” ìº¡ì…˜ì´ ìˆëŠ” ì´ë¯¸ì§€ ì²­í¬ í¬í•¨
                if modality == 'text' and content:
                    text_chunks_list.append(content)
                    text_chunk_indices.append(idx)
                elif modality == 'image' and content:
                    # ì´ë¯¸ì§€ ìº¡ì…˜ë„ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ì¼ë°˜ ê²€ìƒ‰ì—ì„œë„ ì°¾ì„ ìˆ˜ ìˆë„ë¡)
                    text_chunks_list.append(content)
                    text_chunk_indices.append(idx)
                    logger.debug(f"[MULTIMODAL][IMAGE-CAPTION] ì´ë¯¸ì§€ ìº¡ì…˜ ë°°ì¹˜ ì¶”ê°€: idx={idx}, caption='{content[:60]}'")
            
            # ë°°ì¹˜ ì„ë² ë”© ìƒì„± (í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬)
            text_embeddings_batch = []
            if text_chunks_list:
                logger.info(f"[MULTIMODAL][BATCH-EMB] í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ì‹œì‘: {len(text_chunks_list)}ê°œ")
                try:
                    from app.services.core.embedding_service import EmbeddingService
                    emb_service = EmbeddingService()
                    text_embeddings_batch = await emb_service.get_embeddings_batch(text_chunks_list, batch_size=100)
                    logger.info(f"[MULTIMODAL][BATCH-EMB] âœ… í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ: {len(text_embeddings_batch)}ê°œ")
                except Exception as batch_err:
                    logger.error(f"[MULTIMODAL][BATCH-EMB] âŒ ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨ (í´ë°± ì²˜ë¦¬): {batch_err}")
                    # ì‹¤íŒ¨ ì‹œ ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±
                    text_embeddings_batch = []
            
            # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì²­í¬ ì¸ë±ìŠ¤ì— ë§¤í•‘
            text_embedding_map = {}
            for idx, vec in zip(text_chunk_indices, text_embeddings_batch):
                text_embedding_map[idx] = vec
            
            # ì²­í¬ë³„ë¡œ ì„ë² ë”© ì €ì¥ (ë°°ì¹˜ ê²°ê³¼ + CLIP ì´ë¯¸ì§€ ì„ë² ë”©)
            for idx, ch in enumerate(doc_chunks):
                try:
                    modality = getattr(ch, 'modality', 'text')
                    vec = None
                    clip_vec = None
                    
                    # í…ìŠ¤íŠ¸ ì„ë² ë”©: ë°°ì¹˜ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜¤ê¸° (í…ìŠ¤íŠ¸ ì²­í¬ + ì´ë¯¸ì§€ ìº¡ì…˜)
                    if idx in text_embedding_map:
                        vec = text_embedding_map[idx]
                        if modality == 'image':
                            logger.info(f"[MULTIMODAL][IMAGE-CAPTION] âœ… ì´ë¯¸ì§€ ìº¡ì…˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ì ìš©: chunk={ch.chunk_id}")
                        else:
                            logger.debug(f"[MULTIMODAL][BATCH-EMB] í…ìŠ¤íŠ¸ ì„ë² ë”© ë§¤í•‘: chunk={ch.chunk_id}, idx={idx}")
                    elif modality == 'text':
                        # TEXT ì²­í¬ê°€ ë°°ì¹˜ì—ì„œ ëˆ„ë½ëœ ê²½ìš°ì—ë§Œ ê°œë³„ ìƒì„± (í´ë°±)
                        content = getattr(ch, 'content_text', '') or ''
                        if content.strip():
                            vec = await korean_nlp_service.generate_korean_embedding(content)
                            logger.warning(f"[MULTIMODAL][BATCH-EMB] ë°°ì¹˜ ëˆ„ë½ - ê°œë³„ ì„ë² ë”© ìƒì„±: chunk={ch.chunk_id}")
                    
                    # ì´ë¯¸ì§€ ì²­í¬ì¸ ê²½ìš° CLIP ì„ë² ë”© ìƒì„± (ê°œë³„ ì²˜ë¦¬ ìœ ì§€)
                    if modality == 'image' and self.image_embedding_service:
                        try:
                            # ì´ë¯¸ì§€ ê°ì²´ ì¡°íšŒ
                            source_object_ids = getattr(ch, 'source_object_ids', [])
                            if source_object_ids:
                                # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê°ì²´ì—ì„œ CLIP ì„ë² ë”© ìƒì„±
                                img_obj_result = await session.execute(
                                    select(DocExtractedObject)
                                    .where(DocExtractedObject.object_id == source_object_ids[0])
                                )
                                img_obj = img_obj_result.scalar_one_or_none()
                                
                                if img_obj:
                                    page_no_val = getattr(img_obj, 'page_no', 0) or 0
                                    img_blob_key = f"multimodal/{file_bss_info_sno}/objects/image_{img_obj.object_id}_{page_no_val}.png"
                                    caption_text = getattr(ch, 'content_text', '') or ''
                                    img_bytes: Optional[bytes] = None

                                    try:
                                        if settings.storage_backend == 'azure_blob':
                                            azure_factory4 = get_azure_blob_service if callable(get_azure_blob_service) else None
                                            if not azure_factory4:
                                                raise RuntimeError("Azure Blob service factory not available")
                                            azure = azure_factory4()
                                            img_bytes = azure.download_blob_to_bytes(img_blob_key, purpose='intermediate')
                                        elif settings.storage_backend == 's3':
                                            s3_service = self._get_s3_service()
                                            if s3_service:
                                                img_bytes = s3_service.download_bytes(img_blob_key, purpose='intermediate')
                                            else:
                                                logger.warning("[MULTIMODAL][IMAGE-EMB] S3Service unavailable for image download")
                                        else:
                                            logger.debug(f"[MULTIMODAL][IMAGE-EMB] Storage backend '{settings.storage_backend}' does not support blob downloads")
                                    except Exception as storage_err:
                                        logger.warning(f"[MULTIMODAL][IMAGE-EMB] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ chunk={ch.chunk_id}: {storage_err}")

                                    if img_bytes:
                                        # ì¤‘ìš”: ê²€ìƒ‰ ê²½ë¡œì™€ ë™ì¼í•œ "ì´ë¯¸ì§€ ì „ìš©" ì„ë² ë”©ì„ ì‚¬ìš©í•´ ì¼ê´€ëœ ìœ ì‚¬ë„ ë³´ì¥
                                        # captionì„ í•¨ê»˜ ì£¼ëŠ” text_image ëª¨ë“œëŠ” ë™ì¼ ì´ë¯¸ì§€ ì¬ê²€ìƒ‰ ì‹œ ë²¡í„° ë¶ˆì¼ì¹˜ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìŒ
                                        clip_vec = await self.image_embedding_service.generate_image_embedding(
                                            image_bytes=img_bytes,
                                            caption=None
                                        )
                                        if clip_vec:
                                            clip_embed_success += 1
                                            # Provider ë™ì  í‘œì‹œ (bedrock=Marengo, azure_openai=CLIP, local=CLIP)
                                            provider_name = getattr(self.image_embedding_service, 'provider', 'unknown')
                                            if provider_name == 'bedrock':
                                                provider_label = "Marengo"
                                            elif provider_name == 'azure_openai':
                                                provider_label = "Azure-CLIP"
                                            else:
                                                provider_label = "Local-CLIP"
                                            logger.info(f"[MULTIMODAL][{provider_label}] âœ… ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±: chunk={ch.chunk_id}, dim={len(clip_vec)}")
                        except Exception as clip_err:
                            logger.warning(f"[MULTIMODAL][IMAGE-EMB] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ chunk={ch.chunk_id}: {clip_err}")
                    
                    # í…ìŠ¤íŠ¸ ì²­í¬ì˜ ê²½ìš°ì—ë„ ë©€í‹°ëª¨ë‹¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ê°€ëŠ¥ (ì„ íƒì )
                    elif modality == 'text' and self.image_embedding_service and getattr(settings, 'enable_text_clip_embedding', False):
                        try:
                            content_text = getattr(ch, 'content_text', '') or ''
                            if content_text.strip():
                                clip_vec = await self.image_embedding_service.generate_text_embedding(content_text)
                                if clip_vec:
                                    clip_embed_success += 1
                                    provider_name = getattr(self.image_embedding_service, 'provider', 'unknown')
                                    provider_label = "Marengo" if provider_name == 'bedrock' else "CLIP"
                                    logger.info(f"[MULTIMODAL][{provider_label}] âœ… í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±: chunk={ch.chunk_id}, dim={len(clip_vec)}")
                        except Exception as clip_err:
                            logger.warning(f"[MULTIMODAL][TEXT-EMB] í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ chunk={ch.chunk_id}: {clip_err}")
                    
                    # ì„ë² ë”© ë²¡í„° ì €ì¥ (í…ìŠ¤íŠ¸ ì„ë² ë”© ë˜ëŠ” CLIP ì„ë² ë”© ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì €ì¥)
                    if vec or clip_vec:
                        # í…ìŠ¤íŠ¸ ë²¡í„° íŒ¨ë”© (ìˆëŠ” ê²½ìš°ë§Œ)
                        if vec:
                            if len(vec) < max_dim:
                                vec = vec + [0.0] * (max_dim - len(vec))
                            elif len(vec) > max_dim:
                                vec = vec[:max_dim]
                        
                        # CLIP ë²¡í„° íŒ¨ë”© (ìˆëŠ” ê²½ìš°ë§Œ)
                        if clip_vec:
                            if len(clip_vec) < clip_dim:
                                clip_vec = clip_vec + [0.0] * (clip_dim - len(clip_vec))
                            elif len(clip_vec) > clip_dim:
                                clip_vec = clip_vec[:clip_dim]
                        
                        # ë²¤ë”ë³„ ë²¡í„° ì»¬ëŸ¼ í• ë‹¹
                        provider = None
                        azure_vec_1536 = None
                        azure_vec_3072 = None
                        azure_clip_vec = None
                        aws_vec_1024 = None
                        aws_vec_256 = None
                        aws_marengo_vec_512 = None
                        
                        if vec:
                            if max_dim == 1536:
                                provider = 'azure'
                                azure_vec_1536 = vec
                            elif max_dim == 3072:
                                provider = 'azure'
                                azure_vec_3072 = vec
                            elif max_dim == 1024:
                                provider = 'aws'
                                aws_vec_1024 = vec
                            elif max_dim == 256:
                                provider = 'aws'
                                aws_vec_256 = vec
                        
                        # ğŸ”· ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ë²¡í„° í• ë‹¹ (í”„ë¡œë°”ì´ë”ë³„ ëª…í™•í•œ êµ¬ë¶„)
                        multimodal_model_name = None
                        multimodal_dimension = None
                        multimodal_provider = None
                        
                        if clip_vec:
                            # DEFAULT_EMBEDDING_PROVIDER ì„¤ì • ì½ê¸°
                            embedding_provider = settings.get_current_embedding_provider()
                            
                            if embedding_provider == 'bedrock':
                                # âœ… AWS Bedrock: TwelveLabs Marengo (512d) â†’ aws_marengo_vector_512
                                aws_marengo_vec_512 = clip_vec
                                multimodal_model_name = settings.bedrock_multimodal_embedding_model_id
                                multimodal_dimension = 512
                                multimodal_provider = 'aws'
                                logger.info(f"[MULTIMODAL] Bedrock Marengo ë²¡í„° í• ë‹¹ â†’ aws_marengo_vector_512 (512d)")
                            elif embedding_provider == 'azure_openai':
                                # âœ… Azure OpenAI: CLIP (512d) â†’ azure_clip_vector
                                azure_clip_vec = clip_vec
                                multimodal_model_name = settings.azure_openai_multimodal_embedding_deployment or 'openai-clip-vit-base-patch32'
                                multimodal_dimension = 512
                                multimodal_provider = 'azure'
                                logger.info(f"[MULTIMODAL] Azure CLIP ë²¡í„° í• ë‹¹ â†’ azure_clip_vector (512d)")
                            else:
                                # âš ï¸ ê¸°íƒ€ í”„ë¡œë°”ì´ë” (ë¡œì»¬ CLIP ë“±) â†’ ë ˆê±°ì‹œ clip_vector
                                multimodal_model_name = 'openai-clip-vit-base-patch32'
                                multimodal_dimension = 512
                                multimodal_provider = 'local'
                                logger.warning(f"[MULTIMODAL] ì•Œ ìˆ˜ ì—†ëŠ” provider={embedding_provider}, ë ˆê±°ì‹œ clip_vector ì‚¬ìš©")
                        
                        # ğŸ“ ë©”íƒ€ë°ì´í„° ê²°ì •: ì´ë¯¸ì§€ ì²­í¬ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì •ë³´ ìš°ì„  ì‚¬ìš©
                        if modality == 'image' and clip_vec and multimodal_model_name:
                            # âœ… ì´ë¯¸ì§€: ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
                            final_model_name = multimodal_model_name
                            final_dimension = multimodal_dimension
                            final_provider = multimodal_provider or provider
                        else:
                            # í…ìŠ¤íŠ¸: ê¸°ì¡´ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
                            final_model_name = current_embedding_model
                            final_dimension = max_dim
                            final_provider = provider
                        
                        emb = DocEmbedding(
                            chunk_id=ch.chunk_id,
                            file_bss_info_sno=file_bss_info_sno,
                            provider=final_provider,
                            model_name=final_model_name,
                            modality=modality,
                            dimension=final_dimension,
                            azure_vector_1536=azure_vec_1536,
                            azure_vector_3072=azure_vec_3072,
                            azure_clip_vector=azure_clip_vec,
                            aws_vector_1024=aws_vec_1024,
                            aws_vector_256=aws_vec_256,
                            aws_marengo_vector_512=aws_marengo_vec_512,
                            vector=vec,  # ë ˆê±°ì‹œ í˜¸í™˜
                            clip_vector=clip_vec  # ë ˆê±°ì‹œ í˜¸í™˜
                        )
                        session.add(emb)
                        embed_success += 1
                        
                        # ì²­í¬ ì¸ë±ìŠ¤ ë§¤í•‘ ì €ì¥ (vs_doc_contents_chunks ì—…ë°ì´íŠ¸ìš©, í…ìŠ¤íŠ¸ ë²¡í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
                        if vec:
                            chunk_idx = getattr(ch, 'chunk_index', None)
                            if chunk_idx is not None:
                                chunk_embeddings[chunk_idx] = vec
                            
                except Exception as ee:
                    logger.warning(f"[MULTIMODAL] Embedding ì‹¤íŒ¨ chunk={ch.chunk_id}: {ee}")
            await session.flush()
            result["embeddings_count"] = embed_success
            result["clip_embeddings_count"] = clip_embed_success
            _stage("embedding", True, embeddings=embed_success, clip_embeddings=clip_embed_success)

            # -----------------------------
            # 3.1. vs_doc_contents_chunks ì„ë² ë”© ì—…ë°ì´íŠ¸ (RAG ê¸°ëŠ¥ ì§€ì›)
            # í…ìŠ¤íŠ¸ ì„ë² ë”©(Titan 1024d) â†’ aws_embedding_1024
            # ì´ë¯¸ì§€ ì„ë² ë”©(Marengo 512d) â†’ multimodal_embedding
            # -----------------------------
            try:
                logger.info(f"[MULTIMODAL][RAG] vs_doc_contents_chunks ì„ë² ë”© ì—…ë°ì´íŠ¸ ì‹œì‘ - {len(chunk_embeddings)}ê°œ")
                
                # vs_doc_contents_chunks ë ˆì½”ë“œ ì¡°íšŒ ë° ì—…ë°ì´íŠ¸
                from sqlalchemy import update
                
                for chunk_idx, vec in chunk_embeddings.items():
                    # ì„ë² ë”© ì°¨ì›ìœ¼ë¡œ íƒ€ì… íŒë³„
                    embedding_dim = len(vec)
                    
                    if embedding_dim == 1024:
                        # í…ìŠ¤íŠ¸ ì„ë² ë”© (AWS Titan)
                        stmt = (
                            update(VsDocContentsChunks)
                            .where(VsDocContentsChunks.file_bss_info_sno == file_bss_info_sno)
                            .where(VsDocContentsChunks.chunk_index == chunk_idx)
                            .values(
                                aws_embedding_1024=vec,
                                embedding_provider='aws'
                            )
                        )
                        logger.debug(f"[MULTIMODAL][RAG] í…ìŠ¤íŠ¸ ì„ë² ë”© ì €ì¥ (Titan 1024d): chunk_idx={chunk_idx}")
                    elif embedding_dim == 512:
                        # ì´ë¯¸ì§€ ì„ë² ë”© (Marengo)
                        stmt = (
                            update(VsDocContentsChunks)
                            .where(VsDocContentsChunks.file_bss_info_sno == file_bss_info_sno)
                            .where(VsDocContentsChunks.chunk_index == chunk_idx)
                            .values(
                                multimodal_embedding=vec,
                                embedding_provider='aws'
                            )
                        )
                        logger.debug(f"[MULTIMODAL][RAG] ì´ë¯¸ì§€ ì„ë² ë”© ì €ì¥ (Marengo 512d): chunk_idx={chunk_idx}")
                    else:
                        # ë ˆê±°ì‹œ í´ë°±
                        logger.warning(f"[MULTIMODAL][RAG] ì•Œ ìˆ˜ ì—†ëŠ” ì„ë² ë”© ì°¨ì›: {embedding_dim}d, chunk_idx={chunk_idx}")
                        stmt = (
                            update(VsDocContentsChunks)
                            .where(VsDocContentsChunks.file_bss_info_sno == file_bss_info_sno)
                            .where(VsDocContentsChunks.chunk_index == chunk_idx)
                            .values(chunk_embedding=vec)
                        )
                    
                    await session.execute(stmt)
                
                await session.flush()
                logger.info(f"[MULTIMODAL][RAG] âœ… vs_doc_contents_chunks ì„ë² ë”© ì—…ë°ì´íŠ¸ ì™„ë£Œ - {len(chunk_embeddings)}ê°œ")
                
            except Exception as emb_err:
                logger.error(f"[MULTIMODAL][RAG] âŒ vs_doc_contents_chunks ì„ë² ë”© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {emb_err}")
                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

            # -----------------------------
            # 3.5. Blob Storage - ì„ë² ë”© ê²°ê³¼ ì €ì¥ (Azure Blob / S3)
            # -----------------------------
            performed_blob_embedding = False
            try:
                if settings.storage_backend in ['azure_blob', 's3'] and file_bss_info_sno:
                    _start_stage("blob_embedding_save")
                    performed_blob_embedding = True
                    
                    if settings.storage_backend == 'azure_blob':
                        azure_factory5 = get_azure_blob_service if callable(get_azure_blob_service) else None
                        if not azure_factory5:
                            raise RuntimeError("Azure Blob service factory not available")
                        storage = azure_factory5()
                    else:  # s3
                        storage = self._get_s3_service()
                        if not storage:
                            raise RuntimeError("S3 service not available")
                    
                    # ì„ë² ë”© ë©”íƒ€ë°ì´í„° ì €ì¥
                    embedding_metadata_key = f"multimodal/{file_bss_info_sno}/embedding_metadata.json"
                    embedding_metadata = {
                        "model_name": current_embedding_model,
                        "vector_dimension": max_dim,
                        "embeddings_generated": embed_success,
                        "total_chunks": len(doc_chunks),
                        "timestamp": datetime.now().isoformat()
                    }
                    storage.upload_bytes(
                        json.dumps(embedding_metadata, ensure_ascii=False).encode('utf-8'),
                        embedding_metadata_key,
                        purpose='derived'
                    )
                    
                    logger.info(f"[MULTIMODAL-BLOB] ì„ë² ë”© ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ - {embed_success}/{len(doc_chunks)} ì„ë² ë”©")
                    _stage("blob_embedding_save", True, embeddings_saved=embed_success)
                    
            except Exception as blob_err:
                logger.warning(f"[MULTIMODAL-BLOB] ì„ë² ë”© ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {blob_err}")
                if performed_blob_embedding:
                    _stage("blob_embedding_save", False, error=str(blob_err))
            finally:
                if not performed_blob_embedding:
                    _stage("blob_embedding_save", False, skipped=True)

            # -----------------------------
            # 4. Search Index Creation (í†µí•© ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±)
            # -----------------------------
            _start_stage("search_index_creation")
            try:
                # 4.1. íŒŒì¼ ì •ë³´ ì¡°íšŒ
                stmt_file = select(TbFileBssInfo).where(TbFileBssInfo.file_bss_info_sno == file_bss_info_sno)
                file_result = await session.execute(stmt_file)
                file_info = file_result.scalar_one_or_none()
                
                visual_object_types = ("IMAGE", "FIGURE")

                if not file_info:
                    logger.warning(f"[MULTIMODAL] íŒŒì¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_bss_info_sno}")
                    _stage("search_index_creation", False, error="File info not found")
                else:
                    # 4.2. ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ëª¨ë“  ì²­í¬ í†µí•©)
                    full_text_parts = []
                    image_count = 0
                    table_count = 0
                    
                    for chunk in doc_chunks:
                        content = getattr(chunk, 'content_text', '') or ''
                        if content:
                            full_text_parts.append(content)
                    
                    # ì¶”ì¶œëœ ê°ì²´ì—ì„œ ì´ë¯¸ì§€/í…Œì´ë¸” ê°œìˆ˜ í™•ì¸
                    for obj in extracted_objects:
                        obj_type = getattr(obj, 'object_type', '')
                        if obj_type in visual_object_types:
                            image_count += 1
                        elif obj_type == 'TABLE':
                            table_count += 1
                    
                    full_content = '\n\n'.join(full_text_parts)
                    
                    logger.info(f"[MULTIMODAL] ê²€ìƒ‰ ì¸ë±ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - "
                               f"í…ìŠ¤íŠ¸ ì²­í¬: {len(doc_chunks)}ê°œ, "
                               f"ì´ë¯¸ì§€: {image_count}ê°œ, "
                               f"í…Œì´ë¸”: {table_count}ê°œ, "
                               f"ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_content)}")
                    
                    # 4.3. NLP ë¶„ì„ (ì „ì²´ ë¬¸ì„œ ë ˆë²¨)
                    # textsearch_koê°€ ìë™ìœ¼ë¡œ í˜•íƒœì†Œ ë¶„ì„ì„ í•˜ë¯€ë¡œ, ê°„ì†Œí™”ëœ ë¶„ì„ë§Œ ìˆ˜í–‰
                    logger.info(f"[MULTIMODAL] ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ì¤€ë¹„ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_content)}")
                    nlp_result = await korean_nlp_service.analyze_text_for_search(full_content[:10000])  # ì²˜ìŒ 10,000ìë§Œ ë¶„ì„
                    
                    # 4.4. ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° êµ¬ì„± (textsearch_koê°€ tsvector ìƒì„± ì‹œ ìë™ ì²˜ë¦¬)
                    search_metadata = {
                        'keywords': nlp_result.get('keywords', [])[:30],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ (textsearch_koê°€ ìë™ ì²˜ë¦¬)
                        'proper_nouns': nlp_result.get('proper_nouns', [])[:30],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
                        'corp_names': nlp_result.get('entities', {}).get('ORG', [])[:20] if isinstance(nlp_result.get('entities'), dict) else [],
                        'main_topics': nlp_result.get('keywords', [])[:10],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    }
                    
                    logger.info(f"[MULTIMODAL] í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ - textsearch_koê°€ tsvector ìƒì„± ì‹œ ìë™ìœ¼ë¡œ í˜•íƒœì†Œ ë¶„ì„ ìˆ˜í–‰")
                    
                    # 4.5. ì´ë¯¸ì§€ ì •ë³´ ìˆ˜ì§‘ (ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ìš©)
                    image_metadata = []
                    for obj in extracted_objects:
                        obj_type = getattr(obj, 'object_type', '')
                        if obj_type in visual_object_types:
                            # ğŸ¯ Caption ìš°ì„  ì¶”ì¶œ (content_textì— ì €ì¥ë¨)
                            caption = getattr(obj, 'content_text', '') or ''
                            structure_json = getattr(obj, 'structure_json', {}) or {}
                            
                            # Fallback: structure_jsonì—ì„œë„ í™•ì¸
                            if not caption and isinstance(structure_json, dict):
                                caption = structure_json.get('caption', '')
                            
                            img_meta = {
                                'object_id': getattr(obj, 'object_id', None),
                                'page_number': getattr(obj, 'page_no', None),
                                'caption': caption,  # ğŸ¯ Azure DIì—ì„œ ì¶”ì¶œí•œ Figure caption
                                'bounding_box': getattr(obj, 'bbox', None),
                                'has_caption': bool(caption),  # ğŸ¯ Caption ìœ ë¬´ í”Œë˜ê·¸
                                'object_type': obj_type,
                            }
                            image_metadata.append(img_meta)
                            
                            # ğŸ¯ Caption ë°œê²¬ ì‹œ ë¡œê·¸ ì¶œë ¥
                            if caption:
                                logger.info(f"[CAPTION] âœ… ì´ë¯¸ì§€ ìº¡ì…˜ ìˆ˜ì§‘ ì™„ë£Œ - obj_id={img_meta['object_id']}, page={img_meta['page_number']}, caption='{caption[:80]}...'")
                    
                    captions_found = sum(1 for img in image_metadata if img.get('has_caption'))
                    logger.info(f"[MULTIMODAL] ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - {len(image_metadata)}ê°œ (ìº¡ì…˜ ìˆìŒ: {captions_found}ê°œ)")
                    
                    # 4.6. ë¬¸ì„œ ë°ì´í„° ì¤€ë¹„ (ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì§€ì›)
                    document_data = {
                        'title': getattr(file_info, 'file_lgc_nm', 'Untitled'),
                        'file_name': getattr(file_info, 'file_lgc_nm', ''),
                        'file_type': getattr(file_info, 'file_extsn', 'unknown'),
                        'full_content': full_content,  # ì „ì²´ í…ìŠ¤íŠ¸ (ì¤‘ìš”!)
                        'page_count': len(set(getattr(obj, 'page_number', 1) for obj in extracted_objects)) if extracted_objects else 1,
                        'language': 'mixed',  # í•œêµ­ì–´/ì˜ì–´ í˜¼í•©
                        'has_images': image_count > 0,
                        'has_tables': table_count > 0,
                        'image_count': image_count,
                        'table_count': table_count,
                        'images': image_metadata,  # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° (ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ìš©)
                    }
                    
                    logger.info(f"[MULTIMODAL] ë¬¸ì„œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ - "
                               f"ì œëª©: {document_data['title']}, "
                               f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_content)}, "
                               f"ì´ë¯¸ì§€: {image_count}ê°œ, "
                               f"í…Œì´ë¸”: {table_count}ê°œ")
                    
                    # 4.6. ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
                    search_result = await self.search_index_service.store_document_for_search(
                        session=session,
                        file_bss_info_sno=file_bss_info_sno,
                        container_id=container_id,
                        document_data=document_data,
                        nlp_analysis=search_metadata,
                        user_info={'emp_no': user_emp_no} if user_emp_no else None
                    )
                    
                    if search_result.get('success'):
                        logger.info(f"[MULTIMODAL] ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ - search_doc_id: {search_result.get('search_doc_id')}")
                        _stage("search_index_creation", True, 
                              search_doc_id=search_result.get('search_doc_id'),
                              keywords_count=len(search_metadata.get('keywords', [])),
                              content_length=len(full_content))
                    else:
                        logger.warning(f"[MULTIMODAL] ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {search_result.get('error')}")
                        _stage("search_index_creation", False, error=search_result.get('error'))
                        
            except Exception as idx_err:
                logger.error(f"[MULTIMODAL] ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {idx_err}")
                _stage("search_index_creation", False, error=str(idx_err))
            
            # 4.7. ê¸°ì¡´ ê²€ìƒ‰ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ìˆëŠ” ê²½ìš°)
            _start_stage("index_metadata_update")
            try:
                stmt = select(TbDocumentSearchIndex).where(TbDocumentSearchIndex.file_bss_info_sno == file_bss_info_sno)
                sr = await session.execute(stmt)
                search_index = sr.scalar_one_or_none()
                if search_index:
                    search_index.extraction_session_id = extraction_session.extraction_session_id
                    search_index.primary_chunk_session_id = chunk_session.chunk_session_id
                    search_index.last_embedding_model = current_embedding_model
                    search_index.has_table = any(getattr(o, 'object_type', '') == "TABLE" for o in extracted_objects)
                    search_index.has_image = any(getattr(o, 'object_type', '') in visual_object_types for o in extracted_objects)
                    logger.info(f"[MULTIMODAL] ê²€ìƒ‰ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                _stage("index_metadata_update", True)
            except Exception as meta_err:
                logger.warning(f"[MULTIMODAL] ê²€ìƒ‰ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ë¬´ì‹œ): {meta_err}")
                _stage("index_metadata_update", False, error=str(meta_err))

            await session.commit()

            elapsed = (datetime.now() - started).total_seconds()
            # í†µê³„ ê³„ì‚° (SQLAlchemy ê°ì²´ ì†ì„± ì•ˆì „í•˜ê²Œ ì ‘ê·¼)
            tables_count = sum(1 for o in extracted_objects if getattr(o, 'object_type', '') == "TABLE")
            images_count = sum(1 for o in extracted_objects if getattr(o, 'object_type', '') == "IMAGE")  
            figures_count = sum(1 for o in extracted_objects if getattr(o, 'object_type', '') == "FIGURE")
            
            # ì²­í¬ë³„ í†µê³„
            text_chunks_count = sum(1 for c in doc_chunks if getattr(c, 'modality', 'text') == 'text')
            image_chunks_count = sum(1 for c in doc_chunks if getattr(c, 'modality', 'text') == 'image')
            table_chunks_count = sum(1 for c in doc_chunks if getattr(c, 'modality', 'text') == 'table')
            
            total_tokens = sum(getattr(c, 'token_count', 0) or 0 for c in doc_chunks)
            avg_tokens = (total_tokens / len(doc_chunks)) if doc_chunks else 0
            
            result["stats"] = {
                "elapsed_seconds": elapsed,
                "avg_chunk_tokens": avg_tokens,
                "vector_dimension": max_dim,
                "tables": tables_count,
                "images": images_count,
                "figures": figures_count,
                "text_chunks": text_chunks_count,
                "image_chunks": image_chunks_count,
                "table_chunks": table_chunks_count,
            }
            if section_chunking_meta.get("requested"):
                result["stats"]["section_chunking_enabled"] = section_chunking_meta.get("enabled")
                result["stats"]["section_chunking_detected_sections"] = section_chunking_meta.get("detected_sections", [])
                result["section_chunking"] = section_chunking_meta
            result["success"] = True
            logger.info(f"[MULTIMODAL] Pipeline success in {elapsed:.2f}s | "
                       f"chunks={result['chunks_count']} (text={text_chunks_count}, image={image_chunks_count}, table={table_chunks_count}) "
                       f"embeddings={result['embeddings_count']}")
            return result
        except Exception as e:
            logger.error(f"[MULTIMODAL] íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            await session.rollback()
            result["error"] = str(e)
            _stage("fatal", False, error=str(e))
            return result
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if is_temp_file and actual_file_path and os.path.exists(actual_file_path):
                try:
                    os.remove(actual_file_path)
                    logger.debug(f"[MULTIMODAL] ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {actual_file_path}")
                except Exception as cleanup_err:
                    logger.warning(f"[MULTIMODAL] ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {cleanup_err}")

    def _get_s3_service(self) -> Optional['S3Service']:
        """ì§€ì—° ë¡œë”© ë°©ì‹ìœ¼ë¡œ S3Service ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜"""
        if S3Service is None:
            return None
        if self._s3_service is None:
            try:
                self._s3_service = S3Service()
            except Exception as err:
                logger.warning(f"[MULTIMODAL] S3Service ì´ˆê¸°í™” ì‹¤íŒ¨: {err}")
                self._s3_service = None
        return self._s3_service

    def _derive_core_content_page_set(
        self,
        sections: List[Dict[str, Any]],
        object_spans: List[Tuple[DocExtractedObject, int, int]],
    ) -> Optional[Set[int]]:
        """
        ì„¹ì…˜ ìˆœì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ë³¸ë¬¸ êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” í˜ì´ì§€ ì§‘í•© ê³„ì‚°
        
        í•™ìˆ ë…¼ë¬¸ì˜ ê²½ìš°:
        - References ì„¹ì…˜ ì´í›„ëŠ” ì œì™¸ (ì €ì í”„ë¡œí•„ ì‚¬ì§„ ë“± ë…¼ë¬¸ ë‚´ìš©ê³¼ ë¬´ê´€)
        - Introductionë¶€í„° Conclusionê¹Œì§€ê°€ í•µì‹¬ ë³¸ë¬¸
        
        ê°œì„ ì‚¬í•­:
        - bbox ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
        - References ì„¹ì…˜ ê°ì§€ ì‹¤íŒ¨ ì‹œ ì „ì²´ í˜ì´ì§€ì˜ 80% ì´í›„ë¥¼ Referencesë¡œ ê°„ì£¼
        """
        if not sections or not object_spans:
            return None

        # 1. ì „ì²´ ê°ì²´ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ (bbox ê¸°ë°˜)
        all_pages: Set[int] = set()
        for obj, _, _ in object_spans:
            page_no = self._extract_page_from_bbox(obj)
            if page_no and page_no > 0:
                all_pages.add(page_no)
        
        if not all_pages:
            logger.warning("[FIGURE-FILTER] í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ")
            return None
        
        max_page = max(all_pages)
        logger.info(f"[FIGURE-FILTER] ì „ì²´ í˜ì´ì§€ ë²”ìœ„: 1~{max_page}")

        # 2. References ì„¹ì…˜ ê°ì§€
        intro_idx: Optional[int] = None
        conclusion_idx: Optional[int] = None
        references_idx: Optional[int] = None
        references_start_page: Optional[int] = None
        
        for section in sections:
            idx = section.get("index")
            if idx is None:
                continue
            section_type = (section.get("type") or "").lower()
            
            if intro_idx is None and section_type == "introduction":
                intro_idx = idx
            if section_type == "conclusion":
                conclusion_idx = idx
            if section_type == "references":
                references_idx = idx
                # References ì„¹ì…˜ì˜ ì‹œì‘ í˜ì´ì§€ ì €ì¥ (ì„¹ì…˜ ì •ë³´ì—ì„œ)
                references_start_page = section.get("start_page")

        # 3. References ì‹œì‘ í˜ì´ì§€ ê²°ì • (ì„¹ì…˜ ê°ì§€ ì‹¤íŒ¨ ì‹œ bbox ê¸°ë°˜ ì¶”ì •)
        if references_idx is not None and not references_start_page:
            # ì„¹ì…˜ì€ ìˆì§€ë§Œ start_pageê°€ Noneì¸ ê²½ìš° â†’ bboxë¡œ ì¶”ì •
            references_start_page = self._estimate_references_page_from_objects(
                sections, object_spans, references_idx
            )
        
        if not references_start_page:
            # References ì„¹ì…˜ ìì²´ê°€ ì—†ê±°ë‚˜ ì¶”ì • ì‹¤íŒ¨ â†’ ì „ì²´ í˜ì´ì§€ì˜ 80% ì´í›„ë¥¼ Referencesë¡œ ê°„ì£¼
            references_start_page = max(1, int(max_page * 0.8))
            logger.info(f"[FIGURE-FILTER] References ì„¹ì…˜ ë¯¸ê°ì§€ â†’ í˜ì´ì§€ {references_start_page}ë¶€í„°ë¥¼ í›„ë°˜ë¶€ë¡œ ê°„ì£¼ (80% ê¸°ì¤€)")
        else:
            logger.info(f"[FIGURE-FILTER] References ì„¹ì…˜ ê°ì§€ - idx={references_idx}, start_page={references_start_page}")

        # 4. References ì´ì „ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ í—ˆìš© (ì„¹ì…˜ ë²”ìœ„ ë¬´ì‹œ)
        # í•™ìˆ ë…¼ë¬¸ì˜ TABLE/FIGUREëŠ” ë³¸ë¬¸ ì „ì²´ì— ë¶„ì‚°ë˜ì–´ ìˆìœ¼ë¯€ë¡œ
        # ì„¹ì…˜ í…ìŠ¤íŠ¸ ìœ„ì¹˜(span)ì™€ ë§¤ì¹­í•˜ì§€ ë§ê³  ë‹¨ìˆœíˆ í˜ì´ì§€ ë²ˆí˜¸ë§Œìœ¼ë¡œ í•„í„°ë§
        allowed_pages: Set[int] = {p for p in all_pages if p < references_start_page}
        
        # References ì„¹ì…˜ ì •ë³´ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        if intro_idx is not None and conclusion_idx is not None:
            logger.info(f"[FIGURE-FILTER] ì„¹ì…˜ ì¸ë±ìŠ¤: Introduction({intro_idx})~Conclusion({conclusion_idx}), References({references_idx})")
        
        logger.info(f"[FIGURE-FILTER] References({references_start_page}p) ì´í›„ ì œì™¸ â†’ í—ˆìš© í˜ì´ì§€: {sorted(allowed_pages)}")
        
        return allowed_pages or None
    
    def _extract_page_from_bbox(self, obj: DocExtractedObject) -> Optional[int]:
        """
        ê°ì²´ì˜ bbox ë˜ëŠ” structure_jsonì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
        
        ìš°ì„ ìˆœìœ„:
        1. obj.page_no (ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©)
        2. structure_jsonì˜ bbox ì¢Œí‘œì—ì„œ ì¶”ì¶œ
        3. structure_jsonì˜ page_number í•„ë“œ
        """
        # 1. ê¸°ì¡´ page_no ì‚¬ìš©
        page_no = getattr(obj, "page_no", None)
        if isinstance(page_no, int) and page_no > 0:
            return page_no
        
        # 2. structure_jsonì—ì„œ ì¶”ì¶œ
        structure_json = getattr(obj, "structure_json", None)
        if structure_json and isinstance(structure_json, dict):
            # Azure DI bbox êµ¬ì¡°: [{"polygon": [...], "page_number": 5}]
            bboxes = structure_json.get("bounding_regions", [])
            if bboxes and isinstance(bboxes, list) and len(bboxes) > 0:
                first_bbox = bboxes[0]
                if isinstance(first_bbox, dict):
                    bbox_page = first_bbox.get("page_number")
                    if isinstance(bbox_page, int) and bbox_page > 0:
                        return bbox_page
            
            # ì§ì ‘ page_number í•„ë“œ
            direct_page = structure_json.get("page_number")
            if isinstance(direct_page, int) and direct_page > 0:
                return direct_page
        
        return None
    
    def _estimate_references_page_from_objects(
        self,
        sections: List[Dict[str, Any]],
        object_spans: List[Tuple[DocExtractedObject, int, int]],
        references_idx: int
    ) -> Optional[int]:
        """
        References ì„¹ì…˜ì— ì†í•œ ê°ì²´ë“¤ì˜ bboxì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì •
        """
        references_section = None
        for section in sections:
            if section.get("index") == references_idx:
                references_section = section
                break
        
        if not references_section:
            return None
        
        s_start = references_section.get("start_pos", 0)
        s_end = references_section.get("end_pos", 0)
        if s_end < s_start:
            s_start, s_end = s_end, s_start
        
        ref_pages: List[int] = []
        for obj, span_start, span_end in object_spans:
            # ì„¹ì…˜ ë²”ìœ„ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            if span_end <= s_start or span_start >= s_end:
                continue
            page_no = self._extract_page_from_bbox(obj)
            if page_no and page_no > 0:
                ref_pages.append(page_no)
        
        if ref_pages:
            min_page = min(ref_pages)
            logger.info(f"[FIGURE-FILTER] References ì„¹ì…˜ bbox ë¶„ì„ â†’ ì‹œì‘ í˜ì´ì§€: {min_page}")
            return min_page
        
        return None

def _serialize_table_to_text(table_obj: DocExtractedObject) -> str:
    """
    TABLE ê°ì²´ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    ìš°ì„ ìˆœìœ„:
    1. content_textê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    2. structure_jsonì— í‘œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë³€í™˜
    3. ì—†ìœ¼ë©´ í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸ ë°˜í™˜
    """
    # 1. ê¸°ì¡´ content_textê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    content_text = getattr(table_obj, 'content_text', '') or ''
    if content_text and content_text.strip() and not content_text.startswith('[í‘œ '):
        return content_text.strip()
    
    # 2. structure_jsonì—ì„œ í‘œ ë°ì´í„° ì¶”ì¶œ
    structure_json = getattr(table_obj, 'structure_json', None)
    if structure_json and isinstance(structure_json, dict):
        # XLSX ì‹œíŠ¸ ë°ì´í„° (ì „ì²´ ì‹œíŠ¸ê°€ TABLEë¡œ ì €ì¥ë¨)
        if 'text' in structure_json and structure_json.get('text', '').strip():
            return structure_json['text'].strip()
        
        # Azure DI í‘œ êµ¬ì¡° (cells, rows, columns ë“±)
        if 'cells' in structure_json or 'rows' in structure_json:
            try:
                return _convert_azure_table_to_markdown(structure_json)
            except Exception as e:
                logger.debug(f"[TABLE] Azure í‘œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        
        # PDF/PPT í‘œ ì¸ë±ìŠ¤ë§Œ ìˆëŠ” ê²½ìš° (ì‹¤ì œ ë°ì´í„° ì—†ìŒ)
        if 'table_index' in structure_json:
            table_idx = structure_json.get('table_index', 0)
            page_no = getattr(table_obj, 'page_no', None)
            return f"[í‘œ {table_idx + 1} - í˜ì´ì§€ {page_no}]"
    
    # 3. í”Œë ˆì´ìŠ¤í™€ë”
    page_no = getattr(table_obj, 'page_no', None)
    seq = getattr(table_obj, 'sequence_in_page', None)
    return f"[í‘œ - í˜ì´ì§€ {page_no}, ìˆœì„œ {seq}]"


def _convert_azure_table_to_markdown(structure: Dict[str, Any]) -> str:
    """
    Azure Document Intelligence í‘œ êµ¬ì¡°ë¥¼ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë³€í™˜
    
    structure ì˜ˆì‹œ:
    {
        "rowCount": 3,
        "columnCount": 2,
        "cells": [
            {"rowIndex": 0, "columnIndex": 0, "content": "Header1"},
            {"rowIndex": 0, "columnIndex": 1, "content": "Header2"},
            ...
        ]
    }
    """
    if not structure:
        return ""
    
    # ì…€ ë°ì´í„° ì¶”ì¶œ
    cells = structure.get('cells', [])
    if not cells:
        # rows í˜•íƒœë¡œ ì œê³µëœ ê²½ìš°
        rows = structure.get('rows', [])
        if rows:
            lines = []
            for row_data in rows:
                if isinstance(row_data, dict) and 'cells' in row_data:
                    row_cells = [str(c.get('content', '')).strip() for c in row_data['cells']]
                    lines.append(' | '.join(row_cells))
                elif isinstance(row_data, list):
                    lines.append(' | '.join(str(c).strip() for c in row_data))
            return '\n'.join(lines)
        return ""
    
    # í–‰/ì—´ í¬ê¸° í™•ì¸
    row_count = structure.get('rowCount', max((c.get('rowIndex', 0) for c in cells), default=0) + 1)
    col_count = structure.get('columnCount', max((c.get('columnIndex', 0) for c in cells), default=0) + 1)
    
    # 2D ë°°ì—´ ì´ˆê¸°í™”
    table_grid = [['' for _ in range(col_count)] for _ in range(row_count)]
    
    # ì…€ ë°ì´í„° ì±„ìš°ê¸°
    for cell in cells:
        row_idx = cell.get('rowIndex', 0)
        col_idx = cell.get('columnIndex', 0)
        content = str(cell.get('content', '')).strip()
        
        # ë³‘í•©ëœ ì…€ ì²˜ë¦¬
        row_span = cell.get('rowSpan', 1)
        col_span = cell.get('columnSpan', 1)
        
        if 0 <= row_idx < row_count and 0 <= col_idx < col_count:
            table_grid[row_idx][col_idx] = content
            
            # ë³‘í•©ëœ ì…€ ì˜ì—­ í‘œì‹œ
            for r in range(row_idx, min(row_idx + row_span, row_count)):
                for c in range(col_idx, min(col_idx + col_span, col_count)):
                    if r != row_idx or c != col_idx:
                        table_grid[r][c] = ''  # ë³‘í•©ëœ ì…€ì€ ë¹ˆ ë¬¸ìì—´
    
    # ë§ˆí¬ë‹¤ìš´ í‘œ ìƒì„±
    lines = []
    for idx, row in enumerate(table_grid):
        lines.append('| ' + ' | '.join(row) + ' |')
        # ì²« ë²ˆì§¸ í–‰ í›„ êµ¬ë¶„ì„  ì¶”ê°€ (í—¤ë”ë¡œ ê°„ì£¼)
        if idx == 0 and row_count > 1:
            lines.append('| ' + ' | '.join(['---'] * col_count) + ' |')
    
    return '\n'.join(lines)


def _clean_metadata_for_json(metadata: Any) -> Any:
    """
    ë©”íƒ€ë°ì´í„°ì—ì„œ JSON ì§ë ¬í™”í•  ìˆ˜ ì—†ëŠ” binary_data ë“±ì„ ì¬ê·€ì ìœ¼ë¡œ ì œê±°
    """
    if isinstance(metadata, dict):
        result = {}
        for key, value in metadata.items():
            if key == 'binary_data':
                # binary_dataëŠ” ì œê±°í•˜ê³  ëŒ€ì‹  ì°¸ì¡° ì •ë³´ë§Œ ì €ì¥
                result['binary_data_info'] = {
                    'removed': True,
                    'type': type(value).__name__,
                    'size_bytes': len(value) if isinstance(value, bytes) else None
                }
            else:
                result[key] = _clean_metadata_for_json(value)
        return result
    elif isinstance(metadata, list):
        return [_clean_metadata_for_json(item) for item in metadata]
    elif isinstance(metadata, bytes):
        # bytes íƒ€ì…ì€ ì •ë³´ë§Œ ì €ì¥
        return {
            'binary_data_info': {
                'removed': True,
                'type': 'bytes',
                'size_bytes': len(metadata)
            }
        }
    else:
        return metadata


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
multimodal_document_service = MultimodalDocumentService()

"""CLIP ê¸°ë°˜ ì´ë¯¸ì§€ ì„ë² ë”© ì„œë¹„ìŠ¤

ë¡œì»¬ CLIP ë˜ëŠ” Azure CLIP ëª¨ë¸ì„ ì‚¬ìš©í•œ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„±:
1) ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (512d)
2) í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (512d) - í¬ë¡œìŠ¤ ëª¨ë‹¬ ê²€ìƒ‰
3) Perceptual Hash (pHash) ìƒì„±
4) ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

íŠ¹ì§•:
- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ê°™ì€ ë²¡í„° ê³µê°„ì— ë§¤í•‘
- ì‹œê°ì  ìœ ì‚¬ë„ ê²€ìƒ‰ ê°€ëŠ¥
- í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰ ê°€ëŠ¥ (í¬ë¡œìŠ¤ ëª¨ë‹¬)

Fallback ì „ëµ:
1. Azure CLIP API (ìš°ì„ )
2. ë¡œì»¬ Hugging Face CLIP ëª¨ë¸ (ìë™ fallback)
3. Placeholder ì„ë² ë”© (ìµœí›„)
"""

from __future__ import annotations
import io
import base64
from typing import List, Dict, Any, Optional
from PIL import Image
import imagehash
import httpx
import logging

from app.core.config import settings

logger = logging.getLogger(__name__)

try:
    import numpy as np  # noqa
except Exception:  # pragma: no cover
    np = None  # type: ignore

# ë¡œì»¬ CLIP ëª¨ë¸ (Hugging Face)
try:
    from transformers import CLIPProcessor, CLIPModel
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    CLIPProcessor = None
    CLIPModel = None
    torch = None


class ImageEmbeddingService:
    """CLIP ê¸°ë°˜ ì´ë¯¸ì§€ ì„ë² ë”© ì„œë¹„ìŠ¤ (Azure + ë¡œì»¬ Fallback)"""
    
    def __init__(self, target_dim: int = 512):
        self.target_dim = target_dim
        self.endpoint = settings.azure_openai_multimodal_embedding_endpoint
        self.api_key = settings.azure_openai_multimodal_embedding_api_key
        self.deployment = settings.azure_openai_multimodal_embedding_deployment
        self.use_azure_clip = bool(self.endpoint and self.api_key)
        
        # ë¡œì»¬ CLIP ëª¨ë¸ ì´ˆê¸°í™” (lazy loading)
        self.local_clip_model = None
        self.local_clip_processor = None
        self.local_clip_device = "cpu"
        self.local_clip_initialized = False
        self.use_local_clip = TRANSFORMERS_AVAILABLE
        
        # Azure CLIP ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ë¡œì»¬ CLIP ì‚¬ìš©
        self.azure_clip_failed = False
        
        if self.use_azure_clip:
            logger.info(f"âœ… Azure CLIP ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œë„: {self.deployment}")
        
        if self.use_local_clip:
            logger.info("ğŸ’¡ ë¡œì»¬ CLIP ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ (Hugging Face)")
        else:
            logger.warning("âš ï¸ transformers ë¯¸ì„¤ì¹˜ - pip install transformers torch")
    
    def _initialize_local_clip(self):
        """ë¡œì»¬ CLIP ëª¨ë¸ ì´ˆê¸°í™” (Lazy Loading)"""
        if self.local_clip_initialized or not self.use_local_clip:
            return
        
        try:
            logger.info("ğŸ”„ ë¡œì»¬ CLIP ëª¨ë¸ ë¡œë”© ì¤‘ (openai/clip-vit-base-patch32)...")
            
            model_name = "openai/clip-vit-base-patch32"
            self.local_clip_processor = CLIPProcessor.from_pretrained(model_name)
            self.local_clip_model = CLIPModel.from_pretrained(model_name)
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì‚¬ìš©
            if torch and torch.cuda.is_available():
                self.local_clip_device = "cuda"
                self.local_clip_model = self.local_clip_model.to(self.local_clip_device)
                logger.info("âœ… ë¡œì»¬ CLIP ëª¨ë¸ ë¡œë”© ì™„ë£Œ (GPU)")
            else:
                logger.info("âœ… ë¡œì»¬ CLIP ëª¨ë¸ ë¡œë”© ì™„ë£Œ (CPU)")
            
            self.local_clip_initialized = True
            
        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ CLIP ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.use_local_clip = False
    
    def _generate_local_image_embedding(self, image_bytes: bytes) -> Optional[List[float]]:
        """ë¡œì»¬ CLIPìœ¼ë¡œ ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±"""
        try:
            self._initialize_local_clip()
            
            if not self.local_clip_initialized:
                return None
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(io.BytesIO(image_bytes))
            
            with torch.no_grad():
                inputs = self.local_clip_processor(images=image, return_tensors="pt")
                
                if self.local_clip_device == "cuda":
                    inputs = {k: v.to(self.local_clip_device) for k, v in inputs.items()}
                
                image_features = self.local_clip_model.get_image_features(**inputs)
                
                # L2 ì •ê·œí™”
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                # CPUë¡œ ì´ë™ ë° ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                embedding = image_features.cpu().numpy()[0].tolist()
                
                logger.info(f"âœ… ë¡œì»¬ CLIP ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±: {len(embedding)}d")
                return embedding
                
        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ CLIP ì´ë¯¸ì§€ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_local_text_embedding(self, text: str) -> Optional[List[float]]:
        """ë¡œì»¬ CLIPìœ¼ë¡œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            self._initialize_local_clip()
            
            if not self.local_clip_initialized:
                return None
            
            with torch.no_grad():
                inputs = self.local_clip_processor(text=[text], return_tensors="pt", padding=True)
                
                if self.local_clip_device == "cuda":
                    inputs = {k: v.to(self.local_clip_device) for k, v in inputs.items()}
                
                text_features = self.local_clip_model.get_text_features(**inputs)
                
                # L2 ì •ê·œí™”
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # CPUë¡œ ì´ë™ ë° ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                embedding = text_features.cpu().numpy()[0].tolist()
                
                logger.info(f"âœ… ë¡œì»¬ CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±: {len(embedding)}d")
                return embedding
                
        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return None

    def compute_phash(self, img: Image.Image) -> str:
        """Perceptual Hash ìƒì„±"""
        return str(imagehash.phash(img))

    async def generate_image_embedding(
        self, 
        image_path: Optional[str] = None,
        image_bytes: Optional[bytes] = None
    ) -> Optional[List[float]]:
        """ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (Azure CLIP â†’ ë¡œì»¬ CLIP â†’ Placeholder)
        
        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            image_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
            
        Returns:
            512ì°¨ì› ì„ë² ë”© ë²¡í„° ë˜ëŠ” None
        """
        # ì´ë¯¸ì§€ ë°ì´í„° ì¤€ë¹„
        if image_path:
            with open(image_path, "rb") as f:
                img_data = f.read()
        elif image_bytes:
            img_data = image_bytes
        else:
            raise ValueError("image_path ë˜ëŠ” image_bytes ì¤‘ í•˜ë‚˜ í•„ìš”")
        
        # 1ë‹¨ê³„: Azure CLIP ì‹œë„ (ì‹¤íŒ¨ ì „ë ¥ì´ ì—†ì„ ë•Œë§Œ)
        if self.use_azure_clip and not self.azure_clip_failed:
            try:
                # Base64 ì¸ì½”ë”©
                img_base64 = base64.b64encode(img_data).decode("utf-8")
                
                # Azure CLIP API í˜¸ì¶œ (image + text í•„ë“œ ëª¨ë‘ í•„ìš”)
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        self.endpoint,
                        headers={
                            "Content-Type": "application/json",
                            "Accept": "application/json",
                            "Authorization": f"Bearer {self.api_key}"
                        },
                        json={
                            "input_data": {
                                "columns": ["image", "text"],  # ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ìŠ¤í‚¤ë§ˆ
                                "data": [[img_base64, ""]]     # textëŠ” ë¹ˆ ë¬¸ìì—´ (ì´ë¯¸ì§€ë§Œ ì„ë² ë”©)
                            }
                        },
                        timeout=30.0
                    )
                    response.raise_for_status()
                    result = response.json()
                
                # ì‘ë‹µ íŒŒì‹±: [{"image_features": [...], "text_features": [...]}] í˜•ì‹
                embedding = None
                if isinstance(result, list) and len(result) > 0:
                    first_item = result[0]
                    if isinstance(first_item, dict):
                        # image_features ì¶”ì¶œ (ì´ë¯¸ì§€ ì„ë² ë”©)
                        embedding = first_item.get("image_features")
                    elif isinstance(first_item, list):
                        # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì¸ ê²½ìš°
                        embedding = first_item
                elif isinstance(result, dict):
                    # dict ì‘ë‹µì¸ ê²½ìš°
                    embedding = (result.get("image_features") or
                               result.get("output", [[]])[0] or 
                               result.get("embedding") or
                               result.get("data", [[]])[0])
                
                if embedding and isinstance(embedding, list) and len(embedding) > 0:
                    logger.info(f"âœ… Azure CLIP ì´ë¯¸ì§€ ì„ë² ë”©: {len(embedding)}d")
                    return embedding
                else:
                    logger.warning(f"âš ï¸ Azure CLIP ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {type(result)}, ë¡œì»¬ CLIPë¡œ fallback")
                    self.azure_clip_failed = True
                    
            except httpx.HTTPStatusError as e:
                logger.warning(f"âš ï¸ Azure CLIP API ì˜¤ë¥˜ (HTTP {e.response.status_code}), ë¡œì»¬ CLIPë¡œ fallback")
                self.azure_clip_failed = True  # ì´í›„ ìš”ì²­ì€ ë°”ë¡œ ë¡œì»¬ CLIP ì‚¬ìš©
            except Exception as e:
                logger.warning(f"âš ï¸ Azure CLIP ì‹¤íŒ¨: {str(e)[:100]}, ë¡œì»¬ CLIPë¡œ fallback")
                self.azure_clip_failed = True
        
        # 2ë‹¨ê³„: ë¡œì»¬ CLIP ì‹œë„
        if self.use_local_clip:
            embedding = self._generate_local_image_embedding(img_data)
            if embedding:
                return embedding
        
        # 3ë‹¨ê³„: Placeholder (ìµœí›„ì˜ ìˆ˜ë‹¨)
        logger.warning("âš ï¸ CLIP ì‚¬ìš© ë¶ˆê°€ - placeholder ì„ë² ë”© ë°˜í™˜")
        return await self._generate_placeholder_embedding(image_path, image_bytes)

    async def generate_text_embedding(self, text: str) -> Optional[List[float]]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (Azure CLIP â†’ ë¡œì»¬ CLIP)
        
        í¬ë¡œìŠ¤ ëª¨ë‹¬ ê²€ìƒ‰ìš©: í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰ ê°€ëŠ¥
        
        Args:
            text: í…ìŠ¤íŠ¸ ì¿¼ë¦¬
            
        Returns:
            512ì°¨ì› ì„ë² ë”© ë²¡í„° ë˜ëŠ” None
        """
        # 1ë‹¨ê³„: Azure CLIP ì‹œë„ (ì‹¤íŒ¨ ì „ë ¥ì´ ì—†ì„ ë•Œë§Œ)
        if self.use_azure_clip and not self.azure_clip_failed:
            try:
                # Azure CLIP API í˜¸ì¶œ (image + text í•„ë“œ ëª¨ë‘ í•„ìš”)
                # í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•  ë•ŒëŠ” imageë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì „ì†¡
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        self.endpoint,
                        headers={
                            "Content-Type": "application/json",
                            "Accept": "application/json",
                            "Authorization": f"Bearer {self.api_key}"
                        },
                        json={
                            "input_data": {
                                "columns": ["image", "text"],  # ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ìŠ¤í‚¤ë§ˆ
                                "data": [["", text]]           # imageëŠ” ë¹ˆ ë¬¸ìì—´ (í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©)
                            }
                        },
                        timeout=30.0
                    )
                    response.raise_for_status()
                    result = response.json()
                
                # ì‘ë‹µ íŒŒì‹±: [{"image_features": [...], "text_features": [...]}] í˜•ì‹
                embedding = None
                if isinstance(result, list) and len(result) > 0:
                    first_item = result[0]
                    if isinstance(first_item, dict):
                        # text_features ì¶”ì¶œ (í…ìŠ¤íŠ¸ ì„ë² ë”©)
                        embedding = first_item.get("text_features")
                    elif isinstance(first_item, list):
                        embedding = first_item
                elif isinstance(result, dict):
                    embedding = (result.get("text_features") or
                               result.get("output", [[]])[0] or 
                               result.get("embedding") or
                               result.get("data", [[]])[0])
                
                if embedding and isinstance(embedding, list) and len(embedding) > 0:
                    logger.info(f"âœ… Azure CLIP í…ìŠ¤íŠ¸ ì„ë² ë”©: {len(embedding)}d")
                    return embedding
                else:
                    embedding = None
                
                if embedding:
                    logger.info(f"âœ… Azure CLIP í…ìŠ¤íŠ¸ ì„ë² ë”©: {len(embedding)}d")
                    return embedding
                    
            except httpx.HTTPStatusError as e:
                logger.warning(f"âš ï¸ Azure CLIP API ì˜¤ë¥˜ (HTTP {e.response.status_code}), ë¡œì»¬ CLIPë¡œ fallback")
                self.azure_clip_failed = True
            except Exception as e:
                logger.warning(f"âš ï¸ Azure CLIP ì‹¤íŒ¨: {str(e)[:100]}, ë¡œì»¬ CLIPë¡œ fallback")
                self.azure_clip_failed = True
        
        # 2ë‹¨ê³„: ë¡œì»¬ CLIP ì‹œë„
        if self.use_local_clip:
            embedding = self._generate_local_text_embedding(text)
            if embedding:
                return embedding
        
        # 3ë‹¨ê³„: None ë°˜í™˜ (í…ìŠ¤íŠ¸ëŠ” placeholder ì—†ìŒ)
        logger.warning("âš ï¸ CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ë¶ˆê°€")
        return None

    async def _generate_placeholder_embedding(
        self,
        image_path: Optional[str] = None,
        image_bytes: Optional[bytes] = None
    ) -> List[float]:
        """Placeholder ì„ë² ë”© ìƒì„± (Azure CLIP ë¯¸ì„¤ì • ì‹œ)"""
        if image_path:
            with open(image_path, "rb") as f:
                img_bytes = f.read()
        elif image_bytes:
            img_bytes = image_bytes
        else:
            return [0.0] * self.target_dim
        
        with Image.open(io.BytesIO(img_bytes)) as img:
            h = self.compute_phash(img)
            ints = [int(h[i:i+2], 16) for i in range(0, len(h), 2)]
            vec = (ints * (self.target_dim // len(ints) + 1))[: self.target_dim]
            max_v = max(vec) or 1
            return [v / max_v for v in vec]

    async def extract_features(
        self, 
        img_bytes: bytes,
        generate_embedding: bool = True
    ) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        
        Args:
            img_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
            generate_embedding: ì„ë² ë”© ìƒì„± ì—¬ë¶€
            
        Returns:
            {
                "phash": str,
                "embedding": List[float],
                "width": int,
                "height": int,
                "aspect_ratio": float,
                "vector_dimension": int
            }
        """
        with Image.open(io.BytesIO(img_bytes)) as im:
            im = im.convert('RGB')
            phash = self.compute_phash(im)
            
            # ì„ë² ë”© ìƒì„±
            embedding = None
            if generate_embedding:
                embedding = await self.generate_image_embedding(image_bytes=img_bytes)
            
            return {
                "phash": phash,
                "embedding": embedding,
                "width": im.width,
                "height": im.height,
                "aspect_ratio": round(im.width / im.height, 4) if im.height else None,
                "vector_dimension": len(embedding) if embedding else None
            }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
image_embedding_service = ImageEmbeddingService()

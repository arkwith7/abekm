"""
ğŸ”„ í†µí•© ì½˜í…ì¸  ì„œë¹„ìŠ¤ (Integrated Content Service)
===============================================

ğŸ¯ ëª©ì : ë¬¸ì„œ ê´€ë¦¬, ê²€ìƒ‰, RAG ì‹œìŠ¤í…œì˜ ê¸°ëŠ¥ë³„ íŒŒì´í”„ë¼ì¸ ì™„ì„±ë„ í–¥ìƒ

ğŸ“Š í•µì‹¬ í…Œì´ë¸”ë³„ ì—­í•  ì •ì˜:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ í…Œì´ë¸”ëª…              â”‚ ì£¼ìš” ìš©ë„            â”‚ í™œìš© ê¸°ëŠ¥                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ tb_file_bss_info    â”‚ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°      â”‚ ë¬¸ì„œ ëª©ë¡, ê¶Œí•œ ê²€ì¦, íŒŒì¼ ê´€ë¦¬   â”‚
â”‚ tb_file_dtl_info    â”‚ íŒŒì¼ ì²˜ë¦¬ ìƒíƒœ       â”‚ ì—…ë¡œë“œ ì§„í–‰ë¥ , ì²˜ë¦¬ ê²°ê³¼         â”‚
â”‚ vs_doc_contents_indexâ”‚ ë¬¸ì„œ ì „ë¬¸ + ì„ë² ë”©   â”‚ í‚¤ì›Œë“œ ê²€ìƒ‰, ë¬¸ì„œ ë‹¨ìœ„ í‘œì‹œ      â”‚
â”‚ vs_doc_contents_chunksâ”‚ ì²­í‚¹ ë‹¨ìœ„ ì„¸ë¶€ì •ë³´   â”‚ ì˜ë¯¸ ê²€ìƒ‰, RAG ì»¨í…ìŠ¤íŠ¸, ì°¸ì¡°ì •ë³´â”‚
â”‚ tb_chat_history     â”‚ ì±„íŒ… ì„¸ì…˜ ê´€ë¦¬       â”‚ ëŒ€í™” ê¸°ë¡, ì»¨í…ìŠ¤íŠ¸ ì—°ì†ì„±       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”— ê¸°ëŠ¥ë³„ íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜:
â”Œâ”€ ğŸ“„ ë¬¸ì„œ ê´€ë¦¬ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì—…ë¡œë“œ â†’ tb_file_bss_info ì €ì¥ â†’ ì „ì²˜ë¦¬ â†’ NLP ë¶„ì„ â†’ vs_doc_contents_index    â”‚
â”‚        â†“ tb_file_dtl_info ìƒíƒœ   â†“ í…ìŠ¤íŠ¸ì¶”ì¶œ  â†“ í˜•íƒœì†Œë¶„ì„ â†“ ë¬¸ì„œì „ë¬¸+ì„ë² ë”©    â”‚
â”‚        â†’ ì²­í‚¹ â†’ vs_doc_contents_chunks â†’ í‚¤ì›Œë“œ/ì—”í‹°í‹° ì¶”ì¶œ â†’ ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ ğŸ” ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [í‚¤ì›Œë“œ ê²€ìƒ‰] vs_doc_contents_index ì „ë¬¸ â†’ ë¬¸ì„œ ë‹¨ìœ„ ê²°ê³¼ ë°˜í™˜                â”‚
â”‚ [ì˜ë¯¸ ê²€ìƒ‰]   vs_doc_contents_chunks ì„ë² ë”© â†’ ì²­í‚¹ ë‹¨ìœ„ ì •í™•ë„ í–¥ìƒ          â”‚
â”‚ [í•˜ì´ë¸Œë¦¬ë“œ]  ë‘ ë°©ì‹ ê²°í•© â†’ ìŠ¤ì½”ì–´ ê°€ì¤‘ í•©ì‚° â†’ ìµœì  ê²€ìƒ‰ ê²°ê³¼               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ ğŸ¤– RAG íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì§ˆë¬¸ â†’ vs_doc_contents_chunks ì˜ë¯¸ ê²€ìƒ‰ â†’ ì»¨í…ìŠ¤íŠ¸ ìƒì„± â†’ LLM ì‘ë‹µ ìƒì„±      â”‚
â”‚      â†“ ì²­í‚¹ë‹¨ìœ„ ì •ë°€ ê²€ìƒ‰           â†“ ì°¸ì¡°ì •ë³´ ì¶”ì¶œ   â†“ tb_chat_history ì €ì¥  â”‚
â”‚      â†’ í˜ì´ì§€ë²ˆí˜¸, í‚¤ì›Œë“œ, ë¬¸ì„œëª… ë“± ì„¸ë¶€ ì°¸ì¡°ì •ë³´ ì œê³µ                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import json
import uuid
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text, select, and_, or_, func, desc
import numpy as np

from app.core.database import get_async_session_local
from app.services.core.embedding_service import EmbeddingService
from app.services.core.korean_nlp_service import korean_nlp_service
from app.services.auth.permission_service import PermissionService
from app.models import TbFileBssInfo, TbFileDtlInfo, TbDocumentSearchIndex
from app.models import VsDocContentsChunks
from app.models import TbChatHistory
from app.core.config import settings

logger = logging.getLogger(__name__)


class IntegratedContentService:
    """í†µí•© ì½˜í…ì¸  ì„œë¹„ìŠ¤ - ë¬¸ì„œ ê´€ë¦¬, ê²€ìƒ‰, RAG í†µí•©"""
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.async_session_local = get_async_session_local()
        
        # ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì„¤ì •
        self.vector_weight = 0.7      # ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì¦ê°€
        self.keyword_weight = 0.3     # í‚¤ì›Œë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜
        self.similarity_threshold = 0.5
        
        logger.info("ğŸ”„ í†µí•© ì½˜í…ì¸  ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    # =========================================================================
    # ğŸ“„ 1. ë¬¸ì„œ ê´€ë¦¬ íŒŒì´í”„ë¼ì¸ (Document Management Pipeline)
    # =========================================================================
    
    async def complete_document_pipeline(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        file_metadata: Dict[str, Any],
        raw_content: str,
        user_emp_no: str
    ) -> Dict[str, Any]:
        """
        ì™„ì „í•œ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        
        íŒŒì´í”„ë¼ì¸ ë‹¨ê³„:
        1. tb_file_bss_info ë©”íƒ€ë°ì´í„° ì €ì¥/ì—…ë°ì´íŠ¸
        2. tb_file_dtl_info ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬  
        3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹
        4. í˜•íƒœì†Œ ë¶„ì„ ë° NLP ì²˜ë¦¬
        5. vs_doc_contents_index ë¬¸ì„œ ì „ë¬¸ + ì„ë² ë”© ì €ì¥
        6. vs_doc_contents_chunks ì²­í‚¹ ë‹¨ìœ„ ì„¸ë¶€ ì •ë³´ ì €ì¥
        """
        try:
            pipeline_result = {
                "success": False,
                "pipeline_stages": {},
                "file_bss_info_sno": file_bss_info_sno,
                "container_id": container_id,
                "total_chunks": 0,
                "errors": []
            }
            
            # ===== 1ë‹¨ê³„: tb_file_bss_info ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ =====
            stage1_result = await self._process_file_metadata(
                session, file_bss_info_sno, file_metadata, user_emp_no
            )
            pipeline_result["pipeline_stages"]["metadata_processing"] = stage1_result
            
            if not stage1_result["success"]:
                pipeline_result["errors"].append("ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
                return pipeline_result
            
            # ===== 2ë‹¨ê³„: tb_file_dtl_info ìƒíƒœ ê´€ë¦¬ =====
            await self._update_processing_status(session, file_bss_info_sno, "PROCESSING")
            
            # ===== 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹ =====
            stage3_result = await self._preprocess_and_chunk_content(raw_content)
            pipeline_result["pipeline_stages"]["text_processing"] = stage3_result
            
            if not stage3_result["success"]:
                await self._update_processing_status(session, file_bss_info_sno, "FAILED")
                pipeline_result["errors"].append("í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨")
                return pipeline_result
            
            chunks = stage3_result["chunks"]
            pipeline_result["total_chunks"] = len(chunks)
            
            # ===== 4ë‹¨ê³„: í˜•íƒœì†Œ ë¶„ì„ ë° NLP ì²˜ë¦¬ =====
            stage4_result = await self._perform_nlp_analysis(chunks)
            pipeline_result["pipeline_stages"]["nlp_analysis"] = stage4_result
            
            # ===== 5ë‹¨ê³„: vs_doc_contents_index ì €ì¥ (ë¬¸ì„œ ì „ë¬¸ + ì„ë² ë”©) =====
            stage5_result = await self._store_document_fulltext_vectors(
                session, file_bss_info_sno, container_id, chunks, stage4_result["nlp_results"], user_emp_no
            )
            pipeline_result["pipeline_stages"]["fulltext_vector_storage"] = stage5_result
            
            # ===== 6ë‹¨ê³„: vs_doc_contents_chunks ì €ì¥ (ì²­í‚¹ ë‹¨ìœ„ ì„¸ë¶€ì •ë³´) =====
            stage6_result = await self._store_chunk_detail_vectors(
                session, file_bss_info_sno, stage5_result["vector_ids"], chunks, stage4_result["nlp_results"]
            )
            pipeline_result["pipeline_stages"]["chunk_detail_storage"] = stage6_result
            
            # ===== 7ë‹¨ê³„: ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸ =====
            final_status = "COMPLETED" if stage5_result["success"] and stage6_result["success"] else "PARTIAL"
            await self._update_processing_status(session, file_bss_info_sno, final_status)
            
            pipeline_result["success"] = final_status == "COMPLETED"
            
            logger.info(f"ğŸ”„ ë¬¸ì„œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {final_status}, ì²­í¬ {pipeline_result['total_chunks']}ê°œ")
            return pipeline_result
            
        except Exception as e:
            await self._update_processing_status(session, file_bss_info_sno, "ERROR")
            logger.error(f"ë¬¸ì„œ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
            pipeline_result["errors"].append(str(e))
            return pipeline_result
    
    async def _process_file_metadata(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        metadata: Dict[str, Any],
        user_emp_no: str
    ) -> Dict[str, Any]:
        """tb_file_bss_info ë©”íƒ€ë°ì´í„° ì²˜ë¦¬"""
        try:
            # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•œ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            korean_metadata = {
                "file_size": metadata.get("file_size", 0),
                "content_type": metadata.get("content_type", ""),
                "page_count": metadata.get("page_count", 0),
                "language": "ko",
                "encoding": metadata.get("encoding", "utf-8"),
                "processor_version": "v1.0",
                "processed_by": user_emp_no,
                "processing_timestamp": datetime.now().isoformat()
            }
            
            query = text("""
                UPDATE tb_file_bss_info 
                SET 
                    korean_metadata = :korean_metadata,
                    updated_at = NOW(),
                    processing_status = 'ANALYZING'
                WHERE file_bss_info_sno = :file_id
            """)
            
            await session.execute(query, {
                "korean_metadata": json.dumps(korean_metadata),
                "file_id": file_bss_info_sno
            })
            
            return {"success": True, "metadata_updated": True}
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _update_processing_status(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        status: str
    ) -> bool:
        """tb_file_dtl_info ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            # tb_file_dtl_infoì— ì²˜ë¦¬ ìƒíƒœ ê¸°ë¡
            query = text("""
                INSERT INTO tb_file_dtl_info (
                    file_bss_info_sno, processing_stage, status, 
                    started_at, updated_at, details
                ) VALUES (
                    :file_id, :stage, :status, NOW(), NOW(), :details
                )
                ON CONFLICT (file_bss_info_sno, processing_stage) 
                DO UPDATE SET 
                    status = :status,
                    updated_at = NOW(),
                    details = :details
            """)
            
            details = {
                "status_change": status,
                "timestamp": datetime.now().isoformat(),
                "pipeline_stage": self._get_pipeline_stage_from_status(status)
            }
            
            await session.execute(query, {
                "file_id": file_bss_info_sno,
                "stage": "document_processing",
                "status": status,
                "details": json.dumps(details)
            })
            
            return True
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def _preprocess_and_chunk_content(self, raw_content: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹"""
        try:
            # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ì œ
            cleaned_content = await korean_nlp_service.clean_korean_text(raw_content)
            
            # ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹ (ë¬¸ì¥ ê²½ê³„, ë‹¨ë½ ê²½ê³„ ê³ ë ¤)
            chunks = await korean_nlp_service.intelligent_chunking(
                cleaned_content,
                chunk_size=1000,
                overlap_size=200,
                preserve_sentences=True
            )
            
            return {
                "success": True,
                "chunks": chunks,
                "original_length": len(raw_content),
                "cleaned_length": len(cleaned_content),
                "chunk_count": len(chunks)
            }
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e), "chunks": []}
    
    async def _perform_nlp_analysis(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í˜•íƒœì†Œ ë¶„ì„ ë° NLP ì²˜ë¦¬"""
        try:
            nlp_results = []
            
            for chunk in chunks:
                chunk_text = chunk.get("text", "")
                
                # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„
                morpheme_result = await korean_nlp_service.analyze_morphemes(chunk_text)
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = await korean_nlp_service.extract_keywords(
                    chunk_text, max_keywords=10
                )
                
                # ê°œì²´ëª… ì¸ì‹
                entities = await korean_nlp_service.extract_entities(chunk_text)
                
                # ë¬¸ì„œ ìš”ì•½ (ì²­í‚¹ ë‹¨ìœ„)
                summary = await korean_nlp_service.summarize_text(
                    chunk_text, max_length=100
                )
                
                nlp_results.append({
                    "morphemes": morpheme_result,
                    "keywords": keywords,
                    "entities": entities,
                    "summary": summary,
                    "text_stats": {
                        "char_count": len(chunk_text),
                        "word_count": len(chunk_text.split()),
                        "sentence_count": chunk_text.count('.')
                    }
                })
            
            return {
                "success": True,
                "nlp_results": nlp_results,
                "processed_chunks": len(chunks)
            }
            
        except Exception as e:
            logger.error(f"NLP ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e), "nlp_results": []}
    
    def _get_pipeline_stage_from_status(self, status: str) -> str:
        """ìƒíƒœì—ì„œ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì¶”ì¶œ"""
        status_mapping = {
            "PROCESSING": "content_processing",
            "ANALYZING": "nlp_analysis", 
            "VECTORIZING": "embedding_generation",
            "COMPLETED": "storage_complete",
            "FAILED": "processing_failed",
            "ERROR": "system_error"
        }
        return status_mapping.get(status, "unknown")
    
    async def _store_document_fulltext_vectors(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        chunks: List[Dict[str, Any]],
        nlp_results: List[Dict[str, Any]],
        user_emp_no: str
    ) -> Dict[str, Any]:
        """vs_doc_contents_indexì— ë¬¸ì„œ ì „ë¬¸ + ì„ë² ë”© ì €ì¥"""
        try:
            vector_ids = []
            failed_chunks = []
            
            await self._update_processing_status(session, file_bss_info_sno, "VECTORIZING")
            
            for i, (chunk, nlp_result) in enumerate(zip(chunks, nlp_results)):
                vector_result = await self._store_main_vector(
                    session, file_bss_info_sno, container_id, chunk, nlp_result, i, user_emp_no
                )
                
                if vector_result:
                    vector_ids.append(vector_result["vector_id"])
                else:
                    failed_chunks.append(i)
            
            return {
                "success": len(vector_ids) > 0,
                "vector_ids": vector_ids,
                "total_stored": len(vector_ids),
                "failed_chunks": failed_chunks,
                "success_rate": len(vector_ids) / len(chunks) if chunks else 0
            }
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì „ë¬¸ ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e), "vector_ids": []}
    
    async def _store_chunk_detail_vectors(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        vector_ids: List[int],
        chunks: List[Dict[str, Any]],
        nlp_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """vs_doc_contents_chunksì— ì²­í‚¹ ë‹¨ìœ„ ì„¸ë¶€ì •ë³´ ì €ì¥"""
        try:
            stored_count = 0
            failed_details = []
            
            for i, (vector_id, chunk, nlp_result) in enumerate(zip(vector_ids, chunks, nlp_results)):
                success = await self._store_chunk_details(
                    session, file_bss_info_sno, vector_id, chunk, nlp_result, i
                )
                
                if success:
                    stored_count += 1
                else:
                    failed_details.append(i)
            
            return {
                "success": stored_count > 0,
                "total_stored": stored_count,
                "failed_details": failed_details,
                "success_rate": stored_count / len(vector_ids) if vector_ids else 0
            }
            
        except Exception as e:
            logger.error(f"ì²­í¬ ì„¸ë¶€ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _store_main_vector(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        container_id: str,
        chunk: Dict[str, Any],
        nlp_result: Dict[str, Any],
        chunk_index: int,
        user_emp_no: str
    ) -> Optional[Dict[str, Any]]:
        """vs_doc_contents_indexì— ë©”ì¸ ë²¡í„° ë°ì´í„° ì €ì¥"""
        try:
            chunk_text = chunk.get('text', '')
            if not chunk_text.strip():
                return None
            
            # ì„ë² ë”© ìƒì„±
            embedding = await self.embedding_service.get_embedding(chunk_text)
            if not embedding:
                return None
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "chunk_index": chunk_index,
                "chunk_size": len(chunk_text),
                "page_number": chunk.get('page_number', 0),
                "section": chunk.get('section', ''),
                "nlp_keywords": nlp_result.get('keywords', []),
                "nlp_entities": nlp_result.get('entities', []),
                "created_by": user_emp_no,
                "created_at": datetime.now().isoformat()
            }
            
            # vs_doc_contents_indexì— ì €ì¥
            embedding_str = "[" + ",".join(map(str, embedding)) + "]"
            
            query = text("""
                INSERT INTO vs_doc_contents_index (
                    file_bss_info_sno, knowledge_container_id, chunk_text, 
                    chunk_index, chunk_size, metadata_json, embedding, created_at
                ) 
                VALUES (
                    :file_bss_info_sno, :container_id, :chunk_text,
                    :chunk_index, :chunk_size, :metadata_json, :embedding::vector, NOW()
                )
                RETURNING id
            """)
            
            result = await session.execute(query, {
                "file_bss_info_sno": file_bss_info_sno,
                "container_id": container_id,
                "chunk_text": chunk_text,
                "chunk_index": chunk_index,
                "chunk_size": len(chunk_text),
                "metadata_json": json.dumps(metadata),
                "embedding": embedding_str
            })
            
            vector_id = result.scalar()
            
            logger.debug(f"ë²¡í„° ì €ì¥ ì™„ë£Œ: ID {vector_id}, ì²­í¬ {chunk_index}")
            
            return {
                "vector_id": vector_id,
                "chunk_index": chunk_index,
                "embedding_dimension": len(embedding)
            }
            
        except Exception as e:
            logger.error(f"ë©”ì¸ ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def _store_chunk_details(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        vector_id: int,
        chunk: Dict[str, Any],
        nlp_result: Dict[str, Any],
        chunk_index: int
    ) -> bool:
        """vs_doc_contents_chunksì— ì²­í¬ ìƒì„¸ ì •ë³´ ì €ì¥"""
        try:
            query = text("""
                INSERT INTO vs_doc_contents_chunks (
                    vs_doc_contents_index_id, file_bss_info_sno, chunk_index,
                    page_number, section_title, subsection_title,
                    keywords_json, entities_json, summary_text, created_at
                ) 
                VALUES (
                    :vector_id, :file_bss_info_sno, :chunk_index,
                    :page_number, :section_title, :subsection_title,
                    :keywords_json, :entities_json, :summary_text, NOW()
                )
            """)
            
            await session.execute(query, {
                "vector_id": vector_id,
                "file_bss_info_sno": file_bss_info_sno,
                "chunk_index": chunk_index,
                "page_number": chunk.get('page_number', 0),
                "section_title": chunk.get('section', '')[:200] if chunk.get('section') else None,
                "subsection_title": chunk.get('subsection', '')[:200] if chunk.get('subsection') else None,
                "keywords_json": json.dumps(nlp_result.get('keywords', [])),
                "entities_json": json.dumps(nlp_result.get('entities', [])),
                "summary_text": nlp_result.get('summary', '')[:500] if nlp_result.get('summary') else None
            })
            
            return True
            
        except Exception as e:
            logger.error(f"ì²­í¬ ìƒì„¸ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def _update_file_processing_status(
        self,
        session: AsyncSession,
        file_bss_info_sno: int,
        status: str
    ) -> bool:
        """tb_file_bss_info ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            query = text("""
                UPDATE tb_file_bss_info 
                SET 
                    processing_status = :status,
                    processed_at = NOW(),
                    updated_at = NOW()
                WHERE file_bss_info_sno = :file_id
            """)
            
            await session.execute(query, {
                "status": status,
                "file_id": file_bss_info_sno
            })
            
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            return False

    # =========================================================================
    # ğŸ” 2. í†µí•© ê²€ìƒ‰ ì‹œìŠ¤í…œ (Unified Search System)  
    # =========================================================================
    
    async def unified_search(
        self,
        query: str,
        user_emp_no: str,
        container_ids: Optional[List[str]] = None,
        max_results: int = 10,
        search_type: str = "hybrid",
        similarity_threshold: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        í†µí•© ê²€ìƒ‰ ì‹œìŠ¤í…œ - vs_doc_contents_index ì¤‘ì‹¬
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            user_emp_no: ì‚¬ìš©ì ì‚¬ë²ˆ
            container_ids: ê²€ìƒ‰ ëŒ€ìƒ ì»¨í…Œì´ë„ˆ
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            search_type: ê²€ìƒ‰ íƒ€ì… (vector, keyword, hybrid)
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        try:
            threshold = similarity_threshold or self.similarity_threshold
            
            # ê¶Œí•œ í™•ì¸
            accessible_containers = await self._get_accessible_containers(user_emp_no, container_ids)
            if not accessible_containers:
                return {
                    "results": [],
                    "total_count": 0,
                    "message": "ì ‘ê·¼ ê¶Œí•œì´ ìˆëŠ” ì»¨í…Œì´ë„ˆê°€ ì—†ìŠµë‹ˆë‹¤"
                }
            
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬
            processed_query = await self._process_search_query(query)
            
            # ê²€ìƒ‰ ì‹¤í–‰
            if search_type == "vector":
                results = await self._vector_search_unified(processed_query, accessible_containers, max_results, threshold)
            elif search_type == "keyword":
                results = await self._keyword_search_unified(processed_query, accessible_containers, max_results)
            else:  # hybrid
                results = await self._hybrid_search_unified(processed_query, accessible_containers, max_results, threshold)
            
            # ê²°ê³¼ í¬ë§·íŒ… ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
            formatted_results = await self._format_search_results_unified(results, user_emp_no)
            
            # ê²€ìƒ‰ ë¡œê·¸ ì €ì¥ (ì˜µì…˜)
            await self._log_search_activity(user_emp_no, query, len(formatted_results), search_type)
            
            return {
                "results": formatted_results,
                "total_count": len(formatted_results),
                "search_type": search_type,
                "query_processed": processed_query,
                "accessible_containers": accessible_containers,
                "execution_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"í†µí•© ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def _vector_search_unified(
        self,
        processed_query: Dict[str, Any],
        container_ids: List[str],
        max_results: int,
        threshold: float
    ) -> List[Dict[str, Any]]:
        """í†µí•© ë²¡í„° ê²€ìƒ‰ - vs_doc_contents_index ì§ì ‘ í™œìš©"""
        try:
            query_text = processed_query.get("optimized_text", processed_query["original_text"])
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = await self.embedding_service.get_embedding(query_text)
            if not query_embedding:
                return []
            
            async with self.async_session_local() as session:
                container_filter = "', '".join(container_ids)
                embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"
                
                # ğŸ”·ğŸŸ§ ë²¤ë”ë³„ ë²¡í„° ì»¬ëŸ¼ ì„ íƒ (ì°¨ì› ê¸°ë°˜ ìë™ íŒë³„)
                # vs_doc_contents_indexëŠ” ë ˆê±°ì‹œ í…Œì´ë¸”ì´ë¯€ë¡œ í´ë°± ì „ëµ ì‚¬ìš©
                embedding_dim = len(query_embedding)
                logger.info(f"[VECTOR-SEARCH-UNIFIED] ì„ë² ë”© ì°¨ì›: {embedding_dim}d (vs_doc_contents_index í…Œì´ë¸” ì‚¬ìš©)")
                
                query_sql = text(f"""
                    SELECT 
                        v.id as vector_id,
                        v.file_bss_info_sno,
                        v.chunk_text,
                        v.chunk_index,
                        v.chunk_size,
                        v.metadata_json,
                        v.knowledge_container_id,
                        f.file_lgc_nm,
                        f.file_psl_nm,
                        f.path,
                        f.korean_metadata,
                        f.created_at as file_created_at,
                        1 - (v.embedding <=> '{embedding_str}'::vector) as similarity_score
                    FROM vs_doc_contents_index v
                    JOIN tb_file_bss_info f ON v.file_bss_info_sno = f.file_bss_info_sno
                    WHERE v.knowledge_container_id IN ('{container_filter}')
                        AND f.del_yn = 'N'
                        AND v.embedding IS NOT NULL
                        AND 1 - (v.embedding <=> '{embedding_str}'::vector) >= {threshold}
                    ORDER BY similarity_score DESC
                    LIMIT {max_results}
                """)
                
                result = await session.execute(query_sql)
                
                results = []
                for row in result.fetchall():
                    # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                    metadata = {}
                    if row.metadata_json:
                        try:
                            metadata = json.loads(row.metadata_json)
                        except:
                            pass
                    
                    # í•œêµ­ì–´ ë©”íƒ€ë°ì´í„° íŒŒì‹±
                    korean_metadata = {}
                    if row.korean_metadata:
                        try:
                            korean_metadata = json.loads(row.korean_metadata)
                        except:
                            pass
                    
                    results.append({
                        "vector_id": row.vector_id,
                        "file_bss_info_sno": row.file_bss_info_sno,
                        "document_id": f"doc_{row.file_bss_info_sno}_{row.chunk_index}",
                        "title": row.file_lgc_nm or row.file_psl_nm,
                        "content": row.chunk_text,
                        "chunk_index": row.chunk_index,
                        "chunk_size": row.chunk_size,
                        "similarity_score": float(row.similarity_score),
                        "container_id": row.knowledge_container_id,
                        "file_path": row.path,
                        "metadata": metadata,
                        "korean_metadata": korean_metadata,
                        "file_created_at": row.file_created_at.isoformat() if row.file_created_at else None,
                        "search_method": "vector"
                    })
                
                logger.info(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ (ì„ê³„ê°’: {threshold})")
                return results
                
        except Exception as e:
            logger.error(f"í†µí•© ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

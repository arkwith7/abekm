"""
ğŸ“„ í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤
==================================

ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ RAGìš© ë²¡í„°ìŠ¤í† ì–´ê¹Œì§€ ì™„ì „ ì²˜ë¦¬í•˜ëŠ” í†µí•© ì„œë¹„ìŠ¤

íŒŒì´í”„ë¼ì¸:
1. ë¬¸ì„œ ì „ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì²­í‚¹)
2. í•œêµ­ì–´ NLP ë¶„ì„ (í˜•íƒœì†Œ + ì„ë² ë”©) 
3. ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ (ë©”íƒ€ë°ì´í„° + ì„ë² ë”©)
"""

import asyncio
import logging
import os
import json
import uuid
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import tiktoken

from app.core.config import settings
try:
    from app.services.core.azure_blob_service import get_azure_blob_service
except Exception:  # pragma: no cover
    get_azure_blob_service = None  # type: ignore
try:
    from app.utils.storage_paths import (
        build_derived_chunk_key,
        build_derived_chunks_manifest_key,
    )
except Exception:  # pragma: no cover
    build_derived_chunk_key = None  # type: ignore
    build_derived_chunks_manifest_key = None  # type: ignore

# ì„œë¹„ìŠ¤ imports
from app.services.document.processing.document_preprocessing_service import document_preprocessing_service
from app.services.core.korean_nlp_service import korean_nlp_service

# ë°ì´í„°ë² ì´ìŠ¤ imports
from sqlalchemy.ext.asyncio import AsyncSession
from app.core.database import get_db
from app.models import TbDocumentSearchIndex
from app.models import VsDocContentsChunks

logger = logging.getLogger(__name__)

class IntegratedDocumentPipelineService:
    """í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.max_retries = 3
        self.batch_size = 10  # ì²­í¬ ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
    async def process_document_for_rag(
        self,
        file_path: str,
        file_name: str,
        container_id: str,
        user_emp_no: str,
        file_bss_info_sno: int
    ) -> Dict[str, Any]:
        """
        RAGìš© ë¬¸ì„œ ì™„ì „ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        
        Args:
            file_path: ì—…ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ
            file_name: ì›ë³¸ íŒŒì¼ëª…
            container_id: ì§€ì‹ ì»¨í…Œì´ë„ˆ ID
            user_emp_no: ì—…ë¡œë“œ ì‚¬ìš©ì ì‚¬ë²ˆ
            file_bss_info_sno: íŒŒì¼ ê¸°ë³¸ ì •ë³´ ì¼ë ¨ë²ˆí˜¸
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        pipeline_result = {
            "success": False,
            "file_info": {
                "file_path": file_path,
                "file_name": file_name,
                "container_id": container_id,
                "user_emp_no": user_emp_no,
                "file_bss_info_sno": file_bss_info_sno
            },
            "stage_results": {
                "preprocessing": {},
                "nlp_analysis": {},
                "vector_storage": {}
            },
            "rag_ready": False,
            "processing_stats": {}
        }
        
        try:
            start_time = datetime.now()
            logger.info(f"ğŸš€ [PIPELINE-DEBUG] RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            logger.info(f"   ğŸ“„ íŒŒì¼: {file_name}")
            logger.info(f"   ğŸ“ ê²½ë¡œ: {file_path}")
            logger.info(f"   ğŸ”§ ì»¨í…Œì´ë„ˆ: {container_id}")
            logger.info(f"   ğŸ‘¤ ì‚¬ìš©ì: {user_emp_no}")
            logger.info(f"   ğŸ†” DB ID: {file_bss_info_sno}")
            
            # ğŸ”„ 1ë‹¨ê³„: ë¬¸ì„œ ì „ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì²­í‚¹)
            logger.info(f"ğŸ’¾ [PIPELINE-DEBUG] 1ë‹¨ê³„ ì‹œì‘: ë¬¸ì„œ ì „ì²˜ë¦¬")
            preprocessing_result = await document_preprocessing_service.process_document(
                file_path=file_path,
                file_extension=Path(file_path).suffix,
                container_id=container_id,
                user_emp_no=user_emp_no
            )
            
            pipeline_result["stage_results"]["preprocessing"] = preprocessing_result
            
            if not preprocessing_result.get("success"):
                pipeline_result["error"] = f"ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹¤íŒ¨: {preprocessing_result.get('error')}"
                logger.error(f"âŒ [PIPELINE-DEBUG] 1ë‹¨ê³„ ì‹¤íŒ¨: {preprocessing_result.get('error')}")
                return pipeline_result
            
            chunks = preprocessing_result["chunks"]
            logger.info(f"âœ… [PIPELINE-DEBUG] ì „ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            logger.info(f"   ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(preprocessing_result.get('extracted_text', ''))}")
            
            if len(chunks) == 0:
                logger.error(f"âŒ [PIPELINE-DEBUG] ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                pipeline_result["error"] = "ì²­í¬ ìƒì„± ì‹¤íŒ¨"
                return pipeline_result
            
            # ğŸ”„ 2ë‹¨ê³„: í•œêµ­ì–´ NLP ë¶„ì„ + ì„ë² ë”©
            logger.info(f"ğŸ§  [PIPELINE-DEBUG] 2ë‹¨ê³„ ì‹œì‘: NLP ë¶„ì„ - {len(chunks)}ê°œ ì²­í¬")
            analyzed_chunks = []
            
            # ì „ì²˜ë¦¬ì—ì„œ ìƒì„±ëœ ë©”íƒ€ë°ì´í„° ì¬ì‚¬ìš©
            preprocessing_metadata = preprocessing_result.get('metadata', [])
            
            for i, chunk_text in enumerate(chunks):
                logger.info(f"   ğŸ“ [PIPELINE-DEBUG] ì²­í¬ {i+1}/{len(chunks)} ë¶„ì„ ì¤‘...")
                logger.info(f"   ğŸ“„ ì²­í¬ ë‚´ìš© ê¸¸ì´: {len(chunk_text)}")
                
                chunk_analysis = await korean_nlp_service.analyze_chunk_for_search(
                    chunk_text
                )
                
                # ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°ì—ì„œ í† í° ìˆ˜ ì¬ì‚¬ìš© (ì¤‘ë³µ ê³„ì‚° ë°©ì§€)
                existing_meta = preprocessing_metadata[i] if i < len(preprocessing_metadata) else {}
                token_count = existing_meta.get('token_count', len(chunk_text.split()))
                
                if chunk_analysis.get("success"):
                    # ì›ë³¸ ì²­í¬ ë°ì´í„°ì™€ NLP ë¶„ì„ ê²°ê³¼ ë³‘í•©
                    enriched_chunk = {
                        "content": chunk_text,  # ì²­í¬ í…ìŠ¤íŠ¸
                        "chunk_index": i,
                        "char_count": len(chunk_text),
                        "token_count": token_count,  # âœ… ì¤‘ë³µ ê³„ì‚° ì œê±°
                        "korean_keywords": chunk_analysis.get("korean_keywords", []),
                        "pos_tags": chunk_analysis.get("pos_tags", []),
                        "named_entities": chunk_analysis.get("named_entities", []),
                        "embedding": chunk_analysis.get("embedding"),
                        "success": True
                    }
                    analyzed_chunks.append(enriched_chunk)
                    logger.info(f"   âœ… ì²­í¬ {i+1} NLP ì„±ê³µ: {len(enriched_chunk.get('korean_keywords', []))}ê°œ í‚¤ì›Œë“œ")
                else:
                    logger.warning(f"   âš ï¸ [PIPELINE-DEBUG] ì²­í¬ {i+1} NLP ë¶„ì„ ì‹¤íŒ¨: {chunk_analysis.get('error')}")
                    # ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì²­í¬ëŠ” ìœ ì§€
                    analyzed_chunks.append({
                        "content": chunk_text,
                        "chunk_index": i,
                        "char_count": len(chunk_text),
                        "token_count": len(self.tokenizer.encode(chunk_text)) if hasattr(self, 'tokenizer') else len(chunk_text.split()),
                        "korean_keywords": [],
                        "pos_tags": [],
                        "named_entities": [],
                        "embedding": None,
                        "success": False,
                        "error": chunk_analysis.get('error')
                    })
            
            pipeline_result["stage_results"]["nlp_analysis"] = {
                "success": True,
                "processed_chunks": len(analyzed_chunks),
                "successful_chunks": len([c for c in analyzed_chunks if c.get("success")]),
                "failed_chunks": len([c for c in analyzed_chunks if not c.get("success")])
            }
            
            successful_chunks = len([c for c in analyzed_chunks if c.get("success")])
            failed_chunks = len([c for c in analyzed_chunks if not c.get("success")])
            logger.info(f"âœ… [PIPELINE-DEBUG] 2ë‹¨ê³„ NLP ì™„ë£Œ: {successful_chunks}ê°œ ì„±ê³µ, {failed_chunks}ê°œ ì‹¤íŒ¨")
            
            # ğŸ”„ 2.5ë‹¨ê³„: (ì˜µì…˜) íŒŒìƒ ì‚°ì¶œë¬¼ Blob ì €ì¥ (ì²­í¬/ë§¤ë‹ˆí˜ìŠ¤íŠ¸)
            try:
                if settings.storage_backend == 'azure_blob' and get_azure_blob_service and build_derived_chunk_key:
                    azure = get_azure_blob_service()
                    manifest = []
                    for ch_idx, ch_obj in enumerate(analyzed_chunks):
                        chunk_key = build_derived_chunk_key(container_id, file_bss_info_sno, ch_idx)
                        payload = {
                            'chunk_index': ch_idx,
                            'char_count': ch_obj.get('char_count'),
                            'token_count': ch_obj.get('token_count'),
                            'has_embedding': ch_obj.get('embedding') is not None,
                            'korean_keywords': ch_obj.get('korean_keywords'),
                        }
                        azure.upload_bytes(json.dumps(payload, ensure_ascii=False).encode('utf-8'), chunk_key, purpose='derived')
                        manifest.append({'key': chunk_key, 'size': len(ch_obj.get('content',''))})
                    if build_derived_chunks_manifest_key:
                        m_key = build_derived_chunks_manifest_key(container_id, file_bss_info_sno)
                        azure.upload_bytes(json.dumps({'chunks': manifest}, ensure_ascii=False).encode('utf-8'), m_key, purpose='derived')
                    logger.info(f"ğŸ—‚ï¸ [PIPELINE-DEBUG] íŒŒìƒ ì²­í¬ {len(analyzed_chunks)}ê°œ Blob ì €ì¥ ì™„ë£Œ")
            except Exception as derived_err:
                logger.warning(f"[PIPELINE-DEBUG] íŒŒìƒ ì‚°ì¶œë¬¼ Blob ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {derived_err}")

            # ğŸ”„ 3ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
            logger.info(f"ğŸ“¦ [PIPELINE-DEBUG] 3ë‹¨ê³„ ì‹œì‘: ë²¡í„°ìŠ¤í† ì–´ ì €ì¥")
            storage_result = await self._store_document_and_chunks(
                file_bss_info_sno=file_bss_info_sno,
                container_id=container_id,
                user_emp_no=user_emp_no,
                file_name=file_name,
                preprocessing_result=preprocessing_result,
                analyzed_chunks=analyzed_chunks
            )
            
            pipeline_result["stage_results"]["vector_storage"] = storage_result
            
            logger.info(f"ğŸ”„ [PIPELINE-DEBUG] ë²¡í„°ìŠ¤í† ì–´ ê²°ê³¼: success={storage_result.get('success', False)}")
            if storage_result.get("success"):
                logger.info(f"   ğŸ“¦ ì €ì¥ëœ ì²­í¬ ìˆ˜: {storage_result.get('chunks_stored', 0)}")
                pipeline_result["success"] = True
                pipeline_result["rag_ready"] = True
                logger.info(f"âœ… [PIPELINE-DEBUG] ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ")
            else:
                logger.error(f"âŒ [PIPELINE-DEBUG] ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨: {storage_result.get('error')}")
                pipeline_result["error"] = f"ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨: {storage_result.get('error')}"
                return pipeline_result
            
            # ğŸ“Š ì²˜ë¦¬ í†µê³„ ìƒì„±
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            pipeline_result["processing_stats"] = {
                "total_processing_time": processing_time,
                "chunks_created": len(chunks),
                "chunks_with_embeddings": len([c for c in analyzed_chunks if c.get("embedding")]),
                "total_keywords": sum(len(c.get("korean_keywords", [])) for c in analyzed_chunks),
                "avg_chunk_size": sum(c.get("char_count", 0) for c in analyzed_chunks) // len(analyzed_chunks) if analyzed_chunks else 0,
                "vector_dimension": len(analyzed_chunks[0].get("embedding", [])) if analyzed_chunks and analyzed_chunks[0].get("embedding") else 0
            }
            
            logger.info(f"ğŸ¯ [PIPELINE-DEBUG] íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            logger.info(f"   ğŸ“Š í†µê³„: ì²­í¬ {len(chunks)}ê°œ, ì„ë² ë”© {len([c for c in analyzed_chunks if c.get('embedding')])}ê°œ")
            logger.info(f"   âœ… RAG ì¤€ë¹„ ì™„ë£Œ!")
            return pipeline_result
            
        except Exception as e:
            logger.error(f"ğŸ’¥ [PIPELINE-DEBUG] íŒŒì´í”„ë¼ì¸ ì˜ˆì™¸ ë°œìƒ: {e}")
            logger.error(f"   ğŸ“„ íŒŒì¼: {file_name}")
            logger.error(f"   ğŸ†” DB ID: {file_bss_info_sno}")
            import traceback
            logger.error(f"   ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            pipeline_result["error"] = str(e)
            return pipeline_result
    
    async def _store_document_and_chunks(
        self,
        file_bss_info_sno: int,
        container_id: str,
        user_emp_no: str,
        file_name: str,
        preprocessing_result: Dict[str, Any],
        analyzed_chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ê¸°ì¡´ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ë¬¸ì„œ ë° ì²­í¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            logger.info("ğŸ—„ï¸ [STORE-DEBUG] ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì‹œì‘ (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)")
            logger.info(f"   ğŸ“ íŒŒì¼ ID: {file_bss_info_sno}")
            logger.info(f"   ğŸ“¦ ì €ì¥í•  ì²­í¬ ìˆ˜: {len(analyzed_chunks)}")

            async for session in get_db():
                successful_chunks = 0
                vs_doc_records: List[TbDocumentSearchIndex] = []
                chunk_records: List[VsDocContentsChunks] = []

                # 1. ê²€ìƒ‰ ì¸ë±ìŠ¤ ì €ì¥
                logger.info("ğŸ—„ï¸ [STORE-DEBUG] 1ë‹¨ê³„: í‚¤ì›Œë“œ/ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì €ì¥")
                for chunk_data in analyzed_chunks:
                    content = chunk_data.get("content", "")
                    if not content:
                        logger.warning(f"ì²­í¬ {chunk_data.get('chunk_index', 'unknown')}ì— content ì—†ìŒ")
                        continue
                    container_id_to_use = container_id or 'DEFAULT_CONTAINER'
                    if not container_id:
                        logger.warning(f"ğŸš¨ [STORE-DEBUG] ì»¨í…Œì´ë„ˆ ID ëˆ„ë½ -> DEFAULT_CONTAINER ì‚¬ìš© (íŒŒì¼ ID: {file_bss_info_sno})")
                    try:
                        search_record = TbDocumentSearchIndex(
                            file_bss_info_sno=file_bss_info_sno,
                            knowledge_container_id=container_id_to_use,
                            document_title=file_name,
                            full_content=content,
                            content_summary=content[:1000],
                            keywords=chunk_data.get("korean_keywords", []),
                            proper_nouns=chunk_data.get("named_entities", []),
                            document_type=Path(file_name).suffix.upper().replace('.', ''),
                            content_length=len(content),
                            language_code='ko'
                        )
                        session.add(search_record)
                        vs_doc_records.append(search_record)
                        logger.debug(f"   ğŸ”– ì¸ë±ìŠ¤ ë ˆì½”ë“œ ì¶”ê°€ chunk={chunk_data.get('chunk_index')}")
                    except Exception as e:
                        logger.warning(f"ğŸ—„ï¸ [STORE-DEBUG] ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨ chunk={chunk_data.get('chunk_index')}: {e}")
                        continue

                # 2. ë²¡í„°/ì²­í¬ ì €ì¥
                logger.info("ğŸ—„ï¸ [STORE-DEBUG] 2ë‹¨ê³„: ë²¡í„°/ì²­í¬ ì €ì¥")
                expected_dimension = settings.get_current_embedding_dimension()
                for chunk_data in analyzed_chunks:
                    content = chunk_data.get("content", "")
                    if not content:
                        continue
                    container_id_to_use = container_id or 'DEFAULT_CONTAINER'
                    chunk_embedding = chunk_data.get("embedding")
                    try:
                        if chunk_embedding:
                            chunk_embedding = settings.apply_smart_dimension_reduction(chunk_embedding, expected_dimension)
                            if len(chunk_embedding) != expected_dimension:
                                if len(chunk_embedding) < expected_dimension:
                                    chunk_embedding = chunk_embedding + [0.0] * (expected_dimension - len(chunk_embedding))
                                else:
                                    chunk_embedding = chunk_embedding[:expected_dimension]
                        else:
                            chunk_embedding = [0.0] * expected_dimension

                        metadata_payload = json.dumps({
                            "chunk_index": chunk_data.get("chunk_index", 0),
                            "token_count": chunk_data.get("token_count", 0),
                            "char_count": chunk_data.get("char_count", 0),
                            "korean_keywords": chunk_data.get("korean_keywords", []),
                            "named_entities": chunk_data.get("named_entities", []),
                            "pos_tags": chunk_data.get("pos_tags", []),  # âœ… í’ˆì‚¬ íƒœê·¸ ì €ì¥
                            "file_name": file_name,
                            "container_id": container_id
                        })
                        chunk_record = VsDocContentsChunks(
                            file_bss_info_sno=file_bss_info_sno,
                            chunk_index=chunk_data.get("chunk_index", 0),
                            chunk_text=content,
                            chunk_size=chunk_data.get("char_count", len(content)),
                            chunk_embedding=chunk_embedding,
                            page_number=chunk_data.get("page_number"),
                            section_title=chunk_data.get("section_title"),
                            knowledge_container_id=container_id_to_use,
                            metadata_json=metadata_payload,
                            created_by=user_emp_no,
                            last_modified_by=user_emp_no
                        )
                        session.add(chunk_record)
                        chunk_records.append(chunk_record)
                        successful_chunks += 1
                        logger.debug(f"   ğŸ’¾ ë²¡í„° ì²­í¬ ì €ì¥ chunk={chunk_data.get('chunk_index')}")
                    except Exception as e:
                        logger.warning(f"ğŸ—„ï¸ [STORE-DEBUG] ë²¡í„°/ì²­í¬ ì €ì¥ ì‹¤íŒ¨ chunk={chunk_data.get('chunk_index')}: {e}")
                        continue

                # 3. ì»¤ë°‹
                logger.info("ğŸ—„ï¸ [STORE-DEBUG] 3ë‹¨ê³„: íŠ¸ëœì­ì…˜ ì»¤ë°‹")
                logger.info(f"   ğŸ“Š vs_doc_contents_index: {len(vs_doc_records)}ê°œ ë ˆì½”ë“œ")
                logger.info(f"   ğŸ“Š vs_doc_contents_chunks: {len(chunk_records)}ê°œ ë ˆì½”ë“œ")
                await session.commit()
                logger.info("âœ… [STORE-DEBUG] ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ!")
                return {
                    "success": True,
                    "file_bss_info_sno": file_bss_info_sno,
                    "vector_records_stored": len(vs_doc_records),
                    "chunk_records_stored": len(chunk_records),
                    "total_chunks": len(analyzed_chunks),
                    "storage_info": {
                        "vector_table": "vs_doc_contents_index",
                        "chunk_table": "vs_doc_contents_chunks",
                        "vector_dimension": len(analyzed_chunks[0].get("embedding", [])) if analyzed_chunks and analyzed_chunks[0].get("embedding") else 0,
                        "has_korean_analysis": any(c.get("korean_keywords") for c in analyzed_chunks),
                        "has_embeddings": any(c.get("embedding") for c in analyzed_chunks)
                    }
                }
        except Exception as e:
            logger.error(f"ğŸ’¥ [STORE-DEBUG] ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"   ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {"success": False, "error": str(e)}

        # ì´ ìœ„ì¹˜ì— ë„ë‹¬í•˜ë©´ ì„¸ì…˜ ë£¨í”„ì—ì„œ return ë˜ì§€ ì•Šì€ ë¹„ì •ìƒ íë¦„
        logger.error("ğŸ’¥ [STORE-DEBUG] ë¹„ì •ìƒ íë¦„: ì„¸ì…˜ ë£¨í”„ì—ì„œ ë°˜í™˜ë˜ì§€ ì•ŠìŒ")
        return {"success": False, "error": "Unexpected control flow in _store_document_and_chunks"}

        # NOTE: ì •ìƒì ìœ¼ë¡œëŠ” ìœ„ì—ì„œ ì´ë¯¸ ë°˜í™˜ë¨.

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
integrated_pipeline_service = IntegratedDocumentPipelineService()

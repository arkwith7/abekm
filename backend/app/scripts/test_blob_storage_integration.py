#!/usr/bin/env python3
"""
Blob Storage í†µí•© í…ŒìŠ¤íŠ¸

ì—­í• :
- Azure Blob Storage ì—°ê²° í…ŒìŠ¤íŠ¸
- ë¬¸ì„œ ì²˜ë¦¬                        try:
                            self.azure_blob.client.create_container(container_name)
                            logger.info(f"âœ… ì»´í…Œì´ë„ˆ ìƒì„± ì„±ê³µ: {container_name}")í”„ë¼ì¸ì—ì„œ ìƒì„±ë˜ëŠ” íŒŒì¼ë“¤ì´ Blobì— ì €ì¥ë˜ëŠ”ì§€ í™•ì¸
- raw, intermediate, derived ì»¨í…Œì´ë„ˆë³„ ì €ì¥ ìƒíƒœ ì ê²€
- ì¶”ì¶œëœ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ ë“±ì˜ ì €ì¥ ìƒíƒœ ê²€ì¦
"""
import asyncio
import json
import logging
import sys
import os
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

# ê²½ë¡œ ì„¤ì •
sys.path.append('/home/wjadmin/Dev/InsightBridge/backend')

from sqlalchemy.ext.asyncio import AsyncSession
from app.core.database import get_async_session_local
from app.core.config import settings
from app.services.core.azure_blob_service import get_azure_blob_service, AzureBlobService
from app.services.document.multimodal_document_service import multimodal_document_service
from app.services.document.storage.file_storage_service import FileStorageService

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BlobStorageIntegrationTester:
    def __init__(self):
        """Blob Storage í†µí•© í…ŒìŠ¤í„° ì´ˆê¸°í™”"""
        self.azure_blob: Optional[AzureBlobService] = None
        self.file_storage: Optional[FileStorageService] = None
        
    async def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # Azure Blob Service ì´ˆê¸°í™”
            if settings.storage_backend == 'azure_blob':
                self.azure_blob = get_azure_blob_service()
                logger.info(f"âœ… Azure Blob Service ì´ˆê¸°í™” ì„±ê³µ - Account: {self.azure_blob.account_name}")
            else:
                logger.warning(f"âš ï¸ í˜„ì¬ ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ: {settings.storage_backend} (azure_blobê°€ ì•„ë‹˜)")
                
            # File Storage Service ì´ˆê¸°í™”
            self.file_storage = FileStorageService()
            logger.info("âœ… File Storage Service ì´ˆê¸°í™” ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def test_blob_connectivity(self) -> bool:
        """Blob Storage ì—°ê²° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”— Azure Blob Storage ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        if not self.azure_blob:
            logger.warning("âš ï¸ Azure Blob Serviceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return False
            
        try:
            # ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ
            containers = []
            for container in self.azure_blob.client.list_containers():
                containers.append(container.name)
                
            logger.info(f"âœ… ì—°ê²° ì„±ê³µ - ë°œê²¬ëœ ì»¨í…Œì´ë„ˆ: {containers}")
            
            # í•„ìˆ˜ ì»¨í…Œì´ë„ˆ í™•ì¸
            required_containers = [
                settings.azure_blob_container_raw,
                settings.azure_blob_container_intermediate, 
                settings.azure_blob_container_derived
            ]
            
            missing_containers = [c for c in required_containers if c not in containers]
            if missing_containers:
                logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì»¨í…Œì´ë„ˆ: {missing_containers}")
                
                # ìë™ ìƒì„± ì˜µì…˜ì´ ìˆìœ¼ë©´ ìƒì„± ì‹œë„
                if settings.azure_blob_enable_auto_container:
                    for container_name in missing_containers:
                        try:
                            self.azure_blob.client.create_container(container_name)
                            logger.info(f"âœ… ì»¨í…Œì´ë„ˆ ìƒì„± ì„±ê³µ: {container_name}")
                        except Exception as e:
                            logger.error(f"âŒ ì»¨í…Œì´ë„ˆ ìƒì„± ì‹¤íŒ¨ {container_name}: {e}")
                            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Blob Storage ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    async def list_blob_contents(self, container_name: str, prefix: str = "", max_results: int = 10) -> List[Dict[str, Any]]:
        """Blob ì»¨í…Œì´ë„ˆ ë‚´ìš© ì¡°íšŒ"""
        if not self.azure_blob:
            return []
            
        try:
            container_client = self.azure_blob.client.get_container_client(container_name)
            blobs = []
            
            for blob in container_client.list_blobs(name_starts_with=prefix):
                blob_info = {
                    'name': blob.name,
                    'size': blob.size,
                    'last_modified': blob.last_modified,
                    'content_type': getattr(blob, 'content_type', None),
                    'metadata': getattr(blob, 'metadata', {})
                }
                blobs.append(blob_info)
                
                if len(blobs) >= max_results:
                    break
                    
            return blobs
            
        except Exception as e:
            logger.error(f"âŒ Blob ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ - {container_name}: {e}")
            return []

    async def test_file_upload_and_processing(self) -> Dict[str, Any]:
        """íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ í›„ Blob ì €ì¥ ìƒíƒœ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ íŒŒì¼ ìƒì„±
        test_content = """
        ë¸”ë¡­ ìŠ¤í† ë¦¬ì§€ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
        
        ì´ ë¬¸ì„œëŠ” Azure Blob Storageì— ì˜¬ë°”ë¥´ê²Œ ì €ì¥ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ìƒ˜í”Œ ë¬¸ì„œì…ë‹ˆë‹¤.
        
        ì£¼ìš” ê¸°ëŠ¥:
        1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        2. ë©”íƒ€ë°ì´í„° ì €ì¥
        3. ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥
        4. ìµœì¢… ê²°ê³¼ë¬¼ ì €ì¥
        
        í…ŒìŠ¤íŠ¸ ì‹œê°„: {test_time}
        """.format(test_time=datetime.now())
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        test_file_path = "/tmp/blob_test_document.txt"
        with open(test_file_path, 'w', encoding='utf-8') as f:
            f.write(test_content)
            
        try:
            # DBì—ì„œ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì •í™•í•œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
            from sqlalchemy import text
            async_session_local = get_async_session_local()
            async with async_session_local() as session:
                result = await session.execute(
                    text("SELECT file_bss_info_sno, file_lgc_nm, file_psl_nm FROM tb_file_bss_info WHERE del_yn = 'N' LIMIT 1")
                )
                file_row = result.fetchone()
                
                if not file_row:
                    logger.error("âŒ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    return {"success": False, "error": "No test file found"}
                
                file_id = file_row[0]
                file_logical_name = file_row[1] 
                file_physical_name = file_row[2]
                
                logger.info(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: ID={file_id}, ì´ë¦„={file_logical_name}")
                
                # ì²˜ë¦¬ ì „ Blob ìƒíƒœ í™•ì¸
                before_state = await self.get_blob_storage_state()
                logger.info(f"ğŸ“Š ì²˜ë¦¬ ì „ Blob ìƒíƒœ: {before_state}")
                
                # ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                logger.info("ğŸ¨ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰...")
                result = await multimodal_document_service.process_document_multimodal(
                    file_path=test_file_path,
                    file_bss_info_sno=file_id,
                    container_id="test-container",
                    user_emp_no="test-user",
                    session=session
                )
                
                if result.get("success"):
                    logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µ: {result}")
                    
                    # ì²˜ë¦¬ í›„ Blob ìƒíƒœ í™•ì¸
                    after_state = await self.get_blob_storage_state()
                    logger.info(f"ğŸ“Š ì²˜ë¦¬ í›„ Blob ìƒíƒœ: {after_state}")
                    
                    # ìƒíƒœ ë¹„êµ
                    blob_changes = self.compare_blob_states(before_state, after_state)
                    
                    return {
                        "success": True,
                        "file_id": file_id,
                        "file_name": file_logical_name,
                        "pipeline_result": result,
                        "blob_before": before_state,
                        "blob_after": after_state,
                        "blob_changes": blob_changes
                    }
                else:
                    logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result}")
                    return {"success": False, "pipeline_result": result}
                    
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists(test_file_path):
                os.remove(test_file_path)

    async def get_blob_storage_state(self) -> Dict[str, Any]:
        """í˜„ì¬ Blob Storage ìƒíƒœ ì¡°íšŒ"""
        state = {
            "raw": {"count": 0, "total_size": 0, "files": []},
            "intermediate": {"count": 0, "total_size": 0, "files": []},
            "derived": {"count": 0, "total_size": 0, "files": []}
        }
        
        if not self.azure_blob:
            return state
            
        try:
            # Raw ì»¨í…Œì´ë„ˆ
            raw_blobs = await self.list_blob_contents(settings.azure_blob_container_raw, max_results=50)
            state["raw"]["count"] = len(raw_blobs)
            state["raw"]["total_size"] = sum(blob.get("size", 0) for blob in raw_blobs)
            state["raw"]["files"] = [blob["name"] for blob in raw_blobs]
            
            # Intermediate ì»¨í…Œì´ë„ˆ
            intermediate_blobs = await self.list_blob_contents(settings.azure_blob_container_intermediate, max_results=50)
            state["intermediate"]["count"] = len(intermediate_blobs)
            state["intermediate"]["total_size"] = sum(blob.get("size", 0) for blob in intermediate_blobs)  
            state["intermediate"]["files"] = [blob["name"] for blob in intermediate_blobs]
            
            # Derived ì»¨í…Œì´ë„ˆ
            derived_blobs = await self.list_blob_contents(settings.azure_blob_container_derived, max_results=50)
            state["derived"]["count"] = len(derived_blobs)
            state["derived"]["total_size"] = sum(blob.get("size", 0) for blob in derived_blobs)
            state["derived"]["files"] = [blob["name"] for blob in derived_blobs]
            
        except Exception as e:
            logger.error(f"âŒ Blob ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
        return state

    def compare_blob_states(self, before: Dict[str, Any], after: Dict[str, Any]) -> Dict[str, Any]:
        """Blob ìƒíƒœ ë³€í™” ë¹„êµ"""
        changes = {}
        
        for container in ["raw", "intermediate", "derived"]:
            before_count = before.get(container, {}).get("count", 0)
            after_count = after.get(container, {}).get("count", 0)
            before_size = before.get(container, {}).get("total_size", 0)
            after_size = after.get(container, {}).get("total_size", 0)
            
            before_files = set(before.get(container, {}).get("files", []))
            after_files = set(after.get(container, {}).get("files", []))
            new_files = after_files - before_files
            
            changes[container] = {
                "count_change": after_count - before_count,
                "size_change": after_size - before_size,
                "new_files": list(new_files)
            }
            
        return changes

    async def test_specific_file_extraction_results(self, file_id: int) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì¼ì˜ ì¶”ì¶œ ê²°ê³¼ë¬¼ë“¤ì´ Blobì— ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        logger.info(f"ğŸ” íŒŒì¼ {file_id}ì˜ ì¶”ì¶œ ê²°ê³¼ë¬¼ Blob ì €ì¥ ìƒíƒœ í™•ì¸...")
        
        try:
            from sqlalchemy import text
            async_session_local = get_async_session_local()
            async with async_session_local() as session:
                # ì¶”ì¶œ ì„¸ì…˜ ì •ë³´ ì¡°íšŒ
                result = await session.execute(
                    text("""
                    SELECT es.extraction_session_id, es.file_bss_info_sno, es.status,
                           COUNT(eo.extraction_object_id) as object_count
                    FROM extraction_session es
                    LEFT JOIN extraction_object eo ON es.extraction_session_id = eo.extraction_session_id
                    WHERE es.file_bss_info_sno = :file_id
                    GROUP BY es.extraction_session_id, es.file_bss_info_sno, es.status
                    ORDER BY es.extraction_session_id DESC
                    LIMIT 1
                    """),
                    {"file_id": file_id}
                )
                session_row = result.fetchone()
                
                if not session_row:
                    return {"success": False, "error": "ì¶”ì¶œ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"}
                
                extraction_session_id = session_row[0]
                object_count = session_row[3]
                
                logger.info(f"ğŸ“Š ì¶”ì¶œ ì„¸ì…˜ {extraction_session_id}: {object_count}ê°œ ê°ì²´")
                
                # ì¶”ì¶œëœ ê°ì²´ë“¤ ì¡°íšŒ
                objects_result = await session.execute(
                    text("""
                    SELECT object_type, content_length, metadata, storage_path
                    FROM extraction_object 
                    WHERE extraction_session_id = :session_id
                    """),
                    {"session_id": extraction_session_id}
                )
                objects = objects_result.fetchall()
                
                # Blob ì €ì¥ ìƒíƒœ í™•ì¸
                blob_status = {}
                for obj in objects:
                    obj_type = obj[0]
                    storage_path = obj[3]
                    
                    if storage_path:
                        # Blobì—ì„œ íŒŒì¼ ì¡´ì¬ í™•ì¸
                        exists = await self.check_blob_exists(storage_path)
                        blob_status[f"{obj_type}_{storage_path}"] = {
                            "exists": exists,
                            "storage_path": storage_path
                        }
                
                return {
                    "success": True,
                    "extraction_session_id": extraction_session_id,
                    "object_count": object_count,
                    "objects": [
                        {
                            "type": obj[0],
                            "content_length": obj[1], 
                            "metadata": obj[2],
                            "storage_path": obj[3]
                        } for obj in objects
                    ],
                    "blob_status": blob_status
                }
                
        except Exception as e:
            logger.error(f"âŒ ì¶”ì¶œ ê²°ê³¼ë¬¼ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def check_blob_exists(self, blob_path: str) -> bool:
        """íŠ¹ì • Blob íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        if not self.azure_blob or not blob_path:
            return False
            
        try:
            # ê²½ë¡œì—ì„œ ì»¨í…Œì´ë„ˆì™€ blob ì´ë¦„ ë¶„ë¦¬
            parts = blob_path.strip('/').split('/', 1)
            if len(parts) != 2:
                return False
                
            container_name, blob_name = parts
            blob_client = self.azure_blob.client.get_blob_client(
                container=container_name, 
                blob=blob_name
            )
            
            # ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            return blob_client.exists()
            
        except Exception as e:
            logger.error(f"âŒ Blob ì¡´ì¬ í™•ì¸ ì‹¤íŒ¨ {blob_path}: {e}")
            return False

    async def generate_blob_report(self) -> Dict[str, Any]:
        """ì¢…í•©ì ì¸ Blob Storage ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("ğŸ“‹ Blob Storage ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±...")
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "storage_backend": settings.storage_backend,
            "connectivity": False,
            "containers": {},
            "summary": {}
        }
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        report["connectivity"] = await self.test_blob_connectivity()
        if not report["connectivity"]:
            return report
            
        # ê° ì»¨í…Œì´ë„ˆë³„ ìƒì„¸ ì •ë³´
        containers = [
            ("raw", settings.azure_blob_container_raw),
            ("intermediate", settings.azure_blob_container_intermediate), 
            ("derived", settings.azure_blob_container_derived)
        ]
        
        total_files = 0
        total_size = 0
        
        for container_type, container_name in containers:
            blobs = await self.list_blob_contents(container_name, max_results=100)
            container_size = sum(blob.get("size", 0) for blob in blobs)
            
            report["containers"][container_type] = {
                "name": container_name,
                "file_count": len(blobs),
                "total_size": container_size,
                "size_mb": round(container_size / 1024 / 1024, 2),
                "recent_files": [
                    {
                        "name": blob["name"],
                        "size": blob["size"],
                        "last_modified": blob["last_modified"].isoformat() if blob["last_modified"] else None
                    }
                    for blob in sorted(blobs, key=lambda x: x.get("last_modified", datetime.min), reverse=True)[:5]
                ]
            }
            
            total_files += len(blobs)
            total_size += container_size
            
        report["summary"] = {
            "total_files": total_files,
            "total_size": total_size,
            "total_size_mb": round(total_size / 1024 / 1024, 2)
        }
        
        return report

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("ğŸš€ Azure Blob Storage í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    tester = BlobStorageIntegrationTester()
    
    try:
        # ì´ˆê¸°í™”
        await tester.initialize()
        
        # 1. ì—°ê²° í…ŒìŠ¤íŠ¸
        logger.info("\n" + "="*50)
        logger.info("1ï¸âƒ£ Blob Storage ì—°ê²° í…ŒìŠ¤íŠ¸")
        logger.info("="*50)
        connectivity = await tester.test_blob_connectivity()
        
        if not connectivity:
            logger.error("âŒ Blob Storage ì—°ê²° ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
            return
            
        # 2. í˜„ì¬ ìƒíƒœ í™•ì¸
        logger.info("\n" + "="*50) 
        logger.info("2ï¸âƒ£ í˜„ì¬ Blob Storage ìƒíƒœ í™•ì¸")
        logger.info("="*50)
        current_state = await tester.get_blob_storage_state()
        logger.info(f"ğŸ“Š í˜„ì¬ ìƒíƒœ:")
        for container, info in current_state.items():
            logger.info(f"  {container}: {info['count']}ê°œ íŒŒì¼, {info.get('total_size', 0):,}ë°”ì´íŠ¸")
            
        # 3. íŒŒì¼ ì²˜ë¦¬ ë° ì €ì¥ í…ŒìŠ¤íŠ¸
        logger.info("\n" + "="*50)
        logger.info("3ï¸âƒ£ íŒŒì¼ ì²˜ë¦¬ ë° Blob ì €ì¥ í…ŒìŠ¤íŠ¸")
        logger.info("="*50)
        processing_result = await tester.test_file_upload_and_processing()
        
        if processing_result.get("success"):
            logger.info("âœ… íŒŒì¼ ì²˜ë¦¬ ë° ì €ì¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            changes = processing_result.get("blob_changes", {})
            for container, change in changes.items():
                if change["count_change"] > 0:
                    logger.info(f"  ğŸ“ {container}: +{change['count_change']}ê°œ íŒŒì¼, +{change['size_change']:,}ë°”ì´íŠ¸")
                    if change["new_files"]:
                        logger.info(f"    ìƒˆ íŒŒì¼: {change['new_files'][:3]}")
        else:
            logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {processing_result.get('error', 'Unknown error')}")
            
        # 4. ì¢…í•© ë¦¬í¬íŠ¸
        logger.info("\n" + "="*50)
        logger.info("4ï¸âƒ£ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±")
        logger.info("="*50)
        report = await tester.generate_blob_report()
        
        logger.info("ğŸ“‹ === Azure Blob Storage í†µí•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ===")
        logger.info(f"ğŸ• í…ŒìŠ¤íŠ¸ ì‹œê°„: {report['timestamp']}")
        logger.info(f"ğŸ”— ì—°ê²° ìƒíƒœ: {'âœ… ì„±ê³µ' if report['connectivity'] else 'âŒ ì‹¤íŒ¨'}")
        logger.info(f"ğŸ“Š ì „ì²´ í†µê³„: {report['summary']['total_files']}ê°œ íŒŒì¼, {report['summary']['total_size_mb']}MB")
        
        for container_type, container_info in report["containers"].items():
            logger.info(f"ğŸ“ {container_type} ({container_info['name']}): "
                       f"{container_info['file_count']}ê°œ íŒŒì¼, {container_info['size_mb']}MB")
            
        logger.info("\nğŸ‰ Blob Storage í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
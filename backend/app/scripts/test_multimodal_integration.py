#!/usr/bin/env python3
"""ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸
===================================

ìƒˆë¡œ ê°œì„ ëœ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ í…ŒìŠ¤íŠ¸:
1. í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì—…ë¡œë“œ
2. ê³ ê¸‰ ì²­í‚¹ ê²€ì¦
3. ì„ë² ë”© ìƒì„± í™•ì¸
4. ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
5. í†µê³„ ë° ë©”íƒ€ë°ì´í„° ê²€ì¦
"""

import asyncio
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

import logging
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func

from app.core.database import get_async_engine
from app.models.document.multimodal_models import (
    DocExtractionSession, DocExtractedObject, DocChunkSession, DocChunk, DocEmbedding
)
from app.models import TbFileBssInfo
from app.services.document.multimodal_document_service import multimodal_document_service
from app.services.document.search.multimodal_search_service import multimodal_search_service

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MultimodalPipelineIntegrationTest:
    """ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.test_file_id = None
        self.test_container_id = None
        self.extraction_session_id = None
        self.chunk_session_id = None
        
    async def run_full_test(self):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        async with AsyncSession(get_async_engine()) as session:
            try:
                # 1. ê¸°ì¡´ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°
                await self._find_test_file(session)
                if not self.test_file_id:
                    logger.error("âŒ í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                
                # 2. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì „ ìƒíƒœ í™•ì¸
                before_stats = await self._get_current_stats(session)
                logger.info(f"ğŸ“Š ì‹¤í–‰ ì „ ìƒíƒœ: {before_stats}")
                
                # 3. ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                pipeline_result = await self._run_pipeline(session)
                if not pipeline_result.get("success"):
                    logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {pipeline_result.get('error')}")
                    return False
                
                # 4. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í›„ ìƒíƒœ í™•ì¸
                after_stats = await self._get_current_stats(session)
                logger.info(f"ğŸ“Š ì‹¤í–‰ í›„ ìƒíƒœ: {after_stats}")
                
                # 5. ê²°ê³¼ ê²€ì¦
                await self._verify_results(session, pipeline_result)
                
                # 6. ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
                await self._test_search_functionality(session)
                
                # 7. ìµœì¢… ìš”ì•½
                await self._generate_summary(before_stats, after_stats, pipeline_result)
                
                logger.info("âœ… ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                return True
                
            except Exception as e:
                logger.error(f"ğŸ’¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return False
    
    async def _find_test_file(self, session: AsyncSession):
        """í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ì°¾ê¸°"""
        logger.info("ğŸ” í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        
        stmt = select(TbFileBssInfo).where(
            TbFileBssInfo.del_yn == 'N'
        ).order_by(TbFileBssInfo.file_bss_info_sno.desc()).limit(1)
        
        result = await session.execute(stmt)
        file_info = result.scalar_one_or_none()
        
        if file_info:
            # SQLAlchemy ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‹¤ì œ ê°’ ê°€ì ¸ì˜¤ê¸°
            self.test_file_id = file_info.file_bss_info_sno
            self.test_container_id = file_info.knowledge_container_id or "test_container"
            logger.info(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì„ íƒ: ID={self.test_file_id}, íŒŒì¼ëª…={file_info.file_lgc_nm}")
        else:
            logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    async def _get_current_stats(self, session: AsyncSession) -> dict:
        """í˜„ì¬ ë©€í‹°ëª¨ë‹¬ ë°ì´í„° í†µê³„ ì¡°íšŒ"""
        stats = {}
        
        # ì¶”ì¶œ ì„¸ì…˜ ìˆ˜
        stmt = select(func.count()).select_from(DocExtractionSession)
        result = await session.execute(stmt)
        stats["extraction_sessions"] = result.scalar()
        
        # ì¶”ì¶œ ê°ì²´ ìˆ˜
        stmt = select(func.count()).select_from(DocExtractedObject)
        result = await session.execute(stmt)
        stats["extracted_objects"] = result.scalar()
        
        # ì²­í¬ ì„¸ì…˜ ìˆ˜
        stmt = select(func.count()).select_from(DocChunkSession)
        result = await session.execute(stmt)
        stats["chunk_sessions"] = result.scalar()
        
        # ì²­í¬ ìˆ˜
        stmt = select(func.count()).select_from(DocChunk)
        result = await session.execute(stmt)
        stats["chunks"] = result.scalar()
        
        # ì„ë² ë”© ìˆ˜
        stmt = select(func.count()).select_from(DocEmbedding)
        result = await session.execute(stmt)
        stats["embeddings"] = result.scalar()
        
        return stats
    
    async def _run_pipeline(self, session: AsyncSession) -> dict:
        """ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info(f"ğŸ¨ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - íŒŒì¼ ID: {self.test_file_id}")
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë³´ ì¡°íšŒ
        stmt = select(TbFileBssInfo).where(TbFileBssInfo.file_bss_info_sno == self.test_file_id)
        result = await session.execute(stmt)
        file_info = result.scalar_one()
        
        # í…ŒìŠ¤íŠ¸ìš© ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©
        test_file_path = "/home/wjadmin/Dev/InsightBridge/backend/test_document.txt"
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = await multimodal_document_service.process_document_multimodal(
            file_path=test_file_path,
            file_bss_info_sno=self.test_file_id,
            container_id=self.test_container_id,
            user_emp_no="test_user",
            session=session,
            provider="azure",
            model_profile="test"
        )
        
        if result.get("success"):
            self.extraction_session_id = result.get("extraction_session_id")
            self.chunk_session_id = result.get("chunk_session_id")
            logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µ - ì¶”ì¶œ ì„¸ì…˜: {self.extraction_session_id}, ì²­í¬ ì„¸ì…˜: {self.chunk_session_id}")
        
        return result
    
    async def _verify_results(self, session: AsyncSession, pipeline_result: dict):
        """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ê²€ì¦"""
        logger.info("ğŸ” íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ê²€ì¦ ì¤‘...")
        
        # ì¶”ì¶œ ì„¸ì…˜ ê²€ì¦
        if self.extraction_session_id:
            stmt = select(DocExtractionSession).where(
                DocExtractionSession.extraction_session_id == self.extraction_session_id
            )
            result = await session.execute(stmt)
            extraction_session = result.scalar_one_or_none()
            
            if extraction_session:
                logger.info(f"âœ… ì¶”ì¶œ ì„¸ì…˜ ê²€ì¦ ì„±ê³µ: ìƒíƒœ={extraction_session.status}")
            else:
                logger.error("âŒ ì¶”ì¶œ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì²­í¬ ì„¸ì…˜ ê²€ì¦
        if self.chunk_session_id:
            stmt = select(DocChunkSession).where(
                DocChunkSession.chunk_session_id == self.chunk_session_id
            )
            result = await session.execute(stmt)
            chunk_session = result.scalar_one_or_none()
            
            if chunk_session:
                logger.info(f"âœ… ì²­í¬ ì„¸ì…˜ ê²€ì¦ ì„±ê³µ: ì²­í¬ ìˆ˜={chunk_session.chunk_count}")
            else:
                logger.error("âŒ ì²­í¬ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì²­í¬ ë‚´ìš© ìƒ˜í”Œ í™•ì¸
        stmt = select(DocChunk).where(
            DocChunk.file_bss_info_sno == self.test_file_id
        ).limit(3)
        result = await session.execute(stmt)
        sample_chunks = result.scalars().all()
        
        logger.info(f"ğŸ“„ ìƒ˜í”Œ ì²­í¬ {len(sample_chunks)}ê°œ:")
        for i, chunk in enumerate(sample_chunks):
            logger.info(f"  ì²­í¬ {i+1}: í† í°={chunk.token_count}, ê¸¸ì´={len(chunk.content_text or '')}")
    
    async def _test_search_functionality(self, session: AsyncSession):
        """ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        test_queries = [
            "í…ŒìŠ¤íŠ¸",
            "ë¬¸ì„œ",
            "ë‚´ìš©"
        ]
        
        for query in test_queries:
            try:
                search_results = await multimodal_search_service.search_similar_chunks(
                    query_text=query,
                    session=session,
                    top_k=5,
                    file_ids=[self.test_file_id],
                    similarity_threshold=0.1
                )
                
                logger.info(f"ğŸ” ì¿¼ë¦¬ '{query}': {len(search_results)}ê°œ ê²°ê³¼")
                if search_results:
                    best_result = search_results[0]
                    logger.info(f"  ìµœê³  ìœ ì‚¬ë„: {best_result['similarity_score']:.4f}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ('{query}'): {e}")
    
    async def _generate_summary(self, before_stats: dict, after_stats: dict, pipeline_result: dict):
        """ìµœì¢… ìš”ì•½ ìƒì„±"""
        logger.info("ğŸ“‹ === ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìš”ì•½ ===")
        
        # ì¦ê°€ëŸ‰ ê³„ì‚°
        deltas = {key: after_stats[key] - before_stats[key] for key in before_stats}
        
        logger.info(f"ğŸ“Š ë°ì´í„° ì¦ê°€ëŸ‰:")
        for key, delta in deltas.items():
            if delta > 0:
                logger.info(f"  {key}: +{delta}")
        
        # íŒŒì´í”„ë¼ì¸ í†µê³„
        stats = pipeline_result.get("stats", {})
        if stats:
            logger.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {stats.get('elapsed_seconds', 0):.2f}ì´ˆ")
            logger.info(f"ğŸ”¢ ë²¡í„° ì°¨ì›: {stats.get('vector_dimension', 0)}")
            logger.info(f"ğŸ“Š í‰ê·  ì²­í¬ í† í°: {stats.get('avg_chunk_tokens', 0):.1f}")
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€: {stats.get('images', 0)}ê°œ")
            logger.info(f"ğŸ“‹ í‘œ: {stats.get('tables', 0)}ê°œ")
            logger.info(f"ğŸ“ˆ ì°¨íŠ¸: {stats.get('figures', 0)}ê°œ")
        
        # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ê²°ê³¼
        stages = pipeline_result.get("stages", [])
        if stages:
            logger.info("ğŸ”„ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„:")
            for stage in stages:
                status = "âœ…" if stage["success"] else "âŒ"
                logger.info(f"  {status} {stage['name']}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    test = MultimodalPipelineIntegrationTest()
    success = await test.run_full_test()
    
    if success:
        logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return 0
    else:
        logger.error("ğŸ’¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
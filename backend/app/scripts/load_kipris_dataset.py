#!/usr/bin/env python3
"""
KIPRIS ë°ì´í„°ì…‹ ì ì¬ ìŠ¤í¬ë¦½íŠ¸
================================

ëª©ì :
- backend/data/processed/ ì•„ë˜ì˜ JSONL + PDFë¥¼ ì‹œìŠ¤í…œ DB/ë²¡í„° ì¸ë±ìŠ¤ì— ì ì¬
- ì •ì‹ íŒŒì´í”„ë¼ì¸(PatentPipeline) ì‚¬ìš©: PDF íŒŒì‹±â†’ì„¹ì…˜ ì²­í‚¹â†’ì„ë² ë”©â†’ê²€ìƒ‰ ì¸ë±ìŠ¤

ì‚¬ìš©ë²•:
    # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ
    docker exec -it abkms-backend python -m app.scripts.load_kipris_dataset --limit 10

    # ë¡œì»¬ venvì—ì„œ
    source .venv/bin/activate
    python -m app.scripts.load_kipris_dataset --limit 10

ì˜µì…˜:
    --limit N          ì²˜ë¦¬í•  ìµœëŒ€ ê±´ìˆ˜ (ê¸°ë³¸: ì „ì²´)
    --container-id ID  íŠ¹í—ˆë¥¼ ì €ì¥í•  ì»¨í…Œì´ë„ˆ ID (ê¸°ë³¸: KIPRIS_EVAL)
    --user USER        ì‚¬ìš©ì ì‚¬ë²ˆ (ê¸°ë³¸: system)
    --skip-existing    ì´ë¯¸ DBì— ìˆëŠ” íŠ¹í—ˆëŠ” ìŠ¤í‚µ
    --dry-run          ì‹¤ì œ ì ì¬ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ
"""
import asyncio
import argparse
import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime

# SQLAlchemy ë° ëª¨ë¸
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from app.core.database import get_async_session_local
from app.models import TbFileBssInfo

# íŒŒì´í”„ë¼ì¸ ë¼ìš°í„°
from app.services.document.pipeline_router import PipelineRouter

# ì„¤ì •
from app.core.config import settings

logging.basicConfig(
    level=logging.INFO,
    format='[%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)


class KIPRISDatasetLoader:
    """KIPRIS ë°ì´í„°ì…‹ ì ì¬ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        container_id: str = "KIPRIS_EVAL",
        user_emp_no: str = "system",
        skip_existing: bool = True
    ):
        self.container_id = container_id
        self.user_emp_no = user_emp_no
        self.skip_existing = skip_existing
        self.session_factory = get_async_session_local()
        
        # ë°ì´í„° ê²½ë¡œ
        self.base_dir = Path(__file__).parent.parent.parent / "data" / "processed"
        self.jsonl_path = self.base_dir / "kipris_semiconductor_ai_dataset_paper.jsonl"
        self.pdf_dir = self.base_dir / "fulltext_pdfs"
        
        # í†µê³„
        self.stats = {
            "total": 0,
            "loaded": 0,
            "skipped": 0,
            "failed": 0,
            "errors": []
        }
    
    async def load_dataset(self, limit: Optional[int] = None, dry_run: bool = False):
        """ë°ì´í„°ì…‹ ì „ì²´ ì ì¬"""
        logger.info(f"ğŸš€ KIPRIS ë°ì´í„°ì…‹ ì ì¬ ì‹œì‘")
        logger.info(f"   ğŸ“ JSONL: {self.jsonl_path}")
        logger.info(f"   ğŸ“ PDF: {self.pdf_dir}")
        logger.info(f"   ğŸ“¦ ì»¨í…Œì´ë„ˆ: {self.container_id}")
        logger.info(f"   ğŸ‘¤ ì‚¬ìš©ì: {self.user_emp_no}")
        logger.info(f"   âš™ï¸ ìŠ¤í‚µ ê¸°ì¡´: {self.skip_existing}")
        logger.info(f"   ğŸ”¢ ì œí•œ: {limit or 'ì—†ìŒ'}")
        logger.info(f"   ğŸ§ª Dry-run: {dry_run}")
        
        if not self.jsonl_path.exists():
            logger.error(f"âŒ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.jsonl_path}")
            return
        
        if not self.pdf_dir.exists():
            logger.error(f"âŒ PDF ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.pdf_dir}")
            return
        
        # JSONL ì½ê¸°
        patents = []
        with open(self.jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                if limit and len(patents) >= limit:
                    break
                patents.append(json.loads(line))
        
        self.stats["total"] = len(patents)
        logger.info(f"ğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ: {len(patents)}ê±´")
        
        # ê° íŠ¹í—ˆ ì²˜ë¦¬
        for idx, patent_data in enumerate(patents, 1):
            try:
                await self._process_patent(patent_data, idx, dry_run)
            except Exception as e:
                error_msg = f"íŠ¹í—ˆ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
                logger.error(f"âŒ {error_msg}")
                self.stats["failed"] += 1
                self.stats["errors"].append(error_msg)
        
        # ê²°ê³¼ ìš”ì•½
        self._print_summary()
    
    async def _process_patent(self, patent_data: Dict[str, Any], idx: int, dry_run: bool):
        """ê°œë³„ íŠ¹í—ˆ ì²˜ë¦¬"""
        target = patent_data.get("target_patent", {})
        app_no = target.get("application_number")
        pub_no = target.get("publication_number")
        title = target.get("title", "")
        
        if not app_no:
            logger.warning(f"âš ï¸ [{idx}] ì¶œì›ë²ˆí˜¸ ì—†ìŒ, ìŠ¤í‚µ")
            self.stats["skipped"] += 1
            return
        
        # PDF íŒŒì¼ ì°¾ê¸°
        pdf_path = self._find_pdf(app_no, pub_no)
        if not pdf_path:
            logger.warning(f"âš ï¸ [{idx}] {app_no}: PDF ì—†ìŒ, ìŠ¤í‚µ")
            self.stats["skipped"] += 1
            return
        
        # ê¸°ì¡´ ë°ì´í„° ì²´í¬
        if self.skip_existing and not dry_run:
            async with self.session_factory() as session:
                exists = await self._check_existing(session, app_no)
                if exists:
                    logger.info(f"â­ï¸  [{idx}] {app_no}: ì´ë¯¸ ì ì¬ë¨, ìŠ¤í‚µ")
                    self.stats["skipped"] += 1
                    return
        
        logger.info(f"ğŸ“„ [{idx}/{self.stats['total']}] {app_no}: {title[:60]}...")
        
        if dry_run:
            logger.info(f"   ğŸ§ª [DRY-RUN] PDF ê²½ë¡œ: {pdf_path}")
            logger.info(f"   ğŸ§ª [DRY-RUN] ì‹¤ì œ ì ì¬ ìƒëµ")
            self.stats["loaded"] += 1
            return
        
        # ì‹¤ì œ ì ì¬
        try:
            await self._load_patent_document(
                app_no=app_no,
                pub_no=pub_no,
                title=title,
                pdf_path=pdf_path,
                patent_data=target
            )
            logger.info(f"   âœ… [{idx}] {app_no}: ì ì¬ ì™„ë£Œ")
            self.stats["loaded"] += 1
            
        except Exception as e:
            logger.error(f"   âŒ [{idx}] {app_no}: ì ì¬ ì‹¤íŒ¨ - {e}")
            self.stats["failed"] += 1
            self.stats["errors"].append(f"{app_no}: {e}")
    
    def _find_pdf(self, app_no: str, pub_no: Optional[str]) -> Optional[Path]:
        """PDF íŒŒì¼ ì°¾ê¸° (ì¶œì›ë²ˆí˜¸ ë˜ëŠ” ê³µê°œë²ˆí˜¸ ê¸°ì¤€)"""
        # 1) ì¶œì›ë²ˆí˜¸ë¡œ ì°¾ê¸°
        candidates = [
            self.pdf_dir / f"{app_no}.pdf",
            self.pdf_dir / f"KR{app_no}.pdf",
        ]
        
        # 2) ê³µê°œë²ˆí˜¸ë¡œ ì°¾ê¸°
        if pub_no:
            candidates.extend([
                self.pdf_dir / f"{pub_no}.pdf",
                self.pdf_dir / f"KR{pub_no}.pdf",
            ])
        
        for path in candidates:
            if path.exists():
                return path
        
        return None
    
    async def _check_existing(self, session: AsyncSession, app_no: str) -> bool:
        """ì´ë¯¸ ì ì¬ëœ íŠ¹í—ˆì¸ì§€ í™•ì¸"""
        # íŒŒì¼ëª…ì— ì¶œì›ë²ˆí˜¸ê°€ í¬í•¨ëœ ë ˆì½”ë“œ ì°¾ê¸°
        stmt = select(TbFileBssInfo).where(
            TbFileBssInfo.document_type == 'patent',
            TbFileBssInfo.file_lgc_nm.like(f"%{app_no}%"),
            TbFileBssInfo.del_yn != 'Y'
        )
        result = await session.execute(stmt)
        return result.first() is not None
    
    async def _load_patent_document(
        self,
        app_no: str,
        pub_no: Optional[str],
        title: str,
        pdf_path: Path,
        patent_data: Dict[str, Any]
    ):
        """íŠ¹í—ˆ ë¬¸ì„œë¥¼ ì‹œìŠ¤í…œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì ì¬"""
        async with self.session_factory() as session:
            # 1) TbFileBssInfo ìƒì„± (ê¸°ë³¸ ì •ë³´)
            from app.models import TbFileDtlInfo
            import hashlib
            
            file_name = f"{app_no}_{title[:50]}.pdf"
            file_size = pdf_path.stat().st_size
            file_hash = hashlib.md5(pdf_path.read_bytes()).hexdigest()
            
            # ìƒì„¸ ì •ë³´
            file_dtl = TbFileDtlInfo(
                sj=title or app_no,
                cn=patent_data.get("abstract", "")[:1000],
                file_sz=file_size,
                authr=self.user_emp_no,
                created_by=self.user_emp_no,
                last_modified_by=self.user_emp_no
            )
            session.add(file_dtl)
            await session.flush()
            
            # ê¸°ë³¸ ì •ë³´
            file_bss = TbFileBssInfo(
                drcy_sno=1,
                file_dtl_info_sno=file_dtl.file_dtl_info_sno,
                file_lgc_nm=file_name,
                file_psl_nm=file_name,
                file_extsn="pdf",
                path=str(pdf_path),  # ë¡œì»¬ ê²½ë¡œ ì €ì¥
                knowledge_container_id=self.container_id,
                owner_emp_no=self.user_emp_no,
                created_by=self.user_emp_no,
                last_modified_by=self.user_emp_no,
                korean_metadata={
                    "application_number": app_no,
                    "publication_number": pub_no,
                    "file_hash": file_hash,
                    "data_source": "KIPRIS",
                    "ipc": patent_data.get("ipc"),
                    "applicants": patent_data.get("applicants"),
                },
                document_type="patent",
                processing_status="pending",
                processing_options={
                    "extract_claims": True,
                    "priority_claims": True,
                    "technical_field_extraction": True
                }
            )
            session.add(file_bss)
            await session.flush()
            await session.commit()
            
            document_id = file_bss.file_bss_info_sno
            
            # 2) íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (PatentPipeline)
            logger.info(f"   ğŸ”„ íŒŒì´í”„ë¼ì¸ ì‹œì‘: doc_id={document_id}")
            
            result = await PipelineRouter.process_document(
                document_type="patent",
                document_id=document_id,
                file_path=str(pdf_path),
                file_name=file_name,
                container_id=self.container_id,
                processing_options={
                    "extract_claims": True,
                    "priority_claims": True,
                    "technical_field_extraction": True
                },
                user_emp_no=self.user_emp_no
            )
            
            if not result.get("success"):
                # íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ ì‹œ DB ë ˆì½”ë“œ ì‚­ì œ
                await session.rollback()
                raise Exception(f"íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result.get('error')}")
            
            # 3) ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            from sqlalchemy import update
            stmt = (
                update(TbFileBssInfo)
                .where(TbFileBssInfo.file_bss_info_sno == document_id)
                .values(
                    processing_status="completed",
                    processing_completed_at=datetime.now()
                )
            )
            await session.execute(stmt)
            await session.commit()
            
            stats = result.get("statistics", {})
            logger.info(f"   ğŸ“Š ì²­í¬: {stats.get('total_chunks', 0)}")
            logger.info(f"   ğŸ“Š ì„ë² ë”©: {stats.get('total_embeddings', 0)}")
    
    def _print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š ì ì¬ ì™„ë£Œ ìš”ì•½")
        logger.info("=" * 60)
        logger.info(f"   ì´ ëŒ€ìƒ:     {self.stats['total']}ê±´")
        logger.info(f"   âœ… ì ì¬ ì™„ë£Œ:  {self.stats['loaded']}ê±´")
        logger.info(f"   â­ï¸  ìŠ¤í‚µ:      {self.stats['skipped']}ê±´")
        logger.info(f"   âŒ ì‹¤íŒ¨:      {self.stats['failed']}ê±´")
        
        if self.stats["errors"]:
            logger.info("")
            logger.info("ì˜¤ë¥˜ ëª©ë¡:")
            for error in self.stats["errors"][:10]:  # ìµœëŒ€ 10ê°œë§Œ
                logger.info(f"   - {error}")
            if len(self.stats["errors"]) > 10:
                logger.info(f"   ... ì™¸ {len(self.stats['errors']) - 10}ê±´")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="KIPRIS ë°ì´í„°ì…‹ì„ ì‹œìŠ¤í…œ DB/ë²¡í„° ì¸ë±ìŠ¤ì— ì ì¬"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="ì²˜ë¦¬í•  ìµœëŒ€ ê±´ìˆ˜ (ê¸°ë³¸: ì „ì²´)"
    )
    parser.add_argument(
        "--container-id",
        type=str,
        default="KIPRIS_EVAL",
        help="ì»¨í…Œì´ë„ˆ ID (ê¸°ë³¸: KIPRIS_EVAL)"
    )
    parser.add_argument(
        "--user",
        type=str,
        default="system",
        help="ì‚¬ìš©ì ì‚¬ë²ˆ (ê¸°ë³¸: system)"
    )
    parser.add_argument(
        "--skip-existing",
        action="store_true",
        default=True,
        help="ì´ë¯¸ ì ì¬ëœ íŠ¹í—ˆëŠ” ìŠ¤í‚µ (ê¸°ë³¸: True)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ì ì¬ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ"
    )
    
    args = parser.parse_args()
    
    loader = KIPRISDatasetLoader(
        container_id=args.container_id,
        user_emp_no=args.user,
        skip_existing=args.skip_existing
    )
    
    await loader.load_dataset(
        limit=args.limit,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    asyncio.run(main())

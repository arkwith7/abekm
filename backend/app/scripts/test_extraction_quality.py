#!/usr/bin/env python3
"""
ğŸ“Š ë¬¸ì„œ ì¶”ì¶œ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
====================================

ëª©ì : ì‹¤ì œ í…ìŠ¤íŠ¸, í‘œ, ê·¸ë¦¼ì´ ì œëŒ€ë¡œ ì¶”ì¶œë˜ëŠ”ì§€ í™•ì¸
- Blob Storageì— ì €ì¥ëœ ì¶”ì¶œ ê²°ê³¼ ë‚´ìš© ë¶„ì„
- ì‹¤ì œ íŒŒì¼ë¡œ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- ì¶”ì¶œëœ ê°ì²´ë“¤ì˜ ìƒì„¸ ë‚´ìš© ê²€í† 
"""

import asyncio
import json
import logging
import sys
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.config import settings
from app.core.database import get_async_session_local
from app.services.document.multimodal_document_service import multimodal_document_service

# Azure Blob Storage ì„œë¹„ìŠ¤
try:
    from app.services.core.azure_blob_service import get_azure_blob_service
except ImportError:
    get_azure_blob_service = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ExtractionQualityTester:
    """ì¶”ì¶œ í’ˆì§ˆ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.azure_blob = None
        
    async def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        if settings.storage_backend == 'azure_blob' and get_azure_blob_service:
            self.azure_blob = get_azure_blob_service()
            logger.info(f"âœ… Azure Blob Service ì´ˆê¸°í™”: {self.azure_blob.account_name}")
        else:
            logger.error("âŒ Azure Blob Storageê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            
    async def analyze_stored_extraction_results(self) -> Dict[str, Any]:
        """ì €ì¥ëœ ì¶”ì¶œ ê²°ê³¼ ë¶„ì„"""
        if not self.azure_blob:
            return {"error": "Azure Blob Service not available"}
            
        logger.info("ğŸ” ì €ì¥ëœ ì¶”ì¶œ ê²°ê³¼ ë¶„ì„ ì‹œì‘...")
        
        # Intermediate ì»¨í…Œì´ë„ˆì—ì„œ ì¶”ì¶œ ê²°ê³¼ ì°¾ê¸°
        try:
            intermediate_blobs = await self.list_blob_contents('intermediate', prefix='multimodal/')
            logger.info(f"ğŸ“ ë°œê²¬ëœ ì¤‘ê°„ ê²°ê³¼: {len(intermediate_blobs)}ê°œ")
            
            analysis_results = []
            
            for blob in intermediate_blobs:
                blob_name = blob['name']
                logger.info(f"ğŸ“„ ë¶„ì„ ì¤‘: {blob_name}")
                
                # ì¶”ì¶œ ë©”íƒ€ë°ì´í„° ë¶„ì„
                if 'extraction_metadata.json' in blob_name:
                    content = await self.download_blob_content('intermediate', blob_name)
                    if content:
                        metadata = json.loads(content)
                        analysis_results.append({
                            'type': 'extraction_metadata',
                            'file': blob_name,
                            'content': metadata,
                            'summary': {
                                'objects_count': metadata.get('extracted_objects_count', 0),
                                'pages_detected': metadata.get('pages_detected', 0),
                                'provider': metadata.get('provider', 'unknown')
                            }
                        })
                        
                # ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„
                elif 'extraction_full_text.txt' in blob_name:
                    content = await self.download_blob_content('intermediate', blob_name)
                    if content:
                        analysis_results.append({
                            'type': 'full_text',
                            'file': blob_name,
                            'content': content[:500] + "..." if len(content) > 500 else content,
                            'summary': {
                                'char_count': len(content),
                                'line_count': len(content.split('\n')),
                                'has_korean': 'í•œ' in content or 'ã„±' <= max(content, default='') <= 'í£'
                            }
                        })
                        
                # ê°ì²´ë³„ ìƒì„¸ ë¶„ì„
                elif '/objects/' in blob_name:
                    content = await self.download_blob_content('intermediate', blob_name)
                    if content:
                        if blob_name.endswith('.txt'):
                            # í…ìŠ¤íŠ¸ ë¸”ë¡
                            analysis_results.append({
                                'type': 'text_object',
                                'file': blob_name,
                                'content': content[:200] + "..." if len(content) > 200 else content,
                                'summary': {
                                    'char_count': len(content),
                                    'object_type': 'TEXT_BLOCK'
                                }
                            })
                        elif blob_name.endswith('.json'):
                            # í‘œ ë˜ëŠ” ì´ë¯¸ì§€ ê°ì²´
                            obj_data = json.loads(content)
                            analysis_results.append({
                                'type': 'structured_object',
                                'file': blob_name,
                                'content': obj_data,
                                'summary': {
                                    'object_type': obj_data.get('object_type', 'unknown'),
                                    'page_no': obj_data.get('page_no'),
                                    'has_bbox': bool(obj_data.get('bbox')),
                                    'has_structure': bool(obj_data.get('structure_json'))
                                }
                            })
            
            return {
                'success': True,
                'total_files': len(intermediate_blobs),
                'analysis_results': analysis_results,
                'summary': self._generate_extraction_summary(analysis_results)
            }
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    async def test_with_real_document(self, file_path: str) -> Dict[str, Any]:
        """ì‹¤ì œ ë¬¸ì„œë¡œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        if not os.path.exists(file_path):
            return {'success': False, 'error': f'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path}'}
            
        logger.info(f"ğŸ“„ ì‹¤ì œ ë¬¸ì„œ í…ŒìŠ¤íŠ¸: {file_path}")
        
        try:
            # ê°€ìƒì˜ file_bss_info_sno ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
            test_file_id = 9999
            
            # DB ì„¸ì…˜ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
            from app.core.database import get_async_session_local
            async_session_local = get_async_session_local()
            async with async_session_local() as session:
                result = await multimodal_document_service.process_document_multimodal(
                    file_path=file_path,
                    file_bss_info_sno=test_file_id,
                    container_id="TEST_CONTAINER",
                    user_emp_no="test_user",
                    session=session,
                    provider="azure",
                    model_profile="default"
                )
            
            logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {result.get('success', False)}")
            
            if result.get('success'):
                # ì €ì¥ëœ ê²°ê³¼ ë¶„ì„
                analysis = await self.analyze_extraction_for_file(test_file_id)
                result['detailed_analysis'] = analysis
                
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    async def analyze_extraction_for_file(self, file_id: int) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì¼ì˜ ì¶”ì¶œ ê²°ê³¼ ë¶„ì„"""
        if not self.azure_blob:
            return {'error': 'Azure Blob not available'}
            
        prefix = f"multimodal/{file_id}/"
        
        # Intermediate ê²°ê³¼ ë¶„ì„
        intermediate_analysis = await self._analyze_container_for_prefix('intermediate', prefix)
        # Derived ê²°ê³¼ ë¶„ì„  
        derived_analysis = await self._analyze_container_for_prefix('derived', prefix)
        
        return {
            'file_id': file_id,
            'intermediate': intermediate_analysis,
            'derived': derived_analysis,
            'extraction_quality': self._assess_extraction_quality(intermediate_analysis, derived_analysis)
        }
    
    async def _analyze_container_for_prefix(self, container_type: str, prefix: str) -> Dict[str, Any]:
        """íŠ¹ì • ì»¨í…Œì´ë„ˆì™€ prefixì—ì„œ ê²°ê³¼ ë¶„ì„"""
        container_name = getattr(settings, f'azure_blob_container_{container_type}')
        blobs = await self.list_blob_contents(container_type, prefix=prefix)
        
        analysis = {
            'file_count': len(blobs),
            'files': []
        }
        
        for blob in blobs:
            content = await self.download_blob_content(container_type, blob['name'])
            file_analysis = {
                'name': blob['name'],
                'size': blob['size']
            }
            
            if content:
                if blob['name'].endswith('.json'):
                    try:
                        data = json.loads(content)
                        file_analysis['type'] = 'json'
                        file_analysis['keys'] = list(data.keys()) if isinstance(data, dict) else None
                        file_analysis['sample'] = str(data)[:200] + "..." if len(str(data)) > 200 else str(data)
                    except:
                        file_analysis['type'] = 'invalid_json'
                elif blob['name'].endswith('.txt'):
                    file_analysis['type'] = 'text'
                    file_analysis['char_count'] = len(content)
                    file_analysis['sample'] = content[:200] + "..." if len(content) > 200 else content
                    
            analysis['files'].append(file_analysis)
            
        return analysis
    
    def _assess_extraction_quality(self, intermediate: Dict, derived: Dict) -> Dict[str, Any]:
        """ì¶”ì¶œ í’ˆì§ˆ í‰ê°€"""
        quality = {
            'text_extraction': False,
            'structured_objects': False,
            'chunking': False,
            'embedding': False,
            'overall_score': 0
        }
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ í™•ì¸
        for file in intermediate.get('files', []):
            if 'full_text' in file['name'] and file.get('char_count', 0) > 0:
                quality['text_extraction'] = True
                quality['overall_score'] += 25
                break
                
        # êµ¬ì¡°í™”ëœ ê°ì²´ í™•ì¸
        for file in intermediate.get('files', []):
            if '/objects/' in file['name']:
                quality['structured_objects'] = True
                quality['overall_score'] += 25
                break
                
        # ì²­í‚¹ í™•ì¸
        for file in derived.get('files', []):
            if 'chunking_metadata' in file['name']:
                quality['chunking'] = True
                quality['overall_score'] += 25
                break
                
        # ì„ë² ë”© í™•ì¸
        for file in derived.get('files', []):
            if 'embedding_metadata' in file['name']:
                quality['embedding'] = True
                quality['overall_score'] += 25
                break
        
        return quality
    
    def _generate_extraction_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """ì¶”ì¶œ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        summary = {
            'total_objects': 0,
            'text_objects': 0,
            'table_objects': 0,
            'image_objects': 0,
            'total_chars': 0,
            'has_korean_text': False
        }
        
        for result in results:
            if result['type'] == 'structured_object':
                summary['total_objects'] += 1
                obj_type = result['summary'].get('object_type', '')
                if obj_type == 'TABLE':
                    summary['table_objects'] += 1
                elif obj_type == 'IMAGE':
                    summary['image_objects'] += 1
            elif result['type'] == 'text_object':
                summary['text_objects'] += 1
                summary['total_chars'] += result['summary'].get('char_count', 0)
            elif result['type'] == 'full_text':
                summary['total_chars'] += result['summary'].get('char_count', 0)
                summary['has_korean_text'] = result['summary'].get('has_korean', False)
                
        return summary
    
    async def list_blob_contents(self, container_type: str, prefix: str = "", max_results: int = 50) -> List[Dict[str, Any]]:
        """Blob ì»¨í…Œì´ë„ˆ ë‚´ìš© ì¡°íšŒ"""
        if not self.azure_blob:
            return []
            
        container_name = getattr(settings, f'azure_blob_container_{container_type}')
        
        try:
            blobs = []
            # Azure Blob Serviceì˜ ì‹¤ì œ API ì‚¬ìš©
            blobs_iter = self.azure_blob.list_blobs(container_name, prefix=prefix)
            
            for blob_info in blobs_iter[:max_results]:
                if len(blobs) >= max_results:
                    break
                    
                blobs.append({
                    'name': blob_info.get('name', ''),
                    'size': blob_info.get('size', 0),
                    'last_modified': blob_info.get('last_modified'),
                    'content_type': blob_info.get('content_type', 'unknown')
                })
                
            return blobs
            
        except Exception as e:
            logger.error(f"âŒ Blob ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ ({container_name}): {e}")
            return []
    
    async def download_blob_content(self, container_type: str, blob_name: str) -> Optional[str]:
        """Blob ë‚´ìš© ë‹¤ìš´ë¡œë“œ"""
        if not self.azure_blob:
            return None
            
        container_name = getattr(settings, f'azure_blob_container_{container_type}')
        
        try:
            # Azure Blob Serviceë¥¼ í†µí•´ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
            content_bytes = self.azure_blob.download_blob(container_name, blob_name)
            
            # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© ì‹œë„
            try:
                return content.decode('utf-8')
            except UnicodeDecodeError:
                return content.decode('utf-8', errors='ignore')
                
        except Exception as e:
            logger.error(f"âŒ Blob ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({blob_name}): {e}")
            return None

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("ğŸ” ë¬¸ì„œ ì¶”ì¶œ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    tester = ExtractionQualityTester()
    await tester.initialize()
    
    print("\n" + "="*60)
    print("ğŸ“Š 1ë‹¨ê³„: ê¸°ì¡´ ì €ì¥ëœ ì¶”ì¶œ ê²°ê³¼ ë¶„ì„")
    print("="*60)
    
    stored_analysis = await tester.analyze_stored_extraction_results()
    
    if stored_analysis.get('success'):
        summary = stored_analysis.get('summary', {})
        print(f"âœ… ë¶„ì„ ì™„ë£Œ:")
        print(f"  ğŸ“„ ì´ ì¶”ì¶œëœ ê°ì²´: {summary.get('total_objects', 0)}ê°œ")
        print(f"  ğŸ“ í…ìŠ¤íŠ¸ ê°ì²´: {summary.get('text_objects', 0)}ê°œ")
        print(f"  ğŸ“Š í‘œ ê°ì²´: {summary.get('table_objects', 0)}ê°œ")
        print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€ ê°ì²´: {summary.get('image_objects', 0)}ê°œ")
        print(f"  ğŸ’¬ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {summary.get('total_chars', 0):,}ì")
        print(f"  ğŸ‡°ğŸ‡· í•œêµ­ì–´ í¬í•¨: {'âœ…' if summary.get('has_korean_text') else 'âŒ'}")
        
        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ì„ ê²°ê³¼:")
        for result in stored_analysis.get('analysis_results', [])[:5]:  # ì²˜ìŒ 5ê°œë§Œ
            print(f"  ğŸ“ {result['type']}: {result['file']}")
            if result['type'] == 'full_text':
                sample = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
                print(f"     ë‚´ìš© ìƒ˜í”Œ: {sample}")
            elif result['type'] == 'structured_object':
                print(f"     ê°ì²´ íƒ€ì…: {result['summary'].get('object_type')}")
                print(f"     í˜ì´ì§€: {result['summary'].get('page_no')}")
                
    else:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {stored_analysis.get('error')}")
    
    print("\n" + "="*60)
    print("ğŸ“„ 2ë‹¨ê³„: ì‹¤ì œ ë¬¸ì„œë¡œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì œì•ˆ")
    print("="*60)
    
    # í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ë¬¸ì„œ íŒŒì¼ ì°¾ê¸°
    test_documents = [
        "/tmp/test_document.docx",
        "/tmp/test_document.pdf", 
        "/tmp/test_document.pptx",
        "/home/wjadmin/Dev/InsightBridge/test_template.pdf"
    ]
    
    available_docs = [doc for doc in test_documents if os.path.exists(doc)]
    
    if available_docs:
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ë¬¸ì„œ:")
        for doc in available_docs:
            print(f"  - {doc}")
        print(f"\nğŸ’¡ ì‹¤ì œ ë¬¸ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì›í•˜ì‹œë©´ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:")
        print(f"   python app/scripts/test_extraction_quality.py --test-file {available_docs[0]}")
    else:
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”:")
        print(f"   python app/scripts/test_extraction_quality.py --test-file /path/to/document.pdf")
    
    print(f"\nğŸ‰ ì¶”ì¶œ í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    import sys
    
    if "--test-file" in sys.argv:
        file_index = sys.argv.index("--test-file") + 1
        if file_index < len(sys.argv):
            test_file = sys.argv[file_index]
            
            async def test_file():
                tester = ExtractionQualityTester()
                await tester.initialize()
                result = await tester.test_with_real_document(test_file)
                print(f"\nğŸ“„ íŒŒì¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {test_file}")
                print(f"ì„±ê³µ: {'âœ…' if result.get('success') else 'âŒ'}")
                if result.get('success'):
                    stats = result.get('stats', {})
                    print(f"ì¶”ì¶œ ì‹œê°„: {stats.get('elapsed_seconds', 0):.2f}ì´ˆ")
                    print(f"ì²­í¬ ìˆ˜: {result.get('chunks_count', 0)}ê°œ")
                    print(f"ì„ë² ë”© ìˆ˜: {result.get('embeddings_count', 0)}ê°œ")
                else:
                    print(f"ì˜¤ë¥˜: {result.get('error')}")
                    
            asyncio.run(test_file())
        else:
            print("âŒ --test-file ì˜µì…˜ì— íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
    else:
        asyncio.run(main())
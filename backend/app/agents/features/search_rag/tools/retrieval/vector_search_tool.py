"""
Vector Search Tool - ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì „ìš© ë„êµ¬
pgvectorë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
"""
import asyncio
import uuid
from typing import List, Optional, Dict, Any
from datetime import datetime
from loguru import logger
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

try:
    from langchain_core.tools import BaseTool
except ImportError:
    from langchain_core.tools import BaseTool

from app.core.contracts import (
    SearchToolResult, SearchChunk, ToolMetrics
)
from app.services.core.embedding_service import embedding_service


class VectorSearchTool(BaseTool):
    """
    ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ë„êµ¬
    
    ì±…ì„:
    - ì§ˆì˜ ì„ë² ë”© ìƒì„±
    - pgvector <=> ì—°ì‚°ìœ¼ë¡œ í›„ë³´ ê²€ìƒ‰
    - ìœ ì‚¬ë„ threshold í•„í„°ë§
    
    ì±…ì„ ì—†ìŒ:
    - í‚¤ì›Œë“œ ê²€ìƒ‰ (KeywordSearchTool)
    - ì¬ë­í‚¹ (RerankTool)
    - ì¤‘ë³µ ì œê±° (DeduplicateTool)
    - ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ContextBuilderTool)
    """
    name: str = "vector_search"
    description: str = """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì§ˆì˜ì˜ ì˜ë¯¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ í›„ 
pgvectorë¡œ ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸ì´ë‚˜ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì— ì í•©í•©ë‹ˆë‹¤."""
    version: str = "1.0.0"
    
    async def _arun(
        self,
        query: str,
        db_session: AsyncSession,
        query_embedding: Optional[List[float]] = None,
        top_k: int = 20,
        similarity_threshold: float = 0.2,  # 0.25 â†’ 0.2ë¡œ ë‚®ì¶¤ (ì¼ë°˜ RAG adaptive 0.25ì™€ ìœ ì‚¬)
        container_ids: Optional[List[str]] = None,
        document_ids: Optional[List[str]] = None,
        user_emp_no: Optional[str] = None,
        **kwargs
    ) -> SearchToolResult:
        """
        ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            db_session: DB ì„¸ì…˜
            query_embedding: ì§ˆì˜ ì„ë² ë”© (Noneì´ë©´ ìë™ ìƒì„±)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0~1.0)
            container_ids: ê²€ìƒ‰ ëŒ€ìƒ ì»¨í…Œì´ë„ˆ ID ëª©ë¡
            document_ids: ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ID ëª©ë¡
            user_emp_no: ì‚¬ìš©ì ì‚¬ë²ˆ (ê¶Œí•œ í™•ì¸ìš©)
        """
        start_time = datetime.utcnow()
        trace_id = str(uuid.uuid4())
        
        try:
            # 1) ì„ë² ë”© ìƒì„± (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
            if query_embedding is None:
                embedding_start = datetime.utcnow()
                query_embedding = await embedding_service.get_embedding(query)
                embedding_ms = (datetime.utcnow() - embedding_start).total_seconds() * 1000
                logger.info(f"ğŸ” [VectorSearch] ì„ë² ë”© ìƒì„±: {embedding_ms:.1f}ms")
            
            if not query_embedding:
                return SearchToolResult(
                    success=False,
                    data=[],
                    total_found=0,
                    filtered_count=0,
                    search_params={},
                    metrics=ToolMetrics(latency_ms=0, provider="internal", trace_id=trace_id),
                    errors=["ì„ë² ë”© ìƒì„± ì‹¤íŒ¨"],
                    trace_id=trace_id,
                    tool_name=self.name,
                    tool_version=self.version
                )
            
            # 2) SQL ì¿¼ë¦¬ êµ¬ì„±
            embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"
            
            # ê¸°ë³¸ ì¿¼ë¦¬
            # 3) ê¸°ë³¸ ì¿¼ë¦¬
            sql_parts = [
                """
                SELECT 
                    dc.chunk_id,
                    dc.file_bss_info_sno as file_id,
                    dc.content_text as content,
                    dc.chunk_index,
                    dc.token_count,
                    1 - (de.vector <=> :embedding) as similarity_score,
                    fbi.file_lgc_nm as file_name,
                    fbi.path as file_path,
                    fbi.file_extsn as file_ext
                FROM doc_embedding de
                INNER JOIN doc_chunk dc ON de.chunk_id = dc.chunk_id
                LEFT JOIN tb_file_bss_info fbi ON dc.file_bss_info_sno = fbi.file_bss_info_sno
                WHERE de.modality = 'text'
                AND fbi.del_yn = 'N'
                """
            ]
            
            params: Dict[str, Any] = {"embedding": embedding_str}
            
            # ì»¨í…Œì´ë„ˆ ID í•„í„°
            if container_ids:
                # container_idsë¥¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (knowledge_container_idëŠ” String(50))
                normalized_container_ids = [str(c) for c in container_ids]
                sql_parts.append("AND fbi.knowledge_container_id = ANY(:container_ids)")
                params["container_ids"] = normalized_container_ids
            
            if document_ids:
                # ë¬¸ì„œ IDê°€ str/int í˜¼ìš© ê°€ëŠ¥í•˜ë¯€ë¡œ ì •ê·œí™”
                normalized_doc_ids = [
                    int(d) if isinstance(d, str) and d.isdigit() else d 
                    for d in document_ids
                ]
                sql_parts.append("AND dc.file_bss_info_sno = ANY(:document_ids)")
                params["document_ids"] = normalized_doc_ids
            
            # ê¶Œí•œ í™•ì¸ (user_emp_no ì œê³µ ì‹œ)
            if user_emp_no:
                sql_parts.append("""
                    AND fbi.knowledge_container_id IN (
                        SELECT DISTINCT up.container_id
                        FROM tb_user_permissions up
                        WHERE up.user_emp_no = :emp_no
                        AND up.is_active = true
                        AND (up.expires_date IS NULL OR up.expires_date > NOW())
                    )
                """)
                params["emp_no"] = user_emp_no
            
            # 4) ìœ ì‚¬ë„ í•„í„° ë° ì •ë ¬
            sql_parts.append("""
                AND (1 - (de.vector <=> :embedding)) >= :threshold
                ORDER BY de.vector <=> :embedding
                LIMIT :limit
            """)
            params["threshold"] = similarity_threshold
            params["limit"] = top_k
            
            # 5) ì¿¼ë¦¬ ì‹¤í–‰
            full_query = " ".join(sql_parts)
            result = await db_session.execute(text(full_query), params)
            rows = result.fetchall()
            
            # 6) ê²°ê³¼ ë³€í™˜
            chunks = []
            for row in rows:
                chunk = SearchChunk(
                    chunk_id=str(row.chunk_id),
                    content=row.content or "",
                    score=float(row.similarity_score),
                    file_id=str(row.file_id),
                    match_type="vector",
                    metadata={
                        "chunk_index": row.chunk_index,
                        "token_count": row.token_count,
                        "file_name": row.file_name,
                        "file_path": row.file_path,
                        "file_ext": row.file_ext,
                        "search_method": "pgvector_cosine"
                    }
                )
                chunks.append(chunk)
            
            # 7) ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(
                f"âœ… [VectorSearch] ì™„ë£Œ: {len(chunks)}ê°œ ë°œê²¬, "
                f"threshold={similarity_threshold}, latency={latency_ms:.1f}ms"
            )
            
            return SearchToolResult(
                success=True,
                data=chunks,
                total_found=len(chunks),
                filtered_count=len(chunks),
                search_params={
                    "query": query[:100],
                    "top_k": top_k,
                    "threshold": similarity_threshold,
                    "container_ids": container_ids,
                    "document_ids": document_ids
                },
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="internal",
                    cache_hit=False,
                    retries=0
                ),
                errors=[],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        except Exception as e:
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            logger.error(f"âŒ [VectorSearch] ì‹¤íŒ¨: {e}", exc_info=True)
            
            return SearchToolResult(
                success=False,
                data=[],
                total_found=0,
                filtered_count=0,
                search_params={},
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="internal",
                    cache_hit=False,
                    retries=0
                ),
                errors=[str(e)],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
    
    def _run(self, **kwargs) -> SearchToolResult:
        """ë™ê¸° ì‹¤í–‰ (í´ë°±)"""
        try:
            return asyncio.run(self._arun(**kwargs))
        except RuntimeError as e:
            logger.error(f"âŒ [VectorSearch] ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return SearchToolResult(
                success=False,
                data=[],
                total_found=0,
                filtered_count=0,
                search_params={},
                metrics=ToolMetrics(latency_ms=0, provider="internal"),
                errors=["RuntimeError: use _arun in async context"],
                trace_id=str(uuid.uuid4()),
                tool_name=self.name,
                tool_version=self.version
            )
    
    def validate_input(self, **kwargs) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        if "query" not in kwargs or not kwargs["query"]:
            return False
        if "db_session" not in kwargs:
            return False
        return True


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
vector_search_tool = VectorSearchTool()

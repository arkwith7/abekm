"""
Keyword Search Tool - í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ìƒ‰ ì „ìš© ë„êµ¬
ILIKE/regex ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
"""
import asyncio
import uuid
from typing import List, Optional, Dict, Any
from datetime import datetime
from loguru import logger
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

try:
    from langchain_core.tools import BaseTool
except ImportError:
    from langchain_core.tools import BaseTool

from app.core.contracts import SearchToolResult, SearchChunk, ToolMetrics


class KeywordSearchTool(BaseTool):
    """
    í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ìƒ‰ ë„êµ¬
    
    ì±…ì„:
    - í‚¤ì›Œë“œ ì¶”ì¶œ (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ê³µë°± ê¸°ì¤€ ë¶„ë¦¬)
    - ILIKE ê¸°ë°˜ ë¶€ë¶„ ë§¤ì¹­
    - ë§¤ì¹­ ë¹ˆë„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
    
    ì±…ì„ ì—†ìŒ:
    - í˜•íƒœì†Œ ë¶„ì„ (í˜¸ì¶œìê°€ ì œê³µ)
    - ë²¡í„° ê²€ìƒ‰
    - ì¬ë­í‚¹
    """
    name: str = "keyword_search"
    description: str = """í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì§ˆì˜ì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œë¥¼ ë¬¸ì„œ ë‚´ìš©ê³¼ 
ì§ì ‘ ë§¤ì¹­í•©ë‹ˆë‹¤. ê³ ìœ ëª…ì‚¬, íŠ¹ì • ìš©ì–´ ê²€ìƒ‰ì— ì í•©í•©ë‹ˆë‹¤."""
    version: str = "1.0.0"
    
    async def _arun(
        self,
        query: str,
        db_session: AsyncSession,
        keywords: Optional[List[str]] = None,
        top_k: int = 20,
        container_ids: Optional[List[str]] = None,
        document_ids: Optional[List[str]] = None,
        user_emp_no: Optional[str] = None,
        case_sensitive: bool = False,
        **kwargs
    ) -> SearchToolResult:
        """
        í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            db_session: DB ì„¸ì…˜
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ëª©ë¡ (Noneì´ë©´ ìë™ ì¶”ì¶œ)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            container_ids: ê²€ìƒ‰ ëŒ€ìƒ ì»¨í…Œì´ë„ˆ
            document_ids: ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ
            user_emp_no: ì‚¬ìš©ì ì‚¬ë²ˆ
            case_sensitive: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—¬ë¶€
        """
        start_time = datetime.utcnow()
        trace_id = str(uuid.uuid4())
        
        try:
            # 1) í‚¤ì›Œë“œ ì¶”ì¶œ (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ë‹¨ìˆœ ë¶„ë¦¬)
            if keywords is None:
                keywords = [w.strip() for w in query.split() if len(w.strip()) >= 2]
            
            if not keywords:
                return SearchToolResult(
                    success=False,
                    data=[],
                    total_found=0,
                    filtered_count=0,
                    metrics=ToolMetrics(latency_ms=0, provider="internal"),
                    errors=["í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤"],
                    trace_id=trace_id,
                    tool_name=self.name,
                    tool_version=self.version
                )
            
            logger.info(f"ğŸ” [KeywordSearch] í‚¤ì›Œë“œ: {keywords}")
            
            # ìµœì†Œ ë§¤ì¹­ ê°œìˆ˜ ì„¤ì • (í‚¤ì›Œë“œê°€ 2ê°œ ì´ìƒì´ë©´ ìµœì†Œ 2ê°œ, ì•„ë‹ˆë©´ 1ê°œ)
            min_match_count = 2 if len(keywords) >= 2 else 1
            logger.info(f"   - ìµœì†Œ ë§¤ì¹­ ì¡°ê±´: {min_match_count}ê°œ ì´ìƒ")
            
            # 2) SQL ì¿¼ë¦¬ êµ¬ì„± (ILIKE ê¸°ë°˜)
            # ê° í‚¤ì›Œë“œì— ëŒ€í•´ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            keyword_conditions = []
            for i, kw in enumerate(keywords[:10]):  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ
                if case_sensitive:
                    keyword_conditions.append(
                        f"(CASE WHEN dc.content_text LIKE :kw{i} THEN 1 ELSE 0 END)"
                    )
                else:
                    keyword_conditions.append(
                        f"(CASE WHEN LOWER(dc.content_text) LIKE LOWER(:kw{i}) THEN 1 ELSE 0 END)"
                    )
            
            match_score_expr = " + ".join(keyword_conditions)
            
            sql_parts = [
                f"""
                SELECT 
                    dc.chunk_id,
                    dc.file_bss_info_sno as file_id,
                    dc.content_text as content,
                    dc.chunk_index,
                    dc.token_count,
                    ({match_score_expr})::float / :total_keywords as similarity_score,
                    fbi.file_lgc_nm as file_name,
                    fbi.path as file_path,
                    fbi.file_extsn as file_ext
                FROM doc_chunk dc
                LEFT JOIN tb_file_bss_info fbi ON dc.file_bss_info_sno = fbi.file_bss_info_sno
                WHERE ({match_score_expr}) >= :min_match_count
                AND fbi.del_yn = 'N'
                """
            ]
            
            params: Dict[str, Any] = {
                "total_keywords": len(keywords),
                "min_match_count": min_match_count
            }
            for i, kw in enumerate(keywords[:10]):
                params[f"kw{i}"] = f"%{kw}%"
            
            # 3) í•„í„° ì¡°ê±´
            if container_ids:
                # container_idsë¥¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (knowledge_container_idëŠ” String(50))
                normalized_container_ids = [str(c) for c in container_ids]
                sql_parts.append("AND fbi.knowledge_container_id = ANY(:container_ids)")
                params["container_ids"] = normalized_container_ids
            
            if document_ids:
                normalized_doc_ids = [
                    int(d) if isinstance(d, str) and d.isdigit() else d 
                    for d in document_ids
                ]
                sql_parts.append("AND dc.file_bss_info_sno = ANY(:document_ids)")
                params["document_ids"] = normalized_doc_ids
            
            # ê¶Œí•œ í™•ì¸
            if user_emp_no:
                sql_parts.append("""
                    AND fbi.knowledge_container_id IN (
                        SELECT DISTINCT up.container_id
                        FROM tb_user_permissions up
                        WHERE up.user_emp_no = :emp_no
                        AND up.is_active = true
                        AND (up.expires_date IS NULL OR up.expires_date > NOW())
                    )
                """)
                params["emp_no"] = user_emp_no
            
            # 4) ì •ë ¬ ë° ì œí•œ
            sql_parts.append(f"""
                ORDER BY ({match_score_expr}) DESC, dc.chunk_index
                LIMIT :limit
            """)
            params["limit"] = top_k
            
            # 5) ì¿¼ë¦¬ ì‹¤í–‰
            full_query = " ".join(sql_parts)
            result = await db_session.execute(text(full_query), params)
            rows = result.fetchall()
            
            # 6) ê²°ê³¼ ë³€í™˜
            chunks = []
            for row in rows:
                chunk = SearchChunk(
                    chunk_id=str(row.chunk_id),
                    file_id=str(row.file_id),
                    content=row.content or "",
                    score=float(row.similarity_score),
                    match_type="keyword",
                    metadata={
                        "chunk_index": row.chunk_index,
                        "token_count": row.token_count,
                        "file_name": row.file_name,
                        "file_path": row.file_path,
                        "file_ext": row.file_ext,
                        "matched_keywords": keywords,
                        "search_method": "ilike_matching"
                    }
                )
                chunks.append(chunk)
            
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(
                f"âœ… [KeywordSearch] ì™„ë£Œ: {len(chunks)}ê°œ ë°œê²¬, "
                f"keywords={len(keywords)}, latency={latency_ms:.1f}ms"
            )
            
            return SearchToolResult(
                success=True,
                data=chunks,
                total_found=len(chunks),
                filtered_count=len(chunks),
                search_params={
                    "query": query[:100],
                    "keywords": keywords,
                    "top_k": top_k,
                    "case_sensitive": case_sensitive
                },
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="internal",
                    cache_hit=False,
                    retries=0
                ),
                errors=[],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        except Exception as e:
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            logger.error(f"âŒ [KeywordSearch] ì‹¤íŒ¨: {e}", exc_info=True)
            
            return SearchToolResult(
                success=False,
                data=[],
                total_found=0,
                filtered_count=0,
                search_params={},
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="internal"
                ),
                errors=[str(e)],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
    
    def _run(self, **kwargs) -> SearchToolResult:
        """ë™ê¸° ì‹¤í–‰"""
        try:
            return asyncio.run(self._arun(**kwargs))
        except RuntimeError:
            return SearchToolResult(
                success=False,
                data=[],
                total_found=0,
                filtered_count=0,
                search_params={},
                metrics=ToolMetrics(latency_ms=0, provider="internal"),
                errors=["use _arun in async context"],
                trace_id=str(uuid.uuid4()),
                tool_name=self.name,
                tool_version=self.version
            )
    
    def validate_input(self, **kwargs) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        return "query" in kwargs and "db_session" in kwargs


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
keyword_search_tool = KeywordSearchTool()

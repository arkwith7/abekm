"""
Context Builder Tool - ì»¨í…ìŠ¤íŠ¸ í† í° íŒ¨í‚¹ ë° ìµœì í™” ë„êµ¬
"""
import uuid
from typing import List, Optional
from datetime import datetime
from loguru import logger

try:
    from langchain_core.tools import BaseTool
except ImportError:
    from langchain_core.tools import BaseTool

from app.core.contracts import ContextResult, SearchChunk, ToolMetrics


class ContextBuilderTool(BaseTool):
    """
    ì»¨í…ìŠ¤íŠ¸ ë¹Œë” ë„êµ¬
    
    ì±…ì„:
    - ì²­í¬ë¥¼ í† í° ì œí•œ ë‚´ì—ì„œ íŒ¨í‚¹
    - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²­í¬ ì„ íƒ
    - í¬ë§·íŒ… (citation í¬í•¨)
    
    ì±…ì„ ì—†ìŒ:
    - LLM í˜¸ì¶œ
    - ë‹µë³€ ìƒì„±
    """
    name: str = "context_builder"
    description: str = """ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ LLM ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤. 
í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ìœ¼ë¡œ ì²­í¬ë¥¼ ì„ íƒí•˜ê³  í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    version: str = "1.0.0"
    
    def _estimate_tokens(self, text: str) -> int:
        """
        ê°„ë‹¨í•œ í† í° ì¶”ì • (ì‹¤ì œ í† í¬ë‚˜ì´ì € ëŒ€ì‹ )
        í•œê¸€: 1.5ê¸€ìë‹¹ 1í† í°, ì˜ì–´: 4ê¸€ìë‹¹ 1í† í°
        """
        korean_chars = len([c for c in text if '\uac00' <= c <= '\ud7a3'])
        other_chars = len(text) - korean_chars
        
        tokens = int(korean_chars / 1.5) + int(other_chars / 4)
        return max(tokens, len(text) // 4)  # ìµœì†Œ ë³´ì¥
    
    async def _arun(
        self,
        chunks: List[SearchChunk],
        max_tokens: int = 4000,
        include_metadata: bool = True,
        format_style: str = "citation",
        priority_by: str = "similarity",
        **kwargs
    ) -> ContextResult:
        """
        ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ ì‹¤í–‰
        
        Args:
            chunks: ì…ë ¥ ì²­í¬ ëª©ë¡
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€
            format_style: í¬ë§· ìŠ¤íƒ€ì¼ (citation/plain/numbered)
            priority_by: ìš°ì„ ìˆœìœ„ ê¸°ì¤€ (similarity/position/hybrid)
        """
        start_time = datetime.utcnow()
        trace_id = str(uuid.uuid4())
        
        try:
            if not chunks:
                return ContextResult(
                    success=True,
                    data="",
                    used_chunks=[],
                    total_tokens=0,
                    chunks_included=0,
                    chunks_truncated=0,
                    metrics=ToolMetrics(latency_ms=0, provider="internal"),
                    errors=[],
                    trace_id=trace_id,
                    tool_name=self.name,
                    tool_version=self.version
                )
            
            logger.info(f"ğŸ”§ [ContextBuilder] ì…ë ¥: {len(chunks)}ê°œ, max_tokens={max_tokens}")
            
            # 1) ìš°ì„ ìˆœìœ„ ì •ë ¬
            if priority_by == "similarity":
                sorted_chunks = sorted(chunks, key=lambda x: x.similarity_score, reverse=True)
            elif priority_by == "position":
                sorted_chunks = sorted(chunks, key=lambda x: x.metadata.get("chunk_index", 999))
            else:  # hybrid
                sorted_chunks = sorted(
                    chunks,
                    key=lambda x: (x.similarity_score * 0.7 + (1.0 - x.metadata.get("chunk_index", 0) / 1000) * 0.3),
                    reverse=True
                )
            
            # 2) í† í° ì œí•œ ë‚´ì—ì„œ íŒ¨í‚¹
            context_parts = []
            used_chunks = []
            total_tokens = 0
            truncated_count = 0
            
            # í—¤ë” í† í° ì˜ˆì•½ (ì•½ 100 í† í°)
            reserved_tokens = 100
            available_tokens = max_tokens - reserved_tokens
            
            for i, chunk in enumerate(sorted_chunks):
                # í¬ë§·íŒ…
                if format_style == "citation":
                    chunk_text = f"[{i+1}] {chunk.content}"
                    if include_metadata:
                        source = chunk.metadata.get("file_name", "Unknown")
                        chunk_text += f"\n(ì¶œì²˜: {source})"
                elif format_style == "numbered":
                    chunk_text = f"{i+1}. {chunk.content}"
                else:  # plain
                    chunk_text = chunk.content
                
                chunk_tokens = self._estimate_tokens(chunk_text)
                
                if total_tokens + chunk_tokens <= available_tokens:
                    context_parts.append(chunk_text)
                    used_chunks.append(chunk)
                    total_tokens += chunk_tokens
                else:
                    truncated_count += 1
                    logger.debug(f"   - í† í° ì œí•œìœ¼ë¡œ ì²­í¬ {i+1} ìƒëµ")
            
            # 3) ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            if format_style == "citation":
                context_text = "## ì°¸ê³  ë¬¸ì„œ\n\n" + "\n\n".join(context_parts)
            else:
                context_text = "\n\n".join(context_parts)
            
            # 4) ìµœì¢… í† í° ê³„ì‚°
            final_tokens = self._estimate_tokens(context_text)
            
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            logger.info(
                f"âœ… [ContextBuilder] ì™„ë£Œ: {len(used_chunks)}/{len(chunks)}ê°œ í¬í•¨, "
                f"{final_tokens}í† í°, latency={latency_ms:.1f}ms"
            )
            
            return ContextResult(
                success=True,
                data=context_text,
                used_chunks=used_chunks,
                total_tokens=final_tokens,
                chunks_included=len(used_chunks),
                chunks_truncated=truncated_count,
                metrics=ToolMetrics(
                    latency_ms=latency_ms,
                    provider="internal",
                    tokens_used=final_tokens
                ),
                errors=[],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
            
        except Exception as e:
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            logger.error(f"âŒ [ContextBuilder] ì‹¤íŒ¨: {e}", exc_info=True)
            return ContextResult(
                success=False,
                data="",
                used_chunks=[],
                total_tokens=0,
                chunks_included=0,
                chunks_truncated=0,
                metrics=ToolMetrics(latency_ms=latency_ms, provider="internal"),
                errors=[str(e)],
                trace_id=trace_id,
                tool_name=self.name,
                tool_version=self.version
            )
    
    def _run(self, **kwargs) -> ContextResult:
        import asyncio
        try:
            return asyncio.run(self._arun(**kwargs))
        except RuntimeError:
            return ContextResult(
                success=False, data="", used_chunks=[], total_tokens=0,
                chunks_included=0, chunks_truncated=0,
                metrics=ToolMetrics(latency_ms=0, provider="internal"),
                errors=["use _arun"], trace_id=str(uuid.uuid4()),
                tool_name=self.name, tool_version=self.version
            )
    
    def validate_input(self, **kwargs) -> bool:
        return "chunks" in kwargs and isinstance(kwargs["chunks"], list)


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
context_builder_tool = ContextBuilderTool()

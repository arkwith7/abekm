"""Template PPT Comparator Tool - Compares generated PPT with template for quality validation."""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Type

from loguru import logger
from pptx import Presentation
from pydantic import BaseModel, Field

try:
    from langchain_core.tools import BaseTool
except ImportError:
    from langchain_core.tools import BaseTool


class TemplatePPTComparatorInput(BaseModel):
    """Input schema for TemplatePPTComparatorTool."""

    generated_pptx_path: str = Field(..., description="ìƒì„±ëœ PPTX íŒŒì¼ ê²½ë¡œ")
    template_pptx_path: str = Field(..., description="í…œí”Œë¦¿ PPTX íŒŒì¼ ê²½ë¡œ")
    template_metadata_path: Optional[str] = Field(
        default=None, description="í…œí”Œë¦¿ ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ"
    )
    expected_content: Optional[Dict] = Field(
        default=None, description="ê¸°ëŒ€ë˜ëŠ” ì½˜í…ì¸  ì •ë³´ (outline)"
    )


class SlideComparison(BaseModel):
    """Single slide comparison result."""

    slide_index: int
    layout_match: bool
    shape_count_match: bool
    text_replaced_count: int
    text_unchanged_count: int
    table_issues: List[str]
    image_issues: List[str]
    issues: List[str]
    warnings: List[str]


class TableIssue(BaseModel):
    """Table-specific issue."""

    slide_index: int
    table_name: str
    issue_type: str  # "unchanged", "partial", "missing_data"
    description: str
    template_sample: str
    generated_sample: str


class TextIssue(BaseModel):
    """Text-specific issue."""

    slide_index: int
    shape_name: str
    issue_type: str  # "unchanged", "placeholder"
    template_text: str
    generated_text: str


class ComparisonReport(BaseModel):
    """Complete comparison report."""

    overall_quality_score: float  # 0-100
    total_slides: int
    slides_with_issues: int
    critical_issues: List[str]
    warnings: List[str]
    slide_comparisons: List[SlideComparison]
    table_issues: List[TableIssue]
    text_issues: List[TextIssue]
    recommendations: List[str]
    passed: bool


class TemplatePPTComparatorTool(BaseTool):
    """
    Compares generated PPT with template to validate content replacement.

    This tool performs:
    1. Structure Validation: Ensures slide count, layout, and shape structure match
    2. Content Replacement Check: Verifies all template content was replaced
    3. Table Data Validation: Checks if table data was properly replaced
    4. Text Placeholder Check: Identifies unchanged template text
    5. Quality Scoring: Generates pass/fail assessment with detailed issues

    Returns actionable recommendations for the AI agent to fix issues.
    """

    name: str = "template_ppt_comparator_tool"
    description: str = (
        "Compares generated PPT with template PPT to validate content replacement quality. "
        "Detects unchanged template data (especially tables), placeholder text not replaced, "
        "and structural mismatches. Returns detailed issue report with recommendations."
    )
    args_schema: Type[BaseModel] = TemplatePPTComparatorInput

    def _run(
        self,
        generated_pptx_path: str,
        template_pptx_path: str,
        template_metadata_path: Optional[str] = None,
        expected_content: Optional[Dict] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """Synchronous run."""
        import asyncio

        return asyncio.run(
            self._arun(
                generated_pptx_path=generated_pptx_path,
                template_pptx_path=template_pptx_path,
                template_metadata_path=template_metadata_path,
                expected_content=expected_content,
                **kwargs,
            )
        )

    async def _arun(
        self,
        generated_pptx_path: str,
        template_pptx_path: str,
        template_metadata_path: Optional[str] = None,
        expected_content: Optional[Dict] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Compare generated PPT with template asynchronously.

        Args:
            generated_pptx_path: Path to generated PPTX file
            template_pptx_path: Path to template PPTX file
            template_metadata_path: Optional path to template metadata JSON
            expected_content: Optional expected content structure

        Returns:
            Comparison report with pass/fail status and recommendations
        """
        logger.info(
            f"ğŸ” [TemplatePPTComparator] ë¹„êµ ì‹œì‘\n"
            f"  ìƒì„± íŒŒì¼: {generated_pptx_path}\n"
            f"  í…œí”Œë¦¿: {template_pptx_path}"
        )

        try:
            # 1. Load presentations
            gen_path = Path(generated_pptx_path)
            tmp_path = Path(template_pptx_path)

            if not gen_path.exists():
                raise FileNotFoundError(f"ìƒì„± íŒŒì¼ ì—†ìŒ: {generated_pptx_path}")
            if not tmp_path.exists():
                raise FileNotFoundError(f"í…œí”Œë¦¿ íŒŒì¼ ì—†ìŒ: {template_pptx_path}")

            generated = Presentation(str(gen_path))
            template = Presentation(str(tmp_path))

            # 2. Load metadata if provided
            metadata = None
            if template_metadata_path:
                meta_path = Path(template_metadata_path)
                if meta_path.exists():
                    with open(meta_path, "r", encoding="utf-8") as f:
                        metadata = json.load(f)
                    logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ")

            # 3. Compare slides
            slide_comparisons = []
            table_issues = []
            text_issues = []

            for idx in range(min(len(generated.slides), len(template.slides))):
                comparison = self._compare_slide(
                    generated.slides[idx],
                    template.slides[idx],
                    idx,
                    metadata,
                )
                slide_comparisons.append(comparison)

                # Collect detailed issues
                slide_table_issues, slide_text_issues = self._extract_detailed_issues(
                    generated.slides[idx], template.slides[idx], idx
                )
                table_issues.extend(slide_table_issues)
                text_issues.extend(slide_text_issues)

            # 4. Identify critical issues
            critical_issues = self._identify_critical_issues(
                generated, template, slide_comparisons, table_issues, text_issues
            )

            # 5. Generate warnings
            warnings = self._generate_warnings(slide_comparisons, text_issues)

            # 6. Calculate quality score
            quality_score = self._calculate_quality_score(
                slide_comparisons, table_issues, text_issues
            )

            # 7. Generate recommendations
            recommendations = self._generate_recommendations(
                critical_issues, warnings, table_issues, text_issues
            )

            # 8. Determine pass/fail
            passed = len(critical_issues) == 0 and quality_score >= 70.0

            report = ComparisonReport(
                overall_quality_score=quality_score,
                total_slides=len(generated.slides),
                slides_with_issues=len(
                    [s for s in slide_comparisons if len(s.issues) > 0]
                ),
                critical_issues=critical_issues,
                warnings=warnings,
                slide_comparisons=slide_comparisons,
                table_issues=table_issues,
                text_issues=text_issues,
                recommendations=recommendations,
                passed=passed,
            )

            logger.info(
                f"âœ… [TemplatePPTComparator] ë¹„êµ ì™„ë£Œ\n"
                f"  í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}/100\n"
                f"  ê²°ê³¼: {'âœ… PASS' if passed else 'âŒ FAIL'}\n"
                f"  ì¹˜ëª…ì  ë¬¸ì œ: {len(critical_issues)}ê°œ\n"
                f"  ê²½ê³ : {len(warnings)}ê°œ"
            )

            return {
                "success": True,
                "report": report.model_dump(),
                "passed": passed,
                "quality_score": quality_score,
                "critical_issues_count": len(critical_issues),
                "warnings_count": len(warnings),
                "summary": self._generate_summary(report),
            }

        except Exception as e:
            logger.error(f"âŒ [TemplatePPTComparator] ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__,
            }

    def _compare_slide(
        self,
        gen_slide,
        tmp_slide,
        idx: int,
        metadata: Optional[Dict],
    ) -> SlideComparison:
        """Compare a single slide."""
        issues = []
        warnings = []
        table_issues = []
        image_issues = []

        # 1. Layout check
        layout_match = gen_slide.slide_layout.name == tmp_slide.slide_layout.name
        if not layout_match:
            issues.append(
                f"ë ˆì´ì•„ì›ƒ ë¶ˆì¼ì¹˜: '{tmp_slide.slide_layout.name}' â†’ '{gen_slide.slide_layout.name}'"
            )

        # 2. Shape count check
        shape_count_match = len(gen_slide.shapes) == len(tmp_slide.shapes)
        if not shape_count_match:
            issues.append(
                f"Shape ê°œìˆ˜ ë¶ˆì¼ì¹˜: {len(tmp_slide.shapes)} â†’ {len(gen_slide.shapes)}"
            )

        # 3. Text replacement check
        text_replaced = 0
        text_unchanged = 0

        for gen_shape, tmp_shape in zip(gen_slide.shapes, tmp_slide.shapes):
            if hasattr(gen_shape, "text_frame") and gen_shape.text_frame:
                gen_text = gen_shape.text_frame.text.strip()
                tmp_text = tmp_shape.text_frame.text.strip()

                if gen_text and tmp_text:
                    if gen_text != tmp_text:
                        text_replaced += 1
                    else:
                        text_unchanged += 1
                        # ê¸´ í…ìŠ¤íŠ¸ê°€ ë™ì¼í•˜ë©´ ê²½ê³ 
                        if len(gen_text) > 15 and not self._is_template_metadata(
                            gen_text
                        ):
                            warnings.append(
                                f"[{gen_shape.name}] í…œí”Œë¦¿ í…ìŠ¤íŠ¸ ë¯¸êµì²´: '{gen_text[:50]}...'"
                            )

            # 4. Table check
            if hasattr(gen_shape, "table") and hasattr(tmp_shape, "table"):
                table_identical = self._compare_tables(
                    gen_shape.table, tmp_shape.table
                )
                if table_identical:
                    table_issues.append(
                        f"[{gen_shape.name}] í…Œì´ë¸” ë°ì´í„°ê°€ í…œí”Œë¦¿ê³¼ ë™ì¼"
                    )
                    issues.append(
                        f"í…Œì´ë¸” ë°ì´í„° ë¯¸êµì²´: {gen_shape.name}"
                    )

        return SlideComparison(
            slide_index=idx + 1,
            layout_match=layout_match,
            shape_count_match=shape_count_match,
            text_replaced_count=text_replaced,
            text_unchanged_count=text_unchanged,
            table_issues=table_issues,
            image_issues=image_issues,
            issues=issues,
            warnings=warnings,
        )

    def _compare_tables(self, gen_table, tmp_table) -> bool:
        """Check if two tables are identical."""
        if len(gen_table.rows) != len(tmp_table.rows):
            return False
        if len(gen_table.columns) != len(tmp_table.columns):
            return False

        # Compare first 3 rows (enough to detect template data)
        for r_idx in range(min(3, len(gen_table.rows))):
            for c_idx in range(len(gen_table.columns)):
                gen_cell = gen_table.cell(r_idx, c_idx).text.strip()
                tmp_cell = tmp_table.cell(r_idx, c_idx).text.strip()

                if gen_cell != tmp_cell:
                    return False

        return True

    def _is_template_metadata(self, text: str) -> bool:
        """Check if text is template metadata that should remain unchanged."""
        metadata_patterns = [
            "Company or Team Name",
            "Logo",
            "THANK YOU",
            "FOR WATCHING",
        ]
        return any(pattern.lower() in text.lower() for pattern in metadata_patterns)

    def _extract_detailed_issues(
        self, gen_slide, tmp_slide, idx: int
    ) -> tuple[List[TableIssue], List[TextIssue]]:
        """Extract detailed issues for tables and text."""
        table_issues = []
        text_issues = []

        for gen_shape, tmp_shape in zip(gen_slide.shapes, tmp_slide.shapes):
            # Table issues
            if hasattr(gen_shape, "table") and hasattr(tmp_shape, "table"):
                gen_table = gen_shape.table
                tmp_table = tmp_shape.table

                if self._compare_tables(gen_table, tmp_table):
                    # Extract sample data
                    tmp_sample = self._get_table_sample(tmp_table)
                    gen_sample = self._get_table_sample(gen_table)

                    table_issues.append(
                        TableIssue(
                            slide_index=idx + 1,
                            table_name=gen_shape.name,
                            issue_type="unchanged",
                            description="í…Œì´ë¸” ë°ì´í„°ê°€ í…œí”Œë¦¿ê³¼ ì™„ì „íˆ ë™ì¼í•©ë‹ˆë‹¤",
                            template_sample=tmp_sample,
                            generated_sample=gen_sample,
                        )
                    )

            # Text issues
            if hasattr(gen_shape, "text_frame") and gen_shape.text_frame:
                gen_text = gen_shape.text_frame.text.strip()
                tmp_text = tmp_shape.text_frame.text.strip()

                if (
                    gen_text
                    and tmp_text
                    and gen_text == tmp_text
                    and len(gen_text) > 15
                    and not self._is_template_metadata(gen_text)
                ):
                    text_issues.append(
                        TextIssue(
                            slide_index=idx + 1,
                            shape_name=gen_shape.name,
                            issue_type="unchanged",
                            template_text=tmp_text[:100],
                            generated_text=gen_text[:100],
                        )
                    )

        return table_issues, text_issues

    def _get_table_sample(self, table) -> str:
        """Get sample data from table (first 2 rows)."""
        samples = []
        for r_idx in range(min(2, len(table.rows))):
            row_data = []
            for c_idx in range(len(table.columns)):
                cell_text = table.cell(r_idx, c_idx).text.strip()
                row_data.append(cell_text[:20] if cell_text else "(empty)")
            samples.append(" | ".join(row_data))
        return "\n".join(samples)

    def _identify_critical_issues(
        self,
        generated,
        template,
        slide_comparisons: List[SlideComparison],
        table_issues: List[TableIssue],
        text_issues: List[TextIssue],
    ) -> List[str]:
        """Identify critical issues that require fixing."""
        critical = []

        # 1. Slide count mismatch
        if len(generated.slides) != len(template.slides):
            critical.append(
                f"ìŠ¬ë¼ì´ë“œ ìˆ˜ ë¶ˆì¼ì¹˜: í…œí”Œë¦¿({len(template.slides)}) vs ìƒì„±({len(generated.slides)})"
            )

        # 2. Table data not replaced
        if table_issues:
            critical.append(
                f"í…Œì´ë¸” ë°ì´í„° ë¯¸êµì²´: {len(table_issues)}ê°œ í…Œì´ë¸”ì—ì„œ í…œí”Œë¦¿ ë°ì´í„° ê·¸ëŒ€ë¡œ ìœ ì§€ë¨"
            )

        # 3. Layout mismatches
        layout_mismatches = [s for s in slide_comparisons if not s.layout_match]
        if layout_mismatches:
            critical.append(
                f"ë ˆì´ì•„ì›ƒ ë¶ˆì¼ì¹˜: {len(layout_mismatches)}ê°œ ìŠ¬ë¼ì´ë“œ"
            )

        # 4. Shape count mismatches
        shape_mismatches = [s for s in slide_comparisons if not s.shape_count_match]
        if shape_mismatches:
            critical.append(
                f"Shape êµ¬ì¡° ë³€ê²½: {len(shape_mismatches)}ê°œ ìŠ¬ë¼ì´ë“œì—ì„œ Shape ê°œìˆ˜ ë¶ˆì¼ì¹˜"
            )

        return critical

    def _generate_warnings(
        self,
        slide_comparisons: List[SlideComparison],
        text_issues: List[TextIssue],
    ) -> List[str]:
        """Generate warning messages."""
        warnings = []

        # 1. Unchanged text
        if text_issues:
            warnings.append(
                f"í…ìŠ¤íŠ¸ ë¯¸êµì²´: {len(text_issues)}ê°œ í…ìŠ¤íŠ¸ê°€ í…œí”Œë¦¿ê³¼ ë™ì¼"
            )

        # 2. Low replacement rate
        total_replaced = sum(s.text_replaced_count for s in slide_comparisons)
        total_unchanged = sum(s.text_unchanged_count for s in slide_comparisons)

        if total_replaced + total_unchanged > 0:
            replacement_rate = (
                total_replaced / (total_replaced + total_unchanged)
            ) * 100
            if replacement_rate < 50:
                warnings.append(
                    f"ë‚®ì€ í…ìŠ¤íŠ¸ êµì²´ìœ¨: {replacement_rate:.1f}% (ê¸°ëŒ€: 70% ì´ìƒ)"
                )

        return warnings

    def _calculate_quality_score(
        self,
        slide_comparisons: List[SlideComparison],
        table_issues: List[TableIssue],
        text_issues: List[TextIssue],
    ) -> float:
        """Calculate overall quality score (0-100)."""
        score = 100.0

        # Deduct points for issues
        for slide in slide_comparisons:
            if not slide.layout_match:
                score -= 5.0
            if not slide.shape_count_match:
                score -= 5.0
            score -= len(slide.issues) * 3.0

        # Table issues are critical
        score -= len(table_issues) * 15.0

        # Text issues are warnings
        score -= len(text_issues) * 2.0

        return max(0.0, score)

    def _generate_recommendations(
        self,
        critical_issues: List[str],
        warnings: List[str],
        table_issues: List[TableIssue],
        text_issues: List[TextIssue],
    ) -> List[str]:
        """Generate actionable recommendations for the AI agent."""
        recommendations = []

        if critical_issues:
            recommendations.append(
                "ğŸ”´ ì¹˜ëª…ì  ë¬¸ì œ ë°œê²¬ - ì¦‰ì‹œ ìˆ˜ì • í•„ìš”:"
            )
            for issue in critical_issues:
                recommendations.append(f"  â€¢ {issue}")

        if table_issues:
            recommendations.append(
                "\nğŸ“Š í…Œì´ë¸” ë°ì´í„° ìˆ˜ì • ë°©ë²•:"
            )
            recommendations.append(
                "  1. í”„ë¡ íŠ¸ì—”ë“œì—ì„œ tableDataë¥¼ metadataì— í¬í•¨í•˜ì—¬ ì „ì†¡"
            )
            recommendations.append(
                "  2. ë°±ì—”ë“œ enhanced_object_processor.pyì˜ í…Œì´ë¸” ì²˜ë¦¬ ë¡œì§ í™•ì¸"
            )
            recommendations.append(
                "  3. ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° í…Œì´ë¸” ì…€ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”"
            )

            for issue in table_issues[:3]:  # Show first 3
                recommendations.append(
                    f"  â€¢ ìŠ¬ë¼ì´ë“œ {issue.slide_index} [{issue.table_name}]: {issue.description}"
                )

        if text_issues:
            recommendations.append(
                "\nğŸ“ í…ìŠ¤íŠ¸ êµì²´ í•„ìš”:"
            )
            for issue in text_issues[:5]:  # Show first 5
                recommendations.append(
                    f"  â€¢ ìŠ¬ë¼ì´ë“œ {issue.slide_index} [{issue.shape_name}]: '{issue.template_text[:50]}...'"
                )

        if not critical_issues and not warnings:
            recommendations.append(
                "âœ… í’ˆì§ˆ ê²€ì¦ í†µê³¼ - ë¬¸ì œ ì—†ìŒ"
            )

        return recommendations

    def _generate_summary(self, report: ComparisonReport) -> str:
        """Generate human-readable summary."""
        if report.passed:
            return (
                f"âœ… í’ˆì§ˆ ê²€ì¦ í†µê³¼ (ì ìˆ˜: {report.overall_quality_score:.1f}/100)\n"
                f"ëª¨ë“  ì½˜í…ì¸ ê°€ ì •ìƒì ìœ¼ë¡œ êµì²´ë˜ì—ˆìŠµë‹ˆë‹¤."
            )
        else:
            return (
                f"âŒ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨ (ì ìˆ˜: {report.overall_quality_score:.1f}/100)\n"
                f"ì¹˜ëª…ì  ë¬¸ì œ: {len(report.critical_issues)}ê°œ\n"
                f"ê²½ê³ : {len(report.warnings)}ê°œ\n"
                f"ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )


# Singleton instance
template_ppt_comparator_tool = TemplatePPTComparatorTool()

"""
Paper Search Agent - ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ ì—ì´ì „íŠ¸
ë™ì  ë„êµ¬ ì„ íƒê³¼ ì „ëµ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
"""
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger
from sqlalchemy.ext.asyncio import AsyncSession

from app.tools.contracts import (
    AgentIntent, AgentConstraints, AgentResult, AgentStep,
    SearchChunk, ToolResult
)
from app.tools.retrieval.vector_search_tool import vector_search_tool
from app.tools.retrieval.keyword_search_tool import keyword_search_tool
from app.tools.retrieval.fulltext_search_tool import fulltext_search_tool
from app.tools.processing.deduplicate_tool import deduplicate_tool
from app.tools.processing.rerank_tool import rerank_tool
from app.tools.context.context_builder_tool import context_builder_tool
from app.services.core.korean_nlp_service import korean_nlp_service
from app.services.core.ai_service import ai_service


class PaperSearchAgent:
    """
    ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ì—ì´ì „íŠ¸
    
    ì—­í• :
    1. ì§ˆì˜ ë¶„ì„ (ì˜ë„ ë¶„ë¥˜, í‚¤ì›Œë“œ ì¶”ì¶œ, ì–¸ì–´ ê°ì§€)
    2. ê²€ìƒ‰ ì „ëµ ì„ íƒ (ì˜ë„ì™€ ì œì•½ì— ë”°ë¼ ë„êµ¬ ì¡°í•© ê²°ì •)
    3. ë„êµ¬ ìˆœì°¨ ì‹¤í–‰ (ê° ë„êµ¬ëŠ” ë…ë¦½ì )
    4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° ë‹µë³€ ìƒì„±
    
    ë„êµ¬ ëª©ë¡:
    - vector_search: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
    - keyword_search: í‚¤ì›Œë“œ ë§¤ì¹­
    - fulltext_search: ì „ë¬¸ê²€ìƒ‰ (tsvector)
    - deduplicate: ì¤‘ë³µ ì œê±°
    - context_builder: ì»¨í…ìŠ¤íŠ¸ í† í° íŒ¨í‚¹
    """
    
    name: str = "paper_search_agent"
    description: str = "ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ë° QA ì „ë¬¸ ì—ì´ì „íŠ¸"
    version: str = "1.0.0"
    
    def __init__(self):
        # ë„êµ¬ ë“±ë¡ (ëŠìŠ¨í•œ ê²°í•©) - ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
        self.tools = {
            "vector_search": vector_search_tool,
            "keyword_search": keyword_search_tool,
            "rerank": rerank_tool,
            "fulltext_search": fulltext_search_tool,
            "deduplicate": deduplicate_tool,
            "context_builder": context_builder_tool,
            # TODO: PPT ìƒì„± ë„êµ¬ ì¶”ê°€ ì˜ˆì •
            # "ppt_generator": ppt_generator_tool,
            # - ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì•„ ìŠ¬ë¼ì´ë“œ êµ¬ì¡° ìƒì„±
            # - general.promptì˜ PPT ëª¨ë“œ ê·œì¹™ì„ tool ë‚´ë¶€ë¡œ ìº¡ìŠí™”
            # - AgentëŠ” PPT ìš”ì²­ ê°ì§€ ì‹œ ì´ ë„êµ¬ë¥¼ ì „ëµì— í¬í•¨
        }
        
        self.nlp_service = korean_nlp_service
        self.ai_service = ai_service
        self._steps: List[AgentStep] = []
        self._start_time: Optional[datetime] = None
    
    async def execute(
        self,
        query: str,
        db_session: AsyncSession,
        constraints: Optional[AgentConstraints] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> AgentResult:
        """
        ì—ì´ì „íŠ¸ ì‹¤í–‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆì˜
            db_session: DB ì„¸ì…˜
            constraints: ì œì•½ ì¡°ê±´
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (user_emp_no ë“±)
        """
        self._start_time = datetime.utcnow()
        self._steps = []
        
        if constraints is None:
            constraints = AgentConstraints()
        
        try:
            logger.info(f"ğŸ¤– [PaperSearchAgent] ì‹¤í–‰ ì‹œì‘: '{query[:50]}...'")
            
            # Step 1: ì§ˆì˜ ë¶„ì„
            intent = self.classify_intent(query)
            keywords = await self._extract_keywords(query)
            
            logger.info(f"   - ì˜ë„: {intent}, í‚¤ì›Œë“œ: {keywords}")
            
            # Step 2: ì „ëµ ì„ íƒ
            strategy = self.select_strategy(intent, constraints)
            logger.info(f"   - ì „ëµ: {strategy}")
            
            # Step 3: ë„êµ¬ ì‹¤í–‰
            all_chunks: List[SearchChunk] = []
            search_results_by_type = {}  # ê²€ìƒ‰ íƒ€ì…ë³„ ê²°ê³¼ ì¶”ì 
            
            for tool_name in strategy:
                tool = self.tools.get(tool_name)
                if not tool:
                    logger.warning(f"âš  ë„êµ¬ ì—†ìŒ: {tool_name}")
                    continue
                
                try:
                    tool_result = await self._execute_tool(
                        tool_name=tool_name,
                        query=query,
                        db_session=db_session,
                        keywords=keywords,
                        constraints=constraints,
                        chunks=all_chunks,  # ì´ì „ ë„êµ¬ ê²°ê³¼
                        context=context
                    )
                    
                    # ê²°ê³¼ ë³‘í•©/êµì²´
                    if tool_name in ["vector_search", "keyword_search", "fulltext_search"]:
                        # ğŸ†• ê²€ìƒ‰ ë„êµ¬ â†’ ë³‘í•© ë° íƒ€ì…ë³„ ì¶”ì 
                        if tool_result.success and hasattr(tool_result, 'data'):
                            new_chunks = tool_result.data
                            all_chunks.extend(new_chunks)
                            search_results_by_type[tool_name] = len(new_chunks)
                            logger.info(f"   âœ… {tool_name}: {len(new_chunks)}ê°œ ì²­í¬ ì¶”ê°€ (ì´ {len(all_chunks)}ê°œ)")
                    elif tool_name in ["deduplicate", "rerank"]:
                        # í›„ì²˜ë¦¬ ë„êµ¬ â†’ êµì²´
                        if tool_result.success and hasattr(tool_result, 'data'):
                            before_count = len(all_chunks)
                            all_chunks = tool_result.data
                            logger.info(f"   âœ… {tool_name}: {before_count}ê°œ â†’ {len(all_chunks)}ê°œ")
                    
                except Exception as e:
                    logger.error(f"âŒ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {tool_name} - {e}")
                    continue
            
            # ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
            if search_results_by_type:
                logger.info(f"   ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {search_results_by_type}")
            
            # Step 4: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_result = await self._execute_tool(
                tool_name="context_builder",
                query=query,
                db_session=db_session,
                keywords=keywords,
                constraints=constraints,
                chunks=all_chunks,
                context=None
            )
            
            if not context_result.success:
                raise Exception("ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹¤íŒ¨")
            
            # ContextResultëŠ” ToolResultì˜ ì„œë¸Œí´ë˜ìŠ¤ì´ë¯€ë¡œ ì†ì„±ì— ì§ì ‘ ì ‘ê·¼
            context_text = context_result.data if isinstance(context_result.data, str) else ""
            used_chunks = getattr(context_result, 'used_chunks', all_chunks[:5])
            
            # Step 5: ë‹µë³€ ìƒì„±
            answer = await self.generate_answer(query, context_text, intent)
            
            # Step 6: ê²°ê³¼ ë°˜í™˜
            latency_ms = (datetime.utcnow() - self._start_time).total_seconds() * 1000
            
            logger.info(f"âœ… [PaperSearchAgent] ì™„ë£Œ: {latency_ms:.1f}ms, {len(used_chunks)}ê°œ ì°¸ì¡°")
            
            return AgentResult(
                answer=answer,
                references=used_chunks,
                steps=self._steps,
                metrics={
                    "total_latency_ms": latency_ms,
                    "tools_used": len(self._steps),
                    "chunks_found": len(all_chunks),
                    "chunks_used": len(used_chunks),
                    "total_tokens": getattr(context_result, 'total_tokens', 0)
                },
                intent=intent,
                strategy_used=strategy,
                success=True,
                errors=[]
            )
            
        except Exception as e:
            logger.error(f"âŒ [PaperSearchAgent] ì‹¤íŒ¨: {e}", exc_info=True)
            latency_ms = (datetime.utcnow() - self._start_time).total_seconds() * 1000 if self._start_time else 0
            
            return AgentResult(
                answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                references=[],
                steps=self._steps,
                metrics={"total_latency_ms": latency_ms},
                intent=AgentIntent.FACTUAL_QA,
                strategy_used=[],
                success=False,
                errors=[str(e)]
            )
    
    def classify_intent(self, query: str) -> AgentIntent:
        """ì˜ë„ ë¶„ë¥˜ (ê°„ë‹¨í•œ ë£° ê¸°ë°˜)"""
        q = query.lower()
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        if any(kw in q for kw in ["ê²€ìƒ‰", "ì°¾ì•„", "ì°¾ê¸°", "ìˆë‚˜", "ìˆëŠ”ì§€"]):
            return AgentIntent.KEYWORD_SEARCH
        
        # ë¹„êµ
        if any(kw in q for kw in ["ë¹„êµ", "ì°¨ì´", "ë‹¤ë¥¸ì ", "vs"]):
            return AgentIntent.COMPARISON
        
        # ìš”ì•½
        if any(kw in q for kw in ["ìš”ì•½", "ì •ë¦¬", "ê°œìš”"]):
            return AgentIntent.SUMMARIZATION
        
        # ê¸°ë³¸: ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸
        return AgentIntent.FACTUAL_QA
    
    def select_strategy(
        self,
        intent: AgentIntent,
        constraints: AgentConstraints
    ) -> List[str]:
        """
        ì „ëµ ì„ íƒ - í•µì‹¬ ì—ì´ì „íŠ¸ ë¡œì§
        ì˜ë„ì™€ ì œì•½ì— ë”°ë¼ ë„êµ¬ ì¡°í•© ë™ì  ê²°ì •
        
        ğŸ†• ëª¨ë“  ì „ëµì— í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì ìš© (ë²¡í„° + í‚¤ì›Œë“œ ë™ì‹œ ì‹¤í–‰)
        """
        if intent == AgentIntent.FACTUAL_QA:
            # ğŸ†• ì‚¬ì‹¤ í™•ì¸ â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ) + ì¤‘ë³µì œê±° + ë¦¬ë­í‚¹
            return ["vector_search", "keyword_search", "deduplicate", "rerank", "context_builder"]
        
        elif intent == AgentIntent.KEYWORD_SEARCH:
            # í‚¤ì›Œë“œ ì¤‘ì‹¬ â†’ í‚¤ì›Œë“œ + ì „ë¬¸ê²€ìƒ‰ + ì¤‘ë³µì œê±° + ë¦¬ë­í‚¹
            return ["keyword_search", "fulltext_search", "deduplicate", "rerank", "context_builder"]
        
        elif intent == AgentIntent.EXPLORATORY:
            # íƒìƒ‰ â†’ í•˜ì´ë¸Œë¦¬ë“œ (ë²¡í„° + í‚¤ì›Œë“œ + ì „ë¬¸ê²€ìƒ‰) + ë¦¬ë­í‚¹
            return ["vector_search", "keyword_search", "fulltext_search", "deduplicate", "rerank", "context_builder"]
        
        elif intent == AgentIntent.COMPARISON:
            # ë¹„êµ â†’ í•˜ì´ë¸Œë¦¬ë“œ (ë²¡í„° + í‚¤ì›Œë“œ) + í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ + ë¦¬ë­í‚¹
            return ["vector_search", "keyword_search", "deduplicate", "rerank", "context_builder"]
        
        elif intent == AgentIntent.SUMMARIZATION:
            # ìš”ì•½ â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´‘ë²”ìœ„í•œ ìë£Œ ìˆ˜ì§‘
            return ["vector_search", "keyword_search", "deduplicate", "rerank", "context_builder"]
        
        else:
            # ê¸°ë³¸ ì „ëµ: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­í‚¹
            return ["vector_search", "keyword_search", "deduplicate", "rerank", "context_builder"]
    
    async def _execute_tool(
        self,
        tool_name: str,
        query: str,
        db_session: AsyncSession,
        keywords: List[str],
        constraints: AgentConstraints,
        chunks: List[SearchChunk],
        context: Optional[Dict[str, Any]]
    ) -> ToolResult:
        """ë„êµ¬ ì‹¤í–‰ í—¬í¼"""
        tool = self.tools[tool_name]
        
        if tool_name == "vector_search":
            tool_input = {
                "query": query,
                "db_session": db_session,
                "top_k": constraints.max_chunks,
                "similarity_threshold": constraints.similarity_threshold,
                "container_ids": constraints.container_ids,
                "document_ids": constraints.document_ids,
                "user_emp_no": context.get("user_emp_no") if context else None
            }
            reasoning = "ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"
        
        elif tool_name == "keyword_search":
            tool_input = {
                "query": query,
                "db_session": db_session,
                "keywords": keywords,
                "top_k": constraints.max_chunks,
                "container_ids": constraints.container_ids,
                "document_ids": constraints.document_ids,
                "user_emp_no": context.get("user_emp_no") if context else None
            }
            reasoning = "í‚¤ì›Œë“œ ì§ì ‘ ë§¤ì¹­"
        
        elif tool_name == "fulltext_search":
            tool_input = {
                "query": query,
                "db_session": db_session,
                "tsquery_str": " | ".join(keywords) if keywords else None,
                "top_k": constraints.max_chunks,
                "container_ids": constraints.container_ids,
                "document_ids": constraints.document_ids,
                "user_emp_no": context.get("user_emp_no") if context else None
            }
            reasoning = "PostgreSQL ì „ë¬¸ê²€ìƒ‰"
        
        elif tool_name == "deduplicate":
            tool_input = {
                "chunks": chunks,
                "similarity_threshold": 0.95
            }
            reasoning = "ì¤‘ë³µ ì²­í¬ ì œê±°"
        
        elif tool_name == "rerank":
            tool_input = {
                "chunks": chunks,
                "query": query,
                "top_k": constraints.max_chunks
            }
            reasoning = "LLM ê¸°ë°˜ ê´€ë ¨ë„ ì¬í‰ê°€"
        
        elif tool_name == "context_builder":
            tool_input = {
                "chunks": chunks,
                "max_tokens": constraints.max_tokens,
                "include_metadata": True,
                "format_style": "citation"
            }
            reasoning = "í† í° ì œí•œ ë‚´ì—ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"
        
        else:
            tool_input = {}
            reasoning = f"{tool_name} ì‹¤í–‰"
        
        result = await tool._arun(**tool_input)
        
        self._log_step(
            tool_name=tool_name,
            tool_input=tool_input,
            tool_output=result,
            reasoning=reasoning
        )
        
        return result
    
    async def _extract_keywords(self, query: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            analysis = await self.nlp_service.analyze_text_for_search(query)
            return analysis.get("keywords", [])
        except Exception as e:
            logger.warning(f"í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨, ê³µë°± ë¶„ë¦¬ ì‚¬ìš©: {e}")
            return [w.strip() for w in query.split() if len(w.strip()) >= 2]
    
    async def generate_answer(
        self,
        query: str,
        context: str,
        intent: AgentIntent
    ) -> str:
        """
        ë‹µë³€ ìƒì„± (general.prompt ê¸°ë°˜)
        
        Note: PPT ìƒì„± ê´€ë ¨ ë¡œì§ì€ í–¥í›„ ë³„ë„ toolë¡œ ë¶„ë¦¬ ì˜ˆì •
        - í˜„ì¬: general.promptì˜ ëª¨ë“  ê·œì¹™ ì ìš© (ì¼ë°˜ ë‹µë³€ + PPT ëª¨ë“œ í¬í•¨)
        - í–¥í›„: ppt_generator_tool ë¶„ë¦¬ í›„ Agentê°€ ë„êµ¬ë¡œ í˜¸ì¶œí•˜ëŠ” êµ¬ì¡°ë¡œ ë³€ê²½
        """
        from pathlib import Path
        
        # ì»¨í…ìŠ¤íŠ¸ ì—†ì„ ë•Œ ì²˜ë¦¬
        if not context or context.strip() == "":
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ì£¼ì„¸ìš”."
        
        # ğŸ†• general.prompt ë¡œë“œ (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼í•œ í’ˆì§ˆ ë³´ì¥)
        system_prompt = None
        try:
            prompt_path = Path("/home/admin/wkms-aws/backend/prompts/general.prompt")
            if prompt_path.exists():
                system_prompt = prompt_path.read_text(encoding='utf-8').strip()
                logger.info("âœ… Agent: general.prompt ë¡œë“œ ì„±ê³µ")
            else:
                logger.warning("âš ï¸ general.prompt íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                system_prompt = "ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ê°€. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€."
        except Exception as e:
            logger.error(f"âŒ general.prompt ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
            system_prompt = "ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ê°€. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€."
        
        # ì°¸ì¡° ë¬¸ì„œ ê°œìˆ˜ ê³„ì‚° (general.promptì˜ ì°¸ì¡°ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ ì›ì¹™ ì¤€ìˆ˜)
        doc_count = len([c for c in context.split('---') if c.strip()])
        
        # User ë©”ì‹œì§€ êµ¬ì„± (ì°¸ì¡°ë¬¸ì„œ ê°œìˆ˜ ëª…ì‹œ)
        user_message = f"""ì§ˆë¬¸: {query}

ì°¸ì¡° ë¬¸ì„œ:
{context}

ì°¸ì¡°ë¬¸ì„œ ê°œìˆ˜: {doc_count}ê°œ

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì¶œì²˜ëŠ” (íŒŒì¼ëª…) í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”."""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        # max_tokens ì¦ê°€ (general.promptì˜ ìƒì„¸ ë‹µë³€ ì§€ì›ì„ ìœ„í•´)
        response = await self.ai_service.chat_completion(
            messages,
            max_tokens=2000,  # 800 â†’ 2000 (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼í•˜ê²Œ ìƒì„¸ ë‹µë³€ ê°€ëŠ¥)
            temperature=0.3  # ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í–¥ìƒ
        )
        return response.get("response", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
    
    def _log_step(
        self,
        tool_name: str,
        tool_input: Dict[str, Any],
        tool_output: ToolResult,
        reasoning: str
    ):
        """ì‹¤í–‰ ë‹¨ê³„ ë¡œê¹…"""
        step = AgentStep(
            step_number=len(self._steps) + 1,
            tool_name=tool_name,
            tool_input={k: str(v)[:100] for k, v in tool_input.items()},  # ê¸´ ê°’ ìë¥´ê¸°
            tool_output=tool_output,
            reasoning=reasoning
        )
        self._steps.append(step)


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
paper_search_agent = PaperSearchAgent()

"""
Paper Search Agent - ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ ì—ì´ì „íŠ¸
ë™ì  ë„êµ¬ ì„ íƒê³¼ ì „ëµ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
"""
import uuid
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
from loguru import logger
from sqlalchemy.ext.asyncio import AsyncSession

from app.tools.contracts import (
    AgentIntent, AgentConstraints, AgentResult, AgentStep,
    SearchChunk, ToolResult
)
from app.tools.retrieval.vector_search_tool import vector_search_tool
from app.tools.retrieval.keyword_search_tool import keyword_search_tool
from app.tools.retrieval.fulltext_search_tool import fulltext_search_tool
from app.tools.retrieval.internet_search_tool import internet_search_tool
from app.tools.retrieval.multimodal_search_tool import multimodal_search_tool  # ğŸ†• ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ë„êµ¬
from app.tools.processing.deduplicate_tool import deduplicate_tool
from app.tools.processing.rerank_tool import rerank_tool
from app.tools.context.context_builder_tool import context_builder_tool
from app.tools.vision.image_analysis_tool import get_image_analysis_tool  # ğŸ†• ì´ë¯¸ì§€ ë¶„ì„ ë„êµ¬
from app.services.core.korean_nlp_service import korean_nlp_service
from app.services.core.ai_service import ai_service
from app.services.document.extraction.text_extractor_service import TextExtractorService
from app.services.chat.chat_attachment_service import chat_attachment_service


class PaperSearchAgent:
    """
    ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ì—ì´ì „íŠ¸
    
    ì—­í• :
    1. ì§ˆì˜ ë¶„ì„ (ì˜ë„ ë¶„ë¥˜, í‚¤ì›Œë“œ ì¶”ì¶œ, ì–¸ì–´ ê°ì§€)
    2. ê²€ìƒ‰ ì „ëµ ì„ íƒ (ì˜ë„ì™€ ì œì•½ì— ë”°ë¼ ë„êµ¬ ì¡°í•© ê²°ì •)
    3. ë„êµ¬ ìˆœì°¨ ì‹¤í–‰ (ê° ë„êµ¬ëŠ” ë…ë¦½ì )
    4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° ë‹µë³€ ìƒì„±
    
    ë„êµ¬ ëª©ë¡:
    - vector_search: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
    - keyword_search: í‚¤ì›Œë“œ ë§¤ì¹­
    - fulltext_search: ì „ë¬¸ê²€ìƒ‰ (tsvector)
    - deduplicate: ì¤‘ë³µ ì œê±°
    - context_builder: ì»¨í…ìŠ¤íŠ¸ í† í° íŒ¨í‚¹
    """
    
    name: str = "paper_search_agent"
    description: str = "ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ë° QA ì „ë¬¸ ì—ì´ì „íŠ¸"
    version: str = "1.0.0"
    
    def __init__(self):
        # ë„êµ¬ ë“±ë¡ (ëŠìŠ¨í•œ ê²°í•©) - ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
        self.tools = {
            "vector_search": vector_search_tool,
            "keyword_search": keyword_search_tool,
            "rerank": rerank_tool,
            "fulltext_search": fulltext_search_tool,
            "internet_search": internet_search_tool,
            "multimodal_search": multimodal_search_tool,  # ğŸ†• ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ë„êµ¬
            "deduplicate": deduplicate_tool,
            "context_builder": context_builder_tool,
            "image_analysis": get_image_analysis_tool(),  # ğŸ†• ì´ë¯¸ì§€ ë¶„ì„ ë„êµ¬
            # TODO: PPT ìƒì„± ë„êµ¬ ì¶”ê°€ ì˜ˆì •
            # "ppt_generator": ppt_generator_tool,
            # - ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì•„ ìŠ¬ë¼ì´ë“œ êµ¬ì¡° ìƒì„±
            # - general.promptì˜ PPT ëª¨ë“œ ê·œì¹™ì„ tool ë‚´ë¶€ë¡œ ìº¡ìŠí™”
            # - AgentëŠ” PPT ìš”ì²­ ê°ì§€ ì‹œ ì´ ë„êµ¬ë¥¼ ì „ëµì— í¬í•¨
        }
        
        self.nlp_service = korean_nlp_service
        self.ai_service = ai_service
        self.text_extractor = TextExtractorService()
        self._steps: List[AgentStep] = []
        self._start_time: Optional[datetime] = None
    
    async def execute(
        self,
        query: str,
        db_session: AsyncSession,
        constraints: Optional[AgentConstraints] = None,
        context: Optional[Dict[str, Any]] = None,
        history: List[Dict[str, str]] = [],
        images: List[str] = [],
        attachments: List[Dict[str, Any]] = []  # ğŸ†• ì²¨ë¶€ íŒŒì¼ ëª©ë¡ ì¶”ê°€
    ) -> AgentResult:
        """
        ì—ì´ì „íŠ¸ ì‹¤í–‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆì˜
            db_session: DB ì„¸ì…˜
            constraints: ì œì•½ ì¡°ê±´
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (user_emp_no ë“±)
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ë©€í‹°í„´ ì§€ì›)
            images: ì´ë¯¸ì§€ ëª©ë¡ (Base64)
            attachments: ì²¨ë¶€ íŒŒì¼ ë©”íƒ€ë°ì´í„° ëª©ë¡
        """
        self._start_time = datetime.utcnow()
        self._steps = []
        
        if constraints is None:
            constraints = AgentConstraints()
        
        try:
            logger.info(f"ğŸ¤– [PaperSearchAgent] ì‹¤í–‰ ì‹œì‘: '{query[:50]}...'")
            
            # ğŸ†• ì´ë¯¸ì§€ ë¶„ì„
            image_description = ""
            if images:
                image_description = await self.analyze_images(images, query)
            
            # ğŸ†• ë¬¸ì„œ ì²¨ë¶€ ì²˜ë¦¬ (Chat with File)
            attached_document_context = ""
            if attachments:
                # ë¬¸ì„œ íŒŒì¼ í•„í„°ë§ (ì´ë¯¸ì§€/ì˜¤ë””ì˜¤ ì œì™¸)
                doc_attachments = [
                    att for att in attachments 
                    if not att.get('mime_type', '').startswith('image/') and not att.get('mime_type', '').startswith('audio/')
                ]
                
                if doc_attachments:
                    logger.info(f"ğŸ“ ë¬¸ì„œ ì²¨ë¶€ ê°ì§€ ({len(doc_attachments)}ê°œ) - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì»¨í…ìŠ¤íŠ¸ ì£¼ì… ì‹œë„")
                    extracted_texts = []
                    
                    for doc_att in doc_attachments:
                        asset_id = doc_att.get('asset_id')
                        if not asset_id:
                            continue
                            
                        stored_file = chat_attachment_service.get(asset_id)
                        if not stored_file:
                            logger.warning(f"âš ï¸ ì²¨ë¶€ íŒŒì¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {asset_id}")
                            continue
                            
                        # íŒŒì¼ í¬ê¸° ì œí•œ (10MB)
                        MAX_FILE_SIZE = 10 * 1024 * 1024
                        if stored_file.size > MAX_FILE_SIZE:
                            logger.warning(f"âš ï¸ íŒŒì¼ í¬ê¸° ì´ˆê³¼ ({stored_file.size} bytes) - ì²˜ë¦¬ ê±´ë„ˆëœ€: {stored_file.file_name}")
                            extracted_texts.append(f"[íŒŒì¼: {stored_file.file_name}]\n(íŒŒì¼ì´ ë„ˆë¬´ ì»¤ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 10MB ì´í•˜ì˜ íŒŒì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤.)")
                            continue
                            
                        try:
                            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            extraction_result = await self.text_extractor.extract_text_from_file(
                                file_path=str(stored_file.path),
                                file_extension=stored_file.path.suffix
                            )
                            
                            if extraction_result.get('success') and extraction_result.get('text'):
                                text_content = extraction_result['text']
                                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (30,000ì)
                                MAX_TEXT_LENGTH = 30000
                                if len(text_content) > MAX_TEXT_LENGTH:
                                    text_content = text_content[:MAX_TEXT_LENGTH] + "\n...(ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ìƒëµë¨)"
                                    
                                extracted_texts.append(f"[ì²¨ë¶€ íŒŒì¼ ë‚´ìš©: {stored_file.file_name}]\n{text_content}")
                                logger.info(f"âœ… ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {stored_file.file_name} ({len(text_content)}ì)")
                            else:
                                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {stored_file.file_name}")
                        except Exception as e:
                            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            
                    if extracted_texts:
                        attached_document_context = "\n\n".join(extracted_texts)
            
            # ğŸ†• Query Rewrite (ì´ë¯¸ì§€ ì„¤ëª… í¬í•¨)
            rewritten_query = query
            if history or image_description:
                rewritten_query = await self.rewrite_query(query, history, image_description)
                if rewritten_query != query:
                    logger.info(f"   âœï¸ ì§ˆì˜ ì¬ì‘ì„±: '{query}' â†’ '{rewritten_query}'")
            
            # Step 1: ì§ˆì˜ ë¶„ì„
            intent = await self.classify_intent(rewritten_query)
            keywords = await self._extract_keywords(rewritten_query)
            
            logger.info(f"   - ì˜ë„: {intent}, í‚¤ì›Œë“œ: {keywords}")
            
            # Step 2: ì „ëµ ì„ íƒ
            strategy = self.select_strategy(intent, constraints)
            
            # ğŸ†• ì²¨ë¶€ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ ì „ëµ ìˆ˜ì • (ê²€ìƒ‰ ìµœì†Œí™” ë˜ëŠ” ìƒëµ)
            if attached_document_context:
                logger.info("ğŸ“ ì²¨ë¶€ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¡´ì¬ - ì™¸ë¶€ ê²€ìƒ‰ ì „ëµ ì¡°ì •")
                # ì²¨ë¶€ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì¤„ì´ê±°ë‚˜ ì œê±°í•  ìˆ˜ ìˆìŒ
                # ì—¬ê¸°ì„œëŠ” ê²€ìƒ‰ ë„êµ¬ëŠ” ìœ ì§€í•˜ë˜, ì»¨í…ìŠ¤íŠ¸ ë¹Œë”ì— ì²¨ë¶€ ë‚´ìš©ì„ ì „ë‹¬í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
                pass
                
            logger.info(f"   - ì „ëµ: {strategy}")
            
            # Step 3: ë„êµ¬ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ ì ìš©)
            all_chunks: List[SearchChunk] = []
            search_results_by_type = {}  # ê²€ìƒ‰ íƒ€ì…ë³„ ê²°ê³¼ ì¶”ì 
            
            # ê²€ìƒ‰ ë„êµ¬ì™€ í›„ì²˜ë¦¬ ë„êµ¬ ë¶„ë¦¬
            search_tools = ["vector_search", "keyword_search", "fulltext_search", "internet_search", "multimodal_search"]
            parallel_tasks = []
            parallel_tool_names = []
            
            # ì „ëµì— í¬í•¨ëœ ê²€ìƒ‰ ë„êµ¬ ìˆ˜ì§‘
            for tool_name in strategy:
                if tool_name in search_tools:
                    tool = self.tools.get(tool_name)
                    if tool:
                        parallel_tasks.append(self._execute_tool(
                            tool_name=tool_name,
                            query=rewritten_query,
                            db_session=db_session,
                            keywords=keywords,
                            constraints=constraints,
                            chunks=[],  # ê²€ìƒ‰ ë„êµ¬ëŠ” ì´ì „ ì²­í¬ ë¶ˆí•„ìš”
                            context=context
                        ))
                        parallel_tool_names.append(tool_name)
            
            # ê²€ìƒ‰ ë„êµ¬ ë³‘ë ¬ ì‹¤í–‰
            if parallel_tasks:
                logger.info(f"   ğŸš€ ê²€ìƒ‰ ë„êµ¬ ë³‘ë ¬ ì‹¤í–‰: {parallel_tool_names}")
                results = await asyncio.gather(*parallel_tasks, return_exceptions=True)
                
                for tool_name, result in zip(parallel_tool_names, results):
                    if isinstance(result, Exception):
                        logger.error(f"âŒ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {tool_name} - {result}")
                        continue
                        
                    if result.success and hasattr(result, 'data'):
                        new_chunks = result.data
                        all_chunks.extend(new_chunks)
                        search_results_by_type[tool_name] = len(new_chunks)
                        logger.info(f"   âœ… {tool_name}: {len(new_chunks)}ê°œ ì²­í¬ ì¶”ê°€")
                    
                    # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë¡œê¹…
                    if tool_name == "internet_search" and result.success:
                        logger.info(f"   ğŸŒ ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼: {len(new_chunks)}ê±´")

            # ğŸ†• Fallback Search: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê³  ì„ê³„ê°’ì´ ë†’ì€ ê²½ìš° ì™„í™”í•˜ì—¬ ì¬ì‹œë„
            if not all_chunks and constraints.similarity_threshold > 0.25:
                logger.info(f"âš ï¸ ê²€ìƒ‰ ê²°ê³¼ 0ê±´. ì„ê³„ê°’ ì™„í™”í•˜ì—¬ ì¬ê²€ìƒ‰ ì‹œë„ ({constraints.similarity_threshold} â†’ 0.2)")
                
                # ì„ê³„ê°’ ì„ì‹œ ìˆ˜ì •
                original_threshold = constraints.similarity_threshold
                constraints.similarity_threshold = 0.2
                
                # Vector Searchë§Œ ì¬ì‹œë„ (ê°€ì¥ íš¨ê³¼ì )
                if "vector_search" in strategy:
                    try:
                        retry_result = await self._execute_tool(
                            tool_name="vector_search",
                            query=rewritten_query,
                            db_session=db_session,
                            keywords=keywords,
                            constraints=constraints,
                            chunks=[],
                            context=context
                        )
                        
                        if retry_result.success and hasattr(retry_result, 'data'):
                            new_chunks = retry_result.data
                            if new_chunks:
                                all_chunks.extend(new_chunks)
                                search_results_by_type["vector_search_retry"] = len(new_chunks)
                                logger.info(f"   âœ… ì¬ê²€ìƒ‰ ì„±ê³µ: {len(new_chunks)}ê°œ ì²­í¬ í™•ë³´")
                    except Exception as e:
                        logger.error(f"âŒ ì¬ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                
                # ì„ê³„ê°’ ë³µêµ¬
                constraints.similarity_threshold = original_threshold

            # í›„ì²˜ë¦¬ ë„êµ¬ ìˆœì°¨ ì‹¤í–‰ (deduplicate, rerank ë“±)
            # context_builderëŠ” Step 4ì—ì„œ ë³„ë„ë¡œ ì‹¤í–‰í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì œì™¸
            processing_tools = [t for t in strategy if t not in search_tools and t != "context_builder"]
            
            for tool_name in processing_tools:
                tool = self.tools.get(tool_name)
                if not tool:
                    logger.warning(f"âš  ë„êµ¬ ì—†ìŒ: {tool_name}")
                    continue
                
                try:
                    tool_result = await self._execute_tool(
                        tool_name=tool_name,
                        query=rewritten_query,
                        db_session=db_session,
                        keywords=keywords,
                        constraints=constraints,
                        chunks=all_chunks,  # ëˆ„ì ëœ ì²­í¬ ì „ë‹¬
                        context=context
                    )
                    
                    if tool_result.success and hasattr(tool_result, 'data'):
                        before_count = len(all_chunks)
                        all_chunks = tool_result.data
                        logger.info(f"   âœ… {tool_name}: {before_count}ê°œ â†’ {len(all_chunks)}ê°œ")
                        
                except Exception as e:
                    logger.error(f"âŒ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {tool_name} - {e}")
                    continue
            
            # ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
            if search_results_by_type:
                logger.info(f"   ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {search_results_by_type}")
            
            # ğŸ†• ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ (Step 3.5)
            if all_chunks:
                all_chunks = await self._validate_search_quality(all_chunks, rewritten_query)
            
            # Step 4: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_result = await self._execute_tool(
                tool_name="context_builder",
                query=rewritten_query,
                db_session=db_session,
                keywords=keywords,
                constraints=constraints,
                chunks=all_chunks,
                context=None
            )
            
            if not context_result.success:
                raise Exception("ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹¤íŒ¨")
            
            # ContextResultëŠ” ToolResultì˜ ì„œë¸Œí´ë˜ìŠ¤ì´ë¯€ë¡œ ì†ì„±ì— ì§ì ‘ ì ‘ê·¼
            context_text = context_result.data if isinstance(context_result.data, str) else ""
            used_chunks = getattr(context_result, 'used_chunks', all_chunks[:5])
            
            # ğŸ†• ì²¨ë¶€ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            if attached_document_context:
                context_text = f"""[ì²¨ë¶€ëœ ë¬¸ì„œ ë‚´ìš©]
{attached_document_context}

[ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ]
{context_text}"""
            
            # Step 5: ë‹µë³€ ìƒì„±
            answer = await self.generate_answer(rewritten_query, context_text, intent, history)
            
            # Step 6: ê²°ê³¼ ë°˜í™˜
            latency_ms = (datetime.utcnow() - self._start_time).total_seconds() * 1000
            
            logger.info(f"âœ… [PaperSearchAgent] ì™„ë£Œ: {latency_ms:.1f}ms, {len(used_chunks)}ê°œ ì°¸ì¡°")
            
            return AgentResult(
                answer=answer,
                references=used_chunks,
                steps=self._steps,
                metrics={
                    "total_latency_ms": latency_ms,
                    "tools_used": len(self._steps),
                    "chunks_found": len(all_chunks),
                    "chunks_used": len(used_chunks),
                    "total_tokens": getattr(context_result, 'total_tokens', 0)
                },
                intent=intent,
                strategy_used=strategy,
                success=True,
                errors=[]
            )
            
        except Exception as e:
            logger.error(f"âŒ [PaperSearchAgent] ì‹¤íŒ¨: {e}", exc_info=True)
            latency_ms = (datetime.utcnow() - self._start_time).total_seconds() * 1000 if self._start_time else 0
            
            return AgentResult(
                answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                references=[],
                steps=self._steps,
                metrics={"total_latency_ms": latency_ms},
                intent=AgentIntent.FACTUAL_QA,
                strategy_used=[],
                success=False,
                errors=[str(e)]
            )
    
    async def analyze_images(self, images: List[str], query: str) -> str:
        """ì´ë¯¸ì§€ ë¶„ì„ (VLM ì‚¬ìš©)"""
        if not images:
            return ""
            
        try:
            content = [{"type": "text", "text": f"ì‚¬ìš©ìì˜ ì§ˆë¬¸: {query}\n\nì´ ì´ë¯¸ì§€ë“¤ì˜ ë‚´ìš©ì„ ìƒì„¸íˆ ë¬˜ì‚¬í•˜ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”."}]
            
            for img_base64 in images:
                # í—¤ë”ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (data:image/...)
                if "base64," in img_base64:
                    url = img_base64
                else:
                    url = f"data:image/jpeg;base64,{img_base64}"
                    
                content.append({
                    "type": "image_url",
                    "image_url": {"url": url}
                })
            
            messages = [{"role": "user", "content": content}]
            
            # VLM í˜¸ì¶œ (max_tokens ë„‰ë„‰íˆ)
            response = await self.ai_service.chat_completion(
                messages,
                max_tokens=1000,
                temperature=0.0
            )
            
            description = response.get("response", "").strip()
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {description[:100]}...")
            return description
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return ""

    async def rewrite_query(self, query: str, history: List[Dict[str, str]], image_description: str = "") -> str:
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ì´ë¯¸ì§€ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆì˜ ì¬ì‘ì„± (Query Rewrite)
        """
        if not history and not image_description:
            return query
            
        try:
            # ìµœê·¼ 3í„´ë§Œ ì‚¬ìš©
            recent_history = history[-6:]
            history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in recent_history])
            
            prompt = f"""ë‹¹ì‹ ì€ ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì§ˆì˜ ì¬ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë¬¸ë§¥ì´ë‚˜ ì²¨ë¶€ëœ ì´ë¯¸ì§€ ì •ë³´ì— ì˜ì¡´ì ì¸ ê²½ìš°, ì´ë¥¼ ë…ë¦½ì ì¸ ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.
ë¬¸ë§¥ ì˜ì¡´ì„±ì´ ì—†ë‹¤ë©´ ì›ë³¸ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.

ì´ì „ ëŒ€í™”:
{history_text}

ì²¨ë¶€ ì´ë¯¸ì§€ ì„¤ëª…:
{image_description if image_description else "(ì—†ìŒ)"}

í˜„ì¬ ì§ˆë¬¸: {query}

ê·œì¹™:
1. ì§€ì‹œëŒ€ëª…ì‚¬(ê·¸ê²ƒ, ì´ê²ƒ, ê·¸ ê¸°ìˆ , ì´ ê·¸ë¦¼ ë“±)ë¥¼ êµ¬ì²´ì ì¸ ëª…ì‚¬ë‚˜ ì´ë¯¸ì§€ ì„¤ëª… ë‚´ìš©ìœ¼ë¡œ ì¹˜í™˜
2. ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì¸ ê²½ìš°, ì´ë¯¸ì§€ ì„¤ëª…ì˜ í•µì‹¬ ë‚´ìš©ì„ ê²€ìƒ‰ ì§ˆì˜ì— í¬í•¨
   ì˜ˆ) "ì´ ì°¨íŠ¸ì˜ ì¶”ì„¸ëŠ”?" -> "[ì´ë¯¸ì§€ ì„¤ëª…ì˜ ì°¨íŠ¸ ì£¼ì œ]ì˜ ì¶”ì„¸ëŠ”?"
3. ìƒëµëœ ì£¼ì–´ë‚˜ ëª©ì ì–´ë¥¼ ë³µì›
4. ê²€ìƒ‰ ì—”ì§„ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ëª…í™•í™”
5. ë‹µë³€ì€ ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥ (ì„¤ëª… ê¸ˆì§€)

ì¬ì‘ì„±ëœ ì§ˆë¬¸:"""

            messages = [{"role": "user", "content": prompt}]
            
            response = await self.ai_service.chat_completion(
                messages,
                max_tokens=200,
                temperature=0.0
            )
            
            rewritten = response.get("response", "").strip()
            
            # ì›ë³¸ê³¼ ë„ˆë¬´ ë‹¤ë¥´ë©´ ë¡œê¹…
            if rewritten and rewritten != query:
                logger.info(f"âœï¸ [QueryRewrite] '{query}' â†’ '{rewritten}'")
                return rewritten
                
            return query
            
        except Exception as e:
            logger.error(f"âŒ Query Rewrite ì‹¤íŒ¨: {e}")
            return query

    async def classify_intent(self, query: str) -> AgentIntent:
        """ì˜ë„ ë¶„ë¥˜ (LLM ê¸°ë°˜ + ë£° ê¸°ë°˜ ë°±ì—…)"""
        try:
            # 1. LLMì„ ì‚¬ìš©í•œ ì˜ë„ ë¶„ë¥˜
            system_prompt = """You are a query intent classifier. Classify the user query into one of the following categories:
- FACTUAL_QA: General questions asking for facts or information.
- KEYWORD_SEARCH: Requests to find specific documents or keywords.
- COMPARISON: Questions asking to compare two or more things.
- SUMMARIZATION: Requests to summarize a topic or document.
- EXPLORATORY: Broad or open-ended questions requiring exploration.
- WEB_SEARCH: Requests for latest news, external information, or internet search.

Return ONLY the category name (e.g., FACTUAL_QA)."""
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ]
            
            response = await self.ai_service.chat_completion(
                messages,
                max_tokens=10,
                temperature=0.0
            )
            
            intent_str = response.get("response", "").strip().upper()
            
            # ë§¤ì¹­ë˜ëŠ” Enum ì°¾ê¸°
            for intent in AgentIntent:
                if intent.name == intent_str:
                    return intent
                    
            logger.warning(f"âš ï¸ LLM ì˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ì˜ë„: {intent_str}, ë£° ê¸°ë°˜ìœ¼ë¡œ ì „í™˜")
            
        except Exception as e:
            logger.error(f"âŒ LLM ì˜ë„ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {e}, ë£° ê¸°ë°˜ìœ¼ë¡œ ì „í™˜")
            
        # 2. ë£° ê¸°ë°˜ ë¶„ë¥˜ (ë°±ì—…)
        q = query.lower()
        
        # ì¸í„°ë„· ê²€ìƒ‰
        if any(kw in q for kw in ["ì¸í„°ë„·", "ì›¹ê²€ìƒ‰", "êµ¬ê¸€", "ìµœì‹ ", "ë‰´ìŠ¤", "ì™¸ë¶€"]):
            return AgentIntent.WEB_SEARCH

        # í‚¤ì›Œë“œ ê²€ìƒ‰
        if any(kw in q for kw in ["ê²€ìƒ‰", "ì°¾ì•„", "ì°¾ê¸°", "ìˆë‚˜", "ìˆëŠ”ì§€"]):
            return AgentIntent.KEYWORD_SEARCH
        
        # ë¹„êµ
        if any(kw in q for kw in ["ë¹„êµ", "ì°¨ì´", "ë‹¤ë¥¸ì ", "vs"]):
            return AgentIntent.COMPARISON
        
        # ìš”ì•½
        if any(kw in q for kw in ["ìš”ì•½", "ì •ë¦¬", "ê°œìš”"]):
            return AgentIntent.SUMMARIZATION
        
        # ê¸°ë³¸: ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸
        return AgentIntent.FACTUAL_QA
    
    def select_strategy(
        self,
        intent: AgentIntent,
        constraints: AgentConstraints
    ) -> List[str]:
        """
        ì „ëµ ì„ íƒ - í•µì‹¬ ì—ì´ì „íŠ¸ ë¡œì§
        ì˜ë„ì™€ ì œì•½ì— ë”°ë¼ ë„êµ¬ ì¡°í•© ë™ì  ê²°ì •
        
        ğŸ†• ëª¨ë“  ì „ëµì— í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì ìš© (ë²¡í„° + í‚¤ì›Œë“œ ë™ì‹œ ì‹¤í–‰)
        """
        # ğŸ†• íŠ¹ì • ë¬¸ì„œê°€ ì§€ì •ëœ ê²½ìš° (Chat with File)
        # ì¸í„°ë„· ê²€ìƒ‰ ë“± ì™¸ë¶€ ê²€ìƒ‰ì„ ë°°ì œí•˜ê³ , ì§€ì •ëœ ë¬¸ì„œ ë‚´ì—ì„œë§Œ ê²€ìƒ‰í•˜ë„ë¡ ìœ ë„
        if constraints.document_ids and len(constraints.document_ids) > 0:
            logger.info(f"ğŸ“‚ íŠ¹ì • ë¬¸ì„œ ëŒ€ìƒ ê²€ìƒ‰: {constraints.document_ids}")
            # ë¬¸ì„œ ë‚´ ê²€ìƒ‰ì€ ë²¡í„°+í‚¤ì›Œë“œ+ì „ë¬¸ê²€ìƒ‰ ëª¨ë‘ í™œìš©í•˜ì—¬ ì •í™•ë„ ë†’ì„
            return ["vector_search", "keyword_search", "fulltext_search", "deduplicate", "rerank", "context_builder"]

        if intent == AgentIntent.FACTUAL_QA:
            # ğŸ†• ì‚¬ì‹¤ í™•ì¸ â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ) + ì¤‘ë³µì œê±° + ë¦¬ë­í‚¹
            return ["vector_search", "keyword_search", "deduplicate", "rerank", "context_builder"]
        
        elif intent == AgentIntent.KEYWORD_SEARCH:
            # í‚¤ì›Œë“œ ì¤‘ì‹¬ â†’ í‚¤ì›Œë“œ + ì „ë¬¸ê²€ìƒ‰ + ì¤‘ë³µì œê±° + ë¦¬ë­í‚¹
            return ["keyword_search", "fulltext_search", "deduplicate", "rerank", "context_builder"]
        
        elif intent == AgentIntent.EXPLORATORY:
            # íƒìƒ‰ â†’ í•˜ì´ë¸Œë¦¬ë“œ (ë²¡í„° + í‚¤ì›Œë“œ + ì „ë¬¸ê²€ìƒ‰) + ë¦¬ë­í‚¹
            return ["vector_search", "keyword_search", "fulltext_search", "deduplicate", "rerank", "context_builder"]
        
        elif intent == AgentIntent.COMPARISON:
            # ë¹„êµ â†’ í•˜ì´ë¸Œë¦¬ë“œ (ë²¡í„° + í‚¤ì›Œë“œ) + í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ + ë¦¬ë­í‚¹
            return ["vector_search", "keyword_search", "deduplicate", "rerank", "context_builder"]
        
        elif intent == AgentIntent.SUMMARIZATION:
            # ìš”ì•½ â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´‘ë²”ìœ„í•œ ìë£Œ ìˆ˜ì§‘
            return ["vector_search", "keyword_search", "deduplicate", "rerank", "context_builder"]
        
        elif intent == AgentIntent.WEB_SEARCH:
            # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ â†’ ì¸í„°ë„· ê²€ìƒ‰ + ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            return ["internet_search", "context_builder"]
        
        else:
            # ê¸°ë³¸ ì „ëµ: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­í‚¹
            return ["vector_search", "keyword_search", "deduplicate", "rerank", "context_builder"]
    
    async def _execute_tool(
        self,
        tool_name: str,
        query: str,
        db_session: AsyncSession,
        keywords: List[str],
        constraints: AgentConstraints,
        chunks: List[SearchChunk],
        context: Optional[Dict[str, Any]]
    ) -> ToolResult:
        """ë„êµ¬ ì‹¤í–‰ í—¬í¼"""
        tool = self.tools[tool_name]
        
        if tool_name == "vector_search":
            tool_input = {
                "query": query,
                "db_session": db_session,
                "top_k": constraints.max_chunks,
                "similarity_threshold": constraints.similarity_threshold,
                "container_ids": constraints.container_ids,
                "document_ids": constraints.document_ids,
                "user_emp_no": context.get("user_emp_no") if context else None
            }
            reasoning = "ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"
        
        elif tool_name == "keyword_search":
            tool_input = {
                "query": query,
                "db_session": db_session,
                "keywords": keywords,
                "top_k": constraints.max_chunks,
                "container_ids": constraints.container_ids,
                "document_ids": constraints.document_ids,
                "user_emp_no": context.get("user_emp_no") if context else None
            }
            reasoning = "í‚¤ì›Œë“œ ì§ì ‘ ë§¤ì¹­"
        
        elif tool_name == "fulltext_search":
            tool_input = {
                "query": query,
                "db_session": db_session,
                "tsquery_str": " | ".join(keywords) if keywords else None,
                "top_k": constraints.max_chunks,
                "container_ids": constraints.container_ids,
                "document_ids": constraints.document_ids,
                "user_emp_no": context.get("user_emp_no") if context else None
            }
            reasoning = "PostgreSQL ì „ë¬¸ê²€ìƒ‰"
        
        elif tool_name == "internet_search":
            tool_input = {
                "query": query,
                "top_k": 5  # ì¸í„°ë„· ê²€ìƒ‰ì€ ìƒìœ„ 5ê°œë§Œ
            }
            reasoning = "ì™¸ë¶€ ì¸í„°ë„· ê²€ìƒ‰ (DuckDuckGo)"
        
        elif tool_name == "multimodal_search":
            # ğŸ†• ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ (ì´ë¯¸ì§€ ìœ ì‚¬ë„)
            tool_input = {
                "image_data": context.get("image_data") if context else None,
                "query": query,
                "db_session": db_session,
                "top_k": context.get("top_k", 10) if context else 10,
                "container_ids": constraints.container_ids
            }
            reasoning = "CLIP ê¸°ë°˜ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰"
        
        elif tool_name == "deduplicate":
            tool_input = {
                "chunks": chunks,
                "similarity_threshold": 0.95
            }
            reasoning = "ì¤‘ë³µ ì²­í¬ ì œê±°"
        
        elif tool_name == "rerank":
            tool_input = {
                "chunks": chunks,
                "query": query,
                "top_k": constraints.max_chunks,
                "threshold": 0.3  # ğŸ†• ê´€ë ¨ì„± ì„ê³„ê°’ (0.3 ë¯¸ë§Œ ì œì™¸)
            }
            reasoning = "LLM ê¸°ë°˜ ê´€ë ¨ë„ ì¬í‰ê°€"
        
        elif tool_name == "context_builder":
            tool_input = {
                "chunks": chunks,
                "max_tokens": constraints.max_tokens,
                "include_metadata": True,
                "format_style": "citation"
            }
            reasoning = "í† í° ì œí•œ ë‚´ì—ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"
        
        else:
            tool_input = {}
            reasoning = f"{tool_name} ì‹¤í–‰"
        
        result = await tool._arun(**tool_input)
        
        self._log_step(
            tool_name=tool_name,
            tool_input=tool_input,
            tool_output=result,
            reasoning=reasoning
        )
        
        return result
    
    async def _extract_keywords(self, query: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            analysis = await self.nlp_service.analyze_text_for_search(query)
            return analysis.get("keywords", [])
        except Exception as e:
            logger.warning(f"í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨, ê³µë°± ë¶„ë¦¬ ì‚¬ìš©: {e}")
            return [w.strip() for w in query.split() if len(w.strip()) >= 2]
    
    async def generate_answer(
        self,
        query: str,
        context: str,
        intent: AgentIntent,
        history: List[Dict[str, str]] = []
    ) -> str:
        """
        ë‹µë³€ ìƒì„± (general.prompt ê¸°ë°˜)
        
        Note: PPT ìƒì„± ê´€ë ¨ ë¡œì§ì€ í–¥í›„ ë³„ë„ toolë¡œ ë¶„ë¦¬ ì˜ˆì •
        - í˜„ì¬: general.promptì˜ ëª¨ë“  ê·œì¹™ ì ìš© (ì¼ë°˜ ë‹µë³€ + PPT ëª¨ë“œ í¬í•¨)
        - í–¥í›„: ppt_generator_tool ë¶„ë¦¬ í›„ Agentê°€ ë„êµ¬ë¡œ í˜¸ì¶œí•˜ëŠ” êµ¬ì¡°ë¡œ ë³€ê²½
        """
        from pathlib import Path
        
        # ì»¨í…ìŠ¤íŠ¸ ì—†ì„ ë•Œ ì²˜ë¦¬
        if not context or context.strip() == "":
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ì£¼ì„¸ìš”."
        
        # ğŸ†• general.prompt ë¡œë“œ (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼í•œ í’ˆì§ˆ ë³´ì¥)
        system_prompt = None
        try:
            # í˜„ì¬ íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
            current_dir = Path(__file__).parent  # backend/app/agents
            backend_dir = current_dir.parent.parent  # backend
            prompt_path = backend_dir / "prompts" / "general.prompt"
            
            if prompt_path.exists():
                system_prompt = prompt_path.read_text(encoding='utf-8').strip()
                logger.info(f"âœ… Agent: general.prompt ë¡œë“œ ì„±ê³µ ({prompt_path})")
            else:
                logger.warning(f"âš ï¸ general.prompt íŒŒì¼ ì—†ìŒ ({prompt_path}), ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                system_prompt = "ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ê°€. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€."
        except Exception as e:
            logger.error(f"âŒ general.prompt ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
            system_prompt = "ë…¼ë¬¸/ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ê°€. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€."
        
        # ì°¸ì¡° ë¬¸ì„œ ê°œìˆ˜ ê³„ì‚° (general.promptì˜ ì°¸ì¡°ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ ì›ì¹™ ì¤€ìˆ˜)
        doc_count = len([c for c in context.split('---') if c.strip()])
        
        # User ë©”ì‹œì§€ êµ¬ì„± (ì°¸ì¡°ë¬¸ì„œ ê°œìˆ˜ ëª…ì‹œ)
        user_message = f"""ì§ˆë¬¸: {query}

ì°¸ì¡° ë¬¸ì„œ:
{context}

ì°¸ì¡°ë¬¸ì„œ ê°œìˆ˜: {doc_count}ê°œ

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì¶œì²˜ëŠ” (íŒŒì¼ëª…) í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”."""
        
        messages = [{"role": "system", "content": system_prompt}]
        
        # ğŸ†• ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 5ê°œ í„´ë§Œ ìœ ì§€í•˜ì—¬ í† í° ì ˆì•½)
        if history:
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë‹¤ìŒì— íˆìŠ¤í† ë¦¬ ì‚½ì…
            # íˆìŠ¤í† ë¦¬ëŠ” [{'role': 'user', 'content': '...'}, {'role': 'assistant', 'content': '...'}] í˜•íƒœ
            recent_history = history[-10:] # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ (5í„´)
            messages.extend(recent_history)
            logger.info(f"ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬ {len(recent_history)}ê°œ ë©”ì‹œì§€ í¬í•¨")
            
        messages.append({"role": "user", "content": user_message})
        
        # max_tokens ì¦ê°€ (general.promptì˜ ìƒì„¸ ë‹µë³€ ì§€ì›ì„ ìœ„í•´)
        response = await self.ai_service.chat_completion(
            messages,
            max_tokens=2000,  # 800 â†’ 2000 (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼í•˜ê²Œ ìƒì„¸ ë‹µë³€ ê°€ëŠ¥)
            temperature=0.3  # ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í–¥ìƒ
        )
        return response.get("response", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
    
    def _log_step(
        self,
        tool_name: str,
        tool_input: Dict[str, Any],
        tool_output: ToolResult,
        reasoning: str
    ):
        """ì‹¤í–‰ ë‹¨ê³„ ë¡œê¹…"""
        step = AgentStep(
            step_number=len(self._steps) + 1,
            tool_name=tool_name,
            tool_input={k: str(v)[:100] for k, v in tool_input.items()},  # ê¸´ ê°’ ìë¥´ê¸°
            tool_output=tool_output,
            reasoning=reasoning
        )
        self._steps.append(step)
    
    async def _validate_search_quality(self, chunks: List[SearchChunk], query: str) -> List[SearchChunk]:
        """
        ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ (LLM í‰ê°€)
        
        ê° ì²­í¬ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° ìœ ìš©í•œì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ê³ ,
        2ì  ì´í•˜ì¸ ì²­í¬ëŠ” ì œì™¸í•©ë‹ˆë‹¤.
        """
        if not chunks:
            return []
            
        # ë¹„ìš© ì ˆê°ì„ ìœ„í•´ ìƒìœ„ 5ê°œë§Œ ê²€ì¦
        candidates = chunks[:5]
        
        if not candidates:
            return chunks
            
        try:
            # ê²€ì¦ í”„ë¡¬í”„íŠ¸
            chunks_text = "\n\n".join([
                f"ë¬¸ì„œ {i+1}:\n{chunk.content[:500]}"
                for i, chunk in enumerate(candidates)
            ])
            
            prompt = f"""ì§ˆë¬¸: "{query}"

ë‹¤ìŒ ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° ì–¼ë§ˆë‚˜ ìœ ìš©í•œì§€ í‰ê°€í•˜ì„¸ìš”.

ë¬¸ì„œë“¤:
{chunks_text}

ì§€ì‹œì‚¬í•­:
1. ê° ë¬¸ì„œì— ëŒ€í•´ 1~5ì  ì²™ë„ë¡œ í‰ê°€í•˜ì„¸ìš” (1: ì „í˜€ ë¬´ê´€, 5: ë§¤ìš° ìœ ìš©).
2. ë‹µë³€ í˜•ì‹: ë¬¸ì„œë²ˆí˜¸:ì ìˆ˜ (ì˜ˆ: 1:5, 2:3, 3:1)
3. ì ìˆ˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”.

í‰ê°€:"""
            
            messages = [
                {"role": "user", "content": prompt}
            ]
            
            response = await self.ai_service.chat_completion(
                messages,
                max_tokens=100,
                temperature=0.0
            )
            
            content = response.get("response", "")
            logger.debug(f"ğŸ” í’ˆì§ˆ ê²€ì¦ ì‘ë‹µ: {content}")
            
            import re
            matches = re.findall(r'(\d+)\s*:\s*(\d+)', content)
            
            valid_indices = set()
            for idx_str, score_str in matches:
                idx = int(idx_str) - 1
                score = int(score_str)
                
                if score > 2:  # 2ì  ì´ˆê³¼ (3, 4, 5)ë§Œ í—ˆìš©
                    valid_indices.add(idx)
                else:
                    logger.info(f"   âœ‚ï¸ í’ˆì§ˆ ë¯¸ë‹¬ ë¬¸ì„œ ì œì™¸: {idx+1}ë²ˆ (ì ìˆ˜ {score})")
            
            validated_chunks = []
            for i, chunk in enumerate(candidates):
                if i in valid_indices:
                    validated_chunks.append(chunk)
            
            logger.info(f"âœ… í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {len(candidates)}ê°œ ì¤‘ {len(validated_chunks)}ê°œ í†µê³¼")
            
            # ë§Œì•½ ê²€ì¦ í›„ 0ê°œê°€ ë˜ë©´ ìµœìƒìœ„ 1ê°œ ìœ ì§€ (ì•ˆì „ì¥ì¹˜)
            if not validated_chunks and candidates:
                logger.warning("âš ï¸ ëª¨ë“  ë¬¸ì„œê°€ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬. ìµœìƒìœ„ 1ê°œ ìœ ì§€.")
                return [candidates[0]]
                
            return validated_chunks
            
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return chunks  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
paper_search_agent = PaperSearchAgent()

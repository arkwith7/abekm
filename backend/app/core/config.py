from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field
from typing import List, Optional
import os
from pathlib import Path


class Settings(BaseSettings):
    # Pydantic v2 / pydantic-settings v2 configuration
    # NOTE: v1 style Field(..., env="VAR") is deprecated; we rely on case-insensitive
    # matching so ENV_VAR and env_var both map to the field name.
    model_config = SettingsConfigDict(
        env_file=".env",
        case_sensitive=False,
        extra="ignore"
    )
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • - í™˜ê²½ë³€ìˆ˜ì—ì„œ ë™ì  êµ¬ì„±
    database_url: str = Field(
        default_factory=lambda: (
            f"postgresql+asyncpg://"
            f"{os.getenv('DB_USER', 'wkms')}:"
            f"{os.getenv('DB_PASSWORD', 'wkms123')}@"
            f"{os.getenv('DB_HOST', 'localhost')}:"
            f"{os.getenv('DB_PORT', '5432')}/"
            f"{os.getenv('DB_NAME', 'wkms')}"
            if not os.getenv('DATABASE_URL') else os.getenv('DATABASE_URL')
        )
    )
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ì„¤ì •
    db_pool_size: int = 40  # 20 â†’ 40 (Connection pool ì¦ê°€)
    db_max_overflow: int = 60  # 30 â†’ 60
    db_pool_timeout: int = 60  # 30 â†’ 60 (ëŒ€ê¸° ì‹œê°„ ì¦ê°€)
    db_pool_recycle: int = 300
    db_pool_pre_ping: bool = True
    
    # ë””ë²„ê·¸ ëª¨ë“œ (ì „ì—­)
    debug: bool = False
    
    # Redis ì„¤ì •
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_db: int = 0
    redis_url: str = Field(
        default_factory=lambda: (
            f"redis://{os.getenv('REDIS_HOST', 'localhost')}:"
            f"{os.getenv('REDIS_PORT', '6379')}/"
            f"{os.getenv('REDIS_DB', '0')}"
            if not os.getenv('REDIS_URL') else os.getenv('REDIS_URL')
        )
    )
    
    # JWT í† í° ì„¤ì •
    secret_key: str = "your-super-secret-jwt-key-change-this-in-production"
    algorithm: str = "HS256"
    # Access í† í° ë§Œë£Œ ì‹œê°„ (ë¶„) - .envì—ì„œ ACCESS_TOKEN_EXPIRE_MINUTESë¡œ ì„¤ì • ê°€ëŠ¥
    access_token_expire_minutes: int = 480  # ê°œë°œ: 8ì‹œê°„, ìš´ì˜: 30ë¶„ ê¶Œì¥
    # Refresh í† í° ë§Œë£Œ ì‹œê°„ (ë¶„) - .envì—ì„œ REFRESH_TOKEN_EXPIRE_MINUTESë¡œ ì„¤ì • ê°€ëŠ¥
    refresh_token_expire_minutes: int = 60 * 24 * 7  # 7ì¼
    
    # CORS ì„¤ì • - í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜´
    cors_origins: List[str] = Field(
        default=[
            "http://localhost:3000",
            "http://127.0.0.1:3000",
            "http://localhost:3001",
            "http://127.0.0.1:3001"
        ],
        env="CORS_ORIGINS",
        description="CORS allowed origins list"
    )
    
    # íŒŒì¼ ì—…ë¡œë“œ ì„¤ì •
    upload_dir: str = "uploads"
    file_upload_path: str = "uploads"
    max_file_size: int = 104857600  # 100MB (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì§€ì›)
    allowed_file_types: List[str] = [".pdf", ".docx", ".pptx", ".xlsx", ".txt", ".md", ".hwp", ".doc", ".xls", ".ppt"]
    allowed_file_extensions: List[str] = [".pdf", ".docx", ".pptx", ".xlsx", ".txt", ".md", ".hwp", ".doc", ".xls", ".ppt"]
    chat_attachment_dir: str = "uploads/chat_attachments"
    
    # í•œêµ­ì–´ ì²˜ë¦¬ ì„¤ì • (Simplified - 2025-10-16)
    # âŒ ì œê±°ë¨: kiwipiepy ê´€ë ¨ ì„¤ì •
    # korean_nlp_provider: str = "hybrid"
    # kiwi_model_type: str = "sbg"
    # kiwi_typos_correction: str = "basic_with_continual_and_lengthening"
    # user_dictionary_path: str = "dictionaries/company_dict.txt"
    # korean_stopwords_path: str = "dictionaries/korean_stopwords.txt"
    
    # âœ… ìœ ì§€: ì„ë² ë”© ë° í† í¬ë‚˜ì´ì € ì„¤ì •
    korean_tokenizer_model: str = "cl100k_base"  # tiktoken ëª¨ë¸
    
    # ë¬¸ì„œ ì²˜ë¦¬ ì„¤ì •
    supported_document_formats: List[str] = [
        "pdf", "docx", "pptx", "xlsx", "txt", "md", "hwp", "doc", "xls", "ppt"
    ]
    chunk_size: int = 1000
    chunk_overlap: int = 200
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •
    hybrid_search_weights: dict = {
        "semantic": 0.7,  # ì˜ë¯¸ ê²€ìƒ‰ ê°€ì¤‘ì¹˜
        "keyword": 0.3    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜
    }
    korean_embedding_model: str = "jhgan/ko-sroberta-multitask"
    
    # AWS ì„¤ì •
    aws_region: str = "ap-northeast-2"
    aws_access_key_id: Optional[str] = None
    aws_secret_access_key: Optional[str] = None
    
    # íŒŒì¼ ì €ì¥ì†Œ ì„ íƒ (local | s3 | azure_blob)
    storage_backend: str = "local"
    # í‘œì¤€ raw/ í”„ë¦¬í”½ìŠ¤ ì‚¬ìš© ì—¬ë¶€ (S3 ì—…ë¡œë“œ ì‹œ ê²½ë¡œ ìŠ¤í‚´ ì „í™˜ìš© Feature Flag)
    use_standard_raw_prefix: bool = False
    
    # S3 ì„¤ì • (storage_backend == 's3' ì¼ ë•Œ í•„ìˆ˜)
    aws_s3_bucket: Optional[str] = None
    s3_presign_expiry_seconds: int = 3600

    # Azure Blob Storage ì„¤ì • (storage_backend == 'azure_blob' ì¼ ë•Œ ì‚¬ìš©)
    azure_blob_account_name: Optional[str] = None
    azure_blob_account_key: Optional[str] = None
    azure_blob_connection_string: Optional[str] = None  # ì„ íƒ: ì—°ê²° ë¬¸ìì—´ ìš°ì„ 
    azure_blob_container_raw: str = "wkms-raw"  # ì›ë³¸ ì—…ë¡œë“œ
    azure_blob_container_intermediate: str = "wkms-intermediate"  # ì¶”ì¶œ/í˜ì´ì§€/ì„ì‹œ ì‚°ì¶œë¬¼
    azure_blob_container_derived: str = "wkms-derived"  # ì²­í¬/ì„ë² ë”©/ìš”ì•½ ì‚°ì¶œë¬¼
    azure_blob_sas_expiry_seconds: int = 3600
    azure_blob_enable_auto_container: bool = True  # ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ìë™ ìƒì„±
    azure_blob_path_style: bool = False  # ì‚¬ì„¤ ì—ë®¬ë ˆì´í„°(Azurite) ì‚¬ìš© ì‹œ True
    azure_blob_download_mode: str = "proxy"  # redirect: 302 ë¦¬ë‹¤ì´ë ‰íŠ¸ (CORS í•„ìš”), proxy: ì„œë²„ í”„ë¡ì‹œ (CORS ë¶ˆí•„ìš”)
    
    # Azure OpenAI ì„¤ì •
    azure_openai_endpoint: Optional[str] = None
    azure_openai_api_key: Optional[str] = None
    azure_openai_api_version: str = "2024-02-15-preview"
    
    # ë¬¸ì„œ ì²˜ë¦¬ ì œê³µì ì„ íƒ (azure_di | upstage | aws_textract | etc_other)
    # - azure_di: Azure Document Intelligence (í•œêµ­ì–´ ì™„ë²½ ì§€ì›)
    # - upstage: Upstage Document Parse (í•œêµ­ì–´ ìš°ìˆ˜, Azure DI ëŒ€ì•ˆ)
    # - aws_textract: AWS Textract (ì˜ë¬¸ ì¤‘ì‹¬, í•œêµ­ì–´ ì œí•œì )
    # - etc_other: pdfplumber ë“± ê¸°íƒ€ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬
    document_processing_provider: str = Field(
        default="azure_di",
        description="Primary document processing provider"
    )
    document_processing_fallback: Optional[str] = Field(
        default=None,
        description="Fallback document processing provider"
    )
    
    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ì„¤ì • ìœ ì§€ (Deprecated - document_processing_provider ì‚¬ìš© ê¶Œì¥)
    use_azure_document_intelligence_pdf: bool = Field(
        default_factory=lambda: os.getenv("DOCUMENT_PROCESSING_PROVIDER", "azure_di").lower() == "azure_di"
    )
    
    # Azure Document Intelligence ì„¤ì •
    azure_document_intelligence_endpoint: Optional[str] = None
    azure_document_intelligence_api_key: Optional[str] = None
    azure_document_intelligence_api_version: str = "2024-11-30"  # ìµœì‹  API ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (FIGURES feature ì§€ì›)
    azure_document_intelligence_default_model: str = "prebuilt-read"

    @property
    def resolved_upload_dir(self) -> Path:
        """Ensure upload directory is always an absolute path."""
        base = Path(self.file_upload_path or self.upload_dir)
        if not base.is_absolute():
            project_root = Path(__file__).resolve().parents[2]
            base = (project_root / base).resolve()
        return base
    azure_document_intelligence_layout_model: str = "prebuilt-layout"
    azure_document_intelligence_document_model: str = "prebuilt-document"
    azure_document_intelligence_max_pages: int = 150
    azure_document_intelligence_timeout_seconds: int = 300
    azure_document_intelligence_retry_max_attempts: int = 3
    azure_document_intelligence_confidence_threshold: float = 0.8
    azure_document_intelligence_use_korean_optimization: bool = True

    # DI ì„±ëŠ¥/í’ˆì§ˆ ê°œì„  í”Œë˜ê·¸ (Sprint 1)
    # í˜ì´ì§€ ê·¸ë£¹ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
    di_parallel_enabled: bool = False
    # í˜ì´ì§€ ê·¸ë£¹ í¬ê¸° (ì˜ˆ: 3ì´ë©´ 1-3, 4-6, ... ì‹ìœ¼ë¡œ í˜¸ì¶œ)
    di_page_group_size: int = 3
    # ë™ì‹œ ì‹¤í–‰ ê·¸ë£¹ ìˆ˜ ì œí•œ (429 ëŒ€ë¹„)
    di_max_concurrency: int = 3
    # ë™ì¼ íŒŒì¼ í•´ì‹œ ê¸°ë°˜ ì„ì‹œ ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (/tmp/di_cache)
    di_cache_enabled: bool = False
    # 2ì—´ ë ˆì´ì•„ì›ƒ ì¬êµ¬ì„± í™œì„±í™” ì—¬ë¶€ (pdfplumber í•„ìš”, ì—†ìœ¼ë©´ ìë™ ìƒëµ)
    di_two_column_reorder_enabled: bool = True
    
    # Upstage Document Parse ì„¤ì •
    upstage_api_key: Optional[str] = None
    upstage_api_endpoint: str = "https://api.upstage.ai/v1/document-digitization"
    upstage_max_pages: int = 150
    upstage_timeout_seconds: int = 300
    upstage_retry_max_attempts: int = 3
    upstage_model: str = "document-parse"
    upstage_ocr_mode: Optional[str] = None
    upstage_base64_categories: Optional[List[str]] = None
    upstage_merge_multipage_tables: bool = True
    upstage_use_async_api: bool = False
    upstage_async_poll_interval_seconds: int = 5
    upstage_async_timeout_seconds: int = 900
    upstage_async_api_endpoint: Optional[str] = None
    upstage_async_status_endpoint: Optional[str] = None
    
    # OpenAI ì„¤ì •
    openai_api_key: Optional[str] = None
    
    # LLM ì œê³µì ì„¤ì •
    llm_providers: List[str] = Field(default_factory=lambda: ["bedrock", "azure_openai", "openai"])
    default_llm_provider: str = "bedrock"
    default_embedding_provider: Optional[str] = None  # Noneì´ë©´ default_llm_providerì™€ ë™ì¼
    
    # Azure OpenAI ëª¨ë¸ (.envì—ì„œ ì„¤ì • í•„ìˆ˜)
    azure_openai_llm_deployment: str = Field(default="")
    azure_openai_embedding_deployment: str = Field(default="text-embedding-ada-002")
    # ë©€í‹°ëª¨ë‹¬(Vision) ì „ìš© ë°°í¬ (gpt-4o, gpt-4o-mini, gpt-4o-vision ë“±) - ì„ íƒ
    azure_openai_multimodal_deployment: str = Field(default="")
    azure_openai_enable_vision_captioning: bool = True  # Vision ìº¡ì…”ë‹ í™œì„±í™” í”Œë˜ê·¸
    
    # AWS Transcribe ìŒì„± ë³€í™˜ ì„¤ì •
    enable_audio_transcription: bool = False  # ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸ ë³€í™˜ í”Œë˜ê·¸
    aws_transcribe_language_code: str = "ko-KR"  # ê¸°ë³¸ ì–¸ì–´ (ko-KR, en-US, ja-JP, zh-CN ë“±)
    
    # Azure CLIP ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ëª¨ë¸
    azure_openai_multimodal_embedding_endpoint: Optional[str] = None
    azure_openai_multimodal_embedding_api_key: Optional[str] = None
    azure_openai_multimodal_embedding_deployment: str = "openai-clip-image-text-embed-11"
    clip_embedding_dimension: int = 512  # CLIP ì„ë² ë”© ì°¨ì›
    
    # AWS Bedrock ëª¨ë¸
    bedrock_llm_model_id: str = "apac.anthropic.claude-3-5-sonnet-20241022-v2:0"
    bedrock_text_model_id: str = "apac.anthropic.claude-3-5-sonnet-20241022-v2:0"  # bedrock_service.pyì—ì„œ ì‚¬ìš©
    bedrock_embedding_model_id: str = "amazon.titan-embed-text-v2:0"
    bedrock_alt_embedding_model_id: str = "amazon.titan-embed-text-v1:0"  # ëŒ€ì²´ ì„ë² ë”© ëª¨ë¸
    bedrock_embedding_dimension: int = 1024  # Titan V2 ê¸°ë³¸ ì°¨ì› (1024, 512, 256 ì§€ì›)
    
    # AWS Bedrock ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (Cohere Embed v4)
    bedrock_multimodal_embedding_model_id: str = "twelvelabs.marengo-embed-3-0-v1:0"
    bedrock_multimodal_embedding_dimension: int = 512
    bedrock_multimodal_llm_model_id: str = "anthropic.claude-3-haiku-20240307-v1:0"
    
    # OpenAI ëª¨ë¸ (.envì—ì„œ ì„¤ì • í•„ìˆ˜)
    openai_llm_model: str = Field(default="")
    openai_embedding_model: str = Field(default="text-embedding-ada-002")
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„°
    max_tokens: int = 4096
    temperature: float = 0.7
    top_p: float = 0.9
    
    # ë²¡í„° ê²€ìƒ‰ ì„¤ì • (ë©€í‹° ë²¤ë” ì§€ì›)
    vector_dimension: int = 1536  # ê¸°ë³¸ê°’: Azure text-embedding-3-small (.envì—ì„œ ì˜¤ë²„ë¼ì´ë“œ)
    
    # ë²¤ë”ë³„ ë²¡í„° ì°¨ì› (ê³ ì •ê°’)
    azure_vector_dimension_small: int = 1536   # Azure text-embedding-3-small
    azure_vector_dimension_large: int = 3072   # Azure text-embedding-3-large
    azure_clip_dimension: int = 512            # Azure CLIP multimodal
    aws_vector_dimension: int = 1024           # AWS Titan v2 / Cohere v4
    aws_vector_dimension_small: int = 256      # AWS Titan v2 small
    
    similarity_threshold: float = 0.7
    
    # RAG ê²€ìƒ‰ ì„¤ì •
    rag_similarity_threshold: float = 0.3
    rag_max_chunks: int = 30
    rag_use_reranking: bool = True
    
    # ë¦¬ë­í‚¹ ì œê³µì ì„¤ì •
    rag_reranking_provider: str = Field(default="azure_openai")  # azure_openai | bedrock
    
    # ë¦¬ë­í‚¹ ì „ìš© Azure OpenAI ì„¤ì •
    rag_reranking_endpoint: Optional[str] = None
    rag_reranking_api_key: Optional[str] = None
    rag_reranking_deployment: str = Field(default="")
    rag_reranking_api_version: str = Field(default="")
    rag_reranking_max_completion_tokens: int = 500
    rag_reranking_reasoning_effort: Optional[str] = None
    rag_reranking_temperature: float = 0.3
    rag_reranking_max_tokens: int = 500
    
    # ë¦¬ë­í‚¹ ì „ìš© AWS Bedrock ì„¤ì •
    rag_reranking_bedrock_model_id: str = Field(default="")
    rag_reranking_bedrock_region: str = Field(default="")
    
    # OpenSearch ì„¤ì •
    opensearch_endpoint: Optional[str] = None
    opensearch_username: str = "admin"
    opensearch_password: Optional[str] = None
    opensearch_index: str = "wkms-documents"
    
    # Pinecone ì„¤ì •
    pinecone_api_key: Optional[str] = None
    pinecone_environment: Optional[str] = None
    pinecone_index_name: str = "wkms-index"
    
    # ë¡œê¹… ì„¤ì •
    log_level: str = "INFO"
    log_format: str = "json"
    log_dir: str = "logs"
    log_file_name: str = "backend.log"
    log_max_bytes: int = 5 * 1024 * 1024  # 5MB
    log_backup_count: int = 5
    # ë³„ë„ SQL ë¡œê·¸ ì„¤ì •
    sql_query_log_enabled: bool = True
    sql_query_log_file_name: str = "sql.log"
    sql_query_log_level: str = "INFO"
    sql_query_log_format: str = "plain"  # plain | json
    sql_query_log_all: bool = False  # Trueë©´ SLOW/SAMPLE ë¿ ì•„ë‹ˆë¼ ëª¨ë“  ì¿¼ë¦¬ ê¸°ë¡

    # SQL ë¡œê¹… ì œì–´ (ì„¸ë°€ë„ ì¡°ì •)
    sqlalchemy_echo: bool = False  # ê°œë³„ SQL / íŒŒë¼ë¯¸í„° ì¶œë ¥ (ê¸°ë³¸ ë¹„í™œì„±í™”)
    sql_log_slow_threshold_ms: int = 300  # ëŠë¦° ì¿¼ë¦¬ (ms) ì´ìƒë§Œ ìš”ì•½ ë¡œê·¸, 0 ë˜ëŠ” ìŒìˆ˜ë©´ ë¹„í™œì„±í™”
    sql_log_sample_rate: float = 0.0  # 0~1 ì‚¬ì´, ëŠë¦° ì¿¼ë¦¬ ì™¸ ì„ì˜ ìƒ˜í”Œ ë¡œê·¸ (ë¶€í•˜ ë¶„ì„ìš©)
    
    # í”„ë ˆì  í…Œì´ì…˜ ì‚°ì¶œë¬¼ ì €ì¥ ê²½ë¡œ
    presentation_output_dir: str = "data/presentations"
    
    # Office Generator Service ì„¤ì •
    office_generator_url: str = Field(
        default_factory=lambda: os.getenv('OFFICE_GENERATOR_URL', 'http://localhost:3001')
    )
    office_generator_timeout: int = 60  # seconds

    # ì‹¤í–‰ í™˜ê²½
    environment: str = "development"

    # -----------------------------
    # Web Search / External Augmentation ì„¤ì •
    # -----------------------------
    web_search_enabled: bool = True  # ë‚´ë¶€ RAG ì €ì‹ ë¢° ì‹œ ì™¸ë¶€ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ (feature flag)
    web_search_provider: str = "mock"  # mock | serpapi | tavily | bing | brave (ì¶”ê°€ ê°€ëŠ¥)
    serpapi_api_key: Optional[str] = None
    tavily_api_key: Optional[str] = None
    bing_search_api_key: Optional[str] = None
    brave_search_api_key: Optional[str] = None
    web_search_max_results: int = 6
    web_search_timeout_seconds: int = 8
    web_search_cache_ttl_seconds: int = 60 * 60 * 6  # 6ì‹œê°„ ìºì‹œ
    web_search_dual_language: bool = True  # ko/en ë³‘ë ¬ ê²€ìƒ‰
    web_search_result_language: str = "ko"  # ê²°ê³¼ ìš”ì•½ ì–¸ì–´
    web_search_log_queries: bool = False  # ê°œì¸ì •ë³´ í¬í•¨ ì§ˆì˜ ì™¸ë¶€ ì „ì†¡ ì „ì— ë§ˆìŠ¤í‚¹ í•„ìš”
    
    # -----------------------------
    # Patent Search ì„¤ì • (Enterprise Intelligence)
    # -----------------------------
    patent_search_enabled: bool = True  # íŠ¹í—ˆ ê²€ìƒ‰ ê¸°ëŠ¥ í™œì„±í™”
    kipris_api_key: Optional[str] = None  # KIPRIS API í‚¤ (í•œêµ­ íŠ¹í—ˆ)
    kipris_api_endpoint: str = "http://plus.kipris.or.kr/openapi/rest"  # KIPRIS REST API ì—”ë“œí¬ì¸íŠ¸
    # SerpAPI Google Patents (ê¸€ë¡œë²Œ íŠ¹í—ˆ ê²€ìƒ‰)
    serpapi_google_patents_enabled: bool = True  # SerpAPI Google Patents ì‚¬ìš© ì—¬ë¶€
    # serpapi_api_keyëŠ” ìœ„ web_searchì—ì„œ ì´ë¯¸ ì •ì˜ë¨ (ê³µìœ  ì‚¬ìš©)
    uspto_api_endpoint: str = "https://api.patentsview.org/patents"  # USPTO PatentsView API (ë¬´ë£Œ)
    patent_search_max_results: int = 20  # ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
    patent_search_timeout_seconds: int = 30  # API íƒ€ì„ì•„ì›ƒ
    patent_search_cache_ttl_seconds: int = 60 * 60 * 24  # 24ì‹œê°„ ìºì‹œ (íŠ¹í—ˆ ë°ì´í„°ëŠ” ë³€ë™ì´ ì ìŒ)

    # Web page fetch (ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ í˜ì´ì§€ ì¶”ì¶œ) ì„¤ì •
    web_fetch_enabled: bool = True
    web_fetch_timeout_seconds: int = 10
    web_fetch_max_concurrent: int = 4
    web_fetch_max_chars: int = 8000  # í˜ì´ì§€ë‹¹ ìµœëŒ€ ì¶”ì¶œ ê¸¸ì´
    web_fetch_user_agent: str = "WKMSBot/0.1 (+https://example.invalid)"
    web_fetch_allow_domains: List[str] = Field(default_factory=list)  # ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´ í—ˆìš©(ì°¨ë‹¨ ëª©ë¡ ìš°ì„ )
    web_fetch_block_domains: List[str] = Field(default_factory=lambda: ["facebook.com", "instagram.com"])
    
    def get_embedding_dimension(self, model_id: str) -> int:
        """ì„ë² ë”© ëª¨ë¸ì— ë”°ë¥¸ ì‹¤ì œ ì°¨ì› ìˆ˜ ë°˜í™˜ - ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì›ë³¸ ì°¨ì› ì‚¬ìš©"""
        dimension_map = {
            # AWS Bedrock Titan ëª¨ë¸ë“¤
            "amazon.titan-embed-text-v1:0": 1536,
            "amazon.titan-embed-text-v2:0": 1024,  # ê¸°ë³¸ê°’, 512, 256ë„ ì§€ì›
            
            # Azure OpenAI ëª¨ë¸ë“¤ (ì›ë³¸ ì°¨ì› ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            "text-embedding-ada-002": 1536,        # ì›ë³¸ 1536ì°¨ì›
            "text-embedding-3-small": 1536,        # ì›ë³¸ 1536ì°¨ì›
            "text-embedding-3-large": 3072,        # ì›ë³¸ 3072ì°¨ì›
            
            # OpenAI ëª¨ë¸ë“¤ (ì›ë³¸ ì°¨ì› ì‚¬ìš©)
            "text-embedding-ada-002": 1536,
        }
        return dimension_map.get(model_id, self.vector_dimension)
    
    def apply_smart_dimension_reduction(self, embedding: list, target_dim: int = 1024) -> list:
        """ìŠ¤ë§ˆíŠ¸ ì°¨ì› ì¶•ì†Œ - ì„±ëŠ¥ ì €í•˜ ìµœì†Œí™”"""
        if not embedding or len(embedding) <= target_dim:
            return embedding
            
        if len(embedding) == 1536:  # OpenAI ada-002, 3-small
            # 1536 â†’ 1024: ì•ìª½ 1024ê°œ + ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ
            return embedding[:target_dim]
        elif len(embedding) == 3072:  # OpenAI 3-large  
            # 3072 â†’ 1024: 3ë“±ë¶„í•´ì„œ ê° êµ¬ê°„ì—ì„œ ì„ íƒ
            step = len(embedding) // target_dim
            return [embedding[i * step] for i in range(target_dim)]
        else:
            return embedding[:target_dim]
    
    def get_current_embedding_dimension(self) -> int:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© ëª¨ë¸ì˜ ì°¨ì› ìˆ˜ ë°˜í™˜"""
        if self.default_llm_provider == "bedrock":
            return self.get_embedding_dimension(self.bedrock_embedding_model_id)
        elif self.default_llm_provider == "azure_openai":
            return self.get_embedding_dimension(self.azure_openai_embedding_deployment)
        elif self.default_llm_provider == "openai":
            return self.get_embedding_dimension(self.openai_embedding_model)
        else:
            return self.vector_dimension
    
    def get_current_llm_model(self) -> str:
        """í˜„ì¬ ì„¤ì •ëœ LLM ëª¨ë¸ ID ë°˜í™˜"""
        provider = self.get_current_llm_provider()
        
        if provider == "bedrock":
            return self.bedrock_llm_model_id
        elif provider == "azure_openai":
            return self.azure_openai_llm_deployment
        elif provider == "openai":
            return self.openai_llm_model
        else:
            return self.bedrock_llm_model_id  # ê¸°ë³¸ê°’

    def get_current_multimodal_model(self) -> str:
        """ë©€í‹°ëª¨ë‹¬(ë¹„ì „)ìš© ëª¨ë¸ ë°˜í™˜ (LLM for vision)"""
        provider = self.get_current_llm_provider()
        
        if provider == "bedrock":
            return self.bedrock_multimodal_llm_model_id
        elif provider == "azure_openai":
            if self.azure_openai_multimodal_deployment:
                return self.azure_openai_multimodal_deployment
            return self.azure_openai_llm_deployment
        else:
            return self.bedrock_multimodal_llm_model_id
    
    def get_current_embedding_model(self) -> str:
        """í˜„ì¬ ì„¤ì •ëœ ì„ë² ë”© ëª¨ë¸ ID ë°˜í™˜"""
        provider = self.get_current_embedding_provider()
        
        if provider == "bedrock":
            return self.bedrock_embedding_model_id
        elif provider == "azure_openai":
            return self.azure_openai_embedding_deployment
        elif provider == "openai":
            return self.openai_embedding_model
        else:
            return self.bedrock_embedding_model_id  # ê¸°ë³¸ê°’
    
    def get_current_multimodal_embedding_model(self) -> str:
        """í˜„ì¬ ì„¤ì •ëœ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ëª¨ë¸ ë°˜í™˜ (ì´ë¯¸ì§€+í…ìŠ¤íŠ¸)"""
        provider = self.get_current_embedding_provider()
        
        if provider == "bedrock":
            return self.bedrock_multimodal_embedding_model_id
        elif provider == "azure_openai":
            return self.azure_openai_multimodal_embedding_deployment
        else:
            return self.bedrock_multimodal_embedding_model_id
    
    def get_current_multimodal_embedding_dimension(self) -> int:
        """í˜„ì¬ ì„¤ì •ëœ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        provider = self.get_current_embedding_provider()
        
        if provider == "bedrock":
            return self.bedrock_multimodal_embedding_dimension
        elif provider == "azure_openai":
            return self.clip_embedding_dimension
        else:
            return self.bedrock_multimodal_embedding_dimension
    
    def get_current_multimodal_endpoint(self) -> Optional[str]:
        """í˜„ì¬ ì„¤ì •ëœ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ì—”ë“œí¬ì¸íŠ¸ ë°˜í™˜"""
        provider = self.get_current_embedding_provider()
        
        if provider == "azure_openai":
            return self.azure_openai_multimodal_embedding_endpoint
        elif provider == "bedrock":
            return f"AWS Bedrock - {self.aws_region}"
        else:
            return None
    
    def is_multimodal_enabled(self) -> bool:
        """ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ í™œì„±í™” ì—¬ë¶€ ë°˜í™˜"""
        provider = self.get_current_embedding_provider()
        
        if provider == "bedrock":
            # Bedrockì€ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ í™œì„±í™”
            return bool(self.bedrock_multimodal_embedding_model_id)
        elif provider == "azure_openai":
            # AzureëŠ” CLIP ì—”ë“œí¬ì¸íŠ¸ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í™œì„±í™”
            return bool(self.azure_openai_multimodal_embedding_endpoint)
        else:
            return False
    
    def get_current_llm_provider(self) -> str:
        """í˜„ì¬ ì„¤ì •ëœ LLM ê³µê¸‰ì ë°˜í™˜"""
        return self.default_llm_provider
    
    def get_current_embedding_provider(self) -> str:
        """í˜„ì¬ ì„¤ì •ëœ ì„ë² ë”© ê³µê¸‰ì ë°˜í™˜"""
        if self.default_embedding_provider:
            return self.default_embedding_provider
        return self.default_llm_provider  # ì„ë² ë”©ì€ LLMê³¼ ê°™ì€ ê³µê¸‰ì ì‚¬ìš©
    
    def get_query_rewrite_config(self) -> dict:
        """ì§ˆì˜ë¬¸ ì¬ì‘ì„± LLM ì„¤ì • ë°˜í™˜"""
        config = {
            "provider": self.query_rewrite_provider,
            "max_tokens": self.query_rewrite_max_tokens,
            "temperature": self.query_rewrite_temperature,
        }
        
        if self.query_rewrite_provider == "azure_openai":
            config.update({
                "deployment": self.query_rewrite_azure_deployment,
                "endpoint": self.query_rewrite_azure_endpoint or self.azure_openai_endpoint,
                "api_key": self.query_rewrite_azure_api_key or self.azure_openai_api_key,
                "api_version": self.query_rewrite_azure_api_version,
            })
        elif self.query_rewrite_provider == "bedrock":
            config.update({
                "model_id": self.query_rewrite_bedrock_model_id,
                "region": self.query_rewrite_bedrock_region or self.aws_region,
            })
        
        return config
    
    # Bedrock ê´€ë ¨ ì„¤ì • ì¶”ê°€
    bedrock_max_tokens: int = 4096
    bedrock_temperature: float = 0.7
    bedrock_top_p: float = 0.9
    bedrock_top_k: int = 50
    
    # Agent-based RAG ì„¤ì • (Phase 2)
    use_agent_architecture: bool = False  # Feature flag: ì ì§„ì  ë¡¤ì•„ì›ƒ
    agent_enable_observability: bool = True  # Agent ì‹¤í–‰ ë‹¨ê³„ ì¶”ì 
    agent_enable_evaluation: bool = True  # í‰ê°€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    enable_new_summary_agent: bool = False  # ì‹ ê·œ ìš”ì•½ ì—ì´ì „íŠ¸ ì‚¬ìš© ì—¬ë¶€
    enable_new_presentation_agent: bool = False  # ì‹ ê·œ PPT ì—ì´ì „íŠ¸ ì‚¬ìš© ì—¬ë¶€
    
    # Office Generator Service (Node.js PptxGenJS)
    pptxgenjs_service_url: str = "http://localhost:3001"
    pptxgenjs_api_key: str = ""
    presentation_output_dir: str = "uploads/presentations"
    
    # ì§ˆì˜ë¬¸ ì¬ì‘ì„± ë° ì˜ë„ ë¶„ë¥˜ LLM ì„¤ì •
    query_rewrite_provider: str = "azure_openai"  # azure_openai | bedrock
    # Azure OpenAI ì„¤ì •
    query_rewrite_azure_deployment: str = "gpt-4o"
    query_rewrite_azure_endpoint: str = ""
    query_rewrite_azure_api_key: str = ""
    query_rewrite_azure_api_version: str = "2024-12-01-preview"
    # Bedrock ì„¤ì •
    query_rewrite_bedrock_model_id: str = "anthropic.claude-3-haiku-20240307-v1:0"
    query_rewrite_bedrock_region: str = "ap-northeast-2"
    # ê³µí†µ íŒŒë¼ë¯¸í„°
    query_rewrite_max_tokens: int = 500
    query_rewrite_temperature: float = 0.3

    def log_file_path(self) -> str:
        return os.path.join(self.log_dir, self.log_file_name)

    def sql_log_file_path(self) -> str:
        return os.path.join(self.log_dir, self.sql_query_log_file_name)

    # ------------------------------------------------------------------
    # Post init hook (pydantic v2) to normalize CORS origins when provided
    # via environment variable as a single comma-separated string.
    # e.g. CORS_ORIGINS="http://a:3000,http://b:3000" would otherwise
    # become ["http://a:3000,http://b:3000"] (single entry) causing
    # only the first origin to effectively work in practice/logging.
    # ------------------------------------------------------------------
    def model_post_init(self, __context: any) -> None:  # type: ignore[override]
        try:
            if len(self.cors_origins) == 1:
                raw = self.cors_origins[0]
                if "," in raw and raw.count("http") > 1:
                    # Split on commas, strip whitespace
                    split_list = [o.strip() for o in raw.split(",") if o.strip()]
                    if split_list:
                        self.cors_origins = split_list  # type: ignore[assignment]
        except Exception:
            # Fail silently; CORS will just use whatever was parsed
            pass
        
        # ë¦¬ë­í‚¹ ëª¨ë¸ ê²€ì¦ (RAG_USE_RERANKING=trueì¼ ë•Œ í•„ìˆ˜)
        if self.rag_use_reranking:
            import sys
            if self.rag_reranking_provider == "azure_openai":
                if not self.rag_reranking_deployment:
                    print("âŒ ì—ëŸ¬: RAG_RERANKING_PROVIDER=azure_openaiì´ì§€ë§Œ RAG_RERANKING_DEPLOYMENTê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    print("ğŸ’¡ í•´ê²°: backend/.env íŒŒì¼ì— RAG_RERANKING_DEPLOYMENT=gpt-5-nano ë“±ì„ ì¶”ê°€í•˜ì„¸ìš”.")
                    sys.exit(1)
            elif self.rag_reranking_provider == "bedrock":
                if not self.rag_reranking_bedrock_model_id:
                    print("âŒ ì—ëŸ¬: RAG_RERANKING_PROVIDER=bedrockì´ì§€ë§Œ RAG_RERANKING_BEDROCK_MODEL_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    print("ğŸ’¡ í•´ê²°: backend/.env íŒŒì¼ì— RAG_RERANKING_BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0 ë“±ì„ ì¶”ê°€í•˜ì„¸ìš”.")
                    sys.exit(1)


settings = Settings()

"""
ì„ í–‰ê¸°ìˆ  íƒì§€ í‰ê°€ í”„ë ˆì„ì›Œí¬ (ë…¼ë¬¸ ì‹¤í—˜ìš©)

ì´ ëª¨ë“ˆì€ KIPRIS ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì„ í–‰ê¸°ìˆ  íƒì§€ ì‹¤í—˜ì˜ í‰ê°€ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
Ground Truth(ì‹¬ì‚¬ê´€ ì¸ìš© ì„ í–‰ê¸°ìˆ )ì™€ ì—ì´ì „íŠ¸ ì˜ˆì¸¡ì„ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
import numpy as np
from app.evaluation.metrics import (
    calculate_recall_at_k,
    calculate_precision_at_k
)


class EvaluationResult(BaseModel):
    """ë‹¨ì¼ ì¼€ì´ìŠ¤ í‰ê°€ ê²°ê³¼"""
    patent_id: str = Field(..., description="ëŒ€ìƒ íŠ¹í—ˆ ì¶œì›ë²ˆí˜¸")
    ground_truth: List[str] = Field(..., description="Ground Truth ì„ í–‰ê¸°ìˆ  ë¦¬ìŠ¤íŠ¸")
    predictions: List[str] = Field(..., description="ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    recall_at_k: Dict[int, float] = Field(default_factory=dict, description="Recall@K")
    precision_at_k: Dict[int, float] = Field(default_factory=dict, description="Precision@K")
    f1_at_k: Dict[int, float] = Field(default_factory=dict, description="F1-Score@K")
    average_precision: float = Field(0.0, description="Average Precision (AP)")
    
    class Config:
        json_schema_extra = {
            "example": {
                "patent_id": "1020047020979",
                "ground_truth": ["US20050056824 A1", "US20050167836 A1"],
                "predictions": ["US20050056824 A1", "JP2014194966 A", "..."],
                "recall_at_k": {10: 0.5, 20: 0.5, 50: 1.0, 100: 1.0},
                "precision_at_k": {10: 0.1, 20: 0.05, 50: 0.04, 100: 0.02},
                "f1_at_k": {10: 0.167, 20: 0.091, 50: 0.077, 100: 0.039},
                "average_precision": 0.75
            }
        }


class BatchEvaluationSummary(BaseModel):
    """ë°°ì¹˜ í‰ê°€ ìš”ì•½"""
    num_cases: int = Field(..., description="í‰ê°€ ì¼€ì´ìŠ¤ ìˆ˜")
    avg_recall: Dict[int, float] = Field(default_factory=dict, description="í‰ê·  Recall@K")
    avg_precision: Dict[int, float] = Field(default_factory=dict, description="í‰ê·  Precision@K")
    avg_f1: Dict[int, float] = Field(default_factory=dict, description="í‰ê·  F1-Score@K")
    mean_average_precision: float = Field(0.0, description="Mean Average Precision (MAP)")
    
    # í†µê³„
    std_recall: Optional[Dict[int, float]] = Field(default_factory=dict)
    std_precision: Optional[Dict[int, float]] = Field(default_factory=dict)
    std_f1: Optional[Dict[int, float]] = Field(default_factory=dict)
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    target_recall_100: float = Field(0.80, description="Recall@100 ëª©í‘œì¹˜")
    achieved_target: bool = Field(False, description="ëª©í‘œ ë‹¬ì„± ì—¬ë¶€")


class PriorArtEvaluator:
    """ì„ í–‰ê¸°ìˆ  íƒì§€ í‰ê°€ê¸°"""
    
    def __init__(self, k_values: List[int] = [10, 20, 50, 100]):
        """
        Args:
            k_values: í‰ê°€í•  K ê°’ ë¦¬ìŠ¤íŠ¸
        """
        self.k_values = k_values
    
    @staticmethod
    def normalize_patent_id(patent_id: str) -> str:
        """
        íŠ¹í—ˆë²ˆí˜¸ ì •ê·œí™” (ë¹„êµë¥¼ ìœ„í•´)
        
        Examples:
            "US 20050056824 A1" -> "US20050056824A1"
            "JP 2014-194966 A" -> "JP2014194966A"
            "1020047020979" -> "1020047020979"
        """
        # ê³µë°±, í•˜ì´í”ˆ ì œê±° í›„ ëŒ€ë¬¸ì ë³€í™˜
        return patent_id.replace(' ', '').replace('-', '').upper()
    
    def calculate_recall_at_k_normalized(
        self,
        ground_truth: List[str],
        predictions: List[str],
        k: int
    ) -> float:
        """
        íŠ¹í—ˆë²ˆí˜¸ ì •ê·œí™” í›„ Recall@K ê³„ì‚°
        
        Args:
            ground_truth: Ground Truth ì„ í–‰ê¸°ìˆ  ë¦¬ìŠ¤íŠ¸
            predictions: ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ ìˆœ)
            k: ìƒìœ„ Kê°œ ê³ ë ¤
        
        Returns:
            Recall@K ê°’ (0.0 ~ 1.0)
        """
        if not ground_truth:
            return 0.0
        
        # ì •ê·œí™”
        gt_normalized = set(self.normalize_patent_id(p) for p in ground_truth)
        pred_normalized = set(self.normalize_patent_id(p) for p in predictions[:k])
        
        # êµì§‘í•©
        hits = len(gt_normalized & pred_normalized)
        
        return hits / len(ground_truth)
    
    def calculate_precision_at_k_normalized(
        self,
        ground_truth: List[str],
        predictions: List[str],
        k: int
    ) -> float:
        """
        íŠ¹í—ˆë²ˆí˜¸ ì •ê·œí™” í›„ Precision@K ê³„ì‚°
        
        Args:
            ground_truth: Ground Truth ì„ í–‰ê¸°ìˆ  ë¦¬ìŠ¤íŠ¸
            predictions: ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ ìˆœ)
            k: ìƒìœ„ Kê°œ ê³ ë ¤
        
        Returns:
            Precision@K ê°’ (0.0 ~ 1.0)
        """
        if k == 0:
            return 0.0
        
        # ì •ê·œí™”
        gt_normalized = set(self.normalize_patent_id(p) for p in ground_truth)
        pred_normalized = set(self.normalize_patent_id(p) for p in predictions[:k])
        
        # êµì§‘í•©
        hits = len(gt_normalized & pred_normalized)
        
        return hits / k
    
    def calculate_f1_at_k(
        self,
        ground_truth: List[str],
        predictions: List[str],
        k: int
    ) -> float:
        """
        F1-Score@K ê³„ì‚°
        
        Args:
            ground_truth: Ground Truth ì„ í–‰ê¸°ìˆ  ë¦¬ìŠ¤íŠ¸
            predictions: ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            k: ìƒìœ„ Kê°œ ê³ ë ¤
        
        Returns:
            F1-Score@K ê°’ (0.0 ~ 1.0)
        """
        recall = self.calculate_recall_at_k_normalized(ground_truth, predictions, k)
        precision = self.calculate_precision_at_k_normalized(ground_truth, predictions, k)
        
        if recall + precision == 0:
            return 0.0
        
        return 2 * (precision * recall) / (precision + recall)
    
    def calculate_average_precision(
        self,
        ground_truth: List[str],
        predictions: List[str]
    ) -> float:
        """
        Average Precision (AP) ê³„ì‚°
        
        APëŠ” ìˆœìœ„ë¥¼ ê³ ë ¤í•œ í‰ê°€ ì§€í‘œì…ë‹ˆë‹¤.
        ìƒìœ„ì— Ground Truthê°€ ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.
        
        Formula:
            AP = (1 / |GT|) * Î£(Precision@k Ã— rel(k))
            where rel(k) = 1 if kë²ˆì§¸ ê²°ê³¼ê°€ GTì— í¬í•¨, else 0
        
        Args:
            ground_truth: Ground Truth ì„ í–‰ê¸°ìˆ  ë¦¬ìŠ¤íŠ¸
            predictions: ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ ìˆœ)
        
        Returns:
            AP ê°’ (0.0 ~ 1.0)
        """
        if not ground_truth:
            return 0.0
        
        gt_normalized = set(self.normalize_patent_id(p) for p in ground_truth)
        
        num_hits = 0
        sum_precisions = 0.0
        
        for k, pred in enumerate(predictions, start=1):
            pred_normalized = self.normalize_patent_id(pred)
            
            # GTì— í¬í•¨ë˜ë©´
            if pred_normalized in gt_normalized:
                num_hits += 1
                precision_at_k = num_hits / k
                sum_precisions += precision_at_k
        
        return sum_precisions / len(ground_truth) if ground_truth else 0.0
    
    def evaluate_single_case(
        self,
        patent_id: str,
        ground_truth: List[str],
        predictions: List[str]
    ) -> EvaluationResult:
        """
        ë‹¨ì¼ ì¼€ì´ìŠ¤ í‰ê°€
        
        Args:
            patent_id: ëŒ€ìƒ íŠ¹í—ˆ ì¶œì›ë²ˆí˜¸
            ground_truth: Ground Truth ì„ í–‰ê¸°ìˆ  ë¦¬ìŠ¤íŠ¸
            predictions: ì—ì´ì „íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            EvaluationResult ê°ì²´
        """
        recall_at_k = {}
        precision_at_k = {}
        f1_at_k = {}
        
        # ê° K ê°’ì— ëŒ€í•´ ê³„ì‚°
        for k in self.k_values:
            recall_at_k[k] = self.calculate_recall_at_k_normalized(
                ground_truth, predictions, k
            )
            precision_at_k[k] = self.calculate_precision_at_k_normalized(
                ground_truth, predictions, k
            )
            f1_at_k[k] = self.calculate_f1_at_k(
                ground_truth, predictions, k
            )
        
        # Average Precision ê³„ì‚°
        ap = self.calculate_average_precision(ground_truth, predictions)
        
        return EvaluationResult(
            patent_id=patent_id,
            ground_truth=ground_truth,
            predictions=predictions,
            recall_at_k=recall_at_k,
            precision_at_k=precision_at_k,
            f1_at_k=f1_at_k,
            average_precision=ap
        )
    
    def evaluate_batch(
        self,
        test_cases: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ë°°ì¹˜ í‰ê°€ ë° í†µê³„ ê³„ì‚°
        
        Args:
            test_cases: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
                í˜•ì‹: [
                    {
                        "patent_id": "1020047020979",
                        "ground_truth": ["US20050056824 A1", ...],
                        "predictions": ["...", ...]
                    },
                    ...
                ]
        
        Returns:
            {
                "summary": BatchEvaluationSummary,
                "details": List[EvaluationResult]
            }
        """
        results = []
        
        # ê° ì¼€ì´ìŠ¤ í‰ê°€
        for case in test_cases:
            result = self.evaluate_single_case(
                patent_id=case['patent_id'],
                ground_truth=case['ground_truth'],
                predictions=case['predictions']
            )
            results.append(result)
        
        # í†µê³„ ê³„ì‚°
        summary = self._calculate_summary(results)
        
        return {
            'summary': summary,
            'details': results
        }
    
    def _calculate_summary(
        self,
        results: List[EvaluationResult]
    ) -> BatchEvaluationSummary:
        """
        ë°°ì¹˜ í‰ê°€ ìš”ì•½ í†µê³„ ê³„ì‚°
        
        Args:
            results: ê°œë³„ í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            BatchEvaluationSummary ê°ì²´
        """
        num_cases = len(results)
        
        # ê° ë©”íŠ¸ë¦­ë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨
        avg_recall = {}
        avg_precision = {}
        avg_f1 = {}
        std_recall = {}
        std_precision = {}
        std_f1 = {}
        
        for k in self.k_values:
            recall_values = [r.recall_at_k[k] for r in results]
            precision_values = [r.precision_at_k[k] for r in results]
            f1_values = [r.f1_at_k[k] for r in results]
            
            avg_recall[k] = float(np.mean(recall_values))
            avg_precision[k] = float(np.mean(precision_values))
            avg_f1[k] = float(np.mean(f1_values))
            
            std_recall[k] = float(np.std(recall_values))
            std_precision[k] = float(np.std(precision_values))
            std_f1[k] = float(np.std(f1_values))
        
        # MAP ê³„ì‚°
        ap_values = [r.average_precision for r in results]
        mean_average_precision = float(np.mean(ap_values))
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ (Recall@100 >= 80%)
        target_recall_100 = 0.80
        achieved_target = avg_recall.get(100, 0.0) >= target_recall_100
        
        return BatchEvaluationSummary(
            num_cases=num_cases,
            avg_recall=avg_recall,
            avg_precision=avg_precision,
            avg_f1=avg_f1,
            std_recall=std_recall,
            std_precision=std_precision,
            std_f1=std_f1,
            mean_average_precision=mean_average_precision,
            target_recall_100=target_recall_100,
            achieved_target=achieved_target
        )
    
    def print_summary(self, summary: BatchEvaluationSummary) -> None:
        """
        í‰ê°€ ìš”ì•½ ì¶œë ¥ (ì½˜ì†”)
        
        Args:
            summary: BatchEvaluationSummary ê°ì²´
        """
        print("\n" + "="*70)
        print("ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
        print("="*70)
        
        print(f"\nì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {summary.num_cases}ê±´\n")
        
        print("ğŸ“Š Recall@K:")
        for k in sorted(summary.avg_recall.keys()):
            v = summary.avg_recall[k]
            std = summary.std_recall.get(k, 0.0)
            print(f"  @{k:3d}: {v:.4f} ({v*100:5.2f}%) Â± {std:.4f}")
        
        print("\nğŸ“Š Precision@K:")
        for k in sorted(summary.avg_precision.keys()):
            v = summary.avg_precision[k]
            std = summary.std_precision.get(k, 0.0)
            print(f"  @{k:3d}: {v:.4f} ({v*100:5.2f}%) Â± {std:.4f}")
        
        print("\nğŸ“Š F1-Score@K:")
        for k in sorted(summary.avg_f1.keys()):
            v = summary.avg_f1[k]
            std = summary.std_f1.get(k, 0.0)
            print(f"  @{k:3d}: {v:.4f} Â± {std:.4f}")
        
        print(f"\nğŸ“Š MAP: {summary.mean_average_precision:.4f}")
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        print("\n" + "="*70)
        print("ğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€")
        print("="*70)
        
        recall_100 = summary.avg_recall.get(100, 0.0)
        if summary.achieved_target:
            print(f"âœ… Recall@100: {recall_100*100:.2f}% â‰¥ {summary.target_recall_100*100:.0f}% (ëª©í‘œ ë‹¬ì„±!)")
        else:
            print(f"âŒ Recall@100: {recall_100*100:.2f}% < {summary.target_recall_100*100:.0f}% (ëª©í‘œ ë¯¸ë‹¬ì„±)")
        
        print("="*70 + "\n")

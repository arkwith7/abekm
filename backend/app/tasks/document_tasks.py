"""
ë¬¸ì„œ ì²˜ë¦¬ ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬
====================================

Celeryë¥¼ ì‚¬ìš©í•œ ë¬¸ì„œ ì²˜ë¦¬ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
- ë¬¸ì„œ ì—…ë¡œë“œ í›„ DI ë¶„ì„, ì„ë² ë”© ìƒì„±
- ì¥ì‹œê°„ ì‹¤í–‰ ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
- ì²˜ë¦¬ ìƒíƒœë¥¼ DBì— ê¸°ë¡

ì‚¬ìš©ë²•:
-------
from app.tasks.document_tasks import process_document_async

# íƒœìŠ¤í¬ í˜¸ì¶œ
task = process_document_async.delay(
    document_id=123,
    file_path="/path/to/file.pdf",
    container_id="container_1",
    user_emp_no="12345"
)

# íƒœìŠ¤í¬ IDë¡œ ìƒíƒœ ì¡°íšŒ
result = AsyncResult(task.id)
"""

from celery import Task
from app.core.celery_app import celery_app
from datetime import datetime
import logging
import asyncio
import nest_asyncio
from typing import Optional, Dict, Any, cast

# Celery Workerì—ì„œ asyncio.run() ì‚¬ìš©ì„ ìœ„í•œ ì„¤ì •
nest_asyncio.apply()
import traceback

from app.core.config import settings

logger = logging.getLogger(__name__)


class CallbackTask(Task):
    """
    ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì»¤ìŠ¤í…€ Task í´ë˜ìŠ¤
    
    ì‘ì—… ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ DBì— ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ì‘ì—… ì‹¤íŒ¨ ì‹œ í˜¸ì¶œ"""
        document_id = args[0] if args else kwargs.get('document_id')
        if document_id:
            error_msg = f"{type(exc).__name__}: {str(exc)}"
            logger.error(f"âŒ [TASK-FAIL] ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: doc_id={document_id}, error={error_msg}")
            self.update_status(document_id, 'failed', error_msg)
    
    def on_success(self, retval, task_id, args, kwargs):
        """ì‘ì—… ì„±ê³µ ì‹œ í˜¸ì¶œ"""
        document_id = args[0] if args else kwargs.get('document_id')
        logger.info(f"âœ… [TASK-SUCCESS] ë¬¸ì„œ ì²˜ë¦¬ ì„±ê³µ: doc_id={document_id}, task_id={task_id}")
    
    def update_status(self, document_id: int, status: str, error: Optional[str] = None):
        """
        ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸ (ë™ê¸° ë˜í¼)
        
        Args:
            document_id: ë¬¸ì„œ ID
            status: ì²˜ë¦¬ ìƒíƒœ (pending/processing/completed/failed)
            error: ì˜¤ë¥˜ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
        """
        try:
            asyncio.run(self._update_status_async(document_id, status, error))
        except Exception as e:
            logger.error(f"âŒ [STATUS-UPDATE] ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: doc_id={document_id}, error={e}")
    
    async def _update_status_async(self, document_id: int, status: str, error: Optional[str] = None):
        """ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸ (ë¹„ë™ê¸°)"""
        from app.core.database import get_async_session_local
        from app.models import TbFileBssInfo
        from sqlalchemy import update
        
        async_session_factory = get_async_session_local()
        async with async_session_factory() as session:
            try:
                update_data: Dict[str, Any] = {'processing_status': status}
                
                if error:
                    update_data['processing_error'] = error[:1000]  # ì˜¤ë¥˜ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
                
                if status == 'processing':
                    update_data['processing_started_at'] = datetime.now()
                elif status in ('completed', 'failed'):
                    update_data['processing_completed_at'] = datetime.now()
                
                stmt = (
                    update(TbFileBssInfo)
                    .where(TbFileBssInfo.file_bss_info_sno == document_id)
                    .values(**update_data)
                )
                await session.execute(stmt)
                await session.commit()
                
                logger.info(f"âœ… [STATUS-UPDATE] ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: doc_id={document_id}, status={status}")
            except Exception as e:
                logger.error(f"âŒ [STATUS-UPDATE] DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                await session.rollback()


@celery_app.task(bind=True, base=CallbackTask, name='process_document_async')
def process_document_async(
    self,
    document_id: int,
    file_path: str,
    container_id: str,
    user_emp_no: str,
    provider: Optional[str] = None,
    model_profile: str = "default"
):
    """
    ë¬¸ì„œ ë¹„ë™ê¸° ì²˜ë¦¬ íƒœìŠ¤í¬ (ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸)
    
    ì²˜ë¦¬ ë‹¨ê³„:
    1. ìƒíƒœë¥¼ 'processing'ìœ¼ë¡œ ë³€ê²½
    2. Azure DIë¡œ ë¬¸ì„œ ë¶„ì„ (í…ìŠ¤íŠ¸, í‘œ, ì´ë¯¸ì§€ ì¶”ì¶œ)
    3. ê³ ê¸‰ ì²­í‚¹ (ë¬¸ë‹¨/í† í° ê¸°ë°˜)
    4. ì„ë² ë”© ìƒì„± (Azure OpenAI)
    5. ê²€ìƒ‰ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
    6. ìƒíƒœë¥¼ 'completed'ë¡œ ë³€ê²½
    
    Args:
        document_id: ë¬¸ì„œ ID (TbFileBssInfo.file_bss_info_sno)
        file_path: íŒŒì¼ ê²½ë¡œ
        container_id: ì»¨í…Œì´ë„ˆ ID
        user_emp_no: ì‚¬ìš©ì ì‚¬ë²ˆ
        provider: AI ì œê³µì ("azure" ë˜ëŠ” "bedrock")
        model_profile: ëª¨ë¸ í”„ë¡œí•„ ("default")
    
    Returns:
        Dict: ì²˜ë¦¬ ê²°ê³¼
            - success: ì„±ê³µ ì—¬ë¶€
            - document_id: ë¬¸ì„œ ID
            - chunks_count: ìƒì„±ëœ ì²­í¬ ìˆ˜
            - embeddings_count: ìƒì„±ëœ ì„ë² ë”© ìˆ˜
            - processing_time: ì´ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
    """
    start_time = datetime.now()
    logger.info(f"ğŸ”„ [ASYNC-TASK] ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: doc_id={document_id}, container={container_id}")
    
    # ìƒíƒœë¥¼ processingìœ¼ë¡œ ë³€ê²½
    self.update_status(document_id, 'processing')
    
    try:
        # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰ (nest_asyncioê°€ ì´ë¯¸ ì ìš©ë˜ì–´ Event Loop ì¤‘ì²© ê°€ëŠ¥)
        effective_provider = provider or settings.get_current_llm_provider()

        result = asyncio.run(
            _process_document_multimodal(
                document_id=document_id,
                file_path=file_path,
                container_id=container_id,
                user_emp_no=user_emp_no,
                provider=effective_provider,
                model_profile=model_profile
            )
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        if result.get('success'):
            # ìƒíƒœë¥¼ completedë¡œ ë³€ê²½
            self.update_status(document_id, 'completed')
            
            logger.info(
                f"âœ… [ASYNC-TASK] ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: doc_id={document_id}, "
                f"chunks={result.get('chunks_count', 0)}, "
                f"embeddings={result.get('embeddings_count', 0)}, "
                f"time={processing_time:.2f}ì´ˆ"
            )
            
            return {
                'success': True,
                'document_id': document_id,
                'chunks_count': result.get('chunks_count', 0),
                'embeddings_count': result.get('embeddings_count', 0),
                'objects_count': result.get('objects_count', 0),
                'processing_time': processing_time,
                'stages': result.get('stages', [])
            }
        else:
            # ì²˜ë¦¬ ì‹¤íŒ¨
            error_msg = result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
            self.update_status(document_id, 'failed', error_msg)
            
            logger.error(f"âŒ [ASYNC-TASK] ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: doc_id={document_id}, error={error_msg}")
            
            return {
                'success': False,
                'document_id': document_id,
                'error': error_msg,
                'processing_time': processing_time
            }
            
    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        error_trace = traceback.format_exc()
        
        logger.error(f"ğŸ’¥ [ASYNC-TASK] ë¬¸ì„œ ì²˜ë¦¬ ì˜ˆì™¸: doc_id={document_id}")
        logger.error(f"ğŸ’¥ [ASYNC-TASK] ì—ëŸ¬: {error_msg}")
        logger.error(f"ğŸ’¥ [ASYNC-TASK] ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤:\n{error_trace}")
        
        # ìƒíƒœë¥¼ failedë¡œ ë³€ê²½
        self.update_status(document_id, 'failed', error_msg)
        
        # Celeryì— ì˜ˆì™¸ ì „íŒŒ
        raise


async def _process_document_multimodal(
    document_id: int,
    file_path: str,
    container_id: str,
    user_emp_no: str,
    provider: Optional[str] = None,
    model_profile: str = "default"
) -> dict:
    """
    ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë¹„ë™ê¸°) - ë¬¸ì„œ ìœ í˜•ë³„ ë¼ìš°íŒ… ì ìš©
    
    Args:
        document_id: ë¬¸ì„œ ID
        file_path: íŒŒì¼ ê²½ë¡œ
        container_id: ì»¨í…Œì´ë„ˆ ID
        user_emp_no: ì‚¬ìš©ì ì‚¬ë²ˆ
        provider: AI ì œê³µì
        model_profile: ëª¨ë¸ í”„ë¡œí•„
    
    Returns:
        Dict: ì²˜ë¦¬ ê²°ê³¼
    """
    from app.core.database import get_async_session_local
    from app.models import TbFileBssInfo
    from app.services.document.pipeline_router import PipelineRouter
    from sqlalchemy import select
    
    provider = provider or settings.get_current_llm_provider()

    async_session_factory = get_async_session_local()
    async with async_session_factory() as session:
        try:
            logger.info(f"ğŸ“Š [PIPELINE] ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘: doc_id={document_id}, provider={provider}")
            
            # ğŸ†• DBì—ì„œ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ (document_type, processing_options í¬í•¨)
            stmt = select(TbFileBssInfo).where(TbFileBssInfo.file_bss_info_sno == document_id)
            result = await session.execute(stmt)
            file_info = result.scalar_one_or_none()
            
            if not file_info:
                logger.error(f"âŒ [PIPELINE] ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: doc_id={document_id}")
                return {
                    'success': False,
                    'error': f'ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}'
                }
            
            # ë¬¸ì„œ ìœ í˜• ë° ì²˜ë¦¬ ì˜µì…˜ ê°€ì ¸ì˜¤ê¸°
            document_type = cast(str, file_info.document_type or 'general')
            processing_options = cast(Dict[str, Any], file_info.processing_options or {})
            file_name = cast(str, file_info.file_lgc_nm or "unknown_file")
            
            logger.info(f"ğŸ”€ [PIPELINE] ë¬¸ì„œ ìœ í˜•: {document_type}, ì˜µì…˜: {processing_options}")
            
            # ğŸ†• íŒŒì´í”„ë¼ì¸ ë¼ìš°í„°ë¥¼ í†µí•œ ì²˜ë¦¬
            pipeline_result = await PipelineRouter.process_document(
                document_type=document_type,
                document_id=document_id,
                file_path=file_path,
                file_name=file_name,
                container_id=container_id,
                processing_options=processing_options,
                user_emp_no=user_emp_no
            )
            
            if pipeline_result.get('success'):
                stats = pipeline_result.get('statistics', {})
                chunks_count = stats.get('total_chunks', 0)
                
                logger.info(f"ğŸ“Š [PIPELINE] íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: doc_id={document_id}, pipeline={pipeline_result.get('pipeline_type')}")
                logger.info(f"   ğŸ“Š í†µê³„: {stats}")
                
                # ğŸ†• TbFileBssInfoì˜ chunk_count ì—…ë°ì´íŠ¸
                from sqlalchemy import update
                try:
                    update_stmt = (
                        update(TbFileBssInfo)
                        .where(TbFileBssInfo.file_bss_info_sno == document_id)
                        .values(chunk_count=chunks_count)
                    )
                    await session.execute(update_stmt)
                    await session.commit()
                    logger.info(f"âœ… [CHUNK-COUNT] chunk_count ì—…ë°ì´íŠ¸ ì™„ë£Œ: doc_id={document_id}, count={chunks_count}")
                except Exception as e:
                    logger.error(f"âŒ [CHUNK-COUNT] chunk_count ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                    await session.rollback()
                
                # ê¸°ì¡´ multimodal_document_serviceì™€ í˜¸í™˜ë˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                return {
                    'success': True,
                    'chunks_count': chunks_count,
                    'embeddings_count': stats.get('total_embeddings', 0),
                    'objects_count': stats.get('total_objects_extracted', 0),
                    'pipeline_type': pipeline_result.get('pipeline_type'),
                    'document_type': document_type,
                    'stages': ['extract', 'chunk', 'embed', 'index']
                }
            else:
                logger.error(f"âŒ [PIPELINE] íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {pipeline_result.get('error')}")
                return {
                    'success': False,
                    'error': pipeline_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                }
            
        except Exception as e:
            logger.error(f"âŒ [PIPELINE] íŒŒì´í”„ë¼ì¸ ì˜ˆì™¸: doc_id={document_id}, error={e}", exc_info=True)
            return {
                'success': False,
                'error': str(e)
            }

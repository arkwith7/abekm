"""
ğŸ“„ WKMS ë¬¸ì„œ ê´€ë¦¬ API - ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸
===========================================

ğŸ¯ ëª©ì :
- ì›¹ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ë©”ì¸ ë¬¸ì„œ ê´€ë¦¬ API
- ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ê¶Œí•œ ê´€ë¦¬ì™€ í†µí•©ëœ ë¬¸ì„œ CRUD ê¸°ëŠ¥ ì œê³µ
- ë‹¨ìˆœí•˜ê³  ì•ˆì •ì ì¸ ê¸°ë³¸ ê¸°ëŠ¥ë¶€í„° ì‹œì‘í•˜ì—¬ ì ì§„ì  í™•ì¥

ğŸ”— API ê´€ê³„ë„:
```
í”„ë¡ íŠ¸ì—”ë“œ (React)
    â†“ HTTP ìš”ì²­
v1/documents.py (ë©”ì¸ API)
    â†“ ì„œë¹„ìŠ¤ í˜¸ì¶œ
permission_service â† container_service â†’ document_service
    â†“ ë°ì´í„° ì²˜ë¦¬
file_models (tb_file_bss_info, tb_file_dtl_info)
    â†“ ì €ì¥
PostgreSQL Database
```

ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥:
1. ğŸ“‚ ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ (/containers)
2. ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ (/upload)  
3. ğŸ“œ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (/, /list)
4. ğŸ” ë¬¸ì„œ ê²€ìƒ‰ (/search)
5. ğŸ—‘ï¸ ë¬¸ì„œ ì‚­ì œ (/{id})
6. ğŸ” ê¶Œí•œ ê²€ì¦ (/containers/{id}/validate)
7. ğŸ“Š ì—…ë¡œë“œ ì§„í–‰ë¥  (/upload-progress/{id})

ğŸš€ í™•ì¥ ê³„íš:
- [ ] ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ (â†’ v1/search.pyë¡œ ë¶„ë¦¬ ì˜ˆì •)
- [ ] ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬ (â†’ services/processing.py ì—°ë™)
- [ ] ì‹¤ì‹œê°„ ì—…ë¡œë“œ ì§„í–‰ë¥  (WebSocket)
- [ ] ë¬¸ì„œ ë²„ì „ ê´€ë¦¬
- [ ] ìë™ íƒœê¹… ë° ë¶„ë¥˜

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ëŠ” ì‚¬ìš©ì ì¸ì¦ í•„ìˆ˜
- ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê¸°ë°˜ ì ‘ê·¼ ì œì–´ ì ìš©
- íŒŒì¼ í¬ê¸° ì œí•œ: 50MB (ì„¤ì • ê°€ëŠ¥)
- ì§€ì› í˜•ì‹: PDF, DOCX, PPTX, XLSX, TXT, HWP
"""

import os
import uuid
import shutil
from pathlib import Path
from typing import Optional, List
import asyncio
import logging
from datetime import datetime

from fastapi import APIRouter, Depends, UploadFile, File, HTTPException, Query, Form, BackgroundTasks, Response
from fastapi.responses import JSONResponse, FileResponse
from starlette.responses import RedirectResponse
import mimetypes
import urllib.parse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc, func, and_, or_, outerjoin, update

# ğŸ”§ Core dependencies
from app.core.database import get_db
from app.core.dependencies import get_current_user
from app.models import User

# ğŸ›ï¸ Services
from app.services.auth.permission_service import permission_service
from app.services.auth.container_service import ContainerService
from app.services.document.document_service import document_service
from app.services.document.pipeline.integrated_document_pipeline_service import IntegratedDocumentPipelineService
from app.core.config import settings
# ğŸ”® Future extensions (ì£¼ì„ ì²˜ë¦¬ëœ ê³ ê¸‰ ê¸°ëŠ¥ë“¤)
# from app.services.document_processor_service import document_processor_service
# from app.services.vector_storage_service import vector_storage_service
# from app.services.ai_service import ai_service

# ğŸ“Š Models
from app.models import TbKnowledgeContainers as Container, TbUserPermissions
from app.models import TbFileBssInfo, TbFileDtlInfo, TbAcademicDocumentMetadata

# ğŸ“‹ Schemas
from app.schemas.document import (
    DocumentResponse, 
    DocumentListResponse, 
    DocumentUploadResponse,
    DocumentInfo,
    SearchRequest,
    SearchResponse,
    PreprocessResponse,
    ChunkRequest,
    ChunkResponse
)
from app.services.document.processing.document_preprocessing_service import document_preprocessing_service
from app.services.core.azure_blob_service import get_azure_blob_service

logger = logging.getLogger(__name__)

# ğŸ”§ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
pipeline_service = IntegratedDocumentPipelineService()

# ğŸŒ FastAPI ë¼ìš°í„° ì„¤ì •
router = APIRouter(
    prefix="",  # /api/v1/documentsëŠ” main.pyì—ì„œ ì„¤ì •
    tags=["ğŸ“„ Documents"],
    responses={
        400: {"description": "ì˜ëª»ëœ ìš”ì²­"},
        401: {"description": "ì¸ì¦ í•„ìš”"},
        403: {"description": "ê¶Œí•œ ì—†ìŒ"},
        404: {"description": "ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        500: {"description": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜"}
    }
)

# âš™ï¸ ì—…ë¡œë“œ ì„¤ì •
UPLOAD_DIR = Path(os.getenv('UPLOAD_DIR', str(Path(__file__).parent.parent.parent.parent / "uploads")))
UPLOAD_DIR.mkdir(exist_ok=True)

# ğŸ“ íŒŒì¼ ì œí•œ ì„¤ì •
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
ALLOWED_EXTENSIONS = {'.pdf', '.docx', '.pptx', '.xlsx', '.txt', '.hwp'}

# =============================================================================
# ï¿½ ë¬¸ì„œ ìœ í˜• ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/document-types",
           response_model=dict,
           summary="ğŸ“‹ ì§€ì›í•˜ëŠ” ë¬¸ì„œ ìœ í˜• ëª©ë¡",
           description="""
           ì—…ë¡œë“œ ê°€ëŠ¥í•œ ë¬¸ì„œ ìœ í˜• ëª©ë¡ê³¼ ê° ìœ í˜•ë³„ ì²˜ë¦¬ ì˜µì…˜ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
           
           **ë°˜í™˜ ì •ë³´:**
           - ë¬¸ì„œ ìœ í˜• ID, ì´ë¦„, ì„¤ëª…
           - ì§€ì› íŒŒì¼ í˜•ì‹
           - ê¸°ë³¸ ì²˜ë¦¬ ì˜µì…˜
           """)
async def get_document_types():
    """
    ğŸ¯ ê¸°ëŠ¥: ì§€ì›í•˜ëŠ” ë¬¸ì„œ ìœ í˜• ëª©ë¡ ì¡°íšŒ
    ğŸ“Š ì‘ë‹µ: { success: bool, document_types: [], total: int }
    ğŸ” ê¶Œí•œ: ë¡œê·¸ì¸ ì‚¬ìš©ì ì „ì²´
    """
    try:
        from app.schemas.document_types import get_all_document_types
        
        logger.info("ë¬¸ì„œ ìœ í˜• ëª©ë¡ ì¡°íšŒ")
        
        document_types = get_all_document_types()
        
        response = {
            "success": True,
            "document_types": [dt.dict() for dt in document_types],
            "total": len(document_types)
        }
        
        logger.info(f"ë¬¸ì„œ ìœ í˜• ëª©ë¡ ì¡°íšŒ ì™„ë£Œ - ì´ {len(document_types)}ê°œ")
        return response
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ìœ í˜• ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ìœ í˜• ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# =============================================================================
# ï¿½ğŸ“‚ ì»¨í…Œì´ë„ˆ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/containers", 
           response_model=dict,
           summary="ğŸ“‚ ì‚¬ìš©ì ì ‘ê·¼ ê°€ëŠ¥ ì»¨í…Œì´ë„ˆ ëª©ë¡",
           description="""
           ì‚¬ìš©ìê°€ ì—…ë¡œë“œ ê¶Œí•œì„ ê°€ì§„ ì»¨í…Œì´ë„ˆ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
           
           **ë°˜í™˜ ì •ë³´:**
           - ì»¨í…Œì´ë„ˆ ID, ì´ë¦„, ì„¤ëª…
           - ì‚¬ìš©ìì˜ ì ‘ê·¼ ê¶Œí•œ ë ˆë²¨
           - ê³„ì¸µ êµ¬ì¡° ì •ë³´
           """)
async def get_user_accessible_containers(
    user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """
    ğŸ¯ ê¸°ëŠ¥: ì‚¬ìš©ìê°€ ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ
    ğŸ“Š ì‘ë‹µ: { success: bool, containers: [], total: int }
    ğŸ” ê¶Œí•œ: ë¡œê·¸ì¸ ì‚¬ìš©ì ì „ì²´
    """
    try:
        logger.info(f"ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ ì‹œì‘ - ì‚¬ìš©ì: {user.emp_no}")
        
        # ContainerService ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        container_service = ContainerService(session)
        containers = await container_service.get_user_accessible_containers(
            user_emp_no=str(user.emp_no),
            session=session
        )
        
        response = {
            "success": True,
            "containers": containers,
            "total": len(containers),
            "user_emp_no": str(user.emp_no),
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ - ì‚¬ìš©ì: {user.emp_no}, ê°œìˆ˜: {len(containers)}")
        return response
        
    except Exception as e:
        logger.error(f"ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ - ì‚¬ìš©ì: {user.emp_no}, ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# =============================================================================
# ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.post("/upload", 
            response_model=DocumentUploadResponse,
            summary="ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ",
            description="""
            ì»¨í…Œì´ë„ˆì— ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
            
            **ì²˜ë¦¬ ê³¼ì •:**
            1. ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦
            2. íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ (í˜•ì‹, í¬ê¸°)
            3. ì„œë²„ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥
            4. ë°ì´í„°ë² ì´ìŠ¤ì— ë©”íƒ€ë°ì´í„° ì €ì¥
            
            **í–¥í›„ í™•ì¥ ì˜ˆì •:**
            - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° NLP ì²˜ë¦¬
            - ë²¡í„° ì„ë² ë”© ìƒì„±
            - ìë™ íƒœê¹… ë° ë¶„ë¥˜
            """)
async def upload_document(
    file: UploadFile = File(..., description="ì—…ë¡œë“œí•  ë¬¸ì„œ íŒŒì¼"),
    container_id: Optional[str] = Form(..., description="ë¬¸ì„œê°€ ì €ì¥ë  ì»¨í…Œì´ë„ˆ ID"),
    document_type: str = Form("general", description="ë¬¸ì„œ ìœ í˜• (general/academic_paper/patent/...)"),  # âœ… ì¶”ê°€
    processing_options: Optional[str] = Form(None, description="ë¬¸ì„œ ìœ í˜•ë³„ ì²˜ë¦¬ ì˜µì…˜ (JSON string)"),  # âœ… ì¶”ê°€
    use_multimodal: bool = Form(True, description="ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)"),
    user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """
    ğŸ¯ ê¸°ëŠ¥: ë¬¸ì„œ ì—…ë¡œë“œ ë° ê¸°ë³¸ íŒŒì¼ ì €ì¥ (ë©€í‹°ëª¨ë‹¬ ì§€ì› + ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜)
    ğŸ“‹ ë‹¨ê³„:
        1. ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦
        2. íŒŒì¼ ê²€ì¦ ë° ì €ì¥
        3. ë¬¸ì„œ ìœ í˜• ê²€ì¦ ë° ì²˜ë¦¬ ì˜µì…˜ íŒŒì‹±
        4. ë°ì´í„°ë² ì´ìŠ¤ì— íŒŒì¼ ì •ë³´ ì €ì¥ (document_type, processing_options í¬í•¨)
        5. RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìœ í˜•ë³„ ë§ì¶¤ íŒŒì´í”„ë¼ì¸)
    ğŸ” ê¶Œí•œ: ì»¨í…Œì´ë„ˆë³„ ì—…ë¡œë“œ ê¶Œí•œ í•„ìš”
    ğŸ“Š ì‘ë‹µ: DocumentUploadResponse (ë¬¸ì„œ ID, íŒŒì¼ ì •ë³´, ì²˜ë¦¬ í†µê³„)
    ğŸ¨ ë©€í‹°ëª¨ë‹¬: ê°ì²´ ì¶”ì¶œ â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ë²¡í„° ì €ì¥
    """
    upload_start_time = datetime.now()
    
    try:
        # ğŸ” 1ë‹¨ê³„: ì»¨í…Œì´ë„ˆ í•„ìˆ˜ ì²´í¬ ë° ê¶Œí•œ ê²€ì¦
        if not container_id:
            logger.error(f"âŒ [UPLOAD-DEBUG] ì»¨í…Œì´ë„ˆ ID ì—†ìŒ - íŒŒì¼: {file.filename}")
            raise HTTPException(
                status_code=400,
                detail="ì—…ë¡œë“œí•  ì»¨í…Œì´ë„ˆë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
            )
        
        logger.info(f"ğŸš€ [UPLOAD-DEBUG] ë¬¸ì„œ ì—…ë¡œë“œ ì‹œì‘")
        safe_filename = file.filename or "uploaded_file"
        logger.info(f"   ğŸ“„ íŒŒì¼ëª…: {safe_filename}")
        logger.info(f"   ğŸ‘¤ ì‚¬ìš©ì: {user.emp_no}")
        logger.info(f"   ğŸ“ ì»¨í…Œì´ë„ˆ: {container_id}")
        logger.info(f"   ï¿½ ë¬¸ì„œ ìœ í˜•: {document_type}")
        logger.info(f"   ï¿½ğŸ“Š íŒŒì¼ í¬ê¸°: {file.size if file.size else 'Unknown'} bytes")
        
        # ğŸ“ ë¬¸ì„œ ìœ í˜• ë° ì²˜ë¦¬ ì˜µì…˜ ê²€ì¦
        import json
        from app.schemas.document_types import DocumentType, ProcessingOptionsFactory
        
        try:
            doc_type_enum = DocumentType(document_type)
        except ValueError:
            logger.error(f"âŒ [UPLOAD-DEBUG] ì˜ëª»ëœ ë¬¸ì„œ ìœ í˜•: {document_type}")
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ ìœ í˜•ì…ë‹ˆë‹¤: {document_type}"
            )
        
        # ì²˜ë¦¬ ì˜µì…˜ íŒŒì‹± ë° ê²€ì¦
        parsed_options = {}
        if processing_options:
            try:
                parsed_options = json.loads(processing_options)
                logger.info(f"   âš™ï¸ ì²˜ë¦¬ ì˜µì…˜: {parsed_options}")
            except json.JSONDecodeError:
                logger.error(f"âŒ [UPLOAD-DEBUG] ì˜ëª»ëœ JSON í˜•ì‹: {processing_options}")
                raise HTTPException(
                    status_code=400,
                    detail="ì²˜ë¦¬ ì˜µì…˜ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤."
                )
        
        # ì˜µì…˜ ê²€ì¦ ë° ê¸°ë³¸ê°’ ë³‘í•©
        validated_options = ProcessingOptionsFactory.validate_options(
            doc_type_enum, 
            parsed_options
        )
        logger.info(f"   âœ… ê²€ì¦ëœ ì˜µì…˜: {validated_options}")
        
        # ê¶Œí•œ í™•ì¸
        logger.info(f"ğŸ” [UPLOAD-DEBUG] ê¶Œí•œ í™•ì¸ ì‹œì‘")
        can_upload, permission_message = await permission_service.check_upload_permission(
            user_emp_no=str(user.emp_no),
            container_id=container_id
        )
        logger.info(f"ğŸ” [UPLOAD-DEBUG] ê¶Œí•œ í™•ì¸ ê²°ê³¼: {can_upload}, ë©”ì‹œì§€: {permission_message}")
        
        if not can_upload:
            logger.warning(f"âŒ [UPLOAD-DEBUG] ì—…ë¡œë“œ ê¶Œí•œ ì—†ìŒ - ì‚¬ìš©ì: {user.emp_no}, ì»¨í…Œì´ë„ˆ: {container_id}")
            raise HTTPException(
                status_code=403,
                detail=f"ì»¨í…Œì´ë„ˆ ì—…ë¡œë“œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {permission_message}"
            )
        
        # âœ… 2ë‹¨ê³„: íŒŒì¼ ê²€ì¦
        logger.info(f"ğŸ“‹ [UPLOAD-DEBUG] íŒŒì¼ ê²€ì¦ ì‹œì‘")
        validation_result = await _validate_upload_file(file)
        logger.info(f"ğŸ“‹ [UPLOAD-DEBUG] íŒŒì¼ ê²€ì¦ ê²°ê³¼: {validation_result['valid']}")
        
        if not validation_result["valid"]:
            logger.warning(f"âŒ [UPLOAD-DEBUG] íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨ - íŒŒì¼: {file.filename}, ì˜¤ë¥˜: {validation_result['error']}")
            raise HTTPException(
                status_code=400,
                detail=validation_result["error"]
            )

        # ğŸ’¾ 3ë‹¨ê³„: íŒŒì¼ ì €ì¥ (ë¡œì»¬ ì„ì‹œ)
        logger.info(f"ğŸ’¾ [UPLOAD-DEBUG] íŒŒì¼ ì €ì¥ ì‹œì‘")
        saved_file_path = await _save_upload_file(file)
        logger.info(f"ğŸ’¾ [UPLOAD-DEBUG] íŒŒì¼ ì €ì¥ ì™„ë£Œ - ê²½ë¡œ: {saved_file_path}")

        # ğŸª£ 3-1ë‹¨ê³„: ê°ì²´ ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ (S3 ë˜ëŠ” Azure Blob) - ì‹¤íŒ¨ ì‹œ ì¹˜ëª…ì  ì˜¤ë¥˜
        s3_object_key = None
        azure_blob_object_key = None
        try:
            from app.core.config import settings as app_settings
            storage_backend = getattr(app_settings, 'storage_backend', 'local')
        except Exception:
            storage_backend = 'local'

        try:
            from app.utils.storage_paths import build_raw_object_key, classify_key_scheme
        except Exception:
            build_raw_object_key = None  # type: ignore
            classify_key_scheme = lambda k: 'unknown'  # type: ignore

        # ì›ê²© ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ DB ì €ì¥ ì¤‘ë‹¨ì„ ìœ„í•œ í”Œë˜ê·¸
        remote_upload_failed = False
        remote_upload_error = None

        if storage_backend == 's3':
            try:
                from app.services.core.aws_service import S3Service
                container_prefix = container_id.strip('/') if container_id else 'default'
                basename = os.path.basename(saved_file_path)
                # ìƒˆ ìŠ¤í‚´ ì ìš© ì—¬ë¶€ íŒë‹¨
                if getattr(app_settings, 'use_standard_raw_prefix', False) and build_raw_object_key:
                    s3_object_key = build_raw_object_key(container_prefix, safe_filename, saved_file_path)
                else:
                    s3_object_key = f"{container_prefix}/{basename}"
                s3 = S3Service()
                await s3.upload_file(file_path=saved_file_path, object_key=s3_object_key)
                scheme = classify_key_scheme(s3_object_key)
                logger.info(f"ğŸª£ [UPLOAD-DEBUG] S3 ì—…ë¡œë“œ ì™„ë£Œ - í‚¤: {s3_object_key} (scheme={scheme})")
            except Exception as s3e:
                logger.error(f"âŒ [UPLOAD-DEBUG] S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3e}")
                remote_upload_failed = True
                remote_upload_error = f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {str(s3e)}"
        elif storage_backend == 'azure_blob':
            try:
                from app.services.core.azure_blob_service import get_azure_blob_service
                container_prefix = container_id.strip('/') if container_id else 'default'
                basename = os.path.basename(saved_file_path)
                if getattr(app_settings, 'use_standard_raw_prefix', False) and build_raw_object_key:
                    azure_blob_object_key = build_raw_object_key(container_prefix, safe_filename, saved_file_path)
                else:
                    # ë ˆê±°ì‹œ í˜¸í™˜ (container/filename)
                    azure_blob_object_key = f"{container_prefix}/{basename}"
                azure = get_azure_blob_service()
                azure.upload_file(saved_file_path, azure_blob_object_key, purpose='raw')
                scheme = classify_key_scheme(azure_blob_object_key)
                logger.info(f"ğŸª£ [UPLOAD-DEBUG] Azure Blob ì—…ë¡œë“œ ì™„ë£Œ - í‚¤: {azure_blob_object_key} (scheme={scheme})")
            except Exception as aze:
                logger.error(f"âŒ [UPLOAD-DEBUG] Azure Blob ì—…ë¡œë“œ ì‹¤íŒ¨: {aze}")
                remote_upload_failed = True
                remote_upload_error = f"Azure Blob ì—…ë¡œë“œ ì‹¤íŒ¨: {str(aze)}"
        
        # ğŸš¨ ì›ê²© ìŠ¤í† ë¦¬ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨ ë° ë¡œì»¬ íŒŒì¼ ì •ë¦¬
        if remote_upload_failed:
            if os.path.exists(saved_file_path):
                try:
                    os.remove(saved_file_path)
                    logger.info(f"ğŸ§¹ [UPLOAD-DEBUG] ì›ê²© ì—…ë¡œë“œ ì‹¤íŒ¨ í›„ ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ: {saved_file_path}")
                except Exception as cleanup_error:
                    logger.warning(f"âš ï¸ [UPLOAD-DEBUG] ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {cleanup_error}")
            raise HTTPException(
                status_code=500,
                detail=f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {remote_upload_error}"
            )
        
        try:
            # ğŸ“Š 4ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ì— íŒŒì¼ ì •ë³´ ì €ì¥
            logger.info(f"ğŸ“Š [UPLOAD-DEBUG] ë¬¸ì„œ ì •ë³´ ì €ì¥ ì‹œì‘: {file.filename}")
            
            # íŒŒì¼ ì •ë³´ ìƒì„±
            file_size = os.path.getsize(saved_file_path)
            file_extension = Path(safe_filename).suffix
            logger.info(f"ğŸ“Š [UPLOAD-DEBUG] íŒŒì¼ ë©”íƒ€ë°ì´í„° - í¬ê¸°: {file_size}, í™•ì¥ì: {file_extension}")
            
            # ğŸ“‹ tb_file_bss_infoì— ê¸°ë³¸ ì •ë³´ ì €ì¥
            logger.info(f"ğŸ“Š [UPLOAD-DEBUG] document_service.create_document_from_upload í˜¸ì¶œ")
            # DBì—ëŠ” S3 ëª¨ë“œë©´ object key, ì•„ë‹ˆë©´ ë¡œì»¬ ê²½ë¡œ ì €ì¥
            # ì €ì¥ ê²½ë¡œ ê²°ì •: ìš°ì„ ìˆœìœ„ azure_blob > s3 > local
            db_file_path = azure_blob_object_key or s3_object_key or saved_file_path
            
            
            document_result = None
            try:
                # ğŸš€ ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ëª¨ë“œ
                if use_multimodal:
                    # ë©€í‹°ëª¨ë‹¬ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬í•˜ê³  ì¦‰ì‹œ ì‘ë‹µ
                    logger.info(f"ğŸš€ [UPLOAD-DEBUG] ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹œì‘")
                    
                    # 1) ê¸°ë³¸ ë¬¸ì„œ ì •ë³´ë§Œ DBì— ì €ì¥ (RAG íŒŒì´í”„ë¼ì¸ ì œì™¸)
                    document_result = await document_service.create_document_basic_info(
                        file_path=db_file_path,
                        file_name=safe_filename,
                        file_size=file_size,
                        file_extension=file_extension,
                        user_emp_no=str(user.emp_no),
                        container_id=container_id,
                        session=session,
                        processing_status='pending',  # ğŸ†• ì²˜ë¦¬ ëŒ€ê¸° ìƒíƒœ
                        document_type=document_type,  # âœ… ì¶”ê°€
                        processing_options=validated_options  # âœ… ì¶”ê°€
                    )
                    
                    if not document_result["success"]:
                        logger.error(f"âŒ [UPLOAD-DEBUG] ë¬¸ì„œ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {document_result.get('error')}")
                        await session.rollback()
                        raise HTTPException(
                            status_code=500,
                            detail=f"ë¬¸ì„œ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {document_result['error']}"
                        )
                    
                    document_id = document_result["document_id"]
                    logger.info(f"âœ… [UPLOAD-DEBUG] ë¬¸ì„œ ê¸°ë³¸ ì •ë³´ ì €ì¥ ì™„ë£Œ: doc_id={document_id}")
                    
                    # 2) ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë“±ë¡
                    try:
                        from app.tasks.document_tasks import process_document_async
                        
                        # ğŸ”§ Celeryì— ì „ë‹¬í•  íŒŒì¼ ê²½ë¡œ ê²°ì •
                        # S3/Azure Blobì´ ìˆìœ¼ë©´ í•´ë‹¹ í‚¤ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¡œì»¬ ê²½ë¡œ
                        processing_file_path = azure_blob_object_key or s3_object_key or saved_file_path
                        
                        background_provider = settings.get_current_llm_provider()

                        task = process_document_async.delay(
                            document_id=document_id,
                            file_path=processing_file_path,  # S3/Blob í‚¤ ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ
                            container_id=container_id,
                            user_emp_no=str(user.emp_no),
                            provider=background_provider,
                            model_profile="default"
                        )
                        
                        logger.info(f"ğŸ”„ [UPLOAD-DEBUG] ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë“±ë¡ ì™„ë£Œ: task_id={task.id}, doc_id={document_id}, path={processing_file_path}")
                        
                        # ì‘ë‹µì— íƒœìŠ¤í¬ ID í¬í•¨
                        document_result["task_id"] = task.id
                        document_result["processing_status"] = "processing"
                        
                    except Exception as task_error:
                        logger.error(f"âŒ [UPLOAD-DEBUG] ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë“±ë¡ ì‹¤íŒ¨: {task_error}")
                        # ì‘ì—… ë“±ë¡ ì‹¤íŒ¨ ì‹œ ìƒíƒœë¥¼ failedë¡œ ì—…ë°ì´íŠ¸
                        update_stmt = (
                            update(TbFileBssInfo)
                            .where(TbFileBssInfo.file_bss_info_sno == document_id)
                            .values(processing_status='failed', processing_error=f"ì‘ì—… ë“±ë¡ ì‹¤íŒ¨: {str(task_error)}")
                        )
                        await session.execute(update_stmt)
                        await session.commit()
                        raise
                else:
                    # ë©€í‹°ëª¨ë‹¬ ë¹„í™œì„±í™”: ë™ê¸° ë°©ì‹ (ê¸°ì¡´ ë¡œì§)
                    logger.info(f"ğŸ“Š [UPLOAD-DEBUG] ë™ê¸° ì²˜ë¦¬ ëª¨ë“œ (ë©€í‹°ëª¨ë‹¬ ë¹„í™œì„±í™”)")
                    document_result = await document_service.create_document_from_upload(
                        file_path=db_file_path,
                        file_name=safe_filename,
                        file_size=file_size,
                        file_extension=file_extension,
                        user_emp_no=str(user.emp_no),
                        container_id=container_id,
                        session=session,
                        local_source_path=saved_file_path,
                        use_multimodal=False,
                        document_type=document_type,  # âœ… ì¶”ê°€
                        processing_options=validated_options  # âœ… ì¶”ê°€
                    )
                    
                    if not document_result["success"]:
                        logger.error(f"âŒ [UPLOAD-DEBUG] ë¬¸ì„œ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {document_result.get('error')}")
                        await session.rollback()
                        raise HTTPException(
                            status_code=500,
                            detail=f"ë¬¸ì„œ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {document_result['error']}"
                        )
                
                logger.info(f"ğŸ“Š [UPLOAD-DEBUG] document_service ê²°ê³¼: success={document_result.get('success', False)}")
                
                if not document_result["success"]:
                    logger.error(f"âŒ [UPLOAD-DEBUG] ë¬¸ì„œ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {document_result.get('error')}")
                    # ğŸ”„ DB íŠ¸ëœì­ì…˜ ë¡¤ë°±
                    await session.rollback()
                    logger.info(f"ğŸ”„ [UPLOAD-DEBUG] DB íŠ¸ëœì­ì…˜ ë¡¤ë°± ì™„ë£Œ")
                    raise HTTPException(
                        status_code=500,
                        detail=f"ë¬¸ì„œ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {document_result['error']}"
                    )
            except HTTPException:
                raise
            except Exception as db_error:
                logger.error(f"âŒ [UPLOAD-DEBUG] DB ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {db_error}")
                # ğŸ”„ DB íŠ¸ëœì­ì…˜ ë¡¤ë°±
                await session.rollback()
                logger.info(f"ğŸ”„ [UPLOAD-DEBUG] DB íŠ¸ëœì­ì…˜ ë¡¤ë°± ì™„ë£Œ")
                raise HTTPException(
                    status_code=500,
                    detail=f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(db_error)}"
                )
            
            # ğŸ”„ 5ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ì€ DocumentServiceì—ì„œ ì‹¤í–‰ë¨ (ì¤‘ë³µ ì‹¤í–‰ ì œê±°)
            pipeline_result = {
                "success": True
            }
            
            # â±ï¸ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = (datetime.now() - upload_start_time).total_seconds()
            
            # ğŸ‰ 5ë‹¨ê³„: ì„±ê³µ ì‘ë‹µ êµ¬ì„± (ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ë°˜ì˜ + ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„°)
            mm_stats = document_result.get("multimodal")
            
            # ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            multimodal_meta = None
            if use_multimodal and mm_stats:
                multimodal_meta = {
                    "enabled": True,
                    "images": {
                        "count": mm_stats.get("images", 0),
                        "has_content": mm_stats.get("images", 0) > 0
                    },
                    "tables": {
                        "count": mm_stats.get("tables", 0),
                        "has_content": mm_stats.get("tables", 0) > 0
                    },
                    "charts": {
                        "count": mm_stats.get("figures", 0),
                        "has_content": mm_stats.get("figures", 0) > 0
                    },
                    "embeddings": {
                        "text_embeddings": mm_stats.get("embeddings_count", 0),
                        "clip_embeddings": mm_stats.get("clip_embeddings_count", 0),
                        "has_clip": mm_stats.get("clip_embeddings_count", 0) > 0
                    },
                    "visual_content_available": (
                        mm_stats.get("images", 0) > 0 or mm_stats.get("figures", 0) > 0
                    ),
                    "searchable_by_image": mm_stats.get("clip_embeddings_count", 0) > 0,
                    "processing_stages": mm_stats.get("stages", [])
                }
            
            response = DocumentUploadResponse(
                success=True,
                message="ë¬¸ì„œ ì—…ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                document_id=document_result["document_id"],
                file_info={
                    "original_name": safe_filename,
                    "file_size": file_size,
                    "file_type": file_extension,
                    "file_hash": document_result.get("file_hash", ""),
                    "upload_time": upload_start_time.isoformat(),
                    "saved_path": db_file_path,
                    **({"s3_object_key": s3_object_key} if s3_object_key else {}),
                    **({"azure_blob_object_key": azure_blob_object_key} if azure_blob_object_key else {}),
                    # ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ í”Œë˜ê·¸ (í•˜ìœ„ í˜¸í™˜ì„±)
                    "has_images": (mm_stats and mm_stats.get("images", 0) > 0) if use_multimodal else False,
                    "has_tables": (mm_stats and mm_stats.get("tables", 0) > 0) if use_multimodal else False,
                    "has_charts": (mm_stats and mm_stats.get("figures", 0) > 0) if use_multimodal else False,
                    "visual_content_available": use_multimodal and mm_stats and (mm_stats.get("images", 0) > 0 or mm_stats.get("figures", 0) > 0),
                },
                processing_stats={
                    "text_length": 0,  # ğŸ”® ì¶”í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œì‹œ ì—…ë°ì´íŠ¸
                    "chunk_count": mm_stats.get("chunks_count", 0) if mm_stats else 0,
                    "processing_time": processing_time,
                    "quality_score": 1.0,  # ğŸ”® ì¶”í›„ í’ˆì§ˆ ë¶„ì„ì‹œ ì—…ë°ì´íŠ¸
                    "korean_ratio": 0.0,  # ğŸ”® ì¶”í›„ í•œêµ­ì–´ ë¶„ì„ì‹œ ì—…ë°ì´íŠ¸
                    "rag_pipeline_success": mm_stats.get("success", True) if mm_stats else True,
                    "rag_pipeline_error": mm_stats.get("error") if mm_stats else None,
                    # ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ í†µê³„ (ì‹¤ì œ ê²°ê³¼)
                    "image_count": mm_stats.get("images", 0) if mm_stats else 0,
                    "table_count": mm_stats.get("tables", 0) if mm_stats else 0,
                    "chart_count": mm_stats.get("figures", 0) if mm_stats else 0,
                    "embeddings_count": mm_stats.get("embeddings_count", 0) if mm_stats else 0,
                    "vector_dimension": mm_stats.get("vector_dimension", 0) if mm_stats else 0,
                    "pipeline_elapsed": mm_stats.get("elapsed_seconds", 0) if mm_stats else 0,
                },
                korean_analysis={
                    "document_type": "unknown",  # ğŸ”® ì¶”í›„ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ì ìš©
                    "keywords": [],              # ğŸ”® ì¶”í›„ í‚¤ì›Œë“œ ì¶”ì¶œ
                    "proper_nouns": []           # ğŸ”® ì¶”í›„ ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
                },
                container_assignment={
                    "container_id": container_id,
                    "access_level": "VIEWER",  # ğŸ”® ì¶”í›„ ë™ì  ê¶Œí•œ ì„¤ì •
                    "auto_assigned": False
                },
                multimodal_metadata=multimodal_meta  # ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            )
            
            logger.info(f"ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ - ID: {document_result['document_id']}, íŒŒì¼: {file.filename}, ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
            # ğŸ”¢ ì»¨í…Œì´ë„ˆì˜ document_count ì—…ë°ì´íŠ¸ (completed ìƒíƒœë§Œ ì§‘ê³„)
            try:
                from app.services.auth.container_service import ContainerService
                container_service = ContainerService(session)
                updated_count = await container_service.update_container_document_count(container_id)
                logger.info(f"ğŸ“Š [UPLOAD-DEBUG] ì»¨í…Œì´ë„ˆ ë¬¸ì„œ ê°œìˆ˜ ì—…ë°ì´íŠ¸: {container_id} -> {updated_count}ê°œ")
            except Exception as count_error:
                logger.warning(f"âš ï¸ [UPLOAD-DEBUG] ì»¨í…Œì´ë„ˆ ë¬¸ì„œ ê°œìˆ˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ë¬´ì‹œ): {count_error}")
            
            # ğŸ”§ ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì •ë¦¬ (S3/Blob ì—…ë¡œë“œ ì™„ë£Œ í›„)
            # âš ï¸ ì£¼ì˜: Celery ì‘ì—…ì´ S3/Blob í‚¤ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë¡œì»¬ íŒŒì¼ì€ ì•ˆì „í•˜ê²Œ ì‚­ì œ ê°€ëŠ¥
            try:
                if (s3_object_key or azure_blob_object_key) and os.path.exists(saved_file_path):
                    os.remove(saved_file_path)
                    logger.info(f"ğŸ§¹ [UPLOAD-DEBUG] ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ: {saved_file_path}")
            except Exception as cle:
                logger.warning(f"ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {cle}")
            return response
            
        except Exception as processing_error:
            # ğŸ—‘ï¸ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì—…ë¡œë“œëœ íŒŒì¼ ì •ë¦¬
            logger.error(f"âŒ [UPLOAD-DEBUG] ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨, ì •ë¦¬ ì‹œì‘: {processing_error}")
            
            # DB íŠ¸ëœì­ì…˜ ë¡¤ë°±
            try:
                await session.rollback()
                logger.info(f"ğŸ”„ [UPLOAD-DEBUG] ì˜ˆì™¸ ì²˜ë¦¬ ë¸”ë¡ì—ì„œ DB íŠ¸ëœì­ì…˜ ë¡¤ë°± ì™„ë£Œ")
            except Exception as rollback_error:
                logger.warning(f"âš ï¸ [UPLOAD-DEBUG] DB ë¡¤ë°± ì‹¤íŒ¨(ì´ë¯¸ ë¡¤ë°±ë˜ì—ˆì„ ìˆ˜ ìˆìŒ): {rollback_error}")
            
            # ì›ê²© ìŠ¤í† ë¦¬ì§€ íŒŒì¼ ì‚­ì œ ì‹œë„
            if s3_object_key:
                try:
                    from app.services.core.aws_service import S3Service
                    s3 = S3Service()
                    await s3.delete_file(object_key=s3_object_key)
                    logger.info(f"ğŸ§¹ [UPLOAD-DEBUG] S3 íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {s3_object_key}")
                except Exception as s3_cleanup_error:
                    logger.warning(f"âš ï¸ [UPLOAD-DEBUG] S3 íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {s3_cleanup_error}")
            
            if azure_blob_object_key:
                try:
                    from app.services.core.azure_blob_service import get_azure_blob_service
                    azure = get_azure_blob_service()
                    azure.delete_blob(azure_blob_object_key)
                    logger.info(f"ğŸ§¹ [UPLOAD-DEBUG] Azure Blob íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {azure_blob_object_key}")
                except Exception as azure_cleanup_error:
                    logger.warning(f"âš ï¸ [UPLOAD-DEBUG] Azure Blob íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {azure_cleanup_error}")
            
            # ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(saved_file_path):
                try:
                    os.remove(saved_file_path)
                    logger.info(f"ğŸ§¹ [UPLOAD-DEBUG] ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {saved_file_path}")
                except Exception as local_cleanup_error:
                    logger.warning(f"âš ï¸ [UPLOAD-DEBUG] ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {local_cleanup_error}")
            
            raise processing_error
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ - íŒŒì¼: {file.filename}, ì‚¬ìš©ì: {user.emp_no}, ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# =============================================================================
# ğŸ§ª ì „ì²˜ë¦¬/ì²­í‚¹ ë¶„ë¦¬ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.post("/preprocess",
            response_model=PreprocessResponse,
            summary="ğŸ§ª ë¬¸ì„œ ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰",
            description="íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ í…ìŠ¤íŠ¸ ì¶”ì¶œê³¼ ì •ì œë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. (DB ì €ì¥/ì„ë² ë”©/ìƒ‰ì¸ ì—†ìŒ)")
async def preprocess_only(
    file_path: str = Form(..., description="ì„œë²„ ë‚´ ì ‘ê·¼ ê°€ëŠ¥í•œ íŒŒì¼ ê²½ë¡œ"),
    container_id: Optional[str] = Form(None),
    user: User = Depends(get_current_user)
):
    try:
        ext = Path(file_path).suffix
        pre = await document_preprocessing_service.preprocess_document(
            file_path=file_path,
            file_extension=ext,
            container_id=container_id or "",
            user_emp_no=str(user.emp_no)
        )
        if not pre.get("success"):
            raise HTTPException(status_code=400, detail=pre.get("error", "ì „ì²˜ë¦¬ ì‹¤íŒ¨"))

        cleaned = pre.get("cleaned_text", "")
        from app.services.document.processing.document_preprocessing_service import tiktoken
        tokenizer = tiktoken.get_encoding("cl100k_base")
        return PreprocessResponse(
            success=True,
            extracted_text=pre.get("extracted_text", ""),
            cleaned_text=cleaned,
            extraction_metadata=pre.get("extraction_metadata", {}),
            total_chars=len(cleaned),
            total_tokens=len(tokenizer.encode(cleaned))
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ì „ìš© API ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chunk",
            response_model=ChunkResponse,
            summary="ğŸ§ª ì²­í‚¹ë§Œ ìˆ˜í–‰",
            description="ì •ì œëœ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì²­í¬ ë¶„í• ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. (DB ì €ì¥/ì„ë² ë”©/ìƒ‰ì¸ ì—†ìŒ)")
async def chunk_only(
    payload: ChunkRequest,
    user: User = Depends(get_current_user)
):
    try:
        res = document_preprocessing_service.chunk_text(
            payload.text,
            file_path=payload.file_name or "",
            container_id=payload.container_id or "",
            user_emp_no=str(user.emp_no)
        )
        if not res.get("success"):
            raise HTTPException(status_code=400, detail=res.get("error", "ì²­í‚¹ ì‹¤íŒ¨"))
        return ChunkResponse(
            success=True,
            total_chunks=res.get("total_chunks", 0),
            total_tokens=res.get("total_tokens", 0),
            chunks=res.get("chunks", []),
            metadata=res.get("metadata")
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì²­í‚¹ ì „ìš© API ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# ï¿½ ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/{document_id}/status",
           summary="ğŸ“Š ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ",
           description="""
           ë¬¸ì„œì˜ ë¹„ë™ê¸° ì²˜ë¦¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
           
           **ì²˜ë¦¬ ìƒíƒœ:**
           - pending: ì—…ë¡œë“œ ì™„ë£Œ, ì²˜ë¦¬ ëŒ€ê¸° ì¤‘
           - processing: ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ ì¤‘
           - completed: ì²˜ë¦¬ ì™„ë£Œ
           - failed: ì²˜ë¦¬ ì‹¤íŒ¨
           
           **ì§„í–‰ë¥ :**
           - 0%: pending
           - 10-95%: processing (ì‹œê°„ ê¸°ë°˜ ì¶”ì •)
           - 100%: completed
           """)
async def get_document_processing_status(
    document_id: int,
    user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """
    ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ
    
    Returns:
        - document_id: ë¬¸ì„œ ID
        - status: ì²˜ë¦¬ ìƒíƒœ
        - progress: ì§„í–‰ë¥  (0-100)
        - error: ì˜¤ë¥˜ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
        - started_at: ì²˜ë¦¬ ì‹œì‘ ì‹œê°„
        - completed_at: ì²˜ë¦¬ ì™„ë£Œ ì‹œê°„
    """
    try:
        # ë¬¸ì„œ ì¡°íšŒ
        stmt = select(TbFileBssInfo).where(TbFileBssInfo.file_bss_info_sno == document_id)
        result = await session.execute(stmt)
        doc = result.scalar_one_or_none()
        
        if not doc:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê¶Œí•œ í™•ì¸ (ì†Œìœ ì ë˜ëŠ” ì»¨í…Œì´ë„ˆ ì ‘ê·¼ ê¶Œí•œ)
        if str(doc.owner_emp_no) != str(user.emp_no):
            # ì»¨í…Œì´ë„ˆ ì ‘ê·¼ ê¶Œí•œ í™•ì¸
            _container_val = getattr(doc, 'knowledge_container_id', None)
            container_id = str(_container_val) if _container_val is not None else None
            if container_id:
                can_access, _ = await permission_service.check_download_permission(
                    user_emp_no=str(user.emp_no),
                    container_id=container_id
                )
                if not can_access:
                    raise HTTPException(status_code=403, detail="ë¬¸ì„œ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì²˜ë¦¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        status = getattr(doc, 'processing_status', 'unknown')
        
        # ì§„í–‰ë¥  ê³„ì‚°
        progress = 0
        if status == 'pending':
            progress = 0
        elif status == 'processing':
            # ì²˜ë¦¬ ì‹œì‘ í›„ ê²½ê³¼ ì‹œê°„ ê¸°ë°˜ ì¶”ì •
            started = getattr(doc, 'processing_started_at', None)
            if started:
                from datetime import datetime
                elapsed = (datetime.now() - started).total_seconds()
                # í‰ê·  98ì´ˆ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰ë¥  ì¶”ì • (ìµœëŒ€ 95%)
                progress = min(int((elapsed / 100) * 100), 95)
            else:
                progress = 10
        elif status == 'completed':
            progress = 100
        elif status == 'failed':
            progress = 0
        else:
            progress = 0
        
        # ì‹œê°„ ì •ë³´ í¬ë§·íŒ…
        started_at = getattr(doc, 'processing_started_at', None)
        completed_at = getattr(doc, 'processing_completed_at', None)
        
        return {
            "success": True,
            "document_id": document_id,
            "file_name": doc.file_lgc_nm,
            "status": status,
            "progress": progress,
            "error": getattr(doc, 'processing_error', None),
            "started_at": started_at.isoformat() if started_at else None,
            "completed_at": completed_at.isoformat() if completed_at else None,
            "message": _get_status_message(status, progress)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ - doc_id: {document_id}, error: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


def _get_status_message(status: str, progress: int) -> str:
    """ìƒíƒœë³„ ì•ˆë‚´ ë©”ì‹œì§€"""
    if status == 'pending':
        return "ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."
    elif status == 'processing':
        if progress < 30:
            return "ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        elif progress < 70:
            return "í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        else:
            return "ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
    elif status == 'completed':
        return "ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
    elif status == 'failed':
        return "ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    else:
        return "ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


# =============================================================================
# ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.post("/search", 
            response_model=SearchResponse,
            summary="ğŸ” ë¬¸ì„œ ê²€ìƒ‰",
            description="""
            í‚¤ì›Œë“œë¥¼ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
            
            **í˜„ì¬ êµ¬í˜„:**
            - ê¸°ë³¸ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
            
            **í–¥í›„ í™•ì¥ ì˜ˆì •:**
            - ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (semantic)
            - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì˜ë¯¸)
            - ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
            - AI ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
            """)
async def search_documents(
    search_request: SearchRequest,
    user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """
    ğŸ¯ ê¸°ëŠ¥: ë¬¸ì„œ ê²€ìƒ‰ (ê¸°ë³¸ êµ¬í˜„ + ë©€í‹°ëª¨ë‹¬/í•˜ì´ë¸Œë¦¬ë“œ í™•ì¥ ëŒ€ë¹„)
    ğŸ“‹ ë‹¨ê³„: ê¶Œí•œ ê¸°ë°˜ í•„í„°ë§ ë° ê¸°ë³¸ ê²€ìƒ‰
    ğŸ”® í™•ì¥: ë²¡í„° ê²€ìƒ‰ì€ ì¶”í›„ /api/v1/search/ APIë¡œ ë¶„ë¦¬ ì˜ˆì •
    
    **ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ í™•ì¥ í¬ì¸íŠ¸:**
    - search_mode: 'keyword', 'semantic', 'hybrid', 'multimodal'
    - image_query: ì´ë¯¸ì§€ ê¸°ë°˜ ê²€ìƒ‰ (í–¥í›„ ì§€ì›)
    - visual_similarity: ì‹œê°ì  ìœ ì‚¬ë„ ê²€ìƒ‰ (í–¥í›„ ì§€ì›)
    """
    try:
        logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ ìš”ì²­ - ì¿¼ë¦¬: '{search_request.query}', ì‚¬ìš©ì: {user.emp_no}")
        
        # ê²€ìƒ‰ ëª¨ë“œ ê²°ì • (í–¥í›„ í™•ì¥)
        search_mode = getattr(search_request, 'search_mode', 'keyword')  # ê¸°ë³¸ê°’: keyword
        
        # ğŸ”® ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜ (ì¶”í›„ ë²¡í„° ê²€ìƒ‰ êµ¬í˜„ ì˜ˆì •)
        response = SearchResponse(
            success=True,
            query=search_request.query,
            results=[],
            total_found=0,
            search_metadata={
                "search_type": search_mode,  # âœ… ê²€ìƒ‰ ëª¨ë“œ
                "processing_time": 0.1,
                "timestamp": datetime.now().isoformat(),
                "user_emp_no": user.emp_no,
                # ë©€í‹°ëª¨ë‹¬/í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° (í–¥í›„ í™•ì¥)
                "supports_multimodal": False,  # ğŸ”® í–¥í›„ Trueë¡œ ë³€ê²½
                "supports_hybrid": False,  # ğŸ”® í–¥í›„ Trueë¡œ ë³€ê²½
                "image_search_available": False,  # ğŸ”® í–¥í›„ Trueë¡œ ë³€ê²½
            },
            query_analysis={
                "original_query": search_request.query,
                "normalized_query": search_request.query.lower().strip(),
                "query_length": len(search_request.query),
                # í–¥í›„ í™•ì¥: ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼
                "has_image_query": False,  # ğŸ”® ì´ë¯¸ì§€ ì¿¼ë¦¬ í¬í•¨ ì—¬ë¶€
                "query_type": "text",  # ğŸ”® text, image, mixed
            }
        )
        
        logger.info(f"ê²€ìƒ‰ ì™„ë£Œ - ì¿¼ë¦¬: '{search_request.query}', ê²°ê³¼: 0ê°œ (ê¸°ë³¸ êµ¬í˜„)")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ - ì¿¼ë¦¬: '{search_request.query}', ì‚¬ìš©ì: {user.emp_no}, ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ê²€ìƒ‰ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# =============================================================================
# ğŸ“œ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("", 
           response_model=DocumentListResponse,
           summary="ğŸ“œ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ",
           description="""
           ì‚¬ìš©ìê°€ ì ‘ê·¼ ê°€ëŠ¥í•œ ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
           
           **í•„í„°ë§ ì˜µì…˜:**
           - ì»¨í…Œì´ë„ˆë³„ í•„í„°ë§
           - í˜ì´ì§• (skip, limit)
           - ê¶Œí•œ ê¸°ë°˜ ìë™ í•„í„°ë§
           """,
           status_code=200)
async def get_documents(
    skip: int = Query(0, ge=0, description="ê±´ë„ˆë›¸ ë¬¸ì„œ ìˆ˜"),
    limit: int = Query(100, ge=1, le=100, description="ì¡°íšŒí•  ë¬¸ì„œ ìˆ˜ (ìµœëŒ€ 100)"),
    container_id: Optional[str] = Query(None, description="íŠ¹ì • ì»¨í…Œì´ë„ˆ í•„í„°ë§"),
    user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """
    ğŸ¯ ê¸°ëŠ¥: ì‚¬ìš©ìì˜ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
    ğŸ“‹ ë‹¨ê³„: ê¶Œí•œ ê¸°ë°˜ í•„í„°ë§ â†’ í˜ì´ì§• â†’ ì‘ë‹µ ë³€í™˜
    ğŸ” ê¶Œí•œ: ì‚¬ìš©ìë³„ ì ‘ê·¼ ê°€ëŠ¥í•œ ë¬¸ì„œë§Œ ì¡°íšŒ
    """
    import logging
    logger = logging.getLogger(__name__)
    logger.info(f"[DOCUMENTS-API] ğŸ¯ í•¨ìˆ˜ ì§„ì… - user={user.emp_no}, container={container_id}")
    try:
        logger.info(f"[DOCUMENTS-API] ğŸš€ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹œì‘ - ì‚¬ìš©ì: {user.emp_no}, skip: {skip}, limit: {limit}, container_id: {container_id}")
        
        # ğŸ“Š tb_file_bss_infoì™€ tb_file_dtl_info JOINí•˜ì—¬ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
        # ğŸ” ê¶Œí•œ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§: ì‚¬ìš©ìê°€ ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆì˜ ë¬¸ì„œë§Œ í‘œì‹œ
        # ğŸŒ©ï¸ AWS í™˜ê²½ í•„í„°ë§: bedrockìœ¼ë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œë§Œ í‘œì‹œ (Azure ë°ì´í„° ì œì™¸)
        from app.models.document.multimodal_models import DocExtractionSession
        
        accessible_containers_subquery = select(TbUserPermissions.container_id).where(
            and_(
                TbUserPermissions.user_emp_no == user.emp_no,
                TbUserPermissions.is_active == True
            )
        )
        
        # í˜„ì¬ í”„ë¡œë°”ì´ë”(.env ì„¤ì •)ë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œë§Œ í•„í„°ë§
        from app.utils.provider_filters import get_provider_filter_with_status
        
        processed_documents_subquery = select(DocExtractionSession.file_bss_info_sno).where(
            get_provider_filter_with_status(DocExtractionSession, include_pending=False)
        ).distinct()
        
        query = select(TbFileBssInfo, TbFileDtlInfo).select_from(
            outerjoin(TbFileBssInfo, TbFileDtlInfo, 
                     TbFileBssInfo.file_dtl_info_sno == TbFileDtlInfo.file_dtl_info_sno)
        ).where(
            and_(
                TbFileBssInfo.del_yn != 'Y',  # ì‚­ì œë˜ì§€ ì•Šì€ ë¬¸ì„œë§Œ
                or_(
                    TbFileBssInfo.created_by == str(user.emp_no),  # ë³¸ì¸ì´ ìƒì„±í•œ ë¬¸ì„œ
                    TbFileBssInfo.knowledge_container_id.in_(accessible_containers_subquery)  # ê¶Œí•œì´ ìˆëŠ” ì»¨í…Œì´ë„ˆì˜ ë¬¸ì„œ
                ),
                # ğŸŒ©ï¸ í”„ë¡œë°”ì´ë” í™˜ê²½ í•„í„°ë§: í˜„ì¬ í”„ë¡œë°”ì´ë”ë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œ ë˜ëŠ” ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë¬¸ì„œë§Œ í‘œì‹œ
                or_(
                    TbFileBssInfo.file_bss_info_sno.in_(processed_documents_subquery),  # í˜„ì¬ í”„ë¡œë°”ì´ë”ë¡œ ì²˜ë¦¬ ì™„ë£Œëœ ë¬¸ì„œ
                    TbFileBssInfo.processing_status.in_(['pending', 'processing']),  # ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ì¸ ë¬¸ì„œ
                    TbFileBssInfo.document_type == 'patent',  # íŠ¹í—ˆëŠ” URL ê¸°ë°˜ ë¬¸ì„œ ì—”íŠ¸ë¦¬ë¡œ í•­ìƒ í‘œì‹œ
                )
            )
        ).order_by(desc(TbFileBssInfo.created_date))
        
        # ğŸ“¦ ì»¨í…Œì´ë„ˆ í•„í„°ë§
        if container_id:
            query = query.where(TbFileBssInfo.knowledge_container_id == container_id)
        
        # ğŸ“„ í˜ì´ì§• ì ìš© ì „ì— ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        count_query = select(func.count(TbFileBssInfo.file_bss_info_sno)).select_from(
            outerjoin(TbFileBssInfo, TbFileDtlInfo, 
                     TbFileBssInfo.file_dtl_info_sno == TbFileDtlInfo.file_dtl_info_sno)
        ).where(
            and_(
                TbFileBssInfo.del_yn != 'Y',  # ì‚­ì œë˜ì§€ ì•Šì€ ë¬¸ì„œë§Œ
                or_(
                    TbFileBssInfo.created_by == str(user.emp_no),  # ë³¸ì¸ì´ ìƒì„±í•œ ë¬¸ì„œ
                    TbFileBssInfo.knowledge_container_id.in_(accessible_containers_subquery)  # ê¶Œí•œì´ ìˆëŠ” ì»¨í…Œì´ë„ˆì˜ ë¬¸ì„œ
                ),
                # ğŸŒ©ï¸ í”„ë¡œë°”ì´ë” í™˜ê²½ í•„í„°ë§: í˜„ì¬ í”„ë¡œë°”ì´ë”ë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œ ë˜ëŠ” ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë¬¸ì„œë§Œ
                or_(
                    TbFileBssInfo.file_bss_info_sno.in_(processed_documents_subquery),
                    TbFileBssInfo.processing_status.in_(['pending', 'processing']),
                    TbFileBssInfo.document_type == 'patent',
                )
            )
        )
        
        # ğŸ“¦ ì»¨í…Œì´ë„ˆ í•„í„°ë§ (ì¹´ìš´íŠ¸ì—ë„ ì ìš©)
        if container_id:
            count_query = count_query.where(TbFileBssInfo.knowledge_container_id == container_id)
        
        # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        total_count_result = await session.execute(count_query)
        total_count = total_count_result.scalar() or 0
        
        # ğŸ“„ í˜ì´ì§• ì ìš©
        query = query.offset(skip).limit(limit)
        
        result = await session.execute(query)
        rows = result.all()
        
        # ğŸ“‹ DocumentInfo í˜•íƒœë¡œ ë³€í™˜
        documents = []
        for file_info, file_detail in rows:
            # íŒŒì¼ í¬ê¸°ëŠ” ìƒì„¸ ì •ë³´ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
            file_size = file_detail.file_sz if file_detail else 0
            file_extension = ""
            if file_info.file_psl_nm:
                file_extension = Path(file_info.file_psl_nm).suffix.replace('.', '') if Path(file_info.file_psl_nm).suffix else ""
            
            documents.append(DocumentInfo(
                id=file_info.file_bss_info_sno,
                title=(file_detail.sj if file_detail else None) or file_info.file_lgc_nm or "ì œëª© ì—†ìŒ",
                file_name=file_info.file_psl_nm or "",
                file_size=file_size or 0,
                file_extension=file_extension,
                document_type=getattr(file_info, 'document_type', '') or '',  # ë¬¸ì„œ ìœ í˜• (patent ë“±)
                container_path=file_info.knowledge_container_id or "no_container",
                path=getattr(file_info, 'path', None),  # S3 URL ë˜ëŠ” íŒŒì¼ ê²½ë¡œ
                created_at=file_info.created_date,
                updated_at=file_info.last_modified_date,
                uploaded_by=file_info.created_by or "",
                # ë¹„ë™ê¸° ì²˜ë¦¬ ìƒíƒœ í•„ë“œ ì¶”ê°€
                processing_status=getattr(file_info, 'processing_status', 'completed') or 'completed',
                processing_error=getattr(file_info, 'processing_error', None),
                processing_started_at=getattr(file_info, 'processing_started_at', None),
                processing_completed_at=getattr(file_info, 'processing_completed_at', None)
            ))
        
        response = DocumentListResponse(
            success=True,
            documents=documents,
            total=total_count,  # ì „ì²´ ë¬¸ì„œ ìˆ˜
            current_page_count=len(documents),  # í˜„ì¬ í˜ì´ì§€ ë¬¸ì„œ ìˆ˜
            skip=skip,
            limit=limit,
            has_next=skip + limit < total_count,  # ë‹¤ìŒ í˜ì´ì§€ ì—¬ë¶€
            has_previous=skip > 0,  # ì´ì „ í˜ì´ì§€ ì—¬ë¶€
            metadata={
                "user_emp_no": str(user.emp_no),
                "container_filter": container_id,
                "timestamp": datetime.now().isoformat()
            }
        )
        logger.info(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ - ì‚¬ìš©ì: {user.emp_no}, ì¡°íšŒ ê±´ìˆ˜: {len(documents)}")
        return response

    except HTTPException as http_ex:
        logger.error(f"[DOCUMENTS-API] âŒ HTTP ì˜ˆì™¸ ë°œìƒ - status: {http_ex.status_code}, detail: {http_ex.detail}")
        raise
    except Exception as e:
        logger.error(f"[DOCUMENTS-API] âŒ ì¼ë°˜ ì˜ˆì™¸ ë°œìƒ - ì‚¬ìš©ì: {user.emp_no}, ì˜¤ë¥˜: {str(e)}, íƒ€ì…: {type(e).__name__}")
        import traceback
        logger.error(f"[DOCUMENTS-API] ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# =============================================================================
# ğŸ“ í•™ìˆ  ë¬¸ì„œ í•„í„° ì—”ë“œí¬ì¸íŠ¸ (ì„œì§€ì •ë³´ ê¸°ë°˜)
# =============================================================================

@router.get("/filters/academic",
           response_model=DocumentListResponse,
           summary="ğŸ“ í•™ìˆ  ë¬¸ì„œ í•„í„° (ì—°ë„/ì €ë„/DOI)",
           description="""
           tb_academic_document_metadataì— ì €ì¥ëœ ì„œì§€ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.

           - year_gte/year_lte: ì—°ë„ ë²”ìœ„ í•„í„° (ì˜ˆ: 2023ë…„ ì´í›„)
           - journal: ì €ë„ëª… í¬í•¨(ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
           - doi: DOI í¬í•¨(ë¶€ë¶„ ì¼ì¹˜)
           - ê¶Œí•œ: ì‚¬ìš©ìê°€ ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆì˜ ë¬¸ì„œ ë˜ëŠ” ë³¸ì¸ì´ ìƒì„±í•œ ë¬¸ì„œë§Œ
           """)
async def filter_academic_documents(
    year_gte: int = Query(None, ge=1800, le=2100, description="ì´ ì—°ë„ ì´ìƒ"),
    year_lte: int = Query(None, ge=1800, le=2100, description="ì´ ì—°ë„ ì´í•˜"),
    journal: Optional[str] = Query(None, description="ì €ë„ëª… í¬í•¨ ê²€ìƒ‰"),
    doi: Optional[str] = Query(None, description="DOI í¬í•¨ ê²€ìƒ‰"),
    container_id: Optional[str] = Query(None, description="íŠ¹ì • ì»¨í…Œì´ë„ˆ í•„í„°ë§"),
    skip: int = Query(0, ge=0, description="ê±´ë„ˆë›¸ ë¬¸ì„œ ìˆ˜"),
    limit: int = Query(100, ge=1, le=100, description="ì¡°íšŒí•  ë¬¸ì„œ ìˆ˜ (ìµœëŒ€ 100)"),
    user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db),
):
    try:
        logger.info(
            f"í•™ìˆ  ë¬¸ì„œ í•„í„°ë§ - ì‚¬ìš©ì: {user.emp_no}, year_gte={year_gte}, year_lte={year_lte}, journal={journal}, doi={doi}, container_id={container_id}, skip={skip}, limit={limit}"
        )

        # ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ì„œë¸Œì¿¼ë¦¬
        accessible_containers_subquery = select(TbUserPermissions.container_id).where(
            and_(
                TbUserPermissions.user_emp_no == user.emp_no,
                TbUserPermissions.is_active == True,
            )
        )

        # ê¸°ë³¸ FROM: íŒŒì¼ ê¸°ë³¸ + í•™ìˆ  ë©”íƒ€ë°ì´í„° INNER JOIN, íŒŒì¼ ìƒì„¸ëŠ” OUTER JOIN
        base_from = outerjoin(
            TbFileBssInfo,
            TbAcademicDocumentMetadata,
            TbFileBssInfo.file_bss_info_sno == TbAcademicDocumentMetadata.file_bss_info_sno,
        )
        base_from = outerjoin(
            base_from,
            TbFileDtlInfo,
            TbFileBssInfo.file_dtl_info_sno == TbFileDtlInfo.file_dtl_info_sno,
        )

        # WHERE: ê¶Œí•œ + ì‚­ì œ ì•„ë‹˜ + ë©”íƒ€ë°ì´í„° ì¡´ì¬
        conditions = [
            TbFileBssInfo.del_yn != 'Y',
            or_(
                TbFileBssInfo.created_by == str(user.emp_no),
                TbFileBssInfo.knowledge_container_id.in_(accessible_containers_subquery),
            ),
            TbAcademicDocumentMetadata.file_bss_info_sno.isnot(None),
        ]

        # ì„œì§€ í•„í„° ì ìš©
        if journal:
            conditions.append(TbAcademicDocumentMetadata.journal.ilike(f"%{journal}%"))
        if doi:
            conditions.append(TbAcademicDocumentMetadata.doi.ilike(f"%{doi}%"))
        if year_gte is not None:
            # yearëŠ” 4ìë¦¬ ë¬¸ìì—´, ë™ì¼ ê¸¸ì´ì´ë¯€ë¡œ ë¬¸ìì—´ ë¹„êµë¡œë„ ë²”ìœ„ ë™ì‘
            conditions.append(TbAcademicDocumentMetadata.year >= str(year_gte))
        if year_lte is not None:
            conditions.append(TbAcademicDocumentMetadata.year <= str(year_lte))
        if container_id:
            conditions.append(TbFileBssInfo.knowledge_container_id == container_id)

        query = (
            select(TbFileBssInfo, TbFileDtlInfo, TbAcademicDocumentMetadata)
            .select_from(base_from)
            .where(and_(*conditions))
            .order_by(desc(TbFileBssInfo.created_date))
            .offset(skip)
            .limit(limit)
        )

        # ì¹´ìš´íŠ¸ ì¿¼ë¦¬
        count_query = (
            select(func.count(TbFileBssInfo.file_bss_info_sno))
            .select_from(
                outerjoin(
                    TbFileBssInfo,
                    TbAcademicDocumentMetadata,
                    TbFileBssInfo.file_bss_info_sno == TbAcademicDocumentMetadata.file_bss_info_sno,
                )
            )
            .where(and_(*conditions))
        )

        total_count_result = await session.execute(count_query)
        total_count = total_count_result.scalar() or 0

        result = await session.execute(query)
        rows = result.all()

        documents: List[DocumentInfo] = []
        for file_info, file_detail, acad in rows:
            file_size = file_detail.file_sz if file_detail else 0
            file_extension = ""
            if file_info.file_psl_nm:
                file_extension = (
                    Path(file_info.file_psl_nm).suffix.replace('.', '')
                    if Path(file_info.file_psl_nm).suffix
                    else ""
                )

            title = (acad.title if acad and acad.title else None) or (
                file_detail.sj if file_detail and getattr(file_detail, 'sj', None) else None
            ) or file_info.file_lgc_nm or "ì œëª© ì—†ìŒ"

            documents.append(
                DocumentInfo(
                    id=file_info.file_bss_info_sno,
                    title=title,
                    file_name=file_info.file_psl_nm or "",
                    file_size=file_size or 0,
                    file_extension=file_extension,
                    document_type=getattr(file_info, 'document_type', '') or '',  # ë¬¸ì„œ ìœ í˜• (patent ë“±)
                    container_path=file_info.knowledge_container_id or "no_container",
                    path=getattr(file_info, 'path', None),  # S3 URL ë˜ëŠ” íŒŒì¼ ê²½ë¡œ
                    created_at=file_info.created_date,
                    updated_at=file_info.last_modified_date,
                    uploaded_by=file_info.created_by or "",
                    processing_status=getattr(file_info, 'processing_status', 'completed') or 'completed',
                    processing_error=getattr(file_info, 'processing_error', None),
                    processing_started_at=getattr(file_info, 'processing_started_at', None),
                    processing_completed_at=getattr(file_info, 'processing_completed_at', None),
                )
            )

        response = DocumentListResponse(
            success=True,
            documents=documents,
            total=total_count,
            current_page_count=len(documents),
            skip=skip,
            limit=limit,
            has_next=skip + limit < total_count,
            has_previous=skip > 0,
            metadata={
                "user_emp_no": str(user.emp_no),
                "filters": {
                    "year_gte": year_gte,
                    "year_lte": year_lte,
                    "journal": journal,
                    "doi": doi,
                    "container_id": container_id,
                },
                "timestamp": datetime.now().isoformat(),
            },
        )

        logger.info(f"í•™ìˆ  ë¬¸ì„œ í•„í„°ë§ ì™„ë£Œ - ì‚¬ìš©ì: {user.emp_no}, ì¡°íšŒ ê±´ìˆ˜: {len(documents)}/{total_count}")
        return response

    except Exception as e:
        logger.error(f"í•™ìˆ  ë¬¸ì„œ í•„í„°ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ - ì‚¬ìš©ì: {user.emp_no}, ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í•™ìˆ  ë¬¸ì„œ í•„í„°ë§ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜: {str(e)}")

# =============================================================================
# ï¿½ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/{document_id}/download",
            summary="ğŸ“¥ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ",
            description="""
            ë¬¸ì„œë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
            
            **ê¶Œí•œ í™•ì¸:**
            - ë¬¸ì„œì— ëŒ€í•œ ì½ê¸° ê¶Œí•œì´ ìˆëŠ” ì‚¬ìš©ìë§Œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
            """)
async def download_document(
    document_id: str,
    background_tasks: BackgroundTasks,
    user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """
    ğŸ¯ ê¸°ëŠ¥: ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ
    ğŸ“‹ ë‹¨ê³„: ê¶Œí•œ í™•ì¸ â†’ íŒŒì¼ ì¡´ì¬ í™•ì¸ â†’ Storage ê²€ì¦ â†’ íŒŒì¼ ì „ì†¡
    ğŸ” ê¶Œí•œ: ë¬¸ì„œ ì½ê¸° ê¶Œí•œ
    ğŸŒ©ï¸ Storage ê²€ì¦: í˜„ì¬ í”„ë¡œë°”ì´ë”ì™€ ì¼ì¹˜í•˜ëŠ” ì €ì¥ì†Œì¸ì§€ í™•ì¸
    """
    try:
        from app.core.config import settings as app_settings
        
        logger.info(
            f"ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ìš”ì²­ - ë¬¸ì„œ ID: {document_id}, ì‚¬ìš©ì: {getattr(user, 'emp_no', 'unknown')}"
        )

        # ë¬¸ì„œ ì •ë³´ ì¡°íšŒ
        query = select(TbFileBssInfo).where(
            and_(
                TbFileBssInfo.file_bss_info_sno == int(document_id),
                TbFileBssInfo.del_yn != 'Y'
            )
        )
        result = await session.execute(query)
        file_info = result.scalar_one_or_none()

        if not file_info:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ğŸ” ë‹¤ìš´ë¡œë“œ ê¶Œí•œ í™•ì¸
        container_id = getattr(file_info, 'knowledge_container_id', None)
        logger.info(f"ğŸ” ë‹¤ìš´ë¡œë“œ ê¶Œí•œ í™•ì¸ ì‹œì‘ - ì‚¬ìš©ì: {user.emp_no}, ì»¨í…Œì´ë„ˆ: {container_id}")
        
        if container_id:
            can_download, permission_message = await permission_service.check_download_permission(
                user_emp_no=str(user.emp_no),
                container_id=container_id
            )
            logger.info(f"ğŸ” ë‹¤ìš´ë¡œë“œ ê¶Œí•œ í™•ì¸ ê²°ê³¼ - can_download: {can_download}, message: {permission_message}")
            
            if not can_download:
                logger.warning(
                    f"ë‹¤ìš´ë¡œë“œ ê¶Œí•œ ì—†ìŒ - ì‚¬ìš©ì: {user.emp_no}, ë¬¸ì„œ: {document_id}, ì»¨í…Œì´ë„ˆ: {container_id}"
                )
                raise HTTPException(
                    status_code=403,
                    detail=f"ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {permission_message}"
                )
        else:
            logger.warning(f"âš ï¸ ì»¨í…Œì´ë„ˆ ID ì—†ìŒ - ë¬¸ì„œ: {document_id}")
        
        logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ê¶Œí•œ í™•ì¸ ì™„ë£Œ - ì‚¬ìš©ì: {user.emp_no}, ë¬¸ì„œ: {document_id}")

        # âœ… URL ê¸°ë°˜ ë¬¸ì„œ(íŠ¹í—ˆ ë“±)
        # - S3 URL(https://...amazonaws.com/...)ì€ presigned URLë¡œ attachment ë‹¤ìš´ë¡œë“œ ì œê³µ
        # - ê·¸ ì™¸ ì™¸ë¶€ URLì€ URL ìì²´ë¥¼ ë‹´ì€ .url(ë°”ë¡œê°€ê¸°) íŒŒì¼ë¡œ ì œê³µ
        file_path_value = str(getattr(file_info, 'path', '') or '')
        if file_path_value.startswith('http://') or file_path_value.startswith('https://'):
            # S3 URLì¸ ê²½ìš°: presigned redirectëŠ” XHR(blob ë‹¤ìš´ë¡œë“œ)ì—ì„œ CORS ì´ìŠˆê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
            # ì„œë²„ê°€ S3ì—ì„œ ì„ì‹œë¡œ ë‚´ë ¤ë°›ì•„ ë™ì¼ ì˜¤ë¦¬ì§„ìœ¼ë¡œ FileResponse ì œê³µ
            if ('.amazonaws.com' in file_path_value) or ('.s3.' in file_path_value):
                try:
                    from app.services.core.aws_service import S3Service
                    import tempfile
                    import os as _os

                    s3 = S3Service()
                    parsed = urllib.parse.urlparse(file_path_value)

                    # virtual-hosted style: https://bucket.s3.region.amazonaws.com/key  -> /key
                    object_key = parsed.path.lstrip('/')

                    # path-style: https://s3.region.amazonaws.com/bucket/key -> /bucket/key
                    bucket = getattr(settings, 's3_bucket_name', None)
                    if bucket and object_key.startswith(f"{bucket}/"):
                        object_key = object_key[len(bucket) + 1:]

                    filename = (
                        str(getattr(file_info, 'file_psl_nm', '') or '').strip()
                        or str(getattr(file_info, 'file_lgc_nm', '') or '').strip()
                        or f"document_{document_id}.pdf"
                    )
                    # MIME íƒ€ì…
                    mime_type, _ = mimetypes.guess_type(filename)
                    if not mime_type:
                        mime_type = "application/octet-stream"

                    encoded_filename = urllib.parse.quote(filename)
                    disposition = f"attachment; filename*=UTF-8''{encoded_filename}"

                    # ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ í›„ ë™ì¼ ì˜¤ë¦¬ì§„ìœ¼ë¡œ ë°˜í™˜
                    tmp_fd, tmp_path = tempfile.mkstemp(prefix='dl_', suffix=Path(filename).suffix or '.bin')
                    _os.close(tmp_fd)
                    await s3.download_file(object_key=object_key, local_path=tmp_path)

                    background_tasks.add_task(lambda p=tmp_path: _os.path.exists(p) and _os.remove(p))
                    logger.info(f"[DOWNLOAD] S3 URL ì„œë²„ í”„ë¡ì‹œ ë‹¤ìš´ë¡œë“œ: key={object_key}, filename={filename}")
                    return FileResponse(
                        path=tmp_path,
                        media_type=mime_type,
                        headers={"Content-Disposition": disposition},
                    )
                except Exception as e:
                    logger.error(f"[DOWNLOAD] S3 URL presign ì‹¤íŒ¨: {e}")
                    raise HTTPException(status_code=500, detail="S3 íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

            logical_name = (
                str(getattr(file_info, 'file_lgc_nm', '') or '').strip()
                or str(getattr(file_info, 'file_psl_nm', '') or '').strip()
                or f"document_{document_id}"
            )
            # í™•ì¥ì ë³´ì •
            if not logical_name.lower().endswith('.url'):
                logical_name = f"{logical_name}.url"
            encoded_name = urllib.parse.quote(str(logical_name))
            disposition = f"attachment; filename*=UTF-8''{encoded_name}"

            content = f"[InternetShortcut]\nURL={file_path_value}\n"
            response = Response(content=content, media_type='text/plain; charset=utf-8')
            response.headers['Content-Disposition'] = disposition
            response.headers['X-Content-Type-Options'] = 'nosniff'
            logger.info(f"[DOWNLOAD] URL ë°”ë¡œê°€ê¸° íŒŒì¼ ì œê³µ: {logical_name} -> {file_path_value}")
            return response

        # ğŸŒ©ï¸ Storage í”„ë¡œë°”ì´ë” ê²€ì¦ (URL ë¬¸ì„œëŠ” ì œì™¸)
        from app.utils.provider_filters import is_valid_storage_for_provider
        current_provider = app_settings.get_current_embedding_provider()
        if not is_valid_storage_for_provider(file_path_value):
            logger.warning(
                f"Storage ë¶ˆì¼ì¹˜ - í˜„ì¬ í™˜ê²½: {current_provider}, íŒŒì¼ ê²½ë¡œ: {file_path_value}"
            )
            raise HTTPException(
                status_code=400,
                detail=f"ì´ ë¬¸ì„œëŠ” ë‹¤ë¥¸ í™˜ê²½({current_provider})ì—ì„œ ì²˜ë¦¬ë˜ì–´ í˜„ì¬ í™˜ê²½ì—ì„œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                       f"ë¬¸ì„œë¥¼ ì¬ì²˜ë¦¬í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
            )

        # íŒŒì¼ ê²½ë¡œ í™•ì¸ (ìƒëŒ€/ì ˆëŒ€ ê²½ë¡œ ëª¨ë‘ ì²˜ë¦¬)
        original_path_for_name = Path(file_path_value)
        file_path = original_path_for_name
        if not file_path.is_absolute():
            # backend ë£¨íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³´ì • ì‹œë„
            backend_root = Path(__file__).parent.parent.parent.parent
            file_path = (backend_root / file_path).resolve()

        # ë¡œì»¬ íŒŒì¼ ìš°ì„  í™•ì¸
        if not file_path.exists():
            # ë¡œì»¬ì— ì—†ìœ¼ë©´ S3 í”„ë¦¬ì‚¬ì¸ë“œ URLë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹œë„
            try:
                from app.core.config import settings as app_settings
                storage_backend = getattr(app_settings, 'storage_backend', 'local')
            except Exception:
                storage_backend = 'local'

            looks_like_s3_key = (
                bool(file_path_value)
                and not os.path.isabs(file_path_value)
                and '/' in file_path_value
            )

            if storage_backend == 's3' and looks_like_s3_key:
                try:
                    from app.services.core.aws_service import S3Service
                    import tempfile
                    import os as _os

                    s3 = S3Service()

                    # íŒŒì¼ëª… ë° MIME íƒ€ì… ê³„ì‚°
                    logical_name = (
                        str(getattr(file_info, 'file_lgc_nm', '') or '').strip()
                        or str(getattr(file_info, 'file_psl_nm', '') or '').strip()
                        or original_path_for_name.name
                    )
                    if '.' not in Path(logical_name).name:
                        logical_name = f"{logical_name}{original_path_for_name.suffix}"
                    mime_type, _ = mimetypes.guess_type(str(logical_name))
                    if not mime_type:
                        mime_type = 'application/octet-stream'
                    encoded_name = urllib.parse.quote(str(logical_name))
                    disposition = f"attachment; filename*=UTF-8''{encoded_name}"

                    # ì„œë²„ì—ì„œ ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ í›„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                    tmp_fd, tmp_path = tempfile.mkstemp(prefix='dl_', suffix=original_path_for_name.suffix or '')
                    _os.close(tmp_fd)
                    await s3.download_file(object_key=file_path_value, local_path=tmp_path)

                    response = FileResponse(
                        path=str(tmp_path),
                        media_type=mime_type
                    )
                    response.headers["Content-Disposition"] = disposition
                    response.headers["X-Content-Type-Options"] = "nosniff"

                    # ì‘ë‹µ ì´í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ (best-effort)
                    if background_tasks is not None:
                        background_tasks.add_task(_os.remove, tmp_path)

                    logger.info("[DOWNLOAD] S3 ê°ì²´ í”„ë¡ì‹œ ë‹¤ìš´ë¡œë“œ ì œê³µ")
                    return response
                except Exception as e:
                    logger.error(f"S3 ê°ì²´ í”„ë¡ì‹œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    # ê³„ì† ì§„í–‰í•˜ì—¬ 404 ì²˜ë¦¬

            # Azure Blob Storage ì²˜ë¦¬
            elif storage_backend == 'azure_blob' and looks_like_s3_key:
                try:
                    from app.core.config import settings as app_settings
                    import tempfile
                    import os as _os
                    
                    # ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì„¤ì •: "redirect" ë˜ëŠ” "proxy" (ê¸°ë³¸ê°’: redirect)
                    download_mode = getattr(app_settings, 'azure_blob_download_mode', 'redirect')
                    azure_blob = get_azure_blob_service()
                    
                    # file_path_valueì—ì„œ purpose(container)ì™€ blob_path ì¶”ì¶œ
                    # DB ì €ì¥ í˜•ì‹: "raw/WJ_MS_SERVICE/2025/10/filename.docx"
                    # Azure Blob ì‹¤ì œ ê²½ë¡œ: "raw/WJ_MS_SERVICE/2025/10/filename.docx" (í”„ë¦¬í”½ìŠ¤ í¬í•¨!)
                    parts = file_path_value.split('/', 1)
                    if len(parts) == 2 and parts[0] in ['raw', 'intermediate', 'derived']:
                        # purposeê°€ ëª…ì‹œëœ ê²½ìš°
                        purpose = parts[0]  # "raw"
                        # âœ… ìˆ˜ì •: Azure Blobì—ëŠ” raw/ í”„ë¦¬í”½ìŠ¤ê°€ í¬í•¨ë˜ì–´ ì €ì¥ë˜ë¯€ë¡œ ì „ì²´ ê²½ë¡œ ì‚¬ìš©
                        blob_path = file_path_value  # "raw/WJ_MS_SERVICE/2025/10/..."
                    else:
                        # purpose ì—†ìœ¼ë©´ ê¸°ë³¸ raw ì‚¬ìš©
                        purpose = 'raw'
                        blob_path = f"raw/{file_path_value}"  # raw/ í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
                    
                    if download_mode == 'redirect':
                        # ğŸ”„ 302 ë¦¬ë‹¤ì´ë ‰íŠ¸ ë°©ì‹ (ê¸°ì¡´ ë°©ì‹, Azure ì§ì ‘ ì ‘ê·¼)
                        # íŒŒì¼ëª… ë° MIME íƒ€ì… ê³„ì‚°
                        logical_name = (
                            str(getattr(file_info, 'file_lgc_nm', '') or '').strip()
                            or str(getattr(file_info, 'file_psl_nm', '') or '').strip()
                            or original_path_for_name.name
                        )
                        if '.' not in Path(logical_name).name:
                            logical_name = f"{logical_name}{original_path_for_name.suffix}"
                        mime_type, _ = mimetypes.guess_type(str(logical_name))
                        if not mime_type:
                            mime_type = 'application/octet-stream'
                        encoded_name = urllib.parse.quote(str(logical_name))
                        content_disposition = f"attachment; filename*=UTF-8''{encoded_name}"
                        
                        # SAS URL ìƒì„± (1ì‹œê°„ ìœ íš¨, Content-Disposition í—¤ë” í¬í•¨)
                        sas_url = azure_blob.generate_sas_url(
                            blob_path=blob_path,
                            purpose=purpose,
                            expiry_seconds=3600,
                            content_disposition=content_disposition,
                            content_type=mime_type
                        )
                        
                        if sas_url:
                            logger.info(f"[DOWNLOAD] Azure Blob SAS URL ë¦¬ë‹¤ì´ë ‰íŠ¸ - purpose: {purpose}, blob: {blob_path}, filename: {logical_name}")
                            # 302 redirectë¡œ í´ë¼ì´ì–¸íŠ¸ê°€ ì§ì ‘ Azure Blobì—ì„œ ë‹¤ìš´ë¡œë“œ
                            return RedirectResponse(
                                url=sas_url,
                                status_code=302
                            )
                        else:
                            logger.error("Azure Blob SAS URL ìƒì„± ì‹¤íŒ¨")
                    else:
                        # ğŸ“¥ í”„ë¡ì‹œ ë°©ì‹ (ì„œë²„ì—ì„œ ì„ì‹œ ë‹¤ìš´ë¡œë“œ í›„ ì „ì†¡, í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„± í–¥ìƒ)
                        # íŒŒì¼ëª… ë° MIME íƒ€ì… ê³„ì‚°
                        logical_name = (
                            str(getattr(file_info, 'file_lgc_nm', '') or '').strip()
                            or str(getattr(file_info, 'file_psl_nm', '') or '').strip()
                            or original_path_for_name.name
                        )
                        if '.' not in Path(logical_name).name:
                            logical_name = f"{logical_name}{original_path_for_name.suffix}"
                        mime_type, _ = mimetypes.guess_type(str(logical_name))
                        if not mime_type:
                            mime_type = 'application/octet-stream'
                        encoded_name = urllib.parse.quote(str(logical_name))
                        disposition = f"attachment; filename*=UTF-8''{encoded_name}"

                        # ì„œë²„ì—ì„œ ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ í›„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                        tmp_fd, tmp_path = tempfile.mkstemp(prefix='dl_azure_', suffix=original_path_for_name.suffix or '')
                        _os.close(tmp_fd)
                        azure_blob.download_blob_to_file(blob_path, tmp_path, purpose=purpose)

                        response = FileResponse(
                            path=str(tmp_path),
                            media_type=mime_type
                        )
                        response.headers["Content-Disposition"] = disposition
                        response.headers["X-Content-Type-Options"] = "nosniff"

                        # ì‘ë‹µ ì´í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ (best-effort)
                        if background_tasks is not None:
                            background_tasks.add_task(_os.remove, tmp_path)

                        logger.info(f"[DOWNLOAD] Azure Blob í”„ë¡ì‹œ ë‹¤ìš´ë¡œë“œ ì œê³µ - purpose: {purpose}, blob: {blob_path}")
                        return response
                        
                except Exception as e:
                    logger.error(f"Azure Blob ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    # ê³„ì† ì§„í–‰í•˜ì—¬ 404 ì²˜ë¦¬

            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ê²½ë¡œ: {file_path}")
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì ‘ê·¼ íšŸìˆ˜ ì¦ê°€ (ìµœì„ ì˜ ë…¸ë ¥)
        try:
            current_count = int(getattr(file_info, 'access_count', 0) or 0)
            setattr(file_info, 'access_count', current_count + 1)
            setattr(file_info, 'last_accessed_date', datetime.now())
            await session.commit()
        except Exception as met_e:
            logger.warning(f"ë‹¤ìš´ë¡œë“œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {met_e}")

        logger.info(
            f"ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹œì‘ - ë¬¸ì„œ ID: {document_id}, ë…¼ë¦¬ëª…: {getattr(file_info, 'file_lgc_nm', None)}, ë¬¼ë¦¬ëª…: {getattr(file_info, 'file_psl_nm', None)}"
        )

        # MIME íƒ€ì… ì¶”ì • (íŒŒì¼ëª… ìš°ì„ , ì‹¤íŒ¨ ì‹œ ê²½ë¡œ ê¸°ë°˜)
        # Prefer logical (original) filename; fallback to physical
        logical_name = (
            str(getattr(file_info, 'file_lgc_nm', '') or '').strip()
            or str(getattr(file_info, 'file_psl_nm', '') or '').strip()
            or file_path.name
        )
        # Ensure filename has extension; fallback to physical suffix
        if '.' not in Path(logical_name).name:
            # Attach physical extension if missing
            logical_name = f"{logical_name}{file_path.suffix}"
        mime_type, _ = mimetypes.guess_type(str(logical_name))
        if not mime_type:
            mime_type, _ = mimetypes.guess_type(str(file_path))
        if not mime_type:
            # Office ê³„ì—´ ê¸°ë³¸ê°’ ë³´ì •
            suffix = file_path.suffix.lower()
            office_map = {
                ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
                ".ppt": "application/vnd.ms-powerpoint",
                ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                ".doc": "application/msword",
                ".xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                ".xls": "application/vnd.ms-excel",
                ".pdf": "application/pdf",
                ".txt": "text/plain",
            }
            mime_type = office_map.get(suffix, "application/octet-stream")

        # íŒŒì¼ëª… ì¸ì½”ë”© (í•œê¸€ ì•ˆì „) - files.pyì™€ ë™ì¼í•œ ë°©ì‹ ì‚¬ìš©
        safe_name = str(logical_name)
        encoded_name = urllib.parse.quote(safe_name)

        # Debug: log what we're about to send
        logger.info(
            "[DOWNLOAD] íŒŒì¼ëª…/í—¤ë” ì„¤ì • - safe_name=%s, suffix=%s, mime=%s",
            safe_name,
            file_path.suffix,
            mime_type,
        )
        logger.info(
            "[DOWNLOAD] Content-Disposition preview: %s",
            f"attachment; filename*=UTF-8''{encoded_name}"
        )

        # Create FileResponse with only UTF-8 encoded filename (same as files.py)
        response = FileResponse(
            path=str(file_path),
            media_type=mime_type
        )
        
        # Set headers manually - only use filename* (UTF-8) to avoid latin-1 issues
        response.headers["Content-Disposition"] = f"attachment; filename*=UTF-8''{encoded_name}"
        response.headers["X-Content-Type-Options"] = "nosniff"
        
        logger.info("[DOWNLOAD] FileResponse ìƒì„± ì™„ë£Œ")
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ë¬¸ì„œ ID: {document_id}, ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# =============================================================================
# ï¿½ğŸ—‘ï¸ ë¬¸ì„œ ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.delete("/{document_id}",
              summary="ğŸ—‘ï¸ ë¬¸ì„œ ì‚­ì œ",
              description="""
              ë¬¸ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤ (ì†Œí”„íŠ¸ ì‚­ì œ).
              
              **ê¶Œí•œ í™•ì¸:**
              - ë¬¸ì„œ ì—…ë¡œë“œì ë³¸ì¸ë§Œ ì‚­ì œ ê°€ëŠ¥
              - ê´€ë¦¬ìëŠ” ëª¨ë“  ë¬¸ì„œ ì‚­ì œ ê°€ëŠ¥
              """)
async def delete_document(
    document_id: int,
    user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """
    ğŸ¯ ê¸°ëŠ¥: ë¬¸ì„œ ì‚­ì œ (ì†Œí”„íŠ¸ ì‚­ì œ)
    ğŸ“‹ ë‹¨ê³„: ê¶Œí•œ í™•ì¸ â†’ ì†Œí”„íŠ¸ ì‚­ì œ â†’ ë¬¼ë¦¬ì  íŒŒì¼ ì‚­ì œ
    ğŸ” ê¶Œí•œ: ë¬¸ì„œ ì—…ë¡œë“œì ë˜ëŠ” ê´€ë¦¬ì
    """
    try:
        logger.info(f"ë¬¸ì„œ ì‚­ì œ ìš”ì²­ - ë¬¸ì„œ ID: {document_id}, ì‚¬ìš©ì: {user.emp_no}")
        
        result = await document_service.delete_document_by_id(
            document_id=document_id,
            user_emp_no=str(user.emp_no),
            session=session
        )
        
        if not result["success"]:
            if "ì°¾ì„ ìˆ˜ ì—†" in result["error"]:
                raise HTTPException(status_code=404, detail=result["error"])
            elif "ê¶Œí•œ" in result["error"]:
                raise HTTPException(status_code=403, detail=result["error"])
            else:
                raise HTTPException(status_code=500, detail=result["error"])
        
        logger.info(f"ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ - ë¬¸ì„œ ID: {document_id}, ì‚¬ìš©ì: {user.emp_no}")
        return JSONResponse(content={
            **result,
            "timestamp": datetime.now().isoformat(),
            "deleted_by": user.emp_no
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜ˆì™¸ ë°œìƒ - ë¬¸ì„œ ID: {document_id}, ì‚¬ìš©ì: {user.emp_no}, ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# =============================================================================
# ğŸ” ê¶Œí•œ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.post("/containers/{container_id}/validate",
            summary="ğŸ” ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦",
            description="""
            íŠ¹ì • ì»¨í…Œì´ë„ˆì— ëŒ€í•œ ì—…ë¡œë“œ ê¶Œí•œì„ ê²€ì¦í•©ë‹ˆë‹¤.
            
            **ì‚¬ìš© ëª©ì :**
            - í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì—…ë¡œë“œ UI í™œì„±í™”/ë¹„í™œì„±í™” ê²°ì •
            - ì‹¤ì‹œê°„ ê¶Œí•œ ìƒíƒœ í™•ì¸
            """)
async def validate_container_access(
    container_id: str,
    user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """
    ğŸ¯ ê¸°ëŠ¥: íŠ¹ì • ì»¨í…Œì´ë„ˆì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œ ê²€ì¦
    ğŸ“Š ì‘ë‹µ: ê¶Œí•œ ìœ ë¬´, ê¶Œí•œ ë ˆë²¨, ë©”ì‹œì§€
    ğŸ” ê¶Œí•œ: ë¡œê·¸ì¸ ì‚¬ìš©ì ì „ì²´
    """
    try:
        logger.info(f"ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦ - ì»¨í…Œì´ë„ˆ: {container_id}, ì‚¬ìš©ì: {user.emp_no}")
        
        can_upload, permission_message = await permission_service.check_upload_permission(
            user_emp_no=str(user.emp_no),
            container_id=container_id
        )
        
        response = {
            "valid": can_upload,
            "container_id": container_id,
            "permission_message": permission_message,
            "access_level": "UPLOADER" if can_upload else "NONE",
            "user_emp_no": str(user.emp_no),
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦ ì™„ë£Œ - ì»¨í…Œì´ë„ˆ: {container_id}, ì‚¬ìš©ì: {user.emp_no}, ê¶Œí•œ: {can_upload}")
        return JSONResponse(content=response)
        
    except Exception as e:
        logger.error(f"ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦ ì‹¤íŒ¨ - ì»¨í…Œì´ë„ˆ: {container_id}, ì‚¬ìš©ì: {user.emp_no}, ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# ğŸ“Š ëª¨ë‹ˆí„°ë§ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/upload-progress/{upload_id}",
           summary="ğŸ“Š ì—…ë¡œë“œ ì§„í–‰ë¥  ì¡°íšŒ",
           description="""
           ì—…ë¡œë“œ ì§„í–‰ ìƒí™©ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
           
           **í˜„ì¬ êµ¬í˜„:**
           - ê¸°ë³¸ ì™„ë£Œ ìƒíƒœ ë°˜í™˜
           
           **í–¥í›„ í™•ì¥ ì˜ˆì •:**
           - ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì¶”ì  (WebSocket)
           - ë°°ì¹˜ ì—…ë¡œë“œ ì§„í–‰ë¥ 
           - ì˜¤ë¥˜ ìƒì„¸ ì •ë³´
           """)
async def get_upload_progress(
    upload_id: str,
    user: User = Depends(get_current_user)
):
    """
    ğŸ¯ ê¸°ëŠ¥: ì—…ë¡œë“œ ì§„í–‰ ìƒí™© ì¡°íšŒ (ê¸°ë³¸ êµ¬í˜„)
    ğŸ”® í™•ì¥: ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì¶”ì ì€ ì¶”í›„ WebSocketìœ¼ë¡œ êµ¬í˜„ ì˜ˆì •
    """
    try:
        logger.info(f"ì—…ë¡œë“œ ì§„í–‰ë¥  ì¡°íšŒ - ID: {upload_id}, ì‚¬ìš©ì: {user.emp_no}")
        
        # ğŸ”® ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ (ì¶”í›„ ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì¶”ì  êµ¬í˜„ ì˜ˆì •)
        progress = {
            "upload_id": upload_id,
            "status": "completed",  # pending, processing, completed, error
            "progress": 100,
            "message": "ì—…ë¡œë“œ ì™„ë£Œ",
            "current_step": "ì™„ë£Œ",
            "total_steps": 1,
            "processing_time": 0.0,
            "user_emp_no": user.emp_no,
            "timestamp": datetime.now().isoformat()
        }
        
        return JSONResponse(content=progress)
        
    except Exception as e:
        logger.error(f"ì—…ë¡œë“œ ì§„í–‰ ìƒí™© ì¡°íšŒ ì‹¤íŒ¨ - ID: {upload_id}, ì‚¬ìš©ì: {user.emp_no}, ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# ï¿½ ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/{file_bss_info_sno}/chunks",
           summary="ğŸ“‹ ë¬¸ì„œ ì²­í¬ ì¡°íšŒ",
           description="""
           íŠ¹ì • ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
           
           **ê¸°ëŠ¥:**
           - ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ëª©ë¡ ë°˜í™˜
           - ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° í¬í•¨ (í˜ì´ì§€, ì„¹ì…˜, í¬ê¸° ë“±)
           - ê¶Œí•œ ê¸°ë°˜ ì ‘ê·¼ ì œì–´
           
           **ì‘ë‹µ í˜•ì‹:**
           - chunks: ì²­í¬ ëª©ë¡
           - total_chunks: ì´ ì²­í¬ ìˆ˜
           - document_info: ë¬¸ì„œ ê¸°ë³¸ ì •ë³´
           """)
async def get_document_chunks(
    file_bss_info_sno: int,
    chunk_index: Optional[int] = Query(None, description="íŠ¹ì • ì²­í¬ ì¸ë±ìŠ¤ (ì„ íƒì )"),
    db: AsyncSession = Depends(get_db),
    user: User = Depends(get_current_user)
):
    """
    ğŸ¯ ê¸°ëŠ¥: ë¬¸ì„œì˜ ì²­í¬ ì¡°íšŒ
    ğŸ“‹ ë°˜í™˜: ì²­í¬ ëª©ë¡ê³¼ ë©”íƒ€ë°ì´í„°
    """
    try:
        logger.info(f"ë¬¸ì„œ ì²­í¬ ì¡°íšŒ - ë¬¸ì„œ ID: {file_bss_info_sno}, ì‚¬ìš©ì: {user.emp_no}")
        
        # 1. ë¬¸ì„œ ì¡´ì¬ ë° ê¶Œí•œ í™•ì¸
        from sqlalchemy import text
        doc_query = text("""
            SELECT fbi.file_bss_info_sno, fbi.file_lgc_nm, fbi.knowledge_container_id
            FROM tb_file_bss_info fbi
            WHERE fbi.file_bss_info_sno = :file_sno
            AND fbi.del_yn = 'N'
        """)
        
        doc_result = await db.execute(doc_query, {"file_sno": file_bss_info_sno})
        doc_row = doc_result.fetchone()
        
        if not doc_row:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # 2. ì»¨í…Œì´ë„ˆ ê¶Œí•œ í™•ì¸
        has_permission = await permission_service.check_container_permission(
            str(user.emp_no), doc_row.knowledge_container_id, "VIEWER"
        )
        
        if not has_permission:
            raise HTTPException(status_code=403, detail="ë¬¸ì„œ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # 3. ì²­í¬ ì¡°íšŒ ì¿¼ë¦¬ êµ¬ì„±
        if chunk_index is not None:
            # íŠ¹ì • ì²­í¬ë§Œ ì¡°íšŒ
            chunks_query = text("""
                SELECT 
                    chunk_sno,
                    file_bss_info_sno,
                    chunk_index,
                    chunk_text,
                    chunk_size,
                    page_number,
                    section_title,
                    keywords,
                    named_entities,
                    knowledge_container_id,
                    created_date,
                    last_modified_date
                FROM vs_doc_contents_chunks
                WHERE file_bss_info_sno = :file_sno 
                AND chunk_index = :chunk_idx
                AND del_yn = 'N'
                ORDER BY chunk_index
            """)
            chunks_result = await db.execute(chunks_query, {
                "file_sno": file_bss_info_sno,
                "chunk_idx": chunk_index
            })
        else:
            # ëª¨ë“  ì²­í¬ ì¡°íšŒ
            chunks_query = text("""
                SELECT 
                    chunk_sno,
                    file_bss_info_sno,
                    chunk_index,
                    chunk_text,
                    chunk_size,
                    page_number,
                    section_title,
                    keywords,
                    named_entities,
                    knowledge_container_id,
                    created_date,
                    last_modified_date
                FROM vs_doc_contents_chunks
                WHERE file_bss_info_sno = :file_sno 
                AND del_yn = 'N'
                ORDER BY chunk_index
            """)
            chunks_result = await db.execute(chunks_query, {"file_sno": file_bss_info_sno})
        
        # 4. ê²°ê³¼ ì²˜ë¦¬
        chunks = []
        for row in chunks_result.fetchall():
            chunk_data = {
                "chunk_sno": row.chunk_sno,
                "chunk_index": row.chunk_index,
                "chunk_text": row.chunk_text,
                "chunk_size": row.chunk_size,
                "page_number": row.page_number,
                "section_title": row.section_title,
                "keywords": row.keywords.split(',') if row.keywords else [],
                "named_entities": row.named_entities.split(',') if row.named_entities else [],
                "created_date": row.created_date.isoformat() if row.created_date else None,
                "last_modified_date": row.last_modified_date.isoformat() if row.last_modified_date else None
            }
            chunks.append(chunk_data)
        
        # 5. ì‘ë‹µ êµ¬ì„±
        response_data = {
            "success": True,
            "document_info": {
                "file_bss_info_sno": doc_row.file_bss_info_sno,
                "file_name": doc_row.file_lgc_nm,
                "container_id": doc_row.knowledge_container_id
            },
            "chunks": chunks,
            "total_chunks": len(chunks),
            "requested_chunk_index": chunk_index
        }
        
        logger.info(f"âœ… ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì™„ë£Œ - ë¬¸ì„œ ID: {file_bss_info_sno}, ì²­í¬ ìˆ˜: {len(chunks)}")
        return JSONResponse(content=response_data)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨ - ë¬¸ì„œ ID: {file_bss_info_sno}, ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# ï¿½ğŸ› ï¸ í—¬í¼ í•¨ìˆ˜ë“¤
# =============================================================================

async def _validate_upload_file(file: UploadFile) -> dict:
    """
    ğŸ“‹ ì—…ë¡œë“œ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
    
    ğŸ” ê²€ì¦ í•­ëª©:
    - íŒŒì¼ í™•ì¥ì (ALLOWED_EXTENSIONS)
    - íŒŒì¼ í¬ê¸° (MAX_FILE_SIZE)
    - íŒŒì¼ëª… ìœ íš¨ì„±
    
    ğŸ“Š ë°˜í™˜: {"valid": bool, "error": str}
    """
    
    # ğŸ“ íŒŒì¼ í™•ì¥ì ê²€ì¦
    if not file.filename:
        return {
            "valid": False,
            "error": "íŒŒì¼ëª…ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }
    
    safe_name = file.filename or "uploaded_file"
    file_ext = Path(safe_name).suffix.lower()
    if file_ext not in ALLOWED_EXTENSIONS:
        return {
            "valid": False,
            "error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(ALLOWED_EXTENSIONS)}"
        }
    
    # ğŸ“ íŒŒì¼ í¬ê¸° ê²€ì¦ (í—¤ë” ê¸°ë°˜)
    if hasattr(file, 'size') and file.size and file.size > MAX_FILE_SIZE:
        return {
            "valid": False,
            "error": f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ í¬ê¸°: {MAX_FILE_SIZE // (1024*1024)}MB"
        }
    
    # ğŸ“ íŒŒì¼ëª… ê²€ì¦
    if len(safe_name) > 255:
        return {
            "valid": False,
            "error": "íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. (ìµœëŒ€ 255ì)"
        }
    
    # ğŸš« ë³´ì•ˆ: ìœ„í—˜í•œ íŒŒì¼ëª… íŒ¨í„´ ì²´í¬
    dangerous_patterns = ['..', '/', '\\', '<', '>', '|', ':', '*', '?', '"']
    for pattern in dangerous_patterns:
        if pattern in safe_name:
            return {
                "valid": False,
                "error": f"íŒŒì¼ëª…ì— í—ˆìš©ë˜ì§€ ì•Šì€ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {pattern}"
            }
    
    return {"valid": True}

async def _save_upload_file(file: UploadFile) -> str:
    """
    ğŸ’¾ ì—…ë¡œë“œ íŒŒì¼ì„ ì„œë²„ì— ì•ˆì „í•˜ê²Œ ì €ì¥
    
    ğŸ”§ ì²˜ë¦¬ ê³¼ì •:
    1. ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„± (UUID + ì›ë³¸ í™•ì¥ì)
    2. ì„œë²„ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥
    3. ì €ì¥ í›„ íŒŒì¼ í¬ê¸° ì¬ê²€ì¦
    
    ğŸ“Š ë°˜í™˜: ì €ì¥ëœ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ
    ğŸš« ì˜ˆì™¸: íŒŒì¼ í¬ê¸° ì´ˆê³¼ì‹œ ìë™ ì‚­ì œ í›„ HTTPException
    """
    
    # ğŸ†” ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„± (ì¶©ëŒ ë°©ì§€)
    safe_name = file.filename or "uploaded_file"
    file_extension = Path(safe_name).suffix
    unique_filename = f"{uuid.uuid4().hex}_{datetime.now().strftime('%Y%m%d_%H%M%S')}{file_extension}"
    file_path = UPLOAD_DIR / unique_filename
    
    try:
        # ğŸ’¾ íŒŒì¼ ì €ì¥
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # ğŸ“ ì €ì¥ í›„ ì‹¤ì œ íŒŒì¼ í¬ê¸° ê²€ì¦
        actual_file_size = os.path.getsize(file_path)
        if actual_file_size > MAX_FILE_SIZE:
            os.remove(file_path)  # ì¦‰ì‹œ ì‚­ì œ
            raise HTTPException(
                status_code=413,
                detail=f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ì‹¤ì œ í¬ê¸°: {actual_file_size // (1024*1024)}MB, ìµœëŒ€ í—ˆìš©: {MAX_FILE_SIZE // (1024*1024)}MB"
            )
        
        # ğŸ“‚ íŒŒì¼ ê¶Œí•œ ì„¤ì • (ì½ê¸° ì „ìš©)
        os.chmod(file_path, 0o644)

        logger.info(f"íŒŒì¼ ì €ì¥ ì„±ê³µ - ì›ë³¸: {safe_name}, ì €ì¥: {unique_filename}, í¬ê¸°: {actual_file_size:,} bytes")
        return str(file_path)
        
    except HTTPException:
        raise
    except Exception as e:
        # ğŸ—‘ï¸ ì˜¤ë¥˜ ë°œìƒì‹œ ë¶€ë¶„ ì €ì¥ëœ íŒŒì¼ ì •ë¦¬
        if file_path.exists():
            os.remove(file_path)
        logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ - ì›ë³¸: {safe_name}, ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# =============================================================================
# ğŸ“‹ API ì—”ë“œí¬ì¸íŠ¸ ìš”ì•½
# =============================================================================
"""
ğŸŒ WKMS Documents API v1 ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡:

ğŸ“‚ ì»¨í…Œì´ë„ˆ ê´€ë¦¬:
  GET  /containers                     - ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ëª©ë¡
  POST /containers/{id}/validate       - ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦

ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ:
  POST /upload                         - ë¬¸ì„œ ì—…ë¡œë“œ (ë©”ì¸ ê¸°ëŠ¥)
  GET  /upload-progress/{id}           - ì—…ë¡œë“œ ì§„í–‰ë¥  ì¡°íšŒ

ğŸ“œ ë¬¸ì„œ ì¡°íšŒ:
  GET  /                              - ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (í˜ì´ì§• ì§€ì›)
  
ğŸ” ë¬¸ì„œ ê²€ìƒ‰:
  POST /search                        - ë¬¸ì„œ ê²€ìƒ‰ (ê¸°ë³¸ êµ¬í˜„)

ğŸ—‘ï¸ ë¬¸ì„œ ê´€ë¦¬:
  DELETE /{id}                        - ë¬¸ì„œ ì‚­ì œ

ğŸ”® í–¥í›„ í™•ì¥ ì˜ˆì •:
  GET  /{id}                         - ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ
  PUT  /{id}                         - ë¬¸ì„œ ìˆ˜ì •
  GET  /{id}/chunks                  - ë¬¸ì„œ ì²­í¬ ì¡°íšŒ (ë²¡í„° ê²€ìƒ‰ìš©)
  POST /{id}/reindex                 - ë¬¸ì„œ ì¬ì¸ë±ì‹±
  GET  /statistics                   - ë¬¸ì„œ í†µê³„
  POST /batch-upload                 - ë°°ì¹˜ ì—…ë¡œë“œ

ğŸ“¡ URL êµ¬ì¡°:
  /api/v1/documents/                 - ë©”ì¸ ë¬¸ì„œ API (ì´ íŒŒì¼)
  /api/v1/documents/containers       - ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ (í†µí•©ë¨)
  /api/services/processing/          - ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤
  /api/services/large-files/         - ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬
"""

# =============================================================================
# ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²­í¬ ì¡°íšŒ API (ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ìš©)
# =============================================================================

@router.get("/chunks/{chunk_id}/image",
    summary="ì²­í¬ ì´ë¯¸ì§€ ì¡°íšŒ",
    description="ì´ë¯¸ì§€ ì²­í¬ì˜ ì´ë¯¸ì§€ íŒŒì¼ì„ Azure Blob Storageì—ì„œ ê°€ì ¸ì™€ ë°˜í™˜í•©ë‹ˆë‹¤.",
    response_class=FileResponse
)
async def get_chunk_image(
    chunk_id: int,
    user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    ğŸ–¼ï¸ ì²­í¬ ì´ë¯¸ì§€ ì¡°íšŒ
    
    ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ ì²­í¬ì˜ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        chunk_id: ì²­í¬ ID
        user: í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ì
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        
    Returns:
        ì´ë¯¸ì§€ íŒŒì¼ (PNG, JPEG ë“±)
    """
    logger.info(f"[IMAGE_CHUNK] ========== ì—”ë“œí¬ì¸íŠ¸ ì§„ì… ========== chunk_id={chunk_id}")
    try:
        logger.info(f"[IMAGE_CHUNK] ì²­í¬ ì´ë¯¸ì§€ ì¡°íšŒ ì‹œì‘ - chunk_id={chunk_id}, user={user.emp_no}")
        
        # 1. doc_chunk í…Œì´ë¸”ì—ì„œ ì´ë¯¸ì§€ ì²­í¬ ì •ë³´ ì¡°íšŒ
        from app.models.document.multimodal_models import DocChunk, DocChunkSession
        from app.models import TbFileBssInfo
        
        stmt = (
            select(DocChunk, DocChunkSession, TbFileBssInfo)
            .join(DocChunkSession, DocChunk.chunk_session_id == DocChunkSession.chunk_session_id)
            .join(TbFileBssInfo, DocChunk.file_bss_info_sno == TbFileBssInfo.file_bss_info_sno)
            .where(DocChunk.chunk_id == chunk_id)
            .where(TbFileBssInfo.del_yn == 'N')
        )
        
        result = await db.execute(stmt)
        row = result.one_or_none()
        
        if not row:
            logger.warning(f"[IMAGE_CHUNK] ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - chunk_id={chunk_id}")
            raise HTTPException(status_code=404, detail="ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        chunk, chunk_session, file_info = row
        
        # 2. ê¶Œí•œ ê²€ì¦ - ì‚¬ìš©ìê°€ í•´ë‹¹ ì»¨í…Œì´ë„ˆì— ì ‘ê·¼ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸
        from app.services.auth.permission_service import PermissionService
        
        permission_service = PermissionService(db)
        container_id = file_info.knowledge_container_id
        
        has_access = await permission_service.check_container_access(
            user_emp_no=str(user.emp_no),
            container_id=container_id
        )
        
        if not has_access:
            logger.warning(
                f"[IMAGE_CHUNK] ê¶Œí•œ ì—†ìŒ - user={user.emp_no}, "
                f"chunk_id={chunk_id}, container_id={container_id}"
            )
            raise HTTPException(
                status_code=403,
                detail="í•´ë‹¹ ì´ë¯¸ì§€ì— ì ‘ê·¼í•  ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤"
            )
        
        logger.info(
            f"[IMAGE_CHUNK] ê¶Œí•œ í™•ì¸ ì™„ë£Œ - user={user.emp_no}, "
            f"container_id={container_id}"
        )
        
        # 3. modality í™•ì¸ (IMAGE ì²­í¬ì¸ì§€ ê²€ì¦)
        if chunk.modality != "image":
            logger.warning(f"[IMAGE_CHUNK] ì´ë¯¸ì§€ ì²­í¬ê°€ ì•„ë‹˜ - chunk_id={chunk_id}, modality={chunk.modality}")
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ ì²­í¬ê°€ ì•„ë‹™ë‹ˆë‹¤")
        
        # 4. ì´ë¯¸ì§€ blob í‚¤ ê°€ì ¸ì˜¤ê¸°
        doc_id = chunk.file_bss_info_sno
        
        # ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œì— ë”°ë¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ ì„ íƒ
        from app.core.config import settings
        
        async def _download_intermediate_blob(path: str) -> bytes:
            loop = asyncio.get_running_loop()
            
            if settings.storage_backend == "s3":
                # AWS S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
                # pathëŠ” ì´ë¯¸ "multimodal/23/objects/image_3940_5.png" í˜•ì‹
                # S3Service.download_bytes()ëŠ” purpose='intermediate'ë¡œ prefix ìë™ ì¶”ê°€
                from app.services.core.aws_service import S3Service
                s3_service = S3Service()
                return await loop.run_in_executor(
                    None,
                    lambda: s3_service.download_bytes(path, purpose="intermediate")
                )
            else:
                # Azure Blobì—ì„œ ë‹¤ìš´ë¡œë“œ
                blob_service = get_azure_blob_service()
                return await loop.run_in_executor(
                    None,
                    lambda: blob_service.download_blob_to_bytes(path, purpose="intermediate")
                )

        # blob_keyê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš© (ì‹ ê·œ ë°©ì‹)
        if chunk.blob_key:
            image_blob_key = chunk.blob_key
            logger.info(f"[IMAGE_CHUNK] blob_key ì§ì ‘ ì‚¬ìš©: {image_blob_key}")
        else:
            # blob_keyê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ìƒì„± (êµ¬ ë°ì´í„° í˜¸í™˜ì„±)
            logger.warning(f"[IMAGE_CHUNK] blob_key ì—†ìŒ (êµ¬ ë°ì´í„°) - chunk_id={chunk_id}, ë™ì  ìƒì„± ì‹œë„")
            
            # source_object_idsì—ì„œ ì´ë¯¸ì§€ ê°ì²´ ID ì¶”ì¶œ
            if not chunk.source_object_ids or len(chunk.source_object_ids) == 0:
                logger.error(f"[IMAGE_CHUNK] source_object_ids ì—†ìŒ - chunk_id={chunk_id}")
                raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ ê°ì²´ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ì²« ë²ˆì§¸ source_object_id ì‚¬ìš© (ì´ë¯¸ì§€ëŠ” ë³´í†µ í•˜ë‚˜ì˜ ê°ì²´ë§Œ ì°¸ì¡°)
            object_id = chunk.source_object_ids[0]
            
            # page_rangeì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            page_number = 1  # ê¸°ë³¸ê°’
            if chunk.page_range:
                # page_rangeëŠ” Range ê°ì²´ë¡œ ë°˜í™˜ë¨
                page_number = chunk.page_range.lower if hasattr(chunk.page_range, 'lower') else 1
            
            # Blob í‚¤ íŒ¨í„´: multimodal/{doc_id}/objects/image_{object_id}_{page_number}.png
            image_blob_key = f"multimodal/{doc_id}/objects/image_{object_id}_{page_number}.png"
            logger.info(f"[IMAGE_CHUNK] ë™ì  ìƒì„±ëœ blob í‚¤: {image_blob_key} (object_id={object_id}, page={page_number})")
        
        logger.info(f"[IMAGE_CHUNK] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„ - blob_key={image_blob_key}")
        
        # 5. Azure Blob Storageì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        try:
            image_bytes = await _download_intermediate_blob(image_blob_key)
            
            if not image_bytes:
                logger.error(f"[IMAGE_CHUNK] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - blob_key={image_blob_key}")
                raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            logger.info(f"[IMAGE_CHUNK] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ - size={len(image_bytes)} bytes")
            
            # 6. ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ë°˜í™˜
            import tempfile
            import os
            from fastapi.responses import FileResponse
            
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            suffix = ".png"  # ê¸°ë³¸ PNG
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(image_bytes)
                tmp_path = tmp_file.name
            
            logger.info(f"[IMAGE_CHUNK] ì„ì‹œ íŒŒì¼ ìƒì„± ì™„ë£Œ - path={tmp_path}")
            
            # FileResponse ë°˜í™˜ (ìë™ìœ¼ë¡œ íŒŒì¼ ì‚­ì œ)
            return FileResponse(
                path=tmp_path,
                media_type="image/png",
                filename=f"image_chunk_{chunk_id}.png",
                background=None  # ì‘ë‹µ í›„ íŒŒì¼ ìœ ì§€ (cleanupì€ OSê°€ ì²˜ë¦¬)
            )
            
        except Exception as e:
            logger.error(f"[IMAGE_CHUNK] ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[IMAGE_CHUNK] ì´ë¯¸ì§€ ì¡°íšŒ ì‹¤íŒ¨ - chunk_id={chunk_id}, error={str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì´ë¯¸ì§€ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


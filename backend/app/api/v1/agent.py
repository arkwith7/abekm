"""
Agent-based Chat API - PaperSearchAgentë¥¼ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
Feature flagë¡œ ì ì§„ì  ì „í™˜ ê°€ëŠ¥

í†µí•©ëœ ì—”ë“œí¬ì¸íŠ¸:
- /agent/chat - ê¸°ë³¸ ì±„íŒ…
- /agent/chat/stream - ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…
- /agent/chat/assets - ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ
- /agent/chat/assets/{asset_id} - ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
- /agent/chat/transcribe - ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜
- /agent/sessions - ì„¸ì…˜ ê´€ë¦¬
"""
from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Form
from fastapi.responses import StreamingResponse, FileResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime
import uuid
import os
import tempfile
import asyncio
import aiofiles
from botocore.exceptions import ClientError
from urllib.parse import quote

from app.core.database import get_db
from app.core.dependencies import get_current_user
from app.core.config import settings
from app.models import User
from app.agents import paper_search_agent
from app.agents.supervisor_agent import supervisor_agent
from langchain_core.messages import HumanMessage
from app.tools.contracts import AgentConstraints, AgentIntent, AgentResult
from loguru import logger
from app.services.document.extraction.text_extractor_service import TextExtractorService
from app.services.chat.chat_attachment_service import chat_attachment_service
from app.services.core.audio_transcription_service import audio_transcription_service
from pathlib import Path


# íƒœê·¸ëŠ” main.pyì—ì„œ include_router ì‹œ ì„¤ì •ë¨ (tags=["ğŸ¤– Agent RAG"])
router = APIRouter()


def _should_force_ppt_generation(message: str, tool: Optional[str]) -> bool:
    """ì‚¬ìš©ì ì§ˆì˜ë‚˜ ë„êµ¬ ì„ íƒì„ ê¸°ë°˜ìœ¼ë¡œ PPT ìƒì„±ì„ ê°•ì œí• ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨."""
    if tool == 'ppt':
        return True

    if not message:
        return False

    lowered = message.lower()
    ppt_keywords = [
        "ppt",
        "í”„ë ˆì  í…Œì´ì…˜",
        "í”„ë¦¬ì  í…Œì´ì…˜",
        "ë°œí‘œìë£Œ",
        "ë°œí‘œ ìë£Œ",
        "ìŠ¬ë¼ì´ë“œ",
        "presentation"
    ]
    # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆê³  "ë§Œë“¤", "ì‘ì„±", "ìƒì„±" ë“±ì˜ ë™ì‚¬ê°€ í•¨ê»˜ ë“±ì¥í•˜ë©´ PPT ìš”ì²­ìœ¼ë¡œ ê°„ì£¼
    action_keywords = ["ë§Œë“¤", "ì‘ì„±", "ìƒì„±", "ì œì‘", "ì‘ì„±í•´", "ë§Œë“¤ì–´", "create", "generate"]
    contains_ppt = any(keyword in lowered for keyword in ppt_keywords)
    contains_action = any(action in lowered for action in action_keywords)
    return contains_ppt and contains_action


# Request/Response ëª¨ë¸
class AgentChatRequest(BaseModel):
    """Agent ê¸°ë°˜ ì±„íŒ… ìš”ì²­"""
    message: str = Field(..., min_length=1, description="ì‚¬ìš©ì ì§ˆì˜")
    images: Optional[List[str]] = Field(None, description="ì´ë¯¸ì§€ ëª©ë¡ (Base64)")
    session_id: Optional[str] = Field(None, description="ì„¸ì…˜ ID")
    
    # ì œì•½ ì¡°ê±´
    max_chunks: int = Field(10, ge=1, le=50, description="ìµœëŒ€ ì²­í¬ ìˆ˜")
    max_tokens: int = Field(4000, ge=100, le=8000, description="ìµœëŒ€ í† í° ìˆ˜")  # 2000 â†’ 4000 (ì¼ë°˜ RAGì™€ ë™ì¼)
    similarity_threshold: float = Field(0.25, ge=0.0, le=1.0, description="ìœ ì‚¬ë„ ì„ê³„ê°’")  # 0.5 â†’ 0.25ë¡œ ë‚®ì¶¤ (ì¼ë°˜ RAGì™€ ë™ì¼)
    
    # í•„í„°ë§
    container_ids: Optional[List[str]] = Field(None, description="ì»¨í…Œì´ë„ˆ ID í•„í„°")
    document_ids: Optional[List[str]] = Field(None, description="ë¬¸ì„œ ID í•„í„°")
    
    # ğŸ†• ì²¨ë¶€ íŒŒì¼ (Chat with File)
    attachments: Optional[List[Dict[str, Any]]] = Field(None, description="ì²¨ë¶€ íŒŒì¼ ëª©ë¡ (asset_id, mime_type ë“±)")
    
    # ğŸ†• ë„êµ¬ ê°•ì œ ì„ íƒ
    tool: Optional[str] = Field(None, description="ê°•ì œ ì„ íƒí•  ë„êµ¬ (ppt, web-search ë“±)")


class AgentStepResponse(BaseModel):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰ ë‹¨ê³„"""
    step_number: int
    tool_name: str
    reasoning: str
    latency_ms: float
    items_returned: Optional[int] = None
    success: bool


class ReferenceDocument(BaseModel):
    """ì°¸ì¡° ë¬¸ì„œ"""
    chunk_id: str
    content: str
    score: float
    document_id: Optional[str] = None
    title: Optional[str] = None
    page_number: Optional[int] = None


class DetailedChunk(BaseModel):
    """ìƒì„¸ ì²­í¬ ì •ë³´ (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼ í˜•ì‹)"""
    index: int
    file_id: int
    file_name: str
    chunk_index: int
    page_number: Optional[int] = None
    content_preview: str
    similarity_score: float
    search_type: str
    section_title: str = ""


class AgentChatResponse(BaseModel):
    """Agent ê¸°ë°˜ ì±„íŒ… ì‘ë‹µ"""
    answer: str
    intent: str
    strategy_used: List[str]
    references: List[ReferenceDocument]
    detailed_chunks: List[DetailedChunk] = []  # ğŸ†• ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼ í˜•ì‹
    steps: List[AgentStepResponse]
    metrics: Dict[str, Any]
    success: bool
    errors: List[str] = []


@router.post("/agent/chat", response_model=AgentChatResponse)
async def agent_chat(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Agent ê¸°ë°˜ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (Supervisor Architecture)
    
    Supervisor Agentë¥¼ ì‚¬ìš©í•˜ì—¬:
    1. ì‚¬ìš©ì ì˜ë„ íŒŒì•… (ê²€ìƒ‰ vs PPT ìƒì„± vs ê¸°íƒ€)
    2. ì ì ˆí•œ Worker Agent (SearchAgent, PresentationAgent) í˜¸ì¶œ
    3. ê²°ê³¼ í†µí•© ë° ë°˜í™˜
    """
    try:
        user_emp_no = str(current_user.emp_no)
        logger.info(f"ğŸ¤– [AgentChat] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")
        
        # Supervisor ì‹¤í–‰
        initial_state = {
            "messages": [HumanMessage(content=request.message)],
            "next": "",
            "shared_context": {}
        }
        
        # LangGraph ì‹¤í–‰
        final_state = await supervisor_agent.ainvoke(initial_state)
        
        # ê²°ê³¼ ì¶”ì¶œ
        messages = final_state["messages"]
        last_message = messages[-1]
        answer = last_message.content
        shared_context = final_state.get("shared_context", {})
        
        # SearchAgent ê²°ê³¼ ë³µì›
        search_result = shared_context.get("search_agent_result")
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        references_response = []
        detailed_chunks_response = []
        steps_response = []
        metrics = {}
        intent = "general"
        strategy_used = []
        
        if search_result:
            # SearchAgentê°€ ì‹¤í–‰ëœ ê²½ìš° ì •ë³´ ë³µì›
            intent = search_result.intent.value
            strategy_used = search_result.strategy_used
            metrics = search_result.metrics
            
            # Steps
            for step in search_result.steps:
                steps_response.append(AgentStepResponse(
                    step_number=step.step_number,
                    tool_name=step.tool_name,
                    reasoning=step.reasoning,
                    latency_ms=step.tool_output.metrics.latency_ms,
                    items_returned=step.tool_output.metrics.items_returned,
                    success=step.tool_output.success
                ))
            
            # References & Chunks
            for idx, ref in enumerate(search_result.references):
                file_id = ref.file_id
                file_name = None
                chunk_index = 0
                page_number = None
                
                if ref.metadata:
                    file_name = ref.metadata.get("file_name") or ref.metadata.get("title")
                    chunk_index = ref.metadata.get("chunk_index", 0)
                    page_number = ref.metadata.get("page_number")
                
                references_response.append(ReferenceDocument(
                    chunk_id=ref.chunk_id,
                    content=ref.content,
                    score=ref.score,
                    document_id=ref.metadata.get("document_id") if ref.metadata else None,
                    title=file_name,
                    page_number=page_number
                ))
                
                detailed_chunks_response.append(DetailedChunk(
                    index=idx + 1,
                    file_id=int(file_id) if file_id and str(file_id).isdigit() else 0,
                    file_name=file_name or "ë¬¸ì„œ",
                    chunk_index=chunk_index,
                    page_number=page_number,
                    content_preview=ref.content[:200] if ref.content else "",
                    similarity_score=ref.score,
                    search_type="agent",
                    section_title=file_name or ""
                ))
        
        # PresentationAgentê°€ ì‹¤í–‰ëœ ê²½ìš° (ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ PresentationAgentì¸ ê²½ìš°)
        if getattr(last_message, "name", "") == "PresentationAgent":
            intent = "presentation_generation"
            # PPT ìƒì„± ê´€ë ¨ ë©”íŠ¸ë¦­ì´ë‚˜ ìŠ¤í… ì¶”ê°€ ê°€ëŠ¥
            steps_response.append(AgentStepResponse(
                step_number=len(steps_response) + 1,
                tool_name="PresentationAgent",
                reasoning="Generated presentation based on search results.",
                latency_ms=0,
                success=True
            ))

        logger.info(f"âœ… [AgentChat] ì™„ë£Œ: {len(references_response)}ê°œ ì°¸ì¡°")
        
        return AgentChatResponse(
            answer=answer,
            intent=intent,
            strategy_used=strategy_used,
            references=references_response,
            detailed_chunks=detailed_chunks_response,
            steps=steps_response,
            metrics=metrics,
            success=True,
            errors=[]
        )
        
    except Exception as e:
        logger.error(f"âŒ [AgentChat] ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/agent/chat/v2", response_model=AgentChatResponse)
async def agent_chat_v2(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """V2 ë¼ìš°íŠ¸ ìœ ì§€ìš© alias: í˜„ì¬ëŠ” ê¸°ì¡´ `paper_search_agent`ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        user_emp_no = str(current_user.emp_no)
        logger.info(f"ğŸ¤–ğŸ†• [AgentChatV2] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")

        constraints_obj = AgentConstraints(
            max_chunks=request.max_chunks,
            max_tokens=request.max_tokens,
            similarity_threshold=request.similarity_threshold,
            container_ids=request.container_ids,
            document_ids=request.document_ids,
        )

        context = {
            "user_emp_no": user_emp_no,
            "session_id": request.session_id or str(uuid.uuid4()),
        }

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ (ë©€í‹°í„´ ì§€ì›)
        history: List[Dict[str, str]] = []
        if request.session_id:
            try:
                from app.models.chat.chat_models import TbChatHistory
                from sqlalchemy import select

                history_stmt = (
                    select(TbChatHistory)
                    .where(TbChatHistory.session_id == request.session_id)
                    .order_by(TbChatHistory.created_date.asc())
                    .limit(10)
                )
                history_result = await db.execute(history_stmt)
                history_records = history_result.scalars().all()

                for record in history_records:
                    if record.user_message:
                        history.append({"role": "user", "content": record.user_message})
                    if record.assistant_response:
                        history.append({"role": "assistant", "content": record.assistant_response})

                logger.info(f"ğŸ“š [AgentChatV2] íˆìŠ¤í† ë¦¬ ë¡œë“œ: {len(history)}ê°œ ë©”ì‹œì§€")
            except Exception as e:
                logger.warning(f"âš ï¸ íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")

        result: AgentResult = await paper_search_agent.execute(
            query=request.message,
            db_session=db,
            constraints=constraints_obj,
            context=context,
            history=history,
            images=request.images or [],
            attachments=request.attachments or [],
        )

        steps_response: List[AgentStepResponse] = []
        for step in result.steps:
            steps_response.append(
                AgentStepResponse(
                    step_number=step.step_number,
                    tool_name=step.tool_name,
                    reasoning=step.reasoning,
                    latency_ms=0,
                    success=True,
                )
            )

        references_response: List[ReferenceDocument] = []
        detailed_chunks_response: List[DetailedChunk] = []

        for idx, ref in enumerate(result.references or []):
            file_name = None
            chunk_index = 0
            page_number = None

            if ref.metadata:
                file_name = ref.metadata.get("file_name") or ref.metadata.get("title")
                chunk_index = ref.metadata.get("chunk_index", 0)
                page_number = ref.metadata.get("page_number")

            references_response.append(
                ReferenceDocument(
                    chunk_id=ref.chunk_id,
                    content=ref.content,
                    score=ref.score,
                    document_id=ref.metadata.get("document_id") if ref.metadata else None,
                    title=file_name,
                    page_number=page_number,
                )
            )

            detailed_chunks_response.append(
                DetailedChunk(
                    index=idx + 1,
                    file_id=int(ref.file_id) if ref.file_id and str(ref.file_id).isdigit() else 0,
                    file_name=file_name or "ë¬¸ì„œ",
                    chunk_index=chunk_index,
                    page_number=page_number,
                    content_preview=(ref.content or "")[:200],
                    similarity_score=ref.score,
                    search_type="agent",
                    section_title=file_name or "",
                )
            )

        logger.info(f"âœ… [AgentChatV2] ì™„ë£Œ: {len(result.steps)}ê°œ ë‹¨ê³„")

        return AgentChatResponse(
            answer=result.answer,
            intent=result.intent.value if result.intent else "general",
            strategy_used=result.strategy_used or [],
            references=references_response,
            detailed_chunks=detailed_chunks_response,
            steps=steps_response,
            metrics=result.metrics or {},
            success=result.success,
            errors=result.errors or [],
        )

    except Exception as e:
        logger.error(f"âŒ [AgentChatV2] ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Agent V2 ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}",
        )
@router.post("/agent/compare", response_model=Dict[str, Any])
async def compare_architectures(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    A/B ë¹„êµ ì—”ë“œí¬ì¸íŠ¸
    
    ë™ì¼í•œ ì§ˆì˜ë¥¼ ë‘ ê°€ì§€ ì•„í‚¤í…ì²˜ë¡œ ì‹¤í–‰í•˜ì—¬ ë¹„êµ:
    1. ê¸°ì¡´ rag_search_service (monolithic)
    2. ìƒˆë¡œìš´ paper_search_agent (agent-based)
    
    í‰ê°€ ë° ì„±ëŠ¥ ë¶„ì„ì— ì‚¬ìš©
    """
    try:
        user_emp_no = str(current_user.emp_no)
        logger.info(f"ğŸ“Š [Compare] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")
        
        # ì œì•½ ì¡°ê±´
        constraints = AgentConstraints(
            max_chunks=request.max_chunks,
            max_tokens=request.max_tokens,
            similarity_threshold=request.similarity_threshold,
            container_ids=request.container_ids,
            document_ids=request.document_ids
        )
        
        context = {
            "user_emp_no": user_emp_no,
            "session_id": request.session_id or str(uuid.uuid4())
        }
        
        # ìƒˆ ì•„í‚¤í…ì²˜ ì‹¤í–‰
        start_time_new = datetime.utcnow()
        result_new: AgentResult = await paper_search_agent.execute(
            query=request.message,
            db_session=db,
            constraints=constraints,
            context=context
        )
        latency_new = (datetime.utcnow() - start_time_new).total_seconds() * 1000
        
        # ê¸°ì¡´ ì•„í‚¤í…ì²˜ ì‹¤í–‰
        # TODO: rag_search_service í˜¸ì¶œ (í˜„ì¬ëŠ” mock)
        latency_old = 0.0
        result_old = {
            "answer": "[ê¸°ì¡´ ì•„í‚¤í…ì²˜ ê²°ê³¼ - TODO: rag_search_service í†µí•©]",
            "references": [],
            "chunks_found": 0
        }
        
        # ë¹„êµ ê²°ê³¼
        comparison = {
            "query": request.message,
            "old_architecture": {
                "answer": result_old["answer"],
                "latency_ms": latency_old,
                "chunks_found": result_old["chunks_found"],
                "implementation": "rag_search_service (monolithic)"
            },
            "new_architecture": {
                "answer": result_new.answer,
                "latency_ms": latency_new,
                "chunks_found": result_new.metrics.get("chunks_found", 0),
                "chunks_used": result_new.metrics.get("chunks_used", 0),
                "intent": result_new.intent.value,
                "strategy": result_new.strategy_used,
                "tools_used": len(result_new.steps),
                "implementation": "paper_search_agent (agent-based)"
            },
            "improvement": {
                "latency_diff_ms": latency_old - latency_new,
                "latency_improvement_pct": ((latency_old - latency_new) / latency_old * 100) if latency_old > 0 else 0,
            },
            "observability": {
                "agent_steps": [
                    {
                        "tool": step.tool_name,
                        "reasoning": step.reasoning,
                        "latency_ms": step.tool_output.metrics.latency_ms
                    }
                    for step in result_new.steps
                ]
            }
        }
        
        logger.info(
            f"ğŸ“Š [Compare] ì™„ë£Œ - ì‹ ê·œ: {latency_new:.1f}ms, êµ¬: {latency_old:.1f}ms"
        )
        
        return comparison
        
    except Exception as e:
        logger.error(f"âŒ [Compare] ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¹„êµ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/agent/chat/stream")
async def agent_chat_stream(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    ğŸ†• Agent ê¸°ë°˜ ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸
    
    ì‹¤ì‹œê°„ìœ¼ë¡œ AIì˜ ì‚¬ê³  ê³¼ì •(Reasoning)ê³¼ ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°:
    1. reasoning_step: ê° ë„êµ¬ ì‹¤í–‰ ë‹¨ê³„ (ì§ˆì˜ ë¶„ì„, ê²€ìƒ‰, ì¬ì •ë ¬ ë“±)
    2. search_progress: ê²€ìƒ‰ ì§„í–‰ ìƒí™© (ë²¡í„° ê²€ìƒ‰, í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼)
    3. content: ë‹µë³€ í…ìŠ¤íŠ¸ (ì²­í¬ ë‹¨ìœ„)
    4. metadata: ìµœì¢… ë©”íƒ€ë°ì´í„° (ì°¸ê³  ë¬¸ì„œ, ë©”íŠ¸ë¦­)
    5. done: ì™„ë£Œ
    """
    import json
    import asyncio
    from fastapi.responses import StreamingResponse
    
    async def event_generator():
        try:
            user_emp_no = str(current_user.emp_no)
            logger.info(f"ğŸ¤– [AgentChatStream] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")
            
            # ì œì•½ ì¡°ê±´ ìƒì„±
            effective_threshold = request.similarity_threshold
            if effective_threshold >= 0.5:
                effective_threshold = 0.25
            
            constraints = AgentConstraints(
                max_chunks=request.max_chunks,
                max_tokens=request.max_tokens,
                similarity_threshold=effective_threshold,
                container_ids=request.container_ids,
                document_ids=request.document_ids
            )
            
            context = {
                "user_emp_no": user_emp_no,
                "session_id": request.session_id or str(uuid.uuid4())
            }
            
            # ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ (ë©€í‹°í„´ ì§€ì›)
            chat_history_messages = []
            session_attached_files = []  # ğŸ†• ì„¸ì…˜ì— ì €ì¥ëœ ì²¨ë¶€ íŒŒì¼
            if request.session_id:
                try:
                    from app.models.chat.chat_models import TbChatHistory
                    from sqlalchemy import select
                    
                    history_stmt = (
                        select(TbChatHistory)
                        .where(TbChatHistory.session_id == request.session_id)
                        .order_by(TbChatHistory.created_date.asc())
                    )
                    history_result = await db.execute(history_stmt)
                    history_records = history_result.scalars().all()
                    
                    for record in history_records:
                        if record.user_message:
                            chat_history_messages.append({"role": "user", "content": record.user_message})
                        if record.assistant_response:
                            chat_history_messages.append({"role": "assistant", "content": record.assistant_response})
                        
                        # ğŸ†• ì„¸ì…˜ì˜ ì²¨ë¶€ íŒŒì¼ ë¡œë“œ (ê°€ì¥ ìµœê·¼ ê²ƒë§Œ)
                        if record.model_parameters and isinstance(record.model_parameters, dict):
                            attached = record.model_parameters.get('attached_files', [])
                            if attached and isinstance(attached, list):
                                session_attached_files = attached  # ìµœì‹  íŒŒì¼ ëª©ë¡ìœ¼ë¡œ ê°±ì‹ 
                            
                    logger.info(f"ğŸ“š [AgentChatStream] íˆìŠ¤í† ë¦¬ ë¡œë“œ: {len(chat_history_messages)}ê°œ ë©”ì‹œì§€, ì„¸ì…˜ ì²¨ë¶€íŒŒì¼: {len(session_attached_files)}ê°œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ğŸ§  Step 1: ì§ˆì˜ ë¶„ì„
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'started', 'message': 'ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)  # UI ì—…ë°ì´íŠ¸ ì‹œê°„
            
            # ğŸ†• ì´ë¯¸ì§€ ë¶„ì„ (request.images + attachmentsì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ)
            image_description = ""
            images_to_analyze = list(request.images) if request.images else []
            
            # ğŸ†• ë¬¸ì„œ ì²¨ë¶€ ì²˜ë¦¬ (Chat with File)
            attached_document_context = ""
            attached_files = []  # ì²¨ë¶€ íŒŒì¼ ë©”íƒ€ë°ì´í„° (í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš©)
            
            # ğŸ†• ì„¸ì…˜ì— ì €ì¥ëœ ì²¨ë¶€ íŒŒì¼ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
            all_attachments = []
            if session_attached_files:
                # ì„¸ì…˜ì˜ ì²¨ë¶€ íŒŒì¼ì„ ì²¨ë¶€ ëª©ë¡ìœ¼ë¡œ ë³€í™˜
                for sf in session_attached_files:
                    all_attachments.append({
                        'asset_id': sf.get('asset_id') or sf.get('id'),
                        'id': sf.get('asset_id') or sf.get('id'),
                        'category': sf.get('category', 'document'),
                        'file_name': sf.get('file_name', ''),
                        'mime_type': sf.get('mime_type', ''),
                        'file_size': sf.get('file_size', 0)
                    })
                logger.info(f"ğŸ“ [AgentChatStream] ì„¸ì…˜ ì²¨ë¶€ íŒŒì¼ ë³µì›: {len(all_attachments)}ê°œ")
            
            # í˜„ì¬ ìš”ì²­ì˜ ì²¨ë¶€ íŒŒì¼ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
            if request.attachments:
                existing_ids = {att.get('asset_id') or att.get('id') for att in all_attachments}
                for att in request.attachments:
                    att_id = att.get('asset_id') or att.get('id')
                    if att_id and att_id not in existing_ids:
                        all_attachments.append(att)
                        logger.info(f"ğŸ†• [AgentChatStream] ìƒˆ ì²¨ë¶€ íŒŒì¼ ì¶”ê°€: {att.get('file_name', att_id)}")
            
            if all_attachments:
                # ğŸ†• ì²¨ë¶€ íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œí•˜ì—¬ ë¶„ì„ ëŒ€ìƒì— ì¶”ê°€
                image_attachments = [
                    att for att in all_attachments 
                    if att.get('mime_type', '').startswith('image/')
                ]
                
                if image_attachments:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'image_analysis', 'status': 'started', 'message': f'ì²¨ë¶€ëœ ì´ë¯¸ì§€ {len(image_attachments)}ê°œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                    
                    # ì´ë¯¸ì§€ íŒŒì¼ ë¡œë“œ (base64ë¡œ ë³€í™˜)
                    for img_att in image_attachments:
                        asset_id = img_att.get('asset_id') or img_att.get('id')
                        if not asset_id:
                            continue
                        
                        stored_file = chat_attachment_service.get(asset_id)
                        if stored_file:
                            try:
                                import base64
                                img_data = None
                                
                                # S3 ìŠ¤í† ë¦¬ì§€ ì²˜ë¦¬
                                if getattr(stored_file, 'storage_backend', 'local') == 's3':
                                    if chat_attachment_service.s3_client:
                                        response = chat_attachment_service.s3_client.get_object(
                                            Bucket=chat_attachment_service.s3_bucket,
                                            Key=str(stored_file.path)
                                        )
                                        img_data = response['Body'].read()
                                    else:
                                        logger.error(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {asset_id}")
                                        continue
                                # ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ì²˜ë¦¬
                                else:
                                    with open(stored_file.path, 'rb') as f:
                                        img_data = f.read()
                                
                                if img_data:
                                    img_base64 = base64.b64encode(img_data).decode('utf-8')
                                    # MIME íƒ€ì…ì— ë”°ë¼ í—¤ë” ì¶”ê°€
                                    mime = img_att.get('mime_type', 'image/jpeg')
                                    images_to_analyze.append(f"data:{mime};base64,{img_base64}")
                                    logger.info(f"ğŸ“· [AgentChatStream] ì´ë¯¸ì§€ ë¡œë“œ: {img_att.get('file_name')}")
                            except Exception as e:
                                logger.error(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {asset_id}, {e}")
                
                # ğŸ†• ì´ë¯¸ì§€ ë¶„ì„ ë„êµ¬ ì‹¤í–‰
                if images_to_analyze:
                    try:
                        image_tool = paper_search_agent.tools.get('image_analysis')
                        if image_tool:
                            image_description = await image_tool._arun(
                                images=images_to_analyze,
                                query=request.message,
                                detail_level="detailed"
                            )
                            logger.info(f"âœ… [AgentChatStream] ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {len(image_description)}ì")
                            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'image_analysis', 'status': 'completed', 'message': f'ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ ({len(images_to_analyze)}ê°œ)'}, ensure_ascii=False)}\n\n"
                    except Exception as e:
                        logger.error(f"âŒ [AgentChatStream] ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'image_analysis', 'status': 'error', 'message': f'ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}'}, ensure_ascii=False)}\n\n"
                
                # ë¬¸ì„œ íŒŒì¼ í•„í„°ë§ (ì´ë¯¸ì§€/ì˜¤ë””ì˜¤ ì œì™¸)
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'started', 'message': 'ì²¨ë¶€ëœ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                
                doc_attachments = [
                    att for att in all_attachments 
                    if not att.get('mime_type', '').startswith('image/') and not att.get('mime_type', '').startswith('audio/')
                ]
                
                if doc_attachments:
                    text_extractor = TextExtractorService()
                    extracted_texts = []
                    
                    for doc_att in doc_attachments:
                        asset_id = doc_att.get('asset_id') or doc_att.get('id')
                        if not asset_id:
                            continue
                            
                        stored_file = chat_attachment_service.get(asset_id)
                        if not stored_file:
                            logger.warning(f"âš ï¸ ì²¨ë¶€ íŒŒì¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {asset_id}")
                            continue
                            
                        # íŒŒì¼ í¬ê¸° ì œí•œ (3MB - Upstage API ì œí•œ ëŒ€ì‘)
                        MAX_FILE_SIZE = 3 * 1024 * 1024
                        if stored_file.size > MAX_FILE_SIZE:
                            file_size_mb = stored_file.size / (1024 * 1024)
                            extracted_texts.append(f"[íŒŒì¼: {stored_file.file_name}]\n(íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤: {file_size_mb:.1f}MB. ì±„íŒ…ì—ì„œëŠ” 3MB ì´í•˜ì˜ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë¬¸ì„œ ì—…ë¡œë“œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.)")
                            logger.warning(f"âš ï¸ íŒŒì¼ í¬ê¸° ì´ˆê³¼: {stored_file.file_name} ({file_size_mb:.1f}MB)")
                            continue
                            
                        try:
                            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            import tempfile
                            import os
                            
                            extraction_path = str(stored_file.path)
                            is_temp_file = False
                            
                            # S3 ìŠ¤í† ë¦¬ì§€ì¸ ê²½ìš° ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
                            if getattr(stored_file, 'storage_backend', 'local') == 's3':
                                if chat_attachment_service.s3_client:
                                    suffix = Path(stored_file.file_name).suffix
                                    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                                        chat_attachment_service.s3_client.download_fileobj(
                                            chat_attachment_service.s3_bucket,
                                            str(stored_file.path),
                                            tmp
                                        )
                                        extraction_path = tmp.name
                                        is_temp_file = True
                                else:
                                    logger.error(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {asset_id}")
                                    continue
                            
                            file_ext = Path(stored_file.file_name).suffix
                            
                            extraction_result = await text_extractor.extract_text_from_file(
                                file_path=extraction_path,
                                file_extension=file_ext
                            )
                            
                            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                            if is_temp_file and os.path.exists(extraction_path):
                                os.unlink(extraction_path)
                            
                            if extraction_result.get('success') and extraction_result.get('text'):
                                text_content = extraction_result['text']
                                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (30,000ì)
                                MAX_TEXT_LENGTH = 30000
                                if len(text_content) > MAX_TEXT_LENGTH:
                                    text_content = text_content[:MAX_TEXT_LENGTH] + "\n...(ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ìƒëµë¨)"
                                    
                                extracted_texts.append(f"[ì²¨ë¶€ íŒŒì¼ ë‚´ìš©: {stored_file.file_name}]\n{text_content}")
                                attached_files.append({
                                    "file_name": stored_file.file_name,
                                    "file_size": stored_file.size,
                                    "text_length": len(text_content)
                                })
                                logger.info(f"âœ… ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {stored_file.file_name} ({len(text_content)}ì)")
                            else:
                                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {stored_file.file_name}")
                        except Exception as e:
                            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ì—ëŸ¬ ë°œìƒ ì‹œ)
                            if 'is_temp_file' in locals() and is_temp_file and 'extraction_path' in locals() and os.path.exists(extraction_path):
                                try:
                                    os.unlink(extraction_path)
                                except:
                                    pass
                            
                    if extracted_texts:
                        attached_document_context = "\n\n".join(extracted_texts)
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'completed', 'message': f'ì²¨ë¶€ ë¬¸ì„œ {len(extracted_texts)}ê°œ ë‚´ìš©ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"

            # ğŸ†• íŠ¹í—ˆ ì—ì´ì „íŠ¸ëŠ” UIì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì„ íƒë˜ì—ˆì„ ë•Œë§Œ ì‹¤í–‰
            normalized_tool = (request.tool or "").lower()
            if request.tool and request.tool != normalized_tool:
                request.tool = normalized_tool
            skip_rewrite = request.tool == 'patent'

            # ğŸ†• Query Rewrite ì ìš© (íŠ¹í—ˆ ì˜ë„ëŠ” ì›ë¬¸ ìœ ì§€)
            rewritten_query = request.message
            if not skip_rewrite and (chat_history_messages or image_description):
                rewritten_query = await paper_search_agent.rewrite_query(request.message, chat_history_messages, image_description)
                if rewritten_query != request.message:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'started', 'message': f'ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì„ êµ¬ì²´í™”í–ˆìŠµë‹ˆë‹¤: {rewritten_query}'}, ensure_ascii=False)}\n\n"
            elif skip_rewrite:
                logger.info("ğŸ›‘ [AgentChatStream] íŠ¹í—ˆ ë„êµ¬ ì§ˆì˜ëŠ” ë¦¬ë¼ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")

            intent = await paper_search_agent.classify_intent(rewritten_query)
            keywords = await paper_search_agent._extract_keywords(rewritten_query)

            # ğŸ†• PPT ê°•ì œ ëª¨ë“œ (ë„êµ¬ ì„ íƒ ë˜ëŠ” ëª…ì‹œì  ì§ˆì˜)
            if _should_force_ppt_generation(request.message, request.tool):
                if intent != AgentIntent.PPT_GENERATION:
                    logger.info("ğŸ§­ [AgentChatStream] ì‚¬ìš©ì ì§ˆì˜ì—ì„œ PPT ìƒì„± ì˜ë„ë¥¼ ê°•ì œë¡œ ê°ì§€í–ˆìŠµë‹ˆë‹¤")
                intent = AgentIntent.PPT_GENERATION
            
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'completed', 'result': {'intent': intent.value, 'keywords': keywords}, 'message': f'ì˜ë„: {intent.value}, í‚¤ì›Œë“œ: {keywords}'}, ensure_ascii=False)}\n\n"
            
            # ğŸ†• PPT ìƒì„± ì˜ë„ ê°ì§€ ì‹œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ (êµ¬ì¡°í™” ë‹µë³€ + ì¦‰ì‹œ PPT ìƒì„±)
            if intent == AgentIntent.PPT_GENERATION:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': ['hybrid_ppt_generation']}, 'message': 'PPTë¥¼ ë°”ë¡œ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                
                try:
                    # Step 1: êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„± (ë°±ê·¸ë¼ìš´ë“œ ì €ì¥ìš©)
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'ppt_content', 'status': 'started', 'message': 'PPT ì½˜í…ì¸ ë¥¼ êµ¬ì¡°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                    
                    # RAG ê²€ìƒ‰ ì‹¤í–‰
                    strategy = ['keyword_search', 'fulltext_search', 'deduplicate', 'rerank', 'context_builder']
                    retrieval_result = await paper_search_agent.execute_strategy(
                        strategy=strategy,
                        query=rewritten_query,
                        keywords=keywords,
                        constraints=constraints,
                        db_session=db,
                        context=context,
                        attached_document_context=attached_document_context
                    )
                    context_text = retrieval_result.get('context_text', '')
                    
                    # êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
                    structured_answer = await paper_search_agent.generate_answer(
                        query=rewritten_query,
                        context=context_text,
                        intent=intent,
                        history=chat_history_messages
                    )
                    
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'ppt_content', 'status': 'completed', 'message': f'ì½˜í…ì¸  êµ¬ì¡°í™” ì™„ë£Œ ({len(structured_answer)}ì)'}, ensure_ascii=False)}\n\n"
                    
                    # Step 2: ì¦‰ì‹œ PPT ìƒì„± (Unified Agent)
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'ppt_generation', 'status': 'started', 'message': 'PPT íŒŒì¼ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                    
                    from app.agents.presentation.unified_presentation_agent import unified_presentation_agent

                    import uuid

                    run_id = str(uuid.uuid4())
                    
                    ppt_result = await unified_presentation_agent.run(
                        mode="quick",
                        pattern="tool_calling",
                        # Use the (rewritten) user query so PPT title/filename match the request.
                        topic=rewritten_query,
                        context_text=structured_answer,
                        max_slides=8,
                        run_id=run_id,
                        user_id=int(user_emp_no) if str(user_emp_no).isdigit() else None,
                    )

                    # Fallback to deterministic LangGraph-backed flow when tool-calling
                    # doesn't work (unsupported) or doesn't actually create a PPT file.
                    should_fallback_to_react = False
                    if not ppt_result.get("success"):
                        should_fallback_to_react = True
                    if not ppt_result.get("file_name") and not ppt_result.get("filename"):
                        should_fallback_to_react = True
                    if not ppt_result.get("file_path"):
                        should_fallback_to_react = True

                    if should_fallback_to_react:
                        ppt_result = await unified_presentation_agent.run(
                            mode="quick",
                            pattern="react",
                            topic=rewritten_query,
                            context_text=structured_answer,
                            max_slides=8,
                            run_id=run_id,
                            user_id=int(user_emp_no) if str(user_emp_no).isdigit() else None,
                        )
                    
                    success = ppt_result.get("success", False)
                    file_name = ppt_result.get("file_name")
                    file_path = ppt_result.get("file_path")
                    
                    if success and file_name:
                        # ë‹¤ìš´ë¡œë“œ URL ìƒì„±
                        import urllib.parse
                        file_url = f"/api/v1/agent/presentation/download/{urllib.parse.quote(file_name)}"
                        
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'ppt_generation', 'status': 'completed', 'message': f'PPT ìƒì„± ì™„ë£Œ ({file_name})'}, ensure_ascii=False)}\n\n"
                        
                        # ê°„ê²°í•œ ì„±ê³µ ë©”ì‹œì§€
                        final_response = f"âœ… **PPT ìƒì„± ì™„ë£Œ!**\n\nğŸ“ [{file_name}]({file_url})"
                        
                        # ë©”íƒ€ë°ì´í„°ì— êµ¬ì¡°í™” ë‹µë³€ í¬í•¨ (ëª¨ë‹¬ì—ì„œ ì‚¬ìš©)
                        metadata = {
                            "intent": intent.value,
                            "strategy_used": ["hybrid_ppt_generation"],
                            "detailed_chunks": [],
                            "ppt_file_url": file_url,
                            "ppt_file_name": file_name,
                            "structured_content": structured_answer,  # ğŸ†• êµ¬ì¡°í™” ë‹µë³€ ì €ì¥
                            "slide_count": ppt_result.get("slide_count", 0),
                            "iterations": ppt_result.get("iterations", 0),
                            "execution_time": ppt_result.get("execution_time", 0),
                            "run_id": ppt_result.get("run_id") or run_id,
                            "trace_id": ppt_result.get("trace_id") or run_id,
                            "retrieval_metrics": retrieval_result.get("metrics", {})
                        }
                    else:
                        error_msg = ppt_result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                        final_response = f"âŒ PPT ìƒì„± ì‹¤íŒ¨: {error_msg}"
                        metadata = {
                            "intent": intent.value,
                            "strategy_used": ["hybrid_ppt_generation"],
                            "error": error_msg
                        }
                    
                    # ë‹µë³€ ì „ì†¡
                    yield f"event: content\ndata: {json.dumps({'delta': final_response}, ensure_ascii=False)}\n\n"
                    yield f"event: metadata\ndata: {json.dumps(metadata, ensure_ascii=False)}\n\n"
                    yield f"event: done\ndata: {json.dumps({'success': True, 'session_id': context.get('session_id')}, ensure_ascii=False)}\n\n"
                    return
                    
                except Exception as ppt_error:
                    error_text = (str(ppt_error) or "").strip()
                    if not error_text:
                        error_text = f"{type(ppt_error).__name__} (no message)"

                    # NOTE: This project uses loguru; `exc_info=True` is ignored.
                    # Use loguru's exception capture to include traceback.
                    try:
                        logger.opt(exception=ppt_error).error("âŒ [HybridPPT] ìƒì„± ì‹¤íŒ¨: {}", error_text)
                    except Exception:
                        logger.exception("âŒ [HybridPPT] ìƒì„± ì‹¤íŒ¨")

                    yield (
                        "event: reasoning_step\n"
                        f"data: {json.dumps({'stage': 'ppt_generation', 'status': 'error', 'message': f'PPT ìƒì„± ì‹¤íŒ¨: {error_text}'}, ensure_ascii=False)}\n\n"
                    )
                    yield f"event: error\ndata: {json.dumps({'error': error_text}, ensure_ascii=False)}\n\n"
                    return

            # ğŸ” Step 2: ì „ëµ ì„ íƒ
            strategy = paper_search_agent.select_strategy(intent, constraints)
            
            # ğŸ†• ë„êµ¬ ê°•ì œ ì„ íƒ ì ìš©
            if request.tool:
                if request.tool == 'web-search':
                    strategy = ['internet_search', 'context_builder']
                    intent = AgentIntent.WEB_SEARCH
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': strategy}, 'message': 'ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                elif request.tool == 'ppt':
                    intent = AgentIntent.PPT_GENERATION
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': ['presentation_agent']}, 'message': 'ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ PPT ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                elif request.tool == 'patent':
                    # ğŸ†• íŠ¹í—ˆ ë¶„ì„ ì—ì´ì „íŠ¸ ì‹¤í–‰
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': ['patent_analysis']}, 'message': 'íŠ¹í—ˆ ë¶„ì„ ì „ë¬¸ê°€ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                    
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'patent_analysis', 'status': 'started', 'message': 'KIPRIS/Google Patentsì—ì„œ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                    
                    try:
                        from app.agents.patent import patent_analysis_agent_tool
                        
                        # íŠ¹í—ˆ ë¶„ì„ ì‹¤í–‰
                        patent_result = await patent_analysis_agent_tool._arun(
                            query=request.message,
                            analysis_type="search",  # ê¸°ë³¸: ê²€ìƒ‰
                            jurisdiction="KR",
                            max_results=20,
                            include_visualization=True
                        )
                        
                        # íŠ¹í—ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹µë³€ìœ¼ë¡œ í¬ë§·
                        total_patents = patent_result.get("total_patents", 0)
                        summary = patent_result.get("summary", "")
                        
                        completed_msg = f"íŠ¹í—ˆ ê²€ìƒ‰ ì™„ë£Œ: {total_patents}ê±´"
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'patent_analysis', 'status': 'completed', 'message': completed_msg}, ensure_ascii=False)}\n\n"
                        patents = patent_result.get("patents", [])
                        visualizations = patent_result.get("visualizations", [])
                        insights = patent_result.get("insights", [])
                        
                        # ë‹µë³€ ì „ì†¡
                        yield f"event: content\ndata: {json.dumps({'delta': summary}, ensure_ascii=False)}\n\n"
                        
                        # ë©”íƒ€ë°ì´í„° ì „ì†¡ (íŠ¹í—ˆ ëª©ë¡, ì‹œê°í™” í¬í•¨)
                        metadata = {
                            "intent": "patent_analysis",
                            "strategy_used": ["patent_analysis"],
                            "detailed_chunks": [],
                            "patent_results": {
                                "patents": patents[:10],  # ìƒìœ„ 10ê±´
                                "total_patents": patent_result.get("total_patents", 0),
                                "visualizations": visualizations,
                                "insights": insights,
                                "source": patent_result.get("analysis_result", {}).get("source", "kipris")
                            }
                        }
                        yield f"event: metadata\ndata: {json.dumps(metadata, ensure_ascii=False)}\n\n"
                        
                        # íˆìŠ¤í† ë¦¬ ì €ì¥
                        try:
                            from app.models.chat.chat_models import TbChatHistory
                            
                            history_entry = TbChatHistory(
                                session_id=context.get("session_id"),
                                user_emp_no=user_emp_no,
                                user_message=request.message,
                                assistant_response=summary,
                                model_parameters={"tool": "patent", "total_patents": total_patents},
                                created_date=datetime.utcnow()
                            )
                            db.add(history_entry)
                            await db.commit()
                        except Exception as save_error:
                            logger.warning(f"âš ï¸ íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                        
                        yield f"event: done\ndata: {json.dumps({'success': True, 'session_id': context.get('session_id')}, ensure_ascii=False)}\n\n"
                        return
                        
                    except Exception as patent_error:
                        logger.error(f"âŒ íŠ¹í—ˆ ë¶„ì„ ì‹¤íŒ¨: {patent_error}")
                        error_msg = f"íŠ¹í—ˆ ë¶„ì„ ì‹¤íŒ¨: {str(patent_error)}"
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'patent_analysis', 'status': 'error', 'message': error_msg}, ensure_ascii=False)}\n\n"
                        yield f"event: error\ndata: {json.dumps({'error': str(patent_error)}, ensure_ascii=False)}\n\n"
                        return

                elif request.tool == 'deep-research':
                    yield (
                        "event: reasoning_step\n"
                        f"data: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': ['deep_research']}, 'message': 'Deep Researchë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ì›¹ ê²€ìƒ‰ + ë‚´ë¶€ ì§€ì‹).'}, ensure_ascii=False)}\n\n"
                    )

                    yield (
                        "event: reasoning_step\n"
                        f"data: {json.dumps({'stage': 'deep_research', 'status': 'started', 'message': 'ë¦¬ì„œì¹˜ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ìë£Œë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                    )

                    try:
                        from app.agents.deep_research_agent import deep_research_agent
                        from app.agents.base.agent_protocol import AgentExecutionContext

                        deep_ctx = AgentExecutionContext(
                            session_id=context.get('session_id'),
                            user_id=int(user_emp_no) if str(user_emp_no).isdigit() else None,
                            max_tokens=request.max_tokens,
                            max_iterations=10,
                            timeout_seconds=300,
                        )

                        deep_input = {
                            'query': rewritten_query,
                            'db_session': db,
                            'constraints': constraints,
                            'attached_document_context': attached_document_context,
                            'context': context,
                            'chat_history': chat_history_messages,
                            'max_sub_questions': 5,
                            'max_loops': 2,
                        }

                        deep_result = await deep_research_agent.execute(deep_input, deep_ctx)

                        report_md = (deep_result.output or {}).get('report_markdown', '') if deep_result.success else ''
                        sources = (deep_result.output or {}).get('sources', []) if deep_result.success else []

                        if not report_md:
                            raise RuntimeError('Deep Research ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨')

                        # Save report as an attachment
                        safe_ts = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
                        report_filename = f"deep-research-report-{safe_ts}.md"
                        stored = await chat_attachment_service.save_bytes(
                            report_md.encode('utf-8'),
                            owner_emp_no=user_emp_no,
                            file_name=report_filename,
                            mime_type='text/markdown',
                        )
                        download_url = f"/api/v1/agent/chat/assets/{stored.asset_id}"

                        # Attach to metadata (also persisted for session restore)
                        attached_files.append({
                            'asset_id': stored.asset_id,
                            'id': stored.asset_id,
                            'category': stored.category,
                            'file_name': stored.file_name,
                            'mime_type': stored.mime_type,
                            'file_size': stored.size,
                            'download_url': download_url,
                        })

                        yield (
                            "event: reasoning_step\n"
                            f"data: {json.dumps({'stage': 'deep_research', 'status': 'completed', 'message': f'ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ ({stored.file_name})'}, ensure_ascii=False)}\n\n"
                        )

                        # Stream: link + report
                        final_prefix = f"âœ… **Deep Research ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ**\n\nğŸ“ [{stored.file_name}]({download_url})\n\n---\n\n"
                        for chunk in (final_prefix + report_md):
                            yield f"event: content\ndata: {json.dumps({'delta': chunk}, ensure_ascii=False)}\n\n"

                        # Build detailed chunks from sources for UI references
                        detailed_chunks = []
                        has_internet = False
                        for idx_s, s in enumerate(sources[:20]):
                            url = s.get('url')
                            if url:
                                has_internet = True
                            file_name = s.get('title') or 'Source'
                            file_id = s.get('file_id')
                            detailed_chunks.append({
                                'index': idx_s + 1,
                                'file_id': int(file_id) if file_id and str(file_id).isdigit() else 0,
                                'file_name': file_name,
                                'chunk_index': 0,
                                'page_number': s.get('page_number'),
                                'content_preview': file_name,
                                'similarity_score': 0.0,
                                'search_type': 'internet' if url else 'hybrid',
                                'section_title': file_name,
                                'url': url,
                                'full_content': None,
                            })

                        metadata = {
                            'intent': 'deep_research',
                            'strategy_used': ['deep_research'],
                            'detailed_chunks': detailed_chunks,
                            'search_stats': {},
                            'total_chunks_searched': 0,
                            'chunks_used': len(detailed_chunks),
                            'attached_files': attached_files,
                            'answer_source': 'mixed_search' if has_internet else 'database_search',
                            'has_attachments': bool(attached_files),
                            'has_internet_results': has_internet,
                        }
                        yield f"event: metadata\ndata: {json.dumps(metadata, ensure_ascii=False)}\n\n"

                        # Persist to session history
                        try:
                            from app.models.chat.chat_models import TbChatSessions, TbChatHistory
                            from sqlalchemy import select, update
                            from datetime import datetime as dt

                            session_id = context.get('session_id')

                            session_stmt = select(TbChatSessions).where(TbChatSessions.session_id == session_id)
                            session_result = await db.execute(session_stmt)
                            existing_session = session_result.scalar_one_or_none()
                            if not existing_session:
                                new_session = TbChatSessions(
                                    session_id=session_id,
                                    user_emp_no=user_emp_no,
                                    session_name=f"Agent Chat - {request.message[:30]}...",
                                    session_description="AI Agent ì±„íŒ… ì„¸ì…˜",
                                    default_container_id=request.container_ids[0] if request.container_ids else None,
                                    allowed_containers=request.container_ids,
                                    is_active=True,
                                    last_activity=dt.utcnow(),
                                    message_count=1,
                                )
                                db.add(new_session)
                            else:
                                update_stmt = (
                                    update(TbChatSessions)
                                    .where(TbChatSessions.session_id == session_id)
                                    .values(
                                        last_activity=dt.utcnow(),
                                        message_count=TbChatSessions.message_count + 1,
                                    )
                                )
                                await db.execute(update_stmt)

                            history_entry = TbChatHistory(
                                session_id=session_id,
                                user_emp_no=user_emp_no,
                                knowledge_container_id=request.container_ids[0] if request.container_ids else None,
                                accessible_containers=request.container_ids,
                                user_message=request.message,
                                assistant_response=(final_prefix + report_md)[:60000],
                                search_query=rewritten_query,
                                search_results={
                                    'chunks': detailed_chunks[:10],
                                    'total_searched': 0,
                                    'total_used': len(detailed_chunks),
                                },
                                referenced_documents=None,
                                model_used='agent/deep_research',
                                model_parameters={
                                    'tool': 'deep-research',
                                    'attached_files': attached_files,
                                },
                                conversation_context={
                                    'deep_research': True,
                                    'has_internet_results': has_internet,
                                },
                            )
                            db.add(history_entry)
                            await db.commit()
                        except Exception as save_error:
                            logger.warning(f"âš ï¸ Deep Research íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                            try:
                                await db.rollback()
                            except Exception:
                                pass

                        yield f"event: done\ndata: {json.dumps({'success': True, 'session_id': context.get('session_id')}, ensure_ascii=False)}\n\n"
                        return

                    except Exception as deep_error:
                        logger.error(f"âŒ Deep Research ì‹¤íŒ¨: {deep_error}")
                        error_msg = f"Deep Research ì‹¤íŒ¨: {str(deep_error)}"
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'deep_research', 'status': 'error', 'message': error_msg}, ensure_ascii=False)}\n\n"
                        yield f"event: error\ndata: {json.dumps({'error': str(deep_error)}, ensure_ascii=False)}\n\n"
                        return
            
            # ğŸ†• ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰: ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ multimodal_search ì¶”ê°€
            has_images = bool(images_to_analyze)
            has_text_attachments = bool(attached_document_context)
            
            if has_images:
                # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ì„ ì „ëµì— ì¶”ê°€
                if "multimodal_search" not in strategy:
                    # ê²€ìƒ‰ ì „ëµ ì•ë¶€ë¶„ì— ì¶”ê°€ (ë²¡í„° ê²€ìƒ‰ê³¼ ë³‘ë ¬ë¡œ ì‹¤í–‰ë˜ë„ë¡)
                    search_tools = ["vector_search", "keyword_search", "fulltext_search"]
                    first_search_idx = next((i for i, t in enumerate(strategy) if t in search_tools), 0)
                    strategy.insert(first_search_idx, "multimodal_search")
                logger.info(f"ğŸ“· [AgentChatStream] ì´ë¯¸ì§€ ì²¨ë¶€ ê°ì§€ â†’ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì¶”ê°€")
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': strategy, 'multimodal': True}, 'message': 'ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ê²€ìƒ‰í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
            elif has_text_attachments:
                # í…ìŠ¤íŠ¸ ë¬¸ì„œë§Œ ìˆìœ¼ë©´ ê¸°ì¡´ ì „ëµ ìœ ì§€
                logger.info(f"ğŸ“ [AgentChatStream] ì²¨ë¶€ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸: {len(attached_document_context)}ì")
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': strategy}, 'message': 'ì²¨ë¶€ ë¬¸ì„œì™€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í•¨ê»˜ ê²€ìƒ‰í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
            else:
                logger.info(f"ğŸ” [AgentChatStream] ê²€ìƒ‰ ì „ëµ ì„ íƒ: {strategy}")
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': strategy}, 'message': f'ê²€ìƒ‰ ì „ëµ: {strategy}'}, ensure_ascii=False)}\n\n"
            
            # ğŸ“Š Step 3: ë„êµ¬ ì‹¤í–‰ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
            all_chunks = []
            search_stats = {}
            
            for idx, tool_name in enumerate(strategy):
                if tool_name in ["vector_search", "keyword_search", "fulltext_search", "multimodal_search", "internet_search"]:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'search', 'status': 'started', 'tool': tool_name, 'message': f'{tool_name} ì‹¤í–‰ ì¤‘...'}, ensure_ascii=False)}\n\n"
                    
                    try:
                        # ğŸ†• multimodal_searchëŠ” ì´ë¯¸ì§€ ë°ì´í„° ì „ë‹¬
                        if tool_name == "multimodal_search":
                            if not images_to_analyze:
                                logger.warning("âš ï¸ multimodal_search í˜¸ì¶œë¨, í•˜ì§€ë§Œ ì´ë¯¸ì§€ ì—†ìŒ")
                                continue
                            
                            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë§Œ ì‚¬ìš© (ì¶”í›„ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì§€ì› ê°€ëŠ¥)
                            tool_result = await paper_search_agent._execute_tool(
                                tool_name=tool_name,
                                query=rewritten_query,
                                db_session=db,
                                keywords=keywords,
                                constraints=constraints,
                                chunks=all_chunks,
                                context={
                                    **context,
                                    "image_data": images_to_analyze[0],  # Base64 ì´ë¯¸ì§€ (data:image/...;base64,...)
                                    "top_k": 10
                                }
                            )
                        else:
                            tool_result = await paper_search_agent._execute_tool(
                                tool_name=tool_name,
                                query=rewritten_query,
                                db_session=db,
                                keywords=keywords,
                                constraints=constraints,
                                chunks=all_chunks,
                                context=context
                            )
                        
                        if not getattr(tool_result, 'success', False):
                            logger.warning(f"âš ï¸ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {tool_name}, errors={getattr(tool_result, 'errors', [])}")
                            try:
                                await db.rollback()
                            except Exception as rollback_error:
                                logger.error(f"ë¡¤ë°± ì‹¤íŒ¨ ({tool_name}): {rollback_error}")
                            continue

                        if hasattr(tool_result, 'data'):
                            new_chunks = tool_result.data
                            all_chunks.extend(new_chunks)
                            search_stats[tool_name] = {
                                'count': len(new_chunks),
                                'avg_score': sum(c.score for c in new_chunks) / len(new_chunks) if new_chunks else 0
                            }
                            
                            yield f"event: search_progress\ndata: {json.dumps({'tool': tool_name, 'chunks_found': len(new_chunks), 'total_chunks': len(all_chunks), 'avg_similarity': round(search_stats[tool_name]['avg_score'], 3)}, ensure_ascii=False)}\n\n"
                    
                    except Exception as e:
                        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({tool_name}): {e}")
                        try:
                            await db.rollback()
                        except Exception as rollback_error:
                            logger.error(f"ë¡¤ë°± ì‹¤íŒ¨ ({tool_name}): {rollback_error}")
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'search', 'status': 'error', 'tool': tool_name, 'message': f'ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}'}, ensure_ascii=False)}\n\n"
                
                elif tool_name in ["deduplicate", "rerank"]:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'postprocess', 'status': 'started', 'tool': tool_name, 'message': f'{tool_name} ì²˜ë¦¬ ì¤‘...'}, ensure_ascii=False)}\n\n"
                    
                    try:
                        tool_result = await paper_search_agent._execute_tool(
                            tool_name=tool_name,
                            query=rewritten_query,  # ğŸ†• ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ì‚¬ìš©
                            db_session=db,
                            keywords=keywords,
                            constraints=constraints,
                            chunks=all_chunks,
                            context=context
                        )
                        
                        if not getattr(tool_result, 'success', False):
                            logger.warning(f"âš ï¸ í›„ì²˜ë¦¬ ë„êµ¬ ì‹¤íŒ¨: {tool_name}, errors={getattr(tool_result, 'errors', [])}")
                            try:
                                await db.rollback()
                            except Exception as rollback_error:
                                logger.error(f"ë¡¤ë°± ì‹¤íŒ¨ ({tool_name}): {rollback_error}")
                            continue
                        if hasattr(tool_result, 'data'):
                            before_count = len(all_chunks)
                            all_chunks = tool_result.data
                            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'postprocess', 'status': 'completed', 'tool': tool_name, 'before': before_count, 'after': len(all_chunks), 'message': f'{tool_name}: {before_count}ê°œ â†’ {len(all_chunks)}ê°œ'}, ensure_ascii=False)}\n\n"
                    
                    except Exception as e:
                        logger.error(f"í›„ì²˜ë¦¬ ì‹¤íŒ¨ ({tool_name}): {e}")
                        try:
                            await db.rollback()
                        except Exception as rollback_error:
                            logger.error(f"ë¡¤ë°± ì‹¤íŒ¨ ({tool_name}): {rollback_error}")
            
            # ğŸ—ï¸ Step 4: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'context_building', 'status': 'started', 'message': 'ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            
            context_result = await paper_search_agent._execute_tool(
                tool_name="context_builder",
                query=rewritten_query,  # ğŸ†• ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ì‚¬ìš©
                db_session=db,
                keywords=keywords,
                constraints=constraints,
                chunks=all_chunks,
                context=None
            )
            
            context_text = context_result.data if isinstance(context_result.data, str) else ""
            used_chunks = getattr(context_result, 'used_chunks', all_chunks[:5])
            
            # ğŸ†• ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ë¬¸ì„œ + ì´ë¯¸ì§€)
            if attached_document_context or image_description:
                parts = []
                
                # ì´ë¯¸ì§€ ì„¤ëª… ì¶”ê°€
                if image_description:
                    parts.append(f"[ì²¨ë¶€ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼]\n{image_description}")
                
                # ë¬¸ì„œ ë‚´ìš© ì¶”ê°€
                if attached_document_context:
                    parts.append(f"[ì²¨ë¶€ ë¬¸ì„œ ë‚´ìš©]\n{attached_document_context}")
                
                # ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if context_text and context_text.strip():
                    parts.append(f"[ì°¸ê³ : ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼]\n{context_text}")
                
                context_text = "\n\n".join(parts)
            
            token_count = len(context_text.split())  # ê°„ë‹¨í•œ í† í° ì¶”ì •
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ ë©”ì‹œì§€
            if attached_document_context or image_description:
                source_types = []
                if image_description:
                    source_types.append("ì´ë¯¸ì§€")
                if attached_document_context:
                    source_types.append("ë¬¸ì„œ")
                source_msg = " + ".join(source_types)
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'context_building', 'status': 'completed', 'tokens': token_count, 'max_tokens': constraints.max_tokens, 'chunks_used': 0, 'message': f'ì²¨ë¶€ {source_msg} ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {token_count} í† í°'}, ensure_ascii=False)}\n\n"
            else:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'context_building', 'status': 'completed', 'tokens': token_count, 'max_tokens': constraints.max_tokens, 'chunks_used': len(used_chunks), 'message': f'ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {token_count} í† í°, {len(used_chunks)}ê°œ ì²­í¬ ì‚¬ìš©'}, ensure_ascii=False)}\n\n"
            
            # âœï¸ Step 5: ë‹µë³€ ìƒì„±
            if image_description and not attached_document_context:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'answer_generation', 'status': 'started', 'message': 'ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            elif attached_document_context:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'answer_generation', 'status': 'started', 'message': 'ì²¨ë¶€ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            else:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'answer_generation', 'status': 'started', 'message': 'ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            
            # AI ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°) - DEFAULT_LLM_PROVIDER ì„¤ì • ë”°ë¦„
            answer = await paper_search_agent.generate_answer(
                query=request.message, 
                context=context_text, 
                intent=intent,
                history=chat_history_messages
            )
            
            # ë‹µë³€ì„ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì „ì†¡
            if isinstance(answer, str):
                chunk_size = 50
                for i in range(0, len(answer), chunk_size):
                    chunk = answer[i:i+chunk_size]
                    yield f"event: content\ndata: {json.dumps({'delta': chunk}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.05)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
            
            # ğŸ“‹ Step 6: ë©”íƒ€ë°ì´í„° ì „ì†¡
            detailed_chunks = []
            for idx, chunk in enumerate(used_chunks):
                file_id = chunk.file_id
                file_name = chunk.metadata.get("file_name") if chunk.metadata else "ë¬¸ì„œ"
                
                # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ì¸ì§€ í™•ì¸
                is_internet_search = (
                    chunk.match_type == "internet" or 
                    chunk.container_id in ["internet", "tavily", "bing", "duckduckgo"] or
                    (chunk.metadata and chunk.metadata.get("source") in ["internet", "tavily", "bing", "duckduckgo"])
                )
                
                # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ìš© í•„ë“œ
                url = chunk.metadata.get("url") if chunk.metadata else None
                search_type = "internet" if is_internet_search else "hybrid"
                
                # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ì¸ ê²½ìš° íŒŒì¼ëª…ì„ íƒ€ì´í‹€ë¡œ ì„¤ì •
                if is_internet_search and chunk.metadata:
                    file_name = chunk.metadata.get("title") or chunk.metadata.get("file_name") or "ì›¹ ê²€ìƒ‰ ê²°ê³¼"
                
                detailed_chunks.append({
                    "index": idx + 1,
                    "file_id": int(file_id) if file_id and str(file_id).isdigit() else 0,
                    "file_name": file_name,
                    "chunk_index": chunk.metadata.get("chunk_index", 0) if chunk.metadata else 0,
                    "page_number": chunk.metadata.get("page_number") if chunk.metadata else None,
                    "content_preview": chunk.content[:200] if chunk.content else "",
                    "similarity_score": chunk.score,
                    "search_type": search_type,
                    "section_title": file_name,
                    "url": url,  # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ URL
                    "full_content": chunk.content if is_internet_search else None  # ğŸ†• ì „ì²´ ì½˜í…ì¸  (ì¸í„°ë„· ê²€ìƒ‰)
                })
            
            # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ë§Œ ì‚¬ìš©í–ˆëŠ”ì§€ í™•ì¸
            has_internet_only = (
                len(detailed_chunks) > 0 and 
                all(c.get("search_type") == "internet" for c in detailed_chunks)
            )
            has_mixed_search = (
                len(detailed_chunks) > 0 and 
                any(c.get("search_type") == "internet" for c in detailed_chunks) and
                any(c.get("search_type") != "internet" for c in detailed_chunks)
            )
            
            # ğŸ†• answer_source ê²°ì • (ì¸í„°ë„· ê²€ìƒ‰ êµ¬ë¶„)
            if attached_files and not used_chunks:
                answer_source = "attached_documents"
            elif has_internet_only:
                answer_source = "internet_search"
            elif has_mixed_search:
                answer_source = "mixed_search"
            elif used_chunks:
                answer_source = "database_search"
            else:
                answer_source = "general"
            
            metadata = {
                "intent": intent.value,
                "strategy_used": strategy,
                "detailed_chunks": detailed_chunks,
                "search_stats": search_stats,
                "total_chunks_searched": len(all_chunks),
                "chunks_used": len(used_chunks),
                "attached_files": attached_files,  # ğŸ†• ì²¨ë¶€ íŒŒì¼ ë©”íƒ€ë°ì´í„°
                "answer_source": answer_source,  # ğŸ†• ë‹µë³€ ì¶œì²˜ (internet_search, mixed_search, database_search, attached_documents, general)
                "has_attachments": bool(attached_files),  # ğŸ†• ì²¨ë¶€ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€
                "has_internet_results": has_internet_only or has_mixed_search  # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ í¬í•¨ ì—¬ë¶€
            }
            
            yield f"event: metadata\ndata: {json.dumps(metadata, ensure_ascii=False)}\n\n"
            
            # ğŸ’¾ ì„¸ì…˜ ë° ëŒ€í™” ë‚´ì—­ ì €ì¥
            try:
                from app.models.chat.chat_models import TbChatSessions, TbChatHistory
                from sqlalchemy import select, update
                from datetime import datetime as dt
                
                session_id = context.get("session_id")
                user_emp_no = context.get("user_emp_no")
                
                # ì„¸ì…˜ ì¡´ì¬ í™•ì¸
                session_stmt = select(TbChatSessions).where(TbChatSessions.session_id == session_id)
                session_result = await db.execute(session_stmt)
                existing_session = session_result.scalar_one_or_none()
                
                if not existing_session:
                    # ìƒˆ ì„¸ì…˜ ìƒì„±
                    new_session = TbChatSessions(
                        session_id=session_id,
                        user_emp_no=user_emp_no,
                        session_name=f"Agent Chat - {request.message[:30]}...",
                        session_description="AI Agent ì±„íŒ… ì„¸ì…˜",
                        default_container_id=request.container_ids[0] if request.container_ids else None,
                        allowed_containers=request.container_ids,
                        is_active=True,
                        last_activity=dt.utcnow(),
                        message_count=1
                    )
                    db.add(new_session)
                    logger.info(f"âœ… [AgentSession] ìƒˆ ì„¸ì…˜ ìƒì„±: {session_id}")
                else:
                    # ê¸°ì¡´ ì„¸ì…˜ ì—…ë°ì´íŠ¸
                    update_stmt = (
                        update(TbChatSessions)
                        .where(TbChatSessions.session_id == session_id)
                        .values(
                            last_activity=dt.utcnow(),
                            message_count=TbChatSessions.message_count + 1
                        )
                    )
                    await db.execute(update_stmt)
                    logger.info(f"âœ… [AgentSession] ì„¸ì…˜ ì—…ë°ì´íŠ¸: {session_id}")
                
                # ëŒ€í™” ë‚´ì—­ ì €ì¥
                referenced_doc_ids = list(set([
                    int(chunk["file_id"]) 
                    for chunk in detailed_chunks 
                    if chunk.get("file_id") and chunk["file_id"] > 0
                ]))
                
                chat_history = TbChatHistory(
                    session_id=session_id,
                    user_emp_no=user_emp_no,
                    knowledge_container_id=request.container_ids[0] if request.container_ids else None,
                    accessible_containers=request.container_ids,
                    user_message=request.message,
                    assistant_response=answer,
                    search_query=request.message,
                    search_results={
                        "chunks": detailed_chunks[:10],  # ìµœëŒ€ 10ê°œë§Œ ì €ì¥
                        "total_searched": len(all_chunks),
                        "total_used": len(used_chunks)
                    },
                    referenced_documents=referenced_doc_ids if referenced_doc_ids else None,
                    model_used="agent/paper_search_agent",
                    model_parameters={
                        "intent": intent.value,
                        "strategy": strategy,
                        "max_chunks": request.max_chunks,
                        "similarity_threshold": effective_threshold,
                        "attached_files": attached_files  # ğŸ†• ì²¨ë¶€ íŒŒì¼ ì •ë³´ ì €ì¥
                    },
                    conversation_context={
                        "search_stats": search_stats,
                        "reasoning_steps": len(strategy),
                        "has_attachments": bool(attached_document_context),  # ğŸ†• ì²¨ë¶€ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€
                        "attachment_context_length": len(attached_document_context) if attached_document_context else 0
                    }
                )
                db.add(chat_history)
                await db.commit()
                
                logger.info(f"ğŸ’¾ [AgentSession] ëŒ€í™” ì €ì¥ ì™„ë£Œ: session={session_id}, docs={len(referenced_doc_ids)}")
                
            except Exception as save_error:
                logger.error(f"âŒ [AgentSession] ì €ì¥ ì‹¤íŒ¨: {save_error}")
                await db.rollback()
                # ì €ì¥ ì‹¤íŒ¨í•´ë„ ìŠ¤íŠ¸ë¦¬ë°ì€ ê³„ì† ì§„í–‰
            
            # âœ… ì™„ë£Œ
            yield f"event: done\ndata: {json.dumps({'success': True, 'session_id': context.get('session_id')}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ [AgentChatStream] ì˜¤ë¥˜: {error_msg}", exc_info=True)
            yield f"event: error\ndata: {json.dumps({'error': error_msg}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.get("/agent/sessions/{session_id}")
async def get_agent_session(
    session_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Agent ì„¸ì…˜ ë³µì›
    
    ì„¸ì…˜ ë©”íƒ€ë°ì´í„° + ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    """
    try:
        from app.models.chat.chat_models import TbChatSessions, TbChatHistory
        from sqlalchemy import select
        
        user_emp_no = str(current_user.emp_no)
        
        # ì„¸ì…˜ ì¡°íšŒ
        session_stmt = (
            select(TbChatSessions)
            .where(
                TbChatSessions.session_id == session_id,
                TbChatSessions.user_emp_no == user_emp_no  # ê¶Œí•œ í™•ì¸
            )
        )
        session_result = await db.execute(session_stmt)
        session = session_result.scalar_one_or_none()
        
        if not session:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}"
            )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
        history_stmt = (
            select(TbChatHistory)
            .where(TbChatHistory.session_id == session_id)
            .order_by(TbChatHistory.created_date.asc())
        )
        history_result = await db.execute(history_stmt)
        history_records = history_result.scalars().all()
        
        # ì‘ë‹µ ë³€í™˜
        messages = []
        for record in history_records:
            messages.append({
                "chat_id": record.chat_id,
                "user_message": record.user_message,
                "assistant_response": record.assistant_response,
                "referenced_documents": record.referenced_documents or [],
                "search_results": record.search_results,
                "model_used": record.model_used,
                "model_parameters": record.model_parameters,
                "created_date": record.created_date.isoformat()
            })
        
        return {
            "session_id": session.session_id,
            "session_name": session.session_name,
            "session_description": session.session_description,
            "user_emp_no": session.user_emp_no,
            "default_container_id": session.default_container_id,
            "allowed_containers": session.allowed_containers,
            "is_active": session.is_active,
            "last_activity": session.last_activity.isoformat(),
            "message_count": session.message_count,
            "created_date": session.created_date.isoformat(),
            "messages": messages
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [AgentSession] ë³µì› ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„¸ì…˜ ë³µì› ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/agent/sessions")
async def list_agent_sessions(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
    limit: int = 20,
    offset: int = 0
):
    """
    ì‚¬ìš©ìì˜ Agent ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ
    """
    try:
        from app.models.chat.chat_models import TbChatSessions
        from sqlalchemy import select
        
        user_emp_no = str(current_user.emp_no)
        
        stmt = (
            select(TbChatSessions)
            .where(TbChatSessions.user_emp_no == user_emp_no)
            .order_by(TbChatSessions.last_activity.desc())
            .limit(limit)
            .offset(offset)
        )
        
        result = await db.execute(stmt)
        sessions = result.scalars().all()
        
        return {
            "sessions": [
                {
                    "session_id": s.session_id,
                    "session_name": s.session_name,
                    "last_activity": s.last_activity.isoformat(),
                    "message_count": s.message_count,
                    "is_active": s.is_active
                }
                for s in sessions
            ],
            "total": len(sessions),
            "limit": limit,
            "offset": offset
        }
        
    except Exception as e:
        logger.error(f"âŒ [AgentSession] ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/agent/health")
async def agent_health():
    """Agent ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "agent": paper_search_agent.name,
        "version": paper_search_agent.version,
        "tools": list(paper_search_agent.tools.keys()),
        "timestamp": datetime.utcnow().isoformat()
    }


# =============================================================================
# ğŸ“ ì²¨ë¶€íŒŒì¼ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ (chat.pyì—ì„œ í†µí•©)
# =============================================================================

@router.post("/agent/chat/assets")
async def upload_agent_chat_assets(
    files: List[UploadFile] = File(...),
    current_user: User = Depends(get_current_user)
):
    """ì±„íŒ… ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ (ì´ë¯¸ì§€, ë¬¸ì„œ, ì˜¤ë””ì˜¤)
    
    Args:
        files: ì—…ë¡œë“œí•  íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        current_user: í˜„ì¬ ì‚¬ìš©ì (ì¸ì¦)
    
    Returns:
        {"success": true, "assets": [...]}
    """
    if not files:
        raise HTTPException(status_code=400, detail="ì—…ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    assets = []
    for upload in files:
        try:
            stored = await chat_attachment_service.save(upload, str(current_user.emp_no))
            assets.append({
                "asset_id": stored.asset_id,
                "file_name": stored.file_name,
                "mime_type": stored.mime_type,
                "size": stored.size,
                "category": stored.category,
                "preview_url": stored.preview_url,
                "download_url": stored.download_url
            })
        except Exception as exc:
            logger.error(f"âŒ ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {exc}")
            raise HTTPException(status_code=500, detail="ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    return {"success": True, "assets": assets}


@router.get("/agent/chat/assets/{asset_id}")
async def download_agent_chat_asset(
    asset_id: str,
    current_user: User = Depends(get_current_user)
):
    """ì±„íŒ… ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    
    Args:
        asset_id: íŒŒì¼ ì‹ë³„ì
        current_user: í˜„ì¬ ì‚¬ìš©ì (ì¸ì¦)
    
    Returns:
        íŒŒì¼ ìŠ¤íŠ¸ë¦¼ ë˜ëŠ” FileResponse
    """
    stored = chat_attachment_service.get(asset_id)
    if not stored:
        raise HTTPException(status_code=404, detail="ì²¨ë¶€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    stored_owner = str(stored.owner_emp_no) if stored.owner_emp_no else None
    current_emp_no = str(current_user.emp_no)
    logger.info(f"ğŸ” [AgentChatAsset] ì ‘ê·¼ ì‹œë„: asset={asset_id}, stored_owner={stored_owner}, current={current_emp_no}")

    if stored_owner != current_emp_no:
        logger.warning(f"âŒ [AgentChatAsset] ê¶Œí•œ ì—†ìŒ: {current_emp_no} != {stored_owner}")
        raise HTTPException(status_code=403, detail="ì²¨ë¶€ íŒŒì¼ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")

    # S3 ìŠ¤í† ë¦¬ì§€ ì²˜ë¦¬
    if stored.storage_backend == "s3":
        if not chat_attachment_service.s3_client:
            logger.error("S3 client is not initialized but storage_backend is s3")
            raise HTTPException(status_code=500, detail="ìŠ¤í† ë¦¬ì§€ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
        try:
            # S3ì—ì„œ íŒŒì¼ ìŠ¤íŠ¸ë¦¼ ê°€ì ¸ì˜¤ê¸°
            s3_response = chat_attachment_service.s3_client.get_object(
                Bucket=chat_attachment_service.s3_bucket,
                Key=str(stored.path)
            )
            
            # íŒŒì¼ëª… ì¸ì½”ë”© ì²˜ë¦¬ (RFC 5987)
            encoded_filename = quote(stored.file_name)
            
            return StreamingResponse(
                s3_response['Body'],
                media_type=stored.mime_type,
                headers={
                    "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
                }
            )
        except ClientError as e:
            logger.error(f"S3 Download Error: {e}")
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"S3 Streaming Error: {e}")
            raise HTTPException(status_code=500, detail="íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    # ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ì²˜ë¦¬
    return FileResponse(
        path=stored.path,
        media_type=stored.mime_type,
        filename=stored.file_name
    )


# =============================================================================
# ğŸ¤ ìŒì„± ë³€í™˜ ì—”ë“œí¬ì¸íŠ¸ (chat.pyì—ì„œ í†µí•©)
# =============================================================================

@router.post("/agent/chat/transcribe")
async def transcribe_agent_chat_audio(
    file: UploadFile = File(...),
    language: str = Form("ko-KR"),
    current_user: User = Depends(get_current_user)
):
    """ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (AWS Transcribe)
    
    Args:
        file: ì˜¤ë””ì˜¤ íŒŒì¼ (webm, mp3, wav, m4a ë“±)
        language: ì–¸ì–´ ì½”ë“œ (ko-KR, en-US, ja-JP, zh-CN ë“±)
        current_user: í˜„ì¬ ì‚¬ìš©ì (ì¸ì¦)
    
    Returns:
        {"success": true, "transcript": "ë³€í™˜ëœ í…ìŠ¤íŠ¸"}
    """
    if not audio_transcription_service.enabled:
        raise HTTPException(status_code=503, detail="ì˜¤ë””ì˜¤ ì „ì‚¬ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    suffix = Path(file.filename or "audio.webm").suffix or ".webm"
    temp_fd, temp_path_str = tempfile.mkstemp(suffix=suffix)
    os.close(temp_fd)
    temp_path = Path(temp_path_str)

    try:
        # íŒŒì¼ ì €ì¥
        async with aiofiles.open(temp_path, "wb") as out_file:
            while True:
                chunk = await file.read(1024 * 1024)
                if not chunk:
                    break
                await out_file.write(chunk)

        logger.info(
            "ğŸ¤ [AGENT-TRANSCRIBE] ë³€í™˜ ìš”ì²­ - user: %s, file: %s, size: %d bytes, language: %s",
            current_user.username,
            file.filename,
            temp_path.stat().st_size,
            language
        )

        # AWS Transcribe ë³€í™˜ (ë™ê¸° â†’ ë¹„ë™ê¸° ë˜í•‘)
        transcript = await asyncio.to_thread(
            audio_transcription_service.transcribe, 
            temp_path,
            language
        )
        
        logger.info(
            "âœ… [AGENT-TRANSCRIBE] ë³€í™˜ ì™„ë£Œ - user: %s, text_length: %d",
            current_user.username,
            len(transcript)
        )
        
        return {"success": True, "transcript": transcript}
        
    except Exception as exc:
        logger.error(f"âŒ [AGENT-TRANSCRIBE] ë³€í™˜ ì‹¤íŒ¨: {exc}", exc_info=True)
        raise HTTPException(status_code=500, detail="ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    finally:
        try:
            await file.close()
        except Exception:
            pass
        temp_path.unlink(missing_ok=True)


"""
Agent-based Chat API - PaperSearchAgentë¥¼ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
Feature flagë¡œ ì ì§„ì  ì „í™˜ ê°€ëŠ¥
"""
from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime
import uuid

from app.core.database import get_db
from app.core.dependencies import get_current_user
from app.models import User
from app.agents import paper_search_agent
from app.agents.supervisor_agent import supervisor_agent
from langchain_core.messages import HumanMessage
from app.tools.contracts import AgentConstraints, AgentIntent, AgentResult
from loguru import logger
from app.services.document.extraction.text_extractor_service import TextExtractorService
from app.services.chat.chat_attachment_service import chat_attachment_service
from pathlib import Path


router = APIRouter(tags=["agent"])


# Request/Response ëª¨ë¸
class AgentChatRequest(BaseModel):
    """Agent ê¸°ë°˜ ì±„íŒ… ìš”ì²­"""
    message: str = Field(..., min_length=1, description="ì‚¬ìš©ì ì§ˆì˜")
    images: Optional[List[str]] = Field(None, description="ì´ë¯¸ì§€ ëª©ë¡ (Base64)")
    session_id: Optional[str] = Field(None, description="ì„¸ì…˜ ID")
    
    # ì œì•½ ì¡°ê±´
    max_chunks: int = Field(10, ge=1, le=50, description="ìµœëŒ€ ì²­í¬ ìˆ˜")
    max_tokens: int = Field(4000, ge=100, le=8000, description="ìµœëŒ€ í† í° ìˆ˜")  # 2000 â†’ 4000 (ì¼ë°˜ RAGì™€ ë™ì¼)
    similarity_threshold: float = Field(0.25, ge=0.0, le=1.0, description="ìœ ì‚¬ë„ ì„ê³„ê°’")  # 0.5 â†’ 0.25ë¡œ ë‚®ì¶¤ (ì¼ë°˜ RAGì™€ ë™ì¼)
    
    # í•„í„°ë§
    container_ids: Optional[List[str]] = Field(None, description="ì»¨í…Œì´ë„ˆ ID í•„í„°")
    document_ids: Optional[List[str]] = Field(None, description="ë¬¸ì„œ ID í•„í„°")
    
    # ğŸ†• ì²¨ë¶€ íŒŒì¼ (Chat with File)
    attachments: Optional[List[Dict[str, Any]]] = Field(None, description="ì²¨ë¶€ íŒŒì¼ ëª©ë¡ (asset_id, mime_type ë“±)")
    
    # ğŸ†• ë„êµ¬ ê°•ì œ ì„ íƒ
    tool: Optional[str] = Field(None, description="ê°•ì œ ì„ íƒí•  ë„êµ¬ (ppt, web-search ë“±)")


class AgentStepResponse(BaseModel):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰ ë‹¨ê³„"""
    step_number: int
    tool_name: str
    reasoning: str
    latency_ms: float
    items_returned: Optional[int] = None
    success: bool


class ReferenceDocument(BaseModel):
    """ì°¸ì¡° ë¬¸ì„œ"""
    chunk_id: str
    content: str
    score: float
    document_id: Optional[str] = None
    title: Optional[str] = None
    page_number: Optional[int] = None


class DetailedChunk(BaseModel):
    """ìƒì„¸ ì²­í¬ ì •ë³´ (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼ í˜•ì‹)"""
    index: int
    file_id: int
    file_name: str
    chunk_index: int
    page_number: Optional[int] = None
    content_preview: str
    similarity_score: float
    search_type: str
    section_title: str = ""


class AgentChatResponse(BaseModel):
    """Agent ê¸°ë°˜ ì±„íŒ… ì‘ë‹µ"""
    answer: str
    intent: str
    strategy_used: List[str]
    references: List[ReferenceDocument]
    detailed_chunks: List[DetailedChunk] = []  # ğŸ†• ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼ í˜•ì‹
    steps: List[AgentStepResponse]
    metrics: Dict[str, Any]
    success: bool
    errors: List[str] = []


@router.post("/agent/chat", response_model=AgentChatResponse)
async def agent_chat(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Agent ê¸°ë°˜ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (Supervisor Architecture)
    
    Supervisor Agentë¥¼ ì‚¬ìš©í•˜ì—¬:
    1. ì‚¬ìš©ì ì˜ë„ íŒŒì•… (ê²€ìƒ‰ vs PPT ìƒì„± vs ê¸°íƒ€)
    2. ì ì ˆí•œ Worker Agent (SearchAgent, PresentationAgent) í˜¸ì¶œ
    3. ê²°ê³¼ í†µí•© ë° ë°˜í™˜
    """
    try:
        user_emp_no = str(current_user.emp_no)
        logger.info(f"ğŸ¤– [AgentChat] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")
        
        # Supervisor ì‹¤í–‰
        initial_state = {
            "messages": [HumanMessage(content=request.message)],
            "next": "",
            "shared_context": {}
        }
        
        # LangGraph ì‹¤í–‰
        final_state = await supervisor_agent.ainvoke(initial_state)
        
        # ê²°ê³¼ ì¶”ì¶œ
        messages = final_state["messages"]
        last_message = messages[-1]
        answer = last_message.content
        shared_context = final_state.get("shared_context", {})
        
        # SearchAgent ê²°ê³¼ ë³µì›
        search_result = shared_context.get("search_agent_result")
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        references_response = []
        detailed_chunks_response = []
        steps_response = []
        metrics = {}
        intent = "general"
        strategy_used = []
        
        if search_result:
            # SearchAgentê°€ ì‹¤í–‰ëœ ê²½ìš° ì •ë³´ ë³µì›
            intent = search_result.intent.value
            strategy_used = search_result.strategy_used
            metrics = search_result.metrics
            
            # Steps
            for step in search_result.steps:
                steps_response.append(AgentStepResponse(
                    step_number=step.step_number,
                    tool_name=step.tool_name,
                    reasoning=step.reasoning,
                    latency_ms=step.tool_output.metrics.latency_ms,
                    items_returned=step.tool_output.metrics.items_returned,
                    success=step.tool_output.success
                ))
            
            # References & Chunks
            for idx, ref in enumerate(search_result.references):
                file_id = ref.file_id
                file_name = None
                chunk_index = 0
                page_number = None
                
                if ref.metadata:
                    file_name = ref.metadata.get("file_name") or ref.metadata.get("title")
                    chunk_index = ref.metadata.get("chunk_index", 0)
                    page_number = ref.metadata.get("page_number")
                
                references_response.append(ReferenceDocument(
                    chunk_id=ref.chunk_id,
                    content=ref.content,
                    score=ref.score,
                    document_id=ref.metadata.get("document_id") if ref.metadata else None,
                    title=file_name,
                    page_number=page_number
                ))
                
                detailed_chunks_response.append(DetailedChunk(
                    index=idx + 1,
                    file_id=int(file_id) if file_id and str(file_id).isdigit() else 0,
                    file_name=file_name or "ë¬¸ì„œ",
                    chunk_index=chunk_index,
                    page_number=page_number,
                    content_preview=ref.content[:200] if ref.content else "",
                    similarity_score=ref.score,
                    search_type="agent",
                    section_title=file_name or ""
                ))
        
        # PresentationAgentê°€ ì‹¤í–‰ëœ ê²½ìš° (ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ PresentationAgentì¸ ê²½ìš°)
        if getattr(last_message, "name", "") == "PresentationAgent":
            intent = "presentation_generation"
            # PPT ìƒì„± ê´€ë ¨ ë©”íŠ¸ë¦­ì´ë‚˜ ìŠ¤í… ì¶”ê°€ ê°€ëŠ¥
            steps_response.append(AgentStepResponse(
                step_number=len(steps_response) + 1,
                tool_name="PresentationAgent",
                reasoning="Generated presentation based on search results.",
                latency_ms=0,
                success=True
            ))

        logger.info(f"âœ… [AgentChat] ì™„ë£Œ: {len(references_response)}ê°œ ì°¸ì¡°")
        
        return AgentChatResponse(
            answer=answer,
            intent=intent,
            strategy_used=strategy_used,
            references=references_response,
            detailed_chunks=detailed_chunks_response,
            steps=steps_response,
            metrics=metrics,
            success=True,
            errors=[]
        )
        
    except Exception as e:
        logger.error(f"âŒ [AgentChat] ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/agent/compare", response_model=Dict[str, Any])
async def compare_architectures(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    A/B ë¹„êµ ì—”ë“œí¬ì¸íŠ¸
    
    ë™ì¼í•œ ì§ˆì˜ë¥¼ ë‘ ê°€ì§€ ì•„í‚¤í…ì²˜ë¡œ ì‹¤í–‰í•˜ì—¬ ë¹„êµ:
    1. ê¸°ì¡´ rag_search_service (monolithic)
    2. ìƒˆë¡œìš´ paper_search_agent (agent-based)
    
    í‰ê°€ ë° ì„±ëŠ¥ ë¶„ì„ì— ì‚¬ìš©
    """
    try:
        user_emp_no = str(current_user.emp_no)
        logger.info(f"ğŸ“Š [Compare] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")
        
        # ì œì•½ ì¡°ê±´
        constraints = AgentConstraints(
            max_chunks=request.max_chunks,
            max_tokens=request.max_tokens,
            similarity_threshold=request.similarity_threshold,
            container_ids=request.container_ids,
            document_ids=request.document_ids
        )
        
        context = {
            "user_emp_no": user_emp_no,
            "session_id": request.session_id or str(uuid.uuid4())
        }
        
        # ìƒˆ ì•„í‚¤í…ì²˜ ì‹¤í–‰
        start_time_new = datetime.utcnow()
        result_new: AgentResult = await paper_search_agent.execute(
            query=request.message,
            db_session=db,
            constraints=constraints,
            context=context
        )
        latency_new = (datetime.utcnow() - start_time_new).total_seconds() * 1000
        
        # ê¸°ì¡´ ì•„í‚¤í…ì²˜ ì‹¤í–‰
        # TODO: rag_search_service í˜¸ì¶œ (í˜„ì¬ëŠ” mock)
        latency_old = 0.0
        result_old = {
            "answer": "[ê¸°ì¡´ ì•„í‚¤í…ì²˜ ê²°ê³¼ - TODO: rag_search_service í†µí•©]",
            "references": [],
            "chunks_found": 0
        }
        
        # ë¹„êµ ê²°ê³¼
        comparison = {
            "query": request.message,
            "old_architecture": {
                "answer": result_old["answer"],
                "latency_ms": latency_old,
                "chunks_found": result_old["chunks_found"],
                "implementation": "rag_search_service (monolithic)"
            },
            "new_architecture": {
                "answer": result_new.answer,
                "latency_ms": latency_new,
                "chunks_found": result_new.metrics.get("chunks_found", 0),
                "chunks_used": result_new.metrics.get("chunks_used", 0),
                "intent": result_new.intent.value,
                "strategy": result_new.strategy_used,
                "tools_used": len(result_new.steps),
                "implementation": "paper_search_agent (agent-based)"
            },
            "improvement": {
                "latency_diff_ms": latency_old - latency_new,
                "latency_improvement_pct": ((latency_old - latency_new) / latency_old * 100) if latency_old > 0 else 0,
            },
            "observability": {
                "agent_steps": [
                    {
                        "tool": step.tool_name,
                        "reasoning": step.reasoning,
                        "latency_ms": step.tool_output.metrics.latency_ms
                    }
                    for step in result_new.steps
                ]
            }
        }
        
        logger.info(
            f"ğŸ“Š [Compare] ì™„ë£Œ - ì‹ ê·œ: {latency_new:.1f}ms, êµ¬: {latency_old:.1f}ms"
        )
        
        return comparison
        
    except Exception as e:
        logger.error(f"âŒ [Compare] ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¹„êµ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/agent/chat/stream")
async def agent_chat_stream(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    ğŸ†• Agent ê¸°ë°˜ ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸
    
    ì‹¤ì‹œê°„ìœ¼ë¡œ AIì˜ ì‚¬ê³  ê³¼ì •(Reasoning)ê³¼ ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°:
    1. reasoning_step: ê° ë„êµ¬ ì‹¤í–‰ ë‹¨ê³„ (ì§ˆì˜ ë¶„ì„, ê²€ìƒ‰, ì¬ì •ë ¬ ë“±)
    2. search_progress: ê²€ìƒ‰ ì§„í–‰ ìƒí™© (ë²¡í„° ê²€ìƒ‰, í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼)
    3. content: ë‹µë³€ í…ìŠ¤íŠ¸ (ì²­í¬ ë‹¨ìœ„)
    4. metadata: ìµœì¢… ë©”íƒ€ë°ì´í„° (ì°¸ê³  ë¬¸ì„œ, ë©”íŠ¸ë¦­)
    5. done: ì™„ë£Œ
    """
    import json
    import asyncio
    from fastapi.responses import StreamingResponse
    
    async def event_generator():
        try:
            user_emp_no = str(current_user.emp_no)
            logger.info(f"ğŸ¤– [AgentChatStream] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")
            
            # ì œì•½ ì¡°ê±´ ìƒì„±
            effective_threshold = request.similarity_threshold
            if effective_threshold >= 0.5:
                effective_threshold = 0.25
            
            constraints = AgentConstraints(
                max_chunks=request.max_chunks,
                max_tokens=request.max_tokens,
                similarity_threshold=effective_threshold,
                container_ids=request.container_ids,
                document_ids=request.document_ids
            )
            
            context = {
                "user_emp_no": user_emp_no,
                "session_id": request.session_id or str(uuid.uuid4())
            }
            
            # ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ (ë©€í‹°í„´ ì§€ì›)
            chat_history_messages = []
            session_attached_files = []  # ğŸ†• ì„¸ì…˜ì— ì €ì¥ëœ ì²¨ë¶€ íŒŒì¼
            if request.session_id:
                try:
                    from app.models.chat.chat_models import TbChatHistory
                    from sqlalchemy import select
                    
                    history_stmt = (
                        select(TbChatHistory)
                        .where(TbChatHistory.session_id == request.session_id)
                        .order_by(TbChatHistory.created_date.asc())
                    )
                    history_result = await db.execute(history_stmt)
                    history_records = history_result.scalars().all()
                    
                    for record in history_records:
                        if record.user_message:
                            chat_history_messages.append({"role": "user", "content": record.user_message})
                        if record.assistant_response:
                            chat_history_messages.append({"role": "assistant", "content": record.assistant_response})
                        
                        # ğŸ†• ì„¸ì…˜ì˜ ì²¨ë¶€ íŒŒì¼ ë¡œë“œ (ê°€ì¥ ìµœê·¼ ê²ƒë§Œ)
                        if record.model_parameters and isinstance(record.model_parameters, dict):
                            attached = record.model_parameters.get('attached_files', [])
                            if attached and isinstance(attached, list):
                                session_attached_files = attached  # ìµœì‹  íŒŒì¼ ëª©ë¡ìœ¼ë¡œ ê°±ì‹ 
                            
                    logger.info(f"ğŸ“š [AgentChatStream] íˆìŠ¤í† ë¦¬ ë¡œë“œ: {len(chat_history_messages)}ê°œ ë©”ì‹œì§€, ì„¸ì…˜ ì²¨ë¶€íŒŒì¼: {len(session_attached_files)}ê°œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ğŸ§  Step 1: ì§ˆì˜ ë¶„ì„
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'started', 'message': 'ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)  # UI ì—…ë°ì´íŠ¸ ì‹œê°„
            
            # ğŸ†• ì´ë¯¸ì§€ ë¶„ì„ (request.images + attachmentsì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ)
            image_description = ""
            images_to_analyze = list(request.images) if request.images else []
            
            # ğŸ†• ë¬¸ì„œ ì²¨ë¶€ ì²˜ë¦¬ (Chat with File)
            attached_document_context = ""
            attached_files = []  # ì²¨ë¶€ íŒŒì¼ ë©”íƒ€ë°ì´í„° (í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš©)
            
            # ğŸ†• ì„¸ì…˜ì— ì €ì¥ëœ ì²¨ë¶€ íŒŒì¼ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
            all_attachments = []
            if session_attached_files:
                # ì„¸ì…˜ì˜ ì²¨ë¶€ íŒŒì¼ì„ ì²¨ë¶€ ëª©ë¡ìœ¼ë¡œ ë³€í™˜
                for sf in session_attached_files:
                    all_attachments.append({
                        'asset_id': sf.get('asset_id') or sf.get('id'),
                        'id': sf.get('asset_id') or sf.get('id'),
                        'category': sf.get('category', 'document'),
                        'file_name': sf.get('file_name', ''),
                        'mime_type': sf.get('mime_type', ''),
                        'file_size': sf.get('file_size', 0)
                    })
                logger.info(f"ğŸ“ [AgentChatStream] ì„¸ì…˜ ì²¨ë¶€ íŒŒì¼ ë³µì›: {len(all_attachments)}ê°œ")
            
            # í˜„ì¬ ìš”ì²­ì˜ ì²¨ë¶€ íŒŒì¼ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
            if request.attachments:
                existing_ids = {att.get('asset_id') or att.get('id') for att in all_attachments}
                for att in request.attachments:
                    att_id = att.get('asset_id') or att.get('id')
                    if att_id and att_id not in existing_ids:
                        all_attachments.append(att)
                        logger.info(f"ğŸ†• [AgentChatStream] ìƒˆ ì²¨ë¶€ íŒŒì¼ ì¶”ê°€: {att.get('file_name', att_id)}")
            
            if all_attachments:
                # ğŸ†• ì²¨ë¶€ íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œí•˜ì—¬ ë¶„ì„ ëŒ€ìƒì— ì¶”ê°€
                image_attachments = [
                    att for att in all_attachments 
                    if att.get('mime_type', '').startswith('image/')
                ]
                
                if image_attachments:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'image_analysis', 'status': 'started', 'message': f'ì²¨ë¶€ëœ ì´ë¯¸ì§€ {len(image_attachments)}ê°œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                    
                    # ì´ë¯¸ì§€ íŒŒì¼ ë¡œë“œ (base64ë¡œ ë³€í™˜)
                    for img_att in image_attachments:
                        asset_id = img_att.get('asset_id') or img_att.get('id')
                        if not asset_id:
                            continue
                        
                        stored_file = chat_attachment_service.get(asset_id)
                        if stored_file:
                            try:
                                import base64
                                img_data = None
                                
                                # S3 ìŠ¤í† ë¦¬ì§€ ì²˜ë¦¬
                                if getattr(stored_file, 'storage_backend', 'local') == 's3':
                                    if chat_attachment_service.s3_client:
                                        response = chat_attachment_service.s3_client.get_object(
                                            Bucket=chat_attachment_service.s3_bucket,
                                            Key=str(stored_file.path)
                                        )
                                        img_data = response['Body'].read()
                                    else:
                                        logger.error(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {asset_id}")
                                        continue
                                # ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ì²˜ë¦¬
                                else:
                                    with open(stored_file.path, 'rb') as f:
                                        img_data = f.read()
                                
                                if img_data:
                                    img_base64 = base64.b64encode(img_data).decode('utf-8')
                                    # MIME íƒ€ì…ì— ë”°ë¼ í—¤ë” ì¶”ê°€
                                    mime = img_att.get('mime_type', 'image/jpeg')
                                    images_to_analyze.append(f"data:{mime};base64,{img_base64}")
                                    logger.info(f"ğŸ“· [AgentChatStream] ì´ë¯¸ì§€ ë¡œë“œ: {img_att.get('file_name')}")
                            except Exception as e:
                                logger.error(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {asset_id}, {e}")
                
                # ğŸ†• ì´ë¯¸ì§€ ë¶„ì„ ë„êµ¬ ì‹¤í–‰
                if images_to_analyze:
                    try:
                        image_tool = paper_search_agent.tools.get('image_analysis')
                        if image_tool:
                            image_description = await image_tool._arun(
                                images=images_to_analyze,
                                query=request.message,
                                detail_level="detailed"
                            )
                            logger.info(f"âœ… [AgentChatStream] ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {len(image_description)}ì")
                            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'image_analysis', 'status': 'completed', 'message': f'ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ ({len(images_to_analyze)}ê°œ)'}, ensure_ascii=False)}\n\n"
                    except Exception as e:
                        logger.error(f"âŒ [AgentChatStream] ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'image_analysis', 'status': 'error', 'message': f'ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}'}, ensure_ascii=False)}\n\n"
                
                # ë¬¸ì„œ íŒŒì¼ í•„í„°ë§ (ì´ë¯¸ì§€/ì˜¤ë””ì˜¤ ì œì™¸)
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'started', 'message': 'ì²¨ë¶€ëœ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                
                doc_attachments = [
                    att for att in all_attachments 
                    if not att.get('mime_type', '').startswith('image/') and not att.get('mime_type', '').startswith('audio/')
                ]
                
                if doc_attachments:
                    text_extractor = TextExtractorService()
                    extracted_texts = []
                    
                    for doc_att in doc_attachments:
                        asset_id = doc_att.get('asset_id') or doc_att.get('id')
                        if not asset_id:
                            continue
                            
                        stored_file = chat_attachment_service.get(asset_id)
                        if not stored_file:
                            logger.warning(f"âš ï¸ ì²¨ë¶€ íŒŒì¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {asset_id}")
                            continue
                            
                        # íŒŒì¼ í¬ê¸° ì œí•œ (3MB - Upstage API ì œí•œ ëŒ€ì‘)
                        MAX_FILE_SIZE = 3 * 1024 * 1024
                        if stored_file.size > MAX_FILE_SIZE:
                            file_size_mb = stored_file.size / (1024 * 1024)
                            extracted_texts.append(f"[íŒŒì¼: {stored_file.file_name}]\n(íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤: {file_size_mb:.1f}MB. ì±„íŒ…ì—ì„œëŠ” 3MB ì´í•˜ì˜ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë¬¸ì„œ ì—…ë¡œë“œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.)")
                            logger.warning(f"âš ï¸ íŒŒì¼ í¬ê¸° ì´ˆê³¼: {stored_file.file_name} ({file_size_mb:.1f}MB)")
                            continue
                            
                        try:
                            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            import tempfile
                            import os
                            
                            extraction_path = str(stored_file.path)
                            is_temp_file = False
                            
                            # S3 ìŠ¤í† ë¦¬ì§€ì¸ ê²½ìš° ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
                            if getattr(stored_file, 'storage_backend', 'local') == 's3':
                                if chat_attachment_service.s3_client:
                                    suffix = Path(stored_file.file_name).suffix
                                    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                                        chat_attachment_service.s3_client.download_fileobj(
                                            chat_attachment_service.s3_bucket,
                                            str(stored_file.path),
                                            tmp
                                        )
                                        extraction_path = tmp.name
                                        is_temp_file = True
                                else:
                                    logger.error(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {asset_id}")
                                    continue
                            
                            file_ext = Path(stored_file.file_name).suffix
                            
                            extraction_result = await text_extractor.extract_text_from_file(
                                file_path=extraction_path,
                                file_extension=file_ext
                            )
                            
                            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                            if is_temp_file and os.path.exists(extraction_path):
                                os.unlink(extraction_path)
                            
                            if extraction_result.get('success') and extraction_result.get('text'):
                                text_content = extraction_result['text']
                                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (30,000ì)
                                MAX_TEXT_LENGTH = 30000
                                if len(text_content) > MAX_TEXT_LENGTH:
                                    text_content = text_content[:MAX_TEXT_LENGTH] + "\n...(ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ìƒëµë¨)"
                                    
                                extracted_texts.append(f"[ì²¨ë¶€ íŒŒì¼ ë‚´ìš©: {stored_file.file_name}]\n{text_content}")
                                attached_files.append({
                                    "file_name": stored_file.file_name,
                                    "file_size": stored_file.size,
                                    "text_length": len(text_content)
                                })
                                logger.info(f"âœ… ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {stored_file.file_name} ({len(text_content)}ì)")
                            else:
                                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {stored_file.file_name}")
                        except Exception as e:
                            logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ì—ëŸ¬ ë°œìƒ ì‹œ)
                            if 'is_temp_file' in locals() and is_temp_file and 'extraction_path' in locals() and os.path.exists(extraction_path):
                                try:
                                    os.unlink(extraction_path)
                                except:
                                    pass
                            
                    if extracted_texts:
                        attached_document_context = "\n\n".join(extracted_texts)
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'completed', 'message': f'ì²¨ë¶€ ë¬¸ì„œ {len(extracted_texts)}ê°œ ë‚´ìš©ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"

            # ğŸ†• íŠ¹í—ˆ ê´€ë ¨ ì˜ë„ ì„ ê°ì§€ (ë¦¬ë¼ì´í„°/ë„êµ¬ ì„ íƒì— í™œìš©)
            patent_keywords = ['íŠ¹í—ˆ', 'ì¶œì›', 'ë“±ë¡íŠ¹í—ˆ', 'ê³µê°œíŠ¹í—ˆ', 'patent', 'kipris', 'íŠ¹í—ˆë¶„ì„']
            normalized_message = request.message.lower()
            is_patent_query = any(kw in normalized_message for kw in patent_keywords)
            normalized_tool = (request.tool or "").lower()
            if request.tool and request.tool != normalized_tool:
                request.tool = normalized_tool
            skip_rewrite = is_patent_query or (request.tool == 'patent')

            # ğŸ†• Query Rewrite ì ìš© (íŠ¹í—ˆ ì˜ë„ëŠ” ì›ë¬¸ ìœ ì§€)
            rewritten_query = request.message
            if not skip_rewrite and (chat_history_messages or image_description):
                rewritten_query = await paper_search_agent.rewrite_query(request.message, chat_history_messages, image_description)
                if rewritten_query != request.message:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'started', 'message': f'ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì„ êµ¬ì²´í™”í–ˆìŠµë‹ˆë‹¤: {rewritten_query}'}, ensure_ascii=False)}\n\n"
            elif skip_rewrite:
                logger.info("ğŸ›‘ [AgentChatStream] íŠ¹í—ˆ/ê°•ì œ ë„êµ¬ ì§ˆì˜ëŠ” ë¦¬ë¼ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")

            intent = await paper_search_agent.classify_intent(rewritten_query)
            keywords = await paper_search_agent._extract_keywords(rewritten_query)
            
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'completed', 'result': {'intent': intent.value, 'keywords': keywords}, 'message': f'ì˜ë„: {intent.value}, í‚¤ì›Œë“œ: {keywords}'}, ensure_ascii=False)}\n\n"
            
            # ğŸ†• PPT ìƒì„± ì˜ë„ ê°ì§€ ì‹œ PresentationAgentë¡œ ì „í™˜
            if intent == AgentIntent.PPT_GENERATION:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': ['presentation_agent']}, 'message': 'PPT ìƒì„± ì „ë¬¸ê°€ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                
                # PresentationAgent ì‹¤í–‰
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'presentation', 'status': 'started', 'message': 'PPT êµ¬ì¡°ë¥¼ ê¸°íší•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                
                try:
                    # PresentationAgent í˜¸ì¶œ
                    from app.agents.presentation_agent import presentation_agent
                    from langchain_core.messages import HumanMessage
                    
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ í•„ìš”í•œ ê²½ìš° (ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
                    input_state = {
                        "messages": [HumanMessage(content=request.message)],
                        "context": attached_document_context # ì²¨ë¶€ íŒŒì¼ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬
                    }
                    
                    result = await presentation_agent.ainvoke(input_state)
                    
                    ppt_url = result.get("ppt_file_url")
                    final_response = result.get("final_response")
                    
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'presentation', 'status': 'completed', 'message': 'PPT ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                    
                    # ë‹µë³€ ì „ì†¡
                    yield f"event: content\ndata: {json.dumps({'delta': final_response}, ensure_ascii=False)}\n\n"
                    
                    # ë©”íƒ€ë°ì´í„° ì „ì†¡
                    metadata = {
                        "intent": intent.value,
                        "strategy_used": ["presentation_agent"],
                        "detailed_chunks": [],
                        "has_attachments": bool(attached_files),
                        "ppt_url": ppt_url
                    }
                    yield f"event: metadata\ndata: {json.dumps(metadata, ensure_ascii=False)}\n\n"
                    
                    yield f"event: done\ndata: {json.dumps({'success': True, 'session_id': context.get('session_id')}, ensure_ascii=False)}\n\n"
                    return
                except Exception as ppt_error:
                    logger.error(f"PPT ìƒì„± ì‹¤íŒ¨: {ppt_error}")
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'presentation', 'status': 'error', 'message': f'PPT ìƒì„± ì‹¤íŒ¨: {str(ppt_error)}'}, ensure_ascii=False)}\n\n"
                    yield f"event: error\ndata: {json.dumps({'error': str(ppt_error)}, ensure_ascii=False)}\n\n"
                    return

            # ğŸ” Step 2: ì „ëµ ì„ íƒ
            strategy = paper_search_agent.select_strategy(intent, constraints)
            
            if not request.tool and is_patent_query:
                # íŠ¹í—ˆ ê´€ë ¨ ì¿¼ë¦¬ì¸ë° ë„êµ¬ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìë™ìœ¼ë¡œ íŠ¹í—ˆ ì—ì´ì „íŠ¸ í˜¸ì¶œ
                logger.info(f"ğŸ”¬ [AgentChatStream] íŠ¹í—ˆ ì¿¼ë¦¬ ìë™ ê°ì§€: '{request.message[:50]}...'")
                request.tool = 'patent'  # íŠ¹í—ˆ ë„êµ¬ë¡œ ì„¤ì •
            
            # ğŸ†• ë„êµ¬ ê°•ì œ ì„ íƒ ì ìš©
            if request.tool:
                if request.tool == 'web-search':
                    strategy = ['internet_search', 'context_builder']
                    intent = AgentIntent.WEB_SEARCH
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': strategy}, 'message': 'ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                elif request.tool == 'ppt':
                    intent = AgentIntent.PPT_GENERATION
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': ['presentation_agent']}, 'message': 'ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ PPT ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                elif request.tool == 'patent':
                    # ğŸ†• íŠ¹í—ˆ ë¶„ì„ ì—ì´ì „íŠ¸ ì‹¤í–‰
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': ['patent_analysis']}, 'message': 'íŠ¹í—ˆ ë¶„ì„ ì „ë¬¸ê°€ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                    
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'patent_analysis', 'status': 'started', 'message': 'KIPRIS/Google Patentsì—ì„œ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
                    
                    try:
                        from app.agents.patent import patent_analysis_agent_tool
                        
                        # íŠ¹í—ˆ ë¶„ì„ ì‹¤í–‰
                        patent_result = await patent_analysis_agent_tool._arun(
                            query=request.message,
                            analysis_type="search",  # ê¸°ë³¸: ê²€ìƒ‰
                            jurisdiction="KR",
                            max_results=20,
                            include_visualization=True
                        )
                        
                        # íŠ¹í—ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹µë³€ìœ¼ë¡œ í¬ë§·
                        total_patents = patent_result.get("total_patents", 0)
                        summary = patent_result.get("summary", "")
                        
                        completed_msg = f"íŠ¹í—ˆ ê²€ìƒ‰ ì™„ë£Œ: {total_patents}ê±´"
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'patent_analysis', 'status': 'completed', 'message': completed_msg}, ensure_ascii=False)}\n\n"
                        patents = patent_result.get("patents", [])
                        visualizations = patent_result.get("visualizations", [])
                        insights = patent_result.get("insights", [])
                        
                        # ë‹µë³€ ì „ì†¡
                        yield f"event: content\ndata: {json.dumps({'delta': summary}, ensure_ascii=False)}\n\n"
                        
                        # ë©”íƒ€ë°ì´í„° ì „ì†¡ (íŠ¹í—ˆ ëª©ë¡, ì‹œê°í™” í¬í•¨)
                        metadata = {
                            "intent": "patent_analysis",
                            "strategy_used": ["patent_analysis"],
                            "detailed_chunks": [],
                            "patent_results": {
                                "patents": patents[:10],  # ìƒìœ„ 10ê±´
                                "total_patents": patent_result.get("total_patents", 0),
                                "visualizations": visualizations,
                                "insights": insights,
                                "source": patent_result.get("analysis_result", {}).get("source", "kipris")
                            }
                        }
                        yield f"event: metadata\ndata: {json.dumps(metadata, ensure_ascii=False)}\n\n"
                        
                        # íˆìŠ¤í† ë¦¬ ì €ì¥
                        try:
                            from app.models.chat.chat_models import TbChatHistory
                            
                            history_entry = TbChatHistory(
                                session_id=context.get("session_id"),
                                user_emp_no=user_emp_no,
                                user_message=request.message,
                                assistant_response=summary,
                                model_parameters={"tool": "patent", "total_patents": total_patents},
                                created_date=datetime.utcnow()
                            )
                            db.add(history_entry)
                            await db.commit()
                        except Exception as save_error:
                            logger.warning(f"âš ï¸ íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                        
                        yield f"event: done\ndata: {json.dumps({'success': True, 'session_id': context.get('session_id')}, ensure_ascii=False)}\n\n"
                        return
                        
                    except Exception as patent_error:
                        logger.error(f"âŒ íŠ¹í—ˆ ë¶„ì„ ì‹¤íŒ¨: {patent_error}")
                        error_msg = f"íŠ¹í—ˆ ë¶„ì„ ì‹¤íŒ¨: {str(patent_error)}"
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'patent_analysis', 'status': 'error', 'message': error_msg}, ensure_ascii=False)}\n\n"
                        yield f"event: error\ndata: {json.dumps({'error': str(patent_error)}, ensure_ascii=False)}\n\n"
                        return
            
            # ğŸ†• ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰: ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ multimodal_search ì¶”ê°€
            has_images = bool(images_to_analyze)
            has_text_attachments = bool(attached_document_context)
            
            if has_images:
                # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ì„ ì „ëµì— ì¶”ê°€
                if "multimodal_search" not in strategy:
                    # ê²€ìƒ‰ ì „ëµ ì•ë¶€ë¶„ì— ì¶”ê°€ (ë²¡í„° ê²€ìƒ‰ê³¼ ë³‘ë ¬ë¡œ ì‹¤í–‰ë˜ë„ë¡)
                    search_tools = ["vector_search", "keyword_search", "fulltext_search"]
                    first_search_idx = next((i for i, t in enumerate(strategy) if t in search_tools), 0)
                    strategy.insert(first_search_idx, "multimodal_search")
                logger.info(f"ğŸ“· [AgentChatStream] ì´ë¯¸ì§€ ì²¨ë¶€ ê°ì§€ â†’ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì¶”ê°€")
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': strategy, 'multimodal': True}, 'message': 'ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ê²€ìƒ‰í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
            elif has_text_attachments:
                # í…ìŠ¤íŠ¸ ë¬¸ì„œë§Œ ìˆìœ¼ë©´ ê¸°ì¡´ ì „ëµ ìœ ì§€
                logger.info(f"ğŸ“ [AgentChatStream] ì²¨ë¶€ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸: {len(attached_document_context)}ì")
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': strategy}, 'message': 'ì²¨ë¶€ ë¬¸ì„œì™€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í•¨ê»˜ ê²€ìƒ‰í•©ë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
            else:
                logger.info(f"ğŸ” [AgentChatStream] ê²€ìƒ‰ ì „ëµ ì„ íƒ: {strategy}")
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': strategy}, 'message': f'ê²€ìƒ‰ ì „ëµ: {strategy}'}, ensure_ascii=False)}\n\n"
            
            # ğŸ“Š Step 3: ë„êµ¬ ì‹¤í–‰ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
            all_chunks = []
            search_stats = {}
            
            for idx, tool_name in enumerate(strategy):
                if tool_name in ["vector_search", "keyword_search", "fulltext_search", "multimodal_search", "internet_search"]:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'search', 'status': 'started', 'tool': tool_name, 'message': f'{tool_name} ì‹¤í–‰ ì¤‘...'}, ensure_ascii=False)}\n\n"
                    
                    try:
                        # ğŸ†• multimodal_searchëŠ” ì´ë¯¸ì§€ ë°ì´í„° ì „ë‹¬
                        if tool_name == "multimodal_search":
                            if not images_to_analyze:
                                logger.warning("âš ï¸ multimodal_search í˜¸ì¶œë¨, í•˜ì§€ë§Œ ì´ë¯¸ì§€ ì—†ìŒ")
                                continue
                            
                            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë§Œ ì‚¬ìš© (ì¶”í›„ ë‹¤ì¤‘ ì´ë¯¸ì§€ ì§€ì› ê°€ëŠ¥)
                            tool_result = await paper_search_agent._execute_tool(
                                tool_name=tool_name,
                                query=rewritten_query,
                                db_session=db,
                                keywords=keywords,
                                constraints=constraints,
                                chunks=all_chunks,
                                context={
                                    **context,
                                    "image_data": images_to_analyze[0],  # Base64 ì´ë¯¸ì§€ (data:image/...;base64,...)
                                    "top_k": 10
                                }
                            )
                        else:
                            tool_result = await paper_search_agent._execute_tool(
                                tool_name=tool_name,
                                query=rewritten_query,
                                db_session=db,
                                keywords=keywords,
                                constraints=constraints,
                                chunks=all_chunks,
                                context=context
                            )
                        
                        if not getattr(tool_result, 'success', False):
                            logger.warning(f"âš ï¸ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {tool_name}, errors={getattr(tool_result, 'errors', [])}")
                            try:
                                await db.rollback()
                            except Exception as rollback_error:
                                logger.error(f"ë¡¤ë°± ì‹¤íŒ¨ ({tool_name}): {rollback_error}")
                            continue

                        if hasattr(tool_result, 'data'):
                            new_chunks = tool_result.data
                            all_chunks.extend(new_chunks)
                            search_stats[tool_name] = {
                                'count': len(new_chunks),
                                'avg_score': sum(c.score for c in new_chunks) / len(new_chunks) if new_chunks else 0
                            }
                            
                            yield f"event: search_progress\ndata: {json.dumps({'tool': tool_name, 'chunks_found': len(new_chunks), 'total_chunks': len(all_chunks), 'avg_similarity': round(search_stats[tool_name]['avg_score'], 3)}, ensure_ascii=False)}\n\n"
                    
                    except Exception as e:
                        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({tool_name}): {e}")
                        try:
                            await db.rollback()
                        except Exception as rollback_error:
                            logger.error(f"ë¡¤ë°± ì‹¤íŒ¨ ({tool_name}): {rollback_error}")
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'search', 'status': 'error', 'tool': tool_name, 'message': f'ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}'}, ensure_ascii=False)}\n\n"
                
                elif tool_name in ["deduplicate", "rerank"]:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'postprocess', 'status': 'started', 'tool': tool_name, 'message': f'{tool_name} ì²˜ë¦¬ ì¤‘...'}, ensure_ascii=False)}\n\n"
                    
                    try:
                        tool_result = await paper_search_agent._execute_tool(
                            tool_name=tool_name,
                            query=rewritten_query,  # ğŸ†• ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ì‚¬ìš©
                            db_session=db,
                            keywords=keywords,
                            constraints=constraints,
                            chunks=all_chunks,
                            context=context
                        )
                        
                        if not getattr(tool_result, 'success', False):
                            logger.warning(f"âš ï¸ í›„ì²˜ë¦¬ ë„êµ¬ ì‹¤íŒ¨: {tool_name}, errors={getattr(tool_result, 'errors', [])}")
                            try:
                                await db.rollback()
                            except Exception as rollback_error:
                                logger.error(f"ë¡¤ë°± ì‹¤íŒ¨ ({tool_name}): {rollback_error}")
                            continue
                        if hasattr(tool_result, 'data'):
                            before_count = len(all_chunks)
                            all_chunks = tool_result.data
                            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'postprocess', 'status': 'completed', 'tool': tool_name, 'before': before_count, 'after': len(all_chunks), 'message': f'{tool_name}: {before_count}ê°œ â†’ {len(all_chunks)}ê°œ'}, ensure_ascii=False)}\n\n"
                    
                    except Exception as e:
                        logger.error(f"í›„ì²˜ë¦¬ ì‹¤íŒ¨ ({tool_name}): {e}")
                        try:
                            await db.rollback()
                        except Exception as rollback_error:
                            logger.error(f"ë¡¤ë°± ì‹¤íŒ¨ ({tool_name}): {rollback_error}")
            
            # ğŸ—ï¸ Step 4: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'context_building', 'status': 'started', 'message': 'ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            
            context_result = await paper_search_agent._execute_tool(
                tool_name="context_builder",
                query=rewritten_query,  # ğŸ†• ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ì‚¬ìš©
                db_session=db,
                keywords=keywords,
                constraints=constraints,
                chunks=all_chunks,
                context=None
            )
            
            context_text = context_result.data if isinstance(context_result.data, str) else ""
            used_chunks = getattr(context_result, 'used_chunks', all_chunks[:5])
            
            # ğŸ†• ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ë¬¸ì„œ + ì´ë¯¸ì§€)
            if attached_document_context or image_description:
                parts = []
                
                # ì´ë¯¸ì§€ ì„¤ëª… ì¶”ê°€
                if image_description:
                    parts.append(f"[ì²¨ë¶€ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼]\n{image_description}")
                
                # ë¬¸ì„œ ë‚´ìš© ì¶”ê°€
                if attached_document_context:
                    parts.append(f"[ì²¨ë¶€ ë¬¸ì„œ ë‚´ìš©]\n{attached_document_context}")
                
                # ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if context_text and context_text.strip():
                    parts.append(f"[ì°¸ê³ : ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼]\n{context_text}")
                
                context_text = "\n\n".join(parts)
            
            token_count = len(context_text.split())  # ê°„ë‹¨í•œ í† í° ì¶”ì •
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ ë©”ì‹œì§€
            if attached_document_context or image_description:
                source_types = []
                if image_description:
                    source_types.append("ì´ë¯¸ì§€")
                if attached_document_context:
                    source_types.append("ë¬¸ì„œ")
                source_msg = " + ".join(source_types)
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'context_building', 'status': 'completed', 'tokens': token_count, 'max_tokens': constraints.max_tokens, 'chunks_used': 0, 'message': f'ì²¨ë¶€ {source_msg} ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {token_count} í† í°'}, ensure_ascii=False)}\n\n"
            else:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'context_building', 'status': 'completed', 'tokens': token_count, 'max_tokens': constraints.max_tokens, 'chunks_used': len(used_chunks), 'message': f'ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {token_count} í† í°, {len(used_chunks)}ê°œ ì²­í¬ ì‚¬ìš©'}, ensure_ascii=False)}\n\n"
            
            # âœï¸ Step 5: ë‹µë³€ ìƒì„±
            if image_description and not attached_document_context:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'answer_generation', 'status': 'started', 'message': 'ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            elif attached_document_context:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'answer_generation', 'status': 'started', 'message': 'ì²¨ë¶€ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            else:
                yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'answer_generation', 'status': 'started', 'message': 'ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            
            # AI ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°) - DEFAULT_LLM_PROVIDER ì„¤ì • ë”°ë¦„
            answer = await paper_search_agent.generate_answer(
                query=request.message, 
                context=context_text, 
                intent=intent,
                history=chat_history_messages
            )
            
            # ë‹µë³€ì„ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì „ì†¡
            if isinstance(answer, str):
                chunk_size = 50
                for i in range(0, len(answer), chunk_size):
                    chunk = answer[i:i+chunk_size]
                    yield f"event: content\ndata: {json.dumps({'delta': chunk}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.05)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
            
            # ğŸ“‹ Step 6: ë©”íƒ€ë°ì´í„° ì „ì†¡
            detailed_chunks = []
            for idx, chunk in enumerate(used_chunks):
                file_id = chunk.file_id
                file_name = chunk.metadata.get("file_name") if chunk.metadata else "ë¬¸ì„œ"
                
                # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ì¸ì§€ í™•ì¸
                is_internet_search = (
                    chunk.match_type == "internet" or 
                    chunk.container_id in ["internet", "tavily", "bing", "duckduckgo"] or
                    (chunk.metadata and chunk.metadata.get("source") in ["internet", "tavily", "bing", "duckduckgo"])
                )
                
                # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ìš© í•„ë“œ
                url = chunk.metadata.get("url") if chunk.metadata else None
                search_type = "internet" if is_internet_search else "hybrid"
                
                # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ì¸ ê²½ìš° íŒŒì¼ëª…ì„ íƒ€ì´í‹€ë¡œ ì„¤ì •
                if is_internet_search and chunk.metadata:
                    file_name = chunk.metadata.get("title") or chunk.metadata.get("file_name") or "ì›¹ ê²€ìƒ‰ ê²°ê³¼"
                
                detailed_chunks.append({
                    "index": idx + 1,
                    "file_id": int(file_id) if file_id and str(file_id).isdigit() else 0,
                    "file_name": file_name,
                    "chunk_index": chunk.metadata.get("chunk_index", 0) if chunk.metadata else 0,
                    "page_number": chunk.metadata.get("page_number") if chunk.metadata else None,
                    "content_preview": chunk.content[:200] if chunk.content else "",
                    "similarity_score": chunk.score,
                    "search_type": search_type,
                    "section_title": file_name,
                    "url": url,  # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ URL
                    "full_content": chunk.content if is_internet_search else None  # ğŸ†• ì „ì²´ ì½˜í…ì¸  (ì¸í„°ë„· ê²€ìƒ‰)
                })
            
            # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ë§Œ ì‚¬ìš©í–ˆëŠ”ì§€ í™•ì¸
            has_internet_only = (
                len(detailed_chunks) > 0 and 
                all(c.get("search_type") == "internet" for c in detailed_chunks)
            )
            has_mixed_search = (
                len(detailed_chunks) > 0 and 
                any(c.get("search_type") == "internet" for c in detailed_chunks) and
                any(c.get("search_type") != "internet" for c in detailed_chunks)
            )
            
            # ğŸ†• answer_source ê²°ì • (ì¸í„°ë„· ê²€ìƒ‰ êµ¬ë¶„)
            if attached_files and not used_chunks:
                answer_source = "attached_documents"
            elif has_internet_only:
                answer_source = "internet_search"
            elif has_mixed_search:
                answer_source = "mixed_search"
            elif used_chunks:
                answer_source = "database_search"
            else:
                answer_source = "general"
            
            metadata = {
                "intent": intent.value,
                "strategy_used": strategy,
                "detailed_chunks": detailed_chunks,
                "search_stats": search_stats,
                "total_chunks_searched": len(all_chunks),
                "chunks_used": len(used_chunks),
                "attached_files": attached_files,  # ğŸ†• ì²¨ë¶€ íŒŒì¼ ë©”íƒ€ë°ì´í„°
                "answer_source": answer_source,  # ğŸ†• ë‹µë³€ ì¶œì²˜ (internet_search, mixed_search, database_search, attached_documents, general)
                "has_attachments": bool(attached_files),  # ğŸ†• ì²¨ë¶€ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€
                "has_internet_results": has_internet_only or has_mixed_search  # ğŸ†• ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ í¬í•¨ ì—¬ë¶€
            }
            
            yield f"event: metadata\ndata: {json.dumps(metadata, ensure_ascii=False)}\n\n"
            
            # ğŸ’¾ ì„¸ì…˜ ë° ëŒ€í™” ë‚´ì—­ ì €ì¥
            try:
                from app.models.chat.chat_models import TbChatSessions, TbChatHistory
                from sqlalchemy import select, update
                from datetime import datetime as dt
                
                session_id = context.get("session_id")
                user_emp_no = context.get("user_emp_no")
                
                # ì„¸ì…˜ ì¡´ì¬ í™•ì¸
                session_stmt = select(TbChatSessions).where(TbChatSessions.session_id == session_id)
                session_result = await db.execute(session_stmt)
                existing_session = session_result.scalar_one_or_none()
                
                if not existing_session:
                    # ìƒˆ ì„¸ì…˜ ìƒì„±
                    new_session = TbChatSessions(
                        session_id=session_id,
                        user_emp_no=user_emp_no,
                        session_name=f"Agent Chat - {request.message[:30]}...",
                        session_description="AI Agent ì±„íŒ… ì„¸ì…˜",
                        default_container_id=request.container_ids[0] if request.container_ids else None,
                        allowed_containers=request.container_ids,
                        is_active=True,
                        last_activity=dt.utcnow(),
                        message_count=1
                    )
                    db.add(new_session)
                    logger.info(f"âœ… [AgentSession] ìƒˆ ì„¸ì…˜ ìƒì„±: {session_id}")
                else:
                    # ê¸°ì¡´ ì„¸ì…˜ ì—…ë°ì´íŠ¸
                    update_stmt = (
                        update(TbChatSessions)
                        .where(TbChatSessions.session_id == session_id)
                        .values(
                            last_activity=dt.utcnow(),
                            message_count=TbChatSessions.message_count + 1
                        )
                    )
                    await db.execute(update_stmt)
                    logger.info(f"âœ… [AgentSession] ì„¸ì…˜ ì—…ë°ì´íŠ¸: {session_id}")
                
                # ëŒ€í™” ë‚´ì—­ ì €ì¥
                referenced_doc_ids = list(set([
                    int(chunk["file_id"]) 
                    for chunk in detailed_chunks 
                    if chunk.get("file_id") and chunk["file_id"] > 0
                ]))
                
                chat_history = TbChatHistory(
                    session_id=session_id,
                    user_emp_no=user_emp_no,
                    knowledge_container_id=request.container_ids[0] if request.container_ids else None,
                    accessible_containers=request.container_ids,
                    user_message=request.message,
                    assistant_response=answer,
                    search_query=request.message,
                    search_results={
                        "chunks": detailed_chunks[:10],  # ìµœëŒ€ 10ê°œë§Œ ì €ì¥
                        "total_searched": len(all_chunks),
                        "total_used": len(used_chunks)
                    },
                    referenced_documents=referenced_doc_ids if referenced_doc_ids else None,
                    model_used="agent/paper_search_agent",
                    model_parameters={
                        "intent": intent.value,
                        "strategy": strategy,
                        "max_chunks": request.max_chunks,
                        "similarity_threshold": effective_threshold,
                        "attached_files": attached_files  # ğŸ†• ì²¨ë¶€ íŒŒì¼ ì •ë³´ ì €ì¥
                    },
                    conversation_context={
                        "search_stats": search_stats,
                        "reasoning_steps": len(strategy),
                        "has_attachments": bool(attached_document_context),  # ğŸ†• ì²¨ë¶€ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€
                        "attachment_context_length": len(attached_document_context) if attached_document_context else 0
                    }
                )
                db.add(chat_history)
                await db.commit()
                
                logger.info(f"ğŸ’¾ [AgentSession] ëŒ€í™” ì €ì¥ ì™„ë£Œ: session={session_id}, docs={len(referenced_doc_ids)}")
                
            except Exception as save_error:
                logger.error(f"âŒ [AgentSession] ì €ì¥ ì‹¤íŒ¨: {save_error}")
                await db.rollback()
                # ì €ì¥ ì‹¤íŒ¨í•´ë„ ìŠ¤íŠ¸ë¦¬ë°ì€ ê³„ì† ì§„í–‰
            
            # âœ… ì™„ë£Œ
            yield f"event: done\ndata: {json.dumps({'success': True, 'session_id': context.get('session_id')}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ [AgentChatStream] ì˜¤ë¥˜: {error_msg}", exc_info=True)
            yield f"event: error\ndata: {json.dumps({'error': error_msg}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.get("/agent/sessions/{session_id}")
async def get_agent_session(
    session_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Agent ì„¸ì…˜ ë³µì›
    
    ì„¸ì…˜ ë©”íƒ€ë°ì´í„° + ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    """
    try:
        from app.models.chat.chat_models import TbChatSessions, TbChatHistory
        from sqlalchemy import select
        
        user_emp_no = str(current_user.emp_no)
        
        # ì„¸ì…˜ ì¡°íšŒ
        session_stmt = (
            select(TbChatSessions)
            .where(
                TbChatSessions.session_id == session_id,
                TbChatSessions.user_emp_no == user_emp_no  # ê¶Œí•œ í™•ì¸
            )
        )
        session_result = await db.execute(session_stmt)
        session = session_result.scalar_one_or_none()
        
        if not session:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}"
            )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
        history_stmt = (
            select(TbChatHistory)
            .where(TbChatHistory.session_id == session_id)
            .order_by(TbChatHistory.created_date.asc())
        )
        history_result = await db.execute(history_stmt)
        history_records = history_result.scalars().all()
        
        # ì‘ë‹µ ë³€í™˜
        messages = []
        for record in history_records:
            messages.append({
                "chat_id": record.chat_id,
                "user_message": record.user_message,
                "assistant_response": record.assistant_response,
                "referenced_documents": record.referenced_documents or [],
                "search_results": record.search_results,
                "model_used": record.model_used,
                "model_parameters": record.model_parameters,
                "created_date": record.created_date.isoformat()
            })
        
        return {
            "session_id": session.session_id,
            "session_name": session.session_name,
            "session_description": session.session_description,
            "user_emp_no": session.user_emp_no,
            "default_container_id": session.default_container_id,
            "allowed_containers": session.allowed_containers,
            "is_active": session.is_active,
            "last_activity": session.last_activity.isoformat(),
            "message_count": session.message_count,
            "created_date": session.created_date.isoformat(),
            "messages": messages
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [AgentSession] ë³µì› ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„¸ì…˜ ë³µì› ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/agent/sessions")
async def list_agent_sessions(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
    limit: int = 20,
    offset: int = 0
):
    """
    ì‚¬ìš©ìì˜ Agent ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ
    """
    try:
        from app.models.chat.chat_models import TbChatSessions
        from sqlalchemy import select
        
        user_emp_no = str(current_user.emp_no)
        
        stmt = (
            select(TbChatSessions)
            .where(TbChatSessions.user_emp_no == user_emp_no)
            .order_by(TbChatSessions.last_activity.desc())
            .limit(limit)
            .offset(offset)
        )
        
        result = await db.execute(stmt)
        sessions = result.scalars().all()
        
        return {
            "sessions": [
                {
                    "session_id": s.session_id,
                    "session_name": s.session_name,
                    "last_activity": s.last_activity.isoformat(),
                    "message_count": s.message_count,
                    "is_active": s.is_active
                }
                for s in sessions
            ],
            "total": len(sessions),
            "limit": limit,
            "offset": offset
        }
        
    except Exception as e:
        logger.error(f"âŒ [AgentSession] ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/agent/health")
async def agent_health():
    """Agent ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "agent": paper_search_agent.name,
        "version": paper_search_agent.version,
        "tools": list(paper_search_agent.tools.keys()),
        "timestamp": datetime.utcnow().isoformat()
    }

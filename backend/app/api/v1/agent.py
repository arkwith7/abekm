"""
Agent-based Chat API - PaperSearchAgentë¥¼ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
Feature flagë¡œ ì ì§„ì  ì „í™˜ ê°€ëŠ¥
"""
from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime
import uuid

from app.core.database import get_db
from app.core.dependencies import get_current_user
from app.models import User
from app.agents import paper_search_agent
from app.tools.contracts import AgentConstraints, AgentIntent, AgentResult
from loguru import logger


router = APIRouter(tags=["agent"])


# Request/Response ëª¨ë¸
class AgentChatRequest(BaseModel):
    """Agent ê¸°ë°˜ ì±„íŒ… ìš”ì²­"""
    message: str = Field(..., min_length=1, description="ì‚¬ìš©ì ì§ˆì˜")
    session_id: Optional[str] = Field(None, description="ì„¸ì…˜ ID")
    
    # ì œì•½ ì¡°ê±´
    max_chunks: int = Field(10, ge=1, le=50, description="ìµœëŒ€ ì²­í¬ ìˆ˜")
    max_tokens: int = Field(4000, ge=100, le=8000, description="ìµœëŒ€ í† í° ìˆ˜")  # 2000 â†’ 4000 (ì¼ë°˜ RAGì™€ ë™ì¼)
    similarity_threshold: float = Field(0.25, ge=0.0, le=1.0, description="ìœ ì‚¬ë„ ì„ê³„ê°’")  # 0.5 â†’ 0.25ë¡œ ë‚®ì¶¤ (ì¼ë°˜ RAGì™€ ë™ì¼)
    
    # í•„í„°ë§
    container_ids: Optional[List[str]] = Field(None, description="ì»¨í…Œì´ë„ˆ ID í•„í„°")
    document_ids: Optional[List[str]] = Field(None, description="ë¬¸ì„œ ID í•„í„°")


class AgentStepResponse(BaseModel):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰ ë‹¨ê³„"""
    step_number: int
    tool_name: str
    reasoning: str
    latency_ms: float
    items_returned: Optional[int] = None
    success: bool


class ReferenceDocument(BaseModel):
    """ì°¸ì¡° ë¬¸ì„œ"""
    chunk_id: str
    content: str
    score: float
    document_id: Optional[str] = None
    title: Optional[str] = None
    page_number: Optional[int] = None


class DetailedChunk(BaseModel):
    """ìƒì„¸ ì²­í¬ ì •ë³´ (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼ í˜•ì‹)"""
    index: int
    file_id: int
    file_name: str
    chunk_index: int
    page_number: Optional[int] = None
    content_preview: str
    similarity_score: float
    search_type: str
    section_title: str = ""


class AgentChatResponse(BaseModel):
    """Agent ê¸°ë°˜ ì±„íŒ… ì‘ë‹µ"""
    answer: str
    intent: str
    strategy_used: List[str]
    references: List[ReferenceDocument]
    detailed_chunks: List[DetailedChunk] = []  # ğŸ†• ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼ í˜•ì‹
    steps: List[AgentStepResponse]
    metrics: Dict[str, Any]
    success: bool
    errors: List[str] = []


@router.post("/agent/chat", response_model=AgentChatResponse)
async def agent_chat(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Agent ê¸°ë°˜ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
    
    PaperSearchAgentë¥¼ ì‚¬ìš©í•˜ì—¬:
    1. ì§ˆì˜ ì˜ë„ ë¶„ì„
    2. ë™ì  ì „ëµ ì„ íƒ (ë„êµ¬ ì¡°í•©)
    3. ë„êµ¬ ìˆœì°¨ ì‹¤í–‰
    4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° ë‹µë³€ ìƒì„±
    
    ê¸°ì¡´ /chat/messageì™€ ë³‘í–‰ ìš´ì˜ ê°€ëŠ¥ (A/B í…ŒìŠ¤íŠ¸)
    """
    try:
        user_emp_no = str(current_user.emp_no)
        logger.info(f"ğŸ¤– [AgentChat] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")
        
        # similarity_threshold ë³´ì • (0.5 ì´ìƒì´ë©´ 0.25ë¡œ ë‚®ì¶¤)
        effective_threshold = request.similarity_threshold
        if effective_threshold >= 0.5:
            logger.warning(f"âš ï¸ threshold {effective_threshold} â†’ 0.25ë¡œ ë³´ì • (ê²€ìƒ‰ ê²°ê³¼ í™•ë³´)")
            effective_threshold = 0.25
        
        # ì œì•½ ì¡°ê±´ ìƒì„±
        constraints = AgentConstraints(
            max_chunks=request.max_chunks,
            max_tokens=request.max_tokens,
            similarity_threshold=effective_threshold,
            container_ids=request.container_ids,
            document_ids=request.document_ids
        )
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = {
            "user_emp_no": user_emp_no,
            "session_id": request.session_id or str(uuid.uuid4())
        }
        
        # Agent ì‹¤í–‰
        result: AgentResult = await paper_search_agent.execute(
            query=request.message,
            db_session=db,
            constraints=constraints,
            context=context
        )
        
        # ì‘ë‹µ ë³€í™˜
        steps_response = []
        for step in result.steps:
            steps_response.append(AgentStepResponse(
                step_number=step.step_number,
                tool_name=step.tool_name,
                reasoning=step.reasoning,
                latency_ms=step.tool_output.metrics.latency_ms,
                items_returned=step.tool_output.metrics.items_returned,
                success=step.tool_output.success
            ))
        
        references_response = []
        detailed_chunks_response = []
        
        for idx, ref in enumerate(result.references):
            # SearchChunkì—ì„œ file_idì™€ metadata ì •ë³´ ì¶”ì¶œ
            file_id = ref.file_id  # SearchChunk.file_id (ì§ì ‘ í•„ë“œ)
            file_name = None
            chunk_index = 0
            page_number = None
            
            if ref.metadata:
                file_name = ref.metadata.get("file_name") or ref.metadata.get("title")
                chunk_index = ref.metadata.get("chunk_index", 0)
                page_number = ref.metadata.get("page_number")
            
            # ReferenceDocument (ê¸°ì¡´ í˜¸í™˜ì„±)
            references_response.append(ReferenceDocument(
                chunk_id=ref.chunk_id,
                content=ref.content,
                score=ref.score,
                document_id=ref.metadata.get("document_id") if ref.metadata else None,
                title=file_name,  # file_nameì„ titleë¡œ ì‚¬ìš©
                page_number=page_number
            ))
            
            # DetailedChunk (ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼ í˜•ì‹)
            detailed_chunks_response.append(DetailedChunk(
                index=idx + 1,
                file_id=int(file_id) if file_id and str(file_id).isdigit() else 0,
                file_name=file_name or "ë¬¸ì„œ",
                chunk_index=chunk_index,
                page_number=page_number,
                content_preview=ref.content[:200] if ref.content else "",
                similarity_score=ref.score,
                search_type="agent",
                section_title=file_name or ""
            ))
        
        logger.info(
            f"âœ… [AgentChat] ì™„ë£Œ: {result.metrics.get('total_latency_ms', 0):.1f}ms, "
            f"{len(result.references)}ê°œ ì°¸ì¡°, {len(result.steps)}ê°œ ë‹¨ê³„"
        )
        
        return AgentChatResponse(
            answer=result.answer,
            intent=result.intent.value,
            strategy_used=result.strategy_used,
            references=references_response,
            detailed_chunks=detailed_chunks_response,  # ğŸ†• ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼ í˜•ì‹
            steps=steps_response,
            metrics=result.metrics,
            success=result.success,
            errors=result.errors
        )
        
    except Exception as e:
        logger.error(f"âŒ [AgentChat] ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Agent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/agent/compare", response_model=Dict[str, Any])
async def compare_architectures(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    A/B ë¹„êµ ì—”ë“œí¬ì¸íŠ¸
    
    ë™ì¼í•œ ì§ˆì˜ë¥¼ ë‘ ê°€ì§€ ì•„í‚¤í…ì²˜ë¡œ ì‹¤í–‰í•˜ì—¬ ë¹„êµ:
    1. ê¸°ì¡´ rag_search_service (monolithic)
    2. ìƒˆë¡œìš´ paper_search_agent (agent-based)
    
    í‰ê°€ ë° ì„±ëŠ¥ ë¶„ì„ì— ì‚¬ìš©
    """
    try:
        user_emp_no = str(current_user.emp_no)
        logger.info(f"ğŸ“Š [Compare] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")
        
        # ì œì•½ ì¡°ê±´
        constraints = AgentConstraints(
            max_chunks=request.max_chunks,
            max_tokens=request.max_tokens,
            similarity_threshold=request.similarity_threshold,
            container_ids=request.container_ids,
            document_ids=request.document_ids
        )
        
        context = {
            "user_emp_no": user_emp_no,
            "session_id": request.session_id or str(uuid.uuid4())
        }
        
        # ìƒˆ ì•„í‚¤í…ì²˜ ì‹¤í–‰
        start_time_new = datetime.utcnow()
        result_new: AgentResult = await paper_search_agent.execute(
            query=request.message,
            db_session=db,
            constraints=constraints,
            context=context
        )
        latency_new = (datetime.utcnow() - start_time_new).total_seconds() * 1000
        
        # ê¸°ì¡´ ì•„í‚¤í…ì²˜ ì‹¤í–‰
        # TODO: rag_search_service í˜¸ì¶œ (í˜„ì¬ëŠ” mock)
        latency_old = 0.0
        result_old = {
            "answer": "[ê¸°ì¡´ ì•„í‚¤í…ì²˜ ê²°ê³¼ - TODO: rag_search_service í†µí•©]",
            "references": [],
            "chunks_found": 0
        }
        
        # ë¹„êµ ê²°ê³¼
        comparison = {
            "query": request.message,
            "old_architecture": {
                "answer": result_old["answer"],
                "latency_ms": latency_old,
                "chunks_found": result_old["chunks_found"],
                "implementation": "rag_search_service (monolithic)"
            },
            "new_architecture": {
                "answer": result_new.answer,
                "latency_ms": latency_new,
                "chunks_found": result_new.metrics.get("chunks_found", 0),
                "chunks_used": result_new.metrics.get("chunks_used", 0),
                "intent": result_new.intent.value,
                "strategy": result_new.strategy_used,
                "tools_used": len(result_new.steps),
                "implementation": "paper_search_agent (agent-based)"
            },
            "improvement": {
                "latency_diff_ms": latency_old - latency_new,
                "latency_improvement_pct": ((latency_old - latency_new) / latency_old * 100) if latency_old > 0 else 0,
            },
            "observability": {
                "agent_steps": [
                    {
                        "tool": step.tool_name,
                        "reasoning": step.reasoning,
                        "latency_ms": step.tool_output.metrics.latency_ms
                    }
                    for step in result_new.steps
                ]
            }
        }
        
        logger.info(
            f"ğŸ“Š [Compare] ì™„ë£Œ - ì‹ ê·œ: {latency_new:.1f}ms, êµ¬: {latency_old:.1f}ms"
        )
        
        return comparison
        
    except Exception as e:
        logger.error(f"âŒ [Compare] ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¹„êµ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        )


@router.post("/agent/chat/stream")
async def agent_chat_stream(
    request: AgentChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    ğŸ†• Agent ê¸°ë°˜ ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸
    
    ì‹¤ì‹œê°„ìœ¼ë¡œ AIì˜ ì‚¬ê³  ê³¼ì •(Reasoning)ê³¼ ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°:
    1. reasoning_step: ê° ë„êµ¬ ì‹¤í–‰ ë‹¨ê³„ (ì§ˆì˜ ë¶„ì„, ê²€ìƒ‰, ì¬ì •ë ¬ ë“±)
    2. search_progress: ê²€ìƒ‰ ì§„í–‰ ìƒí™© (ë²¡í„° ê²€ìƒ‰, í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼)
    3. content: ë‹µë³€ í…ìŠ¤íŠ¸ (ì²­í¬ ë‹¨ìœ„)
    4. metadata: ìµœì¢… ë©”íƒ€ë°ì´í„° (ì°¸ê³  ë¬¸ì„œ, ë©”íŠ¸ë¦­)
    5. done: ì™„ë£Œ
    """
    import json
    import asyncio
    from fastapi.responses import StreamingResponse
    
    async def event_generator():
        try:
            user_emp_no = str(current_user.emp_no)
            logger.info(f"ğŸ¤– [AgentChatStream] ì‚¬ìš©ì: {user_emp_no}, ì§ˆì˜: '{request.message[:50]}...'")
            
            # ì œì•½ ì¡°ê±´ ìƒì„±
            effective_threshold = request.similarity_threshold
            if effective_threshold >= 0.5:
                effective_threshold = 0.25
            
            constraints = AgentConstraints(
                max_chunks=request.max_chunks,
                max_tokens=request.max_tokens,
                similarity_threshold=effective_threshold,
                container_ids=request.container_ids,
                document_ids=request.document_ids
            )
            
            context = {
                "user_emp_no": user_emp_no,
                "session_id": request.session_id or str(uuid.uuid4())
            }
            
            # ğŸ§  Step 1: ì§ˆì˜ ë¶„ì„
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'started', 'message': 'ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)  # UI ì—…ë°ì´íŠ¸ ì‹œê°„
            
            intent = paper_search_agent.classify_intent(request.message)
            keywords = await paper_search_agent._extract_keywords(request.message)
            
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'query_analysis', 'status': 'completed', 'result': {'intent': intent.value, 'keywords': keywords}, 'message': f'ì˜ë„: {intent.value}, í‚¤ì›Œë“œ: {keywords}'}, ensure_ascii=False)}\n\n"
            
            # ğŸ” Step 2: ì „ëµ ì„ íƒ
            strategy = paper_search_agent.select_strategy(intent, constraints)
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'strategy_selection', 'status': 'completed', 'result': {'strategy': strategy}, 'message': f'ê²€ìƒ‰ ì „ëµ: {strategy}'}, ensure_ascii=False)}\n\n"
            
            # ğŸ“Š Step 3: ë„êµ¬ ì‹¤í–‰ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
            all_chunks = []
            search_stats = {}
            
            for idx, tool_name in enumerate(strategy):
                if tool_name in ["vector_search", "keyword_search", "fulltext_search"]:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'search', 'status': 'started', 'tool': tool_name, 'message': f'{tool_name} ì‹¤í–‰ ì¤‘...'}, ensure_ascii=False)}\n\n"
                    
                    try:
                        tool_result = await paper_search_agent._execute_tool(
                            tool_name=tool_name,
                            query=request.message,
                            db_session=db,
                            keywords=keywords,
                            constraints=constraints,
                            chunks=all_chunks,
                            context=context
                        )
                        
                        if tool_result.success and hasattr(tool_result, 'data'):
                            new_chunks = tool_result.data
                            all_chunks.extend(new_chunks)
                            search_stats[tool_name] = {
                                'count': len(new_chunks),
                                'avg_score': sum(c.score for c in new_chunks) / len(new_chunks) if new_chunks else 0
                            }
                            
                            yield f"event: search_progress\ndata: {json.dumps({'tool': tool_name, 'chunks_found': len(new_chunks), 'total_chunks': len(all_chunks), 'avg_similarity': round(search_stats[tool_name]['avg_score'], 3)}, ensure_ascii=False)}\n\n"
                    
                    except Exception as e:
                        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({tool_name}): {e}")
                        yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'search', 'status': 'error', 'tool': tool_name, 'message': f'ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}'}, ensure_ascii=False)}\n\n"
                
                elif tool_name in ["deduplicate", "rerank"]:
                    yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'postprocess', 'status': 'started', 'tool': tool_name, 'message': f'{tool_name} ì²˜ë¦¬ ì¤‘...'}, ensure_ascii=False)}\n\n"
                    
                    try:
                        tool_result = await paper_search_agent._execute_tool(
                            tool_name=tool_name,
                            query=request.message,
                            db_session=db,
                            keywords=keywords,
                            constraints=constraints,
                            chunks=all_chunks,
                            context=context
                        )
                        
                        if tool_result.success and hasattr(tool_result, 'data'):
                            before_count = len(all_chunks)
                            all_chunks = tool_result.data
                            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'postprocess', 'status': 'completed', 'tool': tool_name, 'before': before_count, 'after': len(all_chunks), 'message': f'{tool_name}: {before_count}ê°œ â†’ {len(all_chunks)}ê°œ'}, ensure_ascii=False)}\n\n"
                    
                    except Exception as e:
                        logger.error(f"í›„ì²˜ë¦¬ ì‹¤íŒ¨ ({tool_name}): {e}")
            
            # ğŸ—ï¸ Step 4: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'context_building', 'status': 'started', 'message': 'ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            
            context_result = await paper_search_agent._execute_tool(
                tool_name="context_builder",
                query=request.message,
                db_session=db,
                keywords=keywords,
                constraints=constraints,
                chunks=all_chunks,
                context=None
            )
            
            context_text = context_result.data if isinstance(context_result.data, str) else ""
            used_chunks = getattr(context_result, 'used_chunks', all_chunks[:5])
            
            token_count = len(context_text.split())  # ê°„ë‹¨í•œ í† í° ì¶”ì •
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'context_building', 'status': 'completed', 'tokens': token_count, 'max_tokens': constraints.max_tokens, 'chunks_used': len(used_chunks), 'message': f'ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {token_count} í† í°, {len(used_chunks)}ê°œ ì²­í¬ ì‚¬ìš©'}, ensure_ascii=False)}\n\n"
            
            # âœï¸ Step 5: ë‹µë³€ ìƒì„±
            yield f"event: reasoning_step\ndata: {json.dumps({'stage': 'answer_generation', 'status': 'started', 'message': 'ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            
            prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì§ˆë¬¸: {request.message}

ë‹µë³€:"""
            
            # AI ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
            answer = await paper_search_agent.ai_service.chat(
                message=prompt,
                provider="azure_openai"
            )
            
            # ë‹µë³€ì„ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì „ì†¡
            if isinstance(answer, str):
                chunk_size = 50
                for i in range(0, len(answer), chunk_size):
                    chunk = answer[i:i+chunk_size]
                    yield f"event: content\ndata: {json.dumps({'delta': chunk}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.05)  # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
            
            # ğŸ“‹ Step 6: ë©”íƒ€ë°ì´í„° ì „ì†¡
            detailed_chunks = []
            for idx, chunk in enumerate(used_chunks):
                file_id = chunk.file_id
                file_name = chunk.metadata.get("file_name") if chunk.metadata else "ë¬¸ì„œ"
                
                detailed_chunks.append({
                    "index": idx + 1,
                    "file_id": int(file_id) if file_id and str(file_id).isdigit() else 0,
                    "file_name": file_name,
                    "chunk_index": chunk.metadata.get("chunk_index", 0) if chunk.metadata else 0,
                    "page_number": chunk.metadata.get("page_number") if chunk.metadata else None,
                    "content_preview": chunk.content[:200] if chunk.content else "",
                    "similarity_score": chunk.score,
                    "search_type": "hybrid",
                    "section_title": file_name
                })
            
            metadata = {
                "intent": intent.value,
                "strategy_used": strategy,
                "detailed_chunks": detailed_chunks,
                "search_stats": search_stats,
                "total_chunks_searched": len(all_chunks),
                "chunks_used": len(used_chunks)
            }
            
            yield f"event: metadata\ndata: {json.dumps(metadata, ensure_ascii=False)}\n\n"
            
            # ğŸ’¾ ì„¸ì…˜ ë° ëŒ€í™” ë‚´ì—­ ì €ì¥
            try:
                from app.models.chat.chat_models import TbChatSessions, TbChatHistory
                from sqlalchemy import select, update
                from datetime import datetime as dt
                
                session_id = context.get("session_id")
                user_emp_no = context.get("user_emp_no")
                
                # ì„¸ì…˜ ì¡´ì¬ í™•ì¸
                session_stmt = select(TbChatSessions).where(TbChatSessions.session_id == session_id)
                session_result = await db.execute(session_stmt)
                existing_session = session_result.scalar_one_or_none()
                
                if not existing_session:
                    # ìƒˆ ì„¸ì…˜ ìƒì„±
                    new_session = TbChatSessions(
                        session_id=session_id,
                        user_emp_no=user_emp_no,
                        session_name=f"Agent Chat - {request.message[:30]}...",
                        session_description="AI Agent ì±„íŒ… ì„¸ì…˜",
                        default_container_id=request.container_ids[0] if request.container_ids else None,
                        allowed_containers=request.container_ids,
                        is_active=True,
                        last_activity=dt.utcnow(),
                        message_count=1
                    )
                    db.add(new_session)
                    logger.info(f"âœ… [AgentSession] ìƒˆ ì„¸ì…˜ ìƒì„±: {session_id}")
                else:
                    # ê¸°ì¡´ ì„¸ì…˜ ì—…ë°ì´íŠ¸
                    update_stmt = (
                        update(TbChatSessions)
                        .where(TbChatSessions.session_id == session_id)
                        .values(
                            last_activity=dt.utcnow(),
                            message_count=TbChatSessions.message_count + 1
                        )
                    )
                    await db.execute(update_stmt)
                    logger.info(f"âœ… [AgentSession] ì„¸ì…˜ ì—…ë°ì´íŠ¸: {session_id}")
                
                # ëŒ€í™” ë‚´ì—­ ì €ì¥
                referenced_doc_ids = list(set([
                    int(chunk["file_id"]) 
                    for chunk in detailed_chunks 
                    if chunk.get("file_id") and chunk["file_id"] > 0
                ]))
                
                chat_history = TbChatHistory(
                    session_id=session_id,
                    user_emp_no=user_emp_no,
                    knowledge_container_id=request.container_ids[0] if request.container_ids else None,
                    accessible_containers=request.container_ids,
                    user_message=request.message,
                    assistant_response=answer,
                    search_query=request.message,
                    search_results={
                        "chunks": detailed_chunks[:10],  # ìµœëŒ€ 10ê°œë§Œ ì €ì¥
                        "total_searched": len(all_chunks),
                        "total_used": len(used_chunks)
                    },
                    referenced_documents=referenced_doc_ids if referenced_doc_ids else None,
                    model_used="agent/paper_search_agent",
                    model_parameters={
                        "intent": intent.value,
                        "strategy": strategy,
                        "max_chunks": request.max_chunks,
                        "similarity_threshold": effective_threshold
                    },
                    conversation_context={
                        "search_stats": search_stats,
                        "reasoning_steps": len(strategy)
                    }
                )
                db.add(chat_history)
                await db.commit()
                
                logger.info(f"ğŸ’¾ [AgentSession] ëŒ€í™” ì €ì¥ ì™„ë£Œ: session={session_id}, docs={len(referenced_doc_ids)}")
                
            except Exception as save_error:
                logger.error(f"âŒ [AgentSession] ì €ì¥ ì‹¤íŒ¨: {save_error}")
                await db.rollback()
                # ì €ì¥ ì‹¤íŒ¨í•´ë„ ìŠ¤íŠ¸ë¦¬ë°ì€ ê³„ì† ì§„í–‰
            
            # âœ… ì™„ë£Œ
            yield f"event: done\ndata: {json.dumps({'success': True, 'session_id': context.get('session_id')}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ [AgentChatStream] ì˜¤ë¥˜: {error_msg}", exc_info=True)
            yield f"event: error\ndata: {json.dumps({'error': error_msg}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.get("/agent/sessions/{session_id}")
async def get_agent_session(
    session_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Agent ì„¸ì…˜ ë³µì›
    
    ì„¸ì…˜ ë©”íƒ€ë°ì´í„° + ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    """
    try:
        from app.models.chat.chat_models import TbChatSessions, TbChatHistory
        from sqlalchemy import select
        
        user_emp_no = str(current_user.emp_no)
        
        # ì„¸ì…˜ ì¡°íšŒ
        session_stmt = (
            select(TbChatSessions)
            .where(
                TbChatSessions.session_id == session_id,
                TbChatSessions.user_emp_no == user_emp_no  # ê¶Œí•œ í™•ì¸
            )
        )
        session_result = await db.execute(session_stmt)
        session = session_result.scalar_one_or_none()
        
        if not session:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}"
            )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
        history_stmt = (
            select(TbChatHistory)
            .where(TbChatHistory.session_id == session_id)
            .order_by(TbChatHistory.created_date.asc())
        )
        history_result = await db.execute(history_stmt)
        history_records = history_result.scalars().all()
        
        # ì‘ë‹µ ë³€í™˜
        messages = []
        for record in history_records:
            messages.append({
                "chat_id": record.chat_id,
                "user_message": record.user_message,
                "assistant_response": record.assistant_response,
                "referenced_documents": record.referenced_documents or [],
                "search_results": record.search_results,
                "model_used": record.model_used,
                "model_parameters": record.model_parameters,
                "created_date": record.created_date.isoformat()
            })
        
        return {
            "session_id": session.session_id,
            "session_name": session.session_name,
            "session_description": session.session_description,
            "user_emp_no": session.user_emp_no,
            "default_container_id": session.default_container_id,
            "allowed_containers": session.allowed_containers,
            "is_active": session.is_active,
            "last_activity": session.last_activity.isoformat(),
            "message_count": session.message_count,
            "created_date": session.created_date.isoformat(),
            "messages": messages
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [AgentSession] ë³µì› ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„¸ì…˜ ë³µì› ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/agent/sessions")
async def list_agent_sessions(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
    limit: int = 20,
    offset: int = 0
):
    """
    ì‚¬ìš©ìì˜ Agent ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ
    """
    try:
        from app.models.chat.chat_models import TbChatSessions
        from sqlalchemy import select
        
        user_emp_no = str(current_user.emp_no)
        
        stmt = (
            select(TbChatSessions)
            .where(TbChatSessions.user_emp_no == user_emp_no)
            .order_by(TbChatSessions.last_activity.desc())
            .limit(limit)
            .offset(offset)
        )
        
        result = await db.execute(stmt)
        sessions = result.scalars().all()
        
        return {
            "sessions": [
                {
                    "session_id": s.session_id,
                    "session_name": s.session_name,
                    "last_activity": s.last_activity.isoformat(),
                    "message_count": s.message_count,
                    "is_active": s.is_active
                }
                for s in sessions
            ],
            "total": len(sessions),
            "limit": limit,
            "offset": offset
        }
        
    except Exception as e:
        logger.error(f"âŒ [AgentSession] ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/agent/health")
async def agent_health():
    """Agent ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "agent": paper_search_agent.name,
        "version": paper_search_agent.version,
        "tools": list(paper_search_agent.tools.keys()),
        "timestamp": datetime.utcnow().isoformat()
    }

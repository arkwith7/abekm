"""Azure CLIP ëª¨ë¸ ì§ì ‘ í…ŒìŠ¤íŠ¸

Hugging Faceì˜ CLIP ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ì—¬
Azure ë°°í¬ì™€ ë™ì¼í•œ ê¸°ëŠ¥ êµ¬í˜„
"""
import asyncio
from typing import List, Optional
import logging

logger = logging.getLogger(__name__)

class LocalCLIPService:
    """ë¡œì»¬ CLIP ëª¨ë¸ ì„œë¹„ìŠ¤ (Fallback)"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.device = "cpu"
        self._initialized = False
    
    def _initialize(self):
        """CLIP ëª¨ë¸ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        
        try:
            from transformers import CLIPProcessor, CLIPModel
            import torch
            
            print("ğŸ”„ Hugging Face CLIP ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # ViT-B/32 ëª¨ë¸ (Azureì™€ ë™ì¼)
            model_name = "openai/clip-vit-base-patch32"
            self.processor = CLIPProcessor.from_pretrained(model_name)
            self.model = CLIPModel.from_pretrained(model_name)
            
            # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©
            if torch.cuda.is_available():
                self.device = "cuda"
                self.model = self.model.to(self.device)
            
            self._initialized = True
            print(f"âœ… CLIP ëª¨ë¸ ë¡œë”© ì™„ë£Œ (device: {self.device})")
            
        except ImportError:
            print("âŒ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: pip install transformers torch pillow")
            raise
        except Exception as e:
            print(f"âŒ CLIP ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def generate_text_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        self._initialize()
        
        import torch
        
        with torch.no_grad():
            inputs = self.processor(text=[text], return_tensors="pt", padding=True)
            
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            text_features = self.model.get_text_features(**inputs)
            
            # L2 ì •ê·œí™”
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            # CPUë¡œ ì´ë™ ë° ë¦¬ìŠ¤íŠ¸ ë³€í™˜
            embedding = text_features.cpu().numpy()[0].tolist()
            
            print(f"âœ… í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±: {len(embedding)}d")
            return embedding
    
    def generate_image_embedding(self, image_bytes: bytes) -> List[float]:
        """ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±"""
        self._initialize()
        
        from PIL import Image
        import io
        import torch
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(io.BytesIO(image_bytes))
        
        with torch.no_grad():
            inputs = self.processor(images=image, return_tensors="pt")
            
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            image_features = self.model.get_image_features(**inputs)
            
            # L2 ì •ê·œí™”
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # CPUë¡œ ì´ë™ ë° ë¦¬ìŠ¤íŠ¸ ë³€í™˜
            embedding = image_features.cpu().numpy()[0].tolist()
            
            print(f"âœ… ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±: {len(embedding)}d")
            return embedding
    
    def compute_similarity(self, text: str, image_bytes: bytes) -> float:
        """í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°"""
        text_emb = self.generate_text_embedding(text)
        image_emb = self.generate_image_embedding(image_bytes)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë‚´ì )
        import numpy as np
        similarity = np.dot(text_emb, image_emb)
        
        return float(similarity)

async def test_local_clip():
    """ë¡œì»¬ CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("="*80)
    print("ë¡œì»¬ CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    try:
        clip_service = LocalCLIPService()
        
        # í…ŒìŠ¤íŠ¸ 1: í…ìŠ¤íŠ¸ ì„ë² ë”©
        print("\n[í…ŒìŠ¤íŠ¸ 1] í…ìŠ¤íŠ¸ ì„ë² ë”©")
        print("-"*60)
        texts = [
            "íŒŒë€ìƒ‰ ìë™ì°¨",
            "ë¹¨ê°„ìƒ‰ ìŠ¤í¬ì¸ ì¹´",
            "ê³ ì–‘ì´ê°€ ì†ŒíŒŒì— ì•‰ì•„ìˆë‹¤"
        ]
        
        text_embeddings = []
        for text in texts:
            emb = clip_service.generate_text_embedding(text)
            text_embeddings.append(emb)
            print(f"'{text}': {len(emb)}d")
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        import numpy as np
        print("\ní…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„:")
        for i in range(len(texts)):
            for j in range(i+1, len(texts)):
                sim = np.dot(text_embeddings[i], text_embeddings[j])
                print(f"  '{texts[i]}' <-> '{texts[j]}': {sim:.4f}")
        
        # í…ŒìŠ¤íŠ¸ 2: ì´ë¯¸ì§€ ì„ë² ë”©
        print("\n[í…ŒìŠ¤íŠ¸ 2] ì´ë¯¸ì§€ ì„ë² ë”©")
        print("-"*60)
        
        from PIL import Image
        import io
        
        # íŒŒë€ìƒ‰ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (224, 224), color='blue')
        img_bytes = io.BytesIO()
        img.save(img_bytes, format='PNG')
        img_bytes = img_bytes.getvalue()
        
        image_emb = clip_service.generate_image_embedding(img_bytes)
        print(f"ì´ë¯¸ì§€ ì„ë² ë”©: {len(image_emb)}d")
        
        # í…ŒìŠ¤íŠ¸ 3: í¬ë¡œìŠ¤ ëª¨ë‹¬ ìœ ì‚¬ë„
        print("\n[í…ŒìŠ¤íŠ¸ 3] í¬ë¡œìŠ¤ ëª¨ë‹¬ ìœ ì‚¬ë„ (í…ìŠ¤íŠ¸ <-> ì´ë¯¸ì§€)")
        print("-"*60)
        
        for text in texts:
            similarity = clip_service.compute_similarity(text, img_bytes)
            print(f"'{text}' <-> íŒŒë€ìƒ‰ ì´ë¯¸ì§€: {similarity:.4f}")
        
        print("\nâœ… ë¡œì»¬ CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("\nğŸ’¡ ì´ ë¡œì»¬ CLIP ì„œë¹„ìŠ¤ë¥¼ Azure CLIP ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except ImportError as e:
        print(f"\nâŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:")
        print("   pip install transformers torch pillow")
        return False
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    asyncio.run(test_local_clip())

"""
ê¸°ì¡´ Azure ì„ë² ë”©ì„ AWS Bedrockìœ¼ë¡œ ì¬ìƒì„±í•˜ëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/migrate_embeddings_to_aws.py --batch-size 10 --dry-run
    python scripts/migrate_embeddings_to_aws.py --batch-size 10  # ì‹¤ì œ ì‹¤í–‰
"""

import asyncio
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import select, update, text
from sqlalchemy.ext.asyncio import AsyncSession
from app.core.database import get_async_session_local
from app.models.document.multimodal_models import DocEmbedding
from app.models.document.vector_models import VsDocContentsChunks
from app.services.core.embedding_service import embedding_service
from app.core.config import settings
import logging
import argparse
from typing import List, Dict, Any
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def migrate_doc_embeddings(
    session: AsyncSession,
    batch_size: int = 10,
    dry_run: bool = False
) -> Dict[str, int]:
    """
    doc_embedding í…Œì´ë¸”ì˜ Azure ì„ë² ë”©ì„ AWSë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
    
    Args:
        session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        dry_run: í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì—¬ë¶€
    
    Returns:
        ë§ˆì´ê·¸ë ˆì´ì…˜ í†µê³„
    """
    stats = {
        "total": 0,
        "migrated": 0,
        "skipped": 0,
        "failed": 0
    }
    
    # Azure ì„ë² ë”©ë§Œ ì¡°íšŒ (AWS ì„ë² ë”©ì´ ì—†ëŠ” ê²ƒë“¤)
    query = select(DocEmbedding).where(
        DocEmbedding.provider == 'azure',
        DocEmbedding.azure_vector_1536.isnot(None),
        DocEmbedding.aws_vector_1024.is_(None)
    ).limit(1000)  # ì•ˆì „ì¥ì¹˜
    
    result = await session.execute(query)
    embeddings = result.scalars().all()
    stats["total"] = len(embeddings)
    
    logger.info(f"ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ìƒ: {stats['total']}ê°œ")
    
    if dry_run:
        logger.info("âš ï¸ DRY-RUN ëª¨ë“œ: ì‹¤ì œ ë³€ê²½ ì—†ìŒ")
        return stats
    
    # ë°°ì¹˜ ì²˜ë¦¬
    for i in range(0, len(embeddings), batch_size):
        batch = embeddings[i:i+batch_size]
        logger.info(f"ğŸ“¦ ë°°ì¹˜ {i//batch_size + 1}/{(len(embeddings)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...")
        
        for emb in batch:
            try:
                # ì²­í¬ í…ìŠ¤íŠ¸ ì¡°íšŒ (doc_chunk í…Œì´ë¸”ì—ì„œ)
                try:
                    from app.models.document.multimodal_models import DocChunk
                except ImportError:
                    # í´ë°±: SQLë¡œ ì§ì ‘ ì¡°íšŒ
                    chunk_result = await session.execute(
                        text("SELECT content_text FROM doc_chunk WHERE chunk_id = :chunk_id"),
                        {"chunk_id": emb.chunk_id}
                    )
                    row = chunk_result.fetchone()
                    content_text = row[0] if row else None
                else:
                    chunk_result = await session.execute(
                        select(DocChunk).where(DocChunk.chunk_id == emb.chunk_id)
                    )
                    chunk = chunk_result.scalar_one_or_none()
                    content_text = chunk.content_text if chunk else None
                
                if not content_text:
                    logger.warning(f"âš ï¸ ì²­í¬ {emb.chunk_id} í…ìŠ¤íŠ¸ ì—†ìŒ - ìŠ¤í‚µ")
                    stats["skipped"] += 1
                    continue
                
                # AWS Bedrock ì„ë² ë”© ìƒì„± (ê°•ì œë¡œ Bedrock ì‚¬ìš©)
                logger.info(f"ğŸ”„ ì²­í¬ {emb.chunk_id} AWS ì„ë² ë”© ìƒì„± ì¤‘...")
                # ì„ì‹œë¡œ providerë¥¼ bedrockìœ¼ë¡œ ë³€ê²½í•˜ì—¬ AWS ì„ë² ë”© ë³´ì¥
                original_provider = embedding_service.default_provider
                embedding_service.default_provider = 'bedrock'
                aws_vector = await embedding_service.get_embedding(content_text)
                embedding_service.default_provider = original_provider
                
                if len(aws_vector) != 1024:
                    logger.error(f"âŒ ì˜ëª»ëœ ì°¨ì›: {len(aws_vector)} (ì˜ˆìƒ: 1024)")
                    stats["failed"] += 1
                    continue
                
                # AWS ë²¡í„° ì—…ë°ì´íŠ¸
                emb.aws_vector_1024 = aws_vector
                emb.provider = 'aws'  # ë˜ëŠ” 'hybrid'ë¡œ ì„¤ì •
                
                stats["migrated"] += 1
                logger.info(f"âœ… ì²­í¬ {emb.chunk_id} ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ({stats['migrated']}/{stats['total']})")
                
            except Exception as e:
                logger.error(f"âŒ ì²­í¬ {emb.chunk_id} ì‹¤íŒ¨: {e}")
                stats["failed"] += 1
        
        # ë°°ì¹˜ ì»¤ë°‹
        await session.commit()
        logger.info(f"ğŸ’¾ ë°°ì¹˜ ì»¤ë°‹ ì™„ë£Œ")
        
        # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
        await asyncio.sleep(1)
    
    return stats


async def migrate_vs_chunks(
    session: AsyncSession,
    batch_size: int = 10,
    dry_run: bool = False
) -> Dict[str, int]:
    """
    vs_doc_contents_chunks í…Œì´ë¸”ì˜ Azure ì„ë² ë”©ì„ AWSë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
    """
    stats = {
        "total": 0,
        "migrated": 0,
        "skipped": 0,
        "failed": 0
    }
    
    # Azure ì„ë² ë”©ë§Œ ì¡°íšŒ
    query = select(VsDocContentsChunks).where(
        VsDocContentsChunks.embedding_provider == 'azure',
        VsDocContentsChunks.azure_embedding_1536.isnot(None),
        VsDocContentsChunks.aws_embedding_1024.is_(None)
    ).limit(1000)
    
    result = await session.execute(query)
    chunks = result.scalars().all()
    stats["total"] = len(chunks)
    
    logger.info(f"ğŸ” vs_chunks ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ìƒ: {stats['total']}ê°œ")
    
    if dry_run:
        return stats
    
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        logger.info(f"ğŸ“¦ ë°°ì¹˜ {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...")
        
        for chunk in batch:
            try:
                if not chunk.chunk_text:
                    stats["skipped"] += 1
                    continue
                
                # AWS ì„ë² ë”© ìƒì„± (ê°•ì œë¡œ Bedrock ì‚¬ìš©)
                original_provider = embedding_service.default_provider
                embedding_service.default_provider = 'bedrock'
                aws_vector = await embedding_service.get_embedding(chunk.chunk_text)
                embedding_service.default_provider = original_provider
                
                if len(aws_vector) != 1024:
                    logger.error(f"âŒ ì˜ëª»ëœ ì°¨ì›: {len(aws_vector)}")
                    stats["failed"] += 1
                    continue
                
                # ì—…ë°ì´íŠ¸
                chunk.aws_embedding_1024 = aws_vector
                chunk.embedding_provider = 'aws'
                
                stats["migrated"] += 1
                logger.info(f"âœ… vs_chunk {chunk.chunk_sno} ì™„ë£Œ ({stats['migrated']}/{stats['total']})")
                
            except Exception as e:
                logger.error(f"âŒ ì‹¤íŒ¨: {e}")
                stats["failed"] += 1
        
        await session.commit()
        await asyncio.sleep(1)
    
    return stats


async def main():
    parser = argparse.ArgumentParser(description="Azure â†’ AWS ì„ë² ë”© ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--batch-size", type=int, default=10, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 10)")
    parser.add_argument("--dry-run", action="store_true", help="í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë³€ê²½ ì—†ìŒ)")
    parser.add_argument("--table", choices=["doc_embedding", "vs_chunks", "all"], 
                        default="all", help="ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ìƒ í…Œì´ë¸”")
    
    args = parser.parse_args()
    
    logger.info("="*80)
    logger.info("ğŸš€ AWS ì„ë² ë”© ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    logger.info(f"ğŸ“Š ì„¤ì •: batch_size={args.batch_size}, dry_run={args.dry_run}, table={args.table}")
    logger.info(f"ğŸ”§ ì„ë² ë”© í”„ë¡œë°”ì´ë”: {settings.default_embedding_provider}")
    logger.info(f"ğŸ“ ë²¡í„° ì°¨ì›: {settings.vector_dimension}")
    logger.info("="*80)
    
    # ì„ë² ë”© í”„ë¡œë°”ì´ë” í™•ì¸
    if settings.default_embedding_provider != 'bedrock':
        logger.error("âŒ DEFAULT_EMBEDDING_PROVIDERë¥¼ 'bedrock'ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    async_session_local = get_async_session_local()
    async with async_session_local() as session:
        start_time = datetime.now()
        
        # doc_embedding ë§ˆì´ê·¸ë ˆì´ì…˜
        if args.table in ["doc_embedding", "all"]:
            logger.info("\nğŸ“„ doc_embedding í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
            doc_stats = await migrate_doc_embeddings(session, args.batch_size, args.dry_run)
            logger.info(f"âœ… doc_embedding ì™„ë£Œ: {doc_stats}")
        
        # vs_doc_contents_chunks ë§ˆì´ê·¸ë ˆì´ì…˜
        if args.table in ["vs_chunks", "all"]:
            logger.info("\nğŸ“„ vs_doc_contents_chunks í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
            vs_stats = await migrate_vs_chunks(session, args.batch_size, args.dry_run)
            logger.info(f"âœ… vs_chunks ì™„ë£Œ: {vs_stats}")
        
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    
    logger.info("="*80)
    logger.info("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
    logger.info("="*80)


if __name__ == "__main__":
    asyncio.run(main())

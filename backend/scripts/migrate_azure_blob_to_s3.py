"""
Azure Blob Storage â†’ AWS S3 íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

ëª©ì :
- Azure Blob Storageì˜ ëª¨ë“  ì»¨í…Œì´ë„ˆ(raw, intermediate, derived)ë¥¼ S3ë¡œ ë³µì‚¬
- ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (ë™ì¼í•œ prefix êµ¬ì¡°)
- ë©€í‹°ëª¨ë‹¬ ê°ì²´ ì´ë¯¸ì§€ í¬í•¨ ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜

ì‚¬ìš©ë²•:
    python scripts/migrate_azure_blob_to_s3.py --dry-run  # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    python scripts/migrate_azure_blob_to_s3.py            # ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜
"""

import asyncio
import sys
import os
from pathlib import Path
from typing import List, Dict, Tuple
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.core.config import settings
import boto3

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from azure.storage.blob import BlobServiceClient
except ImportError:
    logger.error("âŒ azure-storage-blob ë¯¸ì„¤ì¹˜. pip install azure-storage-blob í•„ìš”")
    sys.exit(1)


class AzureToS3Migrator:
    """Azure Blob â†’ S3 ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    def __init__(self, dry_run: bool = False):
        self.dry_run = dry_run
        
        # Azure Blob í´ë¼ì´ì–¸íŠ¸
        self.azure_conn_str = settings.azure_blob_connection_string or os.getenv("AZURE_BLOB_CONNECTION_STRING")
        if not self.azure_conn_str:
            account_name = settings.azure_blob_account_name
            account_key = settings.azure_blob_account_key
            if not account_name or not account_key:
                raise RuntimeError("Azure Blob ì¸ì¦ ì •ë³´ ì—†ìŒ (connection string or account name/key)")
            self.azure_conn_str = f"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net"
        
        self.azure_client = BlobServiceClient.from_connection_string(self.azure_conn_str)
        
        # S3 í´ë¼ì´ì–¸íŠ¸
        self.s3_client = boto3.client(
            's3',
            region_name=settings.aws_region,
            aws_access_key_id=settings.aws_access_key_id,
            aws_secret_access_key=settings.aws_secret_access_key
        )
        self.s3_bucket = settings.aws_s3_bucket
        
        # ì»¨í…Œì´ë„ˆ ë§¤í•‘ (Azure container â†’ S3 prefix)
        self.container_map = {
            settings.azure_blob_container_raw: "",  # S3ëŠ” ë²„í‚· ë£¨íŠ¸ì— ë°”ë¡œ ì €ìž¥
            settings.azure_blob_container_intermediate: "",
            settings.azure_blob_container_derived: ""
        }
        
        logger.info(f"ðŸ“¦ Azure ê³„ì •: {settings.azure_blob_account_name}")
        logger.info(f"ðŸ“¦ S3 ë²„í‚·: {self.s3_bucket}")
        logger.info(f"ðŸŒ S3 ë¦¬ì „: {settings.aws_region}")
        if dry_run:
            logger.info("âš ï¸ DRY-RUN ëª¨ë“œ í™œì„±í™”")
    
    def list_azure_blobs(self, container_name: str) -> List[Tuple[str, int]]:
        """Azure ì»¨í…Œì´ë„ˆì˜ ëª¨ë“  Blob ëª©ë¡ ì¡°íšŒ"""
        try:
            container_client = self.azure_client.get_container_client(container_name)
            blobs = []
            
            for blob in container_client.list_blobs():
                blobs.append((blob.name, blob.size))
            
            logger.info(f"âœ… Azure ì»¨í…Œì´ë„ˆ '{container_name}': {len(blobs)}ê°œ íŒŒì¼")
            return blobs
            
        except Exception as e:
            logger.error(f"âŒ Azure ì»¨í…Œì´ë„ˆ '{container_name}' ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def copy_blob_to_s3(self, azure_container: str, blob_name: str, blob_size: int) -> bool:
        """ë‹¨ì¼ Blobì„ S3ë¡œ ë³µì‚¬"""
        try:
            # Azureì—ì„œ ë‹¤ìš´ë¡œë“œ
            blob_client = self.azure_client.get_blob_client(
                container=azure_container,
                blob=blob_name
            )
            
            if self.dry_run:
                logger.info(f"[DRY-RUN] {azure_container}/{blob_name} â†’ s3://{self.s3_bucket}/{blob_name} ({blob_size:,} bytes)")
                return True
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
            blob_data = blob_client.download_blob()
            file_bytes = blob_data.readall()
            
            # S3ì— ì—…ë¡œë“œ (ë™ì¼í•œ í‚¤ ì‚¬ìš©)
            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key=blob_name,
                Body=file_bytes
            )
            
            logger.info(f"âœ… {blob_name} ({blob_size:,} bytes)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {blob_name} ë³µì‚¬ ì‹¤íŒ¨: {e}")
            return False
    
    async def migrate_container(self, container_name: str) -> Dict[str, int]:
        """ì»¨í…Œì´ë„ˆ ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "total_bytes": 0
        }
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ðŸ“¦ ì»¨í…Œì´ë„ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œìž‘: {container_name}")
        logger.info(f"{'='*80}")
        
        # Azure Blob ëª©ë¡ ì¡°íšŒ
        blobs = self.list_azure_blobs(container_name)
        stats["total"] = len(blobs)
        
        if not blobs:
            logger.warning(f"âš ï¸ ì»¨í…Œì´ë„ˆ '{container_name}'ì— íŒŒì¼ ì—†ìŒ")
            return stats
        
        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = 10
        for i in range(0, len(blobs), batch_size):
            batch = blobs[i:i+batch_size]
            
            for blob_name, blob_size in batch:
                stats["total_bytes"] += blob_size
                
                if self.copy_blob_to_s3(container_name, blob_name, blob_size):
                    stats["success"] += 1
                else:
                    stats["failed"] += 1
            
            # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
            if not self.dry_run:
                await asyncio.sleep(0.5)
        
        logger.info(f"\nâœ… ì»¨í…Œì´ë„ˆ '{container_name}' ì™„ë£Œ: {stats['success']}/{stats['total']} ì„±ê³µ")
        return stats
    
    async def migrate_all(self) -> Dict[str, Dict[str, int]]:
        """ëª¨ë“  ì»¨í…Œì´ë„ˆ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("\n" + "="*80)
        logger.info("ðŸš€ Azure Blob â†’ S3 ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œìž‘")
        logger.info("="*80)
        
        results = {}
        
        for container_name in self.container_map.keys():
            stats = await self.migrate_container(container_name)
            results[container_name] = stats
        
        # ì „ì²´ í†µê³„
        total_files = sum(s["total"] for s in results.values())
        total_success = sum(s["success"] for s in results.values())
        total_failed = sum(s["failed"] for s in results.values())
        total_bytes = sum(s["total_bytes"] for s in results.values())
        
        logger.info("\n" + "="*80)
        logger.info("ðŸŽ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
        logger.info("="*80)
        logger.info(f"ðŸ“Š ì „ì²´ íŒŒì¼: {total_files}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {total_success}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {total_failed}ê°œ")
        logger.info(f"ðŸ’¾ ì „ì†¡ëŸ‰: {total_bytes / (1024**2):.2f} MB")
        logger.info("="*80)
        
        return results


async def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Azure Blob â†’ S3 ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--dry-run", action="store_true", help="í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹¤ì œ ë³µì‚¬ ì—†ìŒ)")
    
    args = parser.parse_args()
    
    migrator = AzureToS3Migrator(dry_run=args.dry_run)
    results = await migrator.migrate_all()
    
    # ì‹¤íŒ¨í•œ íŒŒì¼ ìžˆìœ¼ë©´ ì¢…ë£Œ ì½”ë“œ 1
    total_failed = sum(s["failed"] for s in results.values())
    sys.exit(1 if total_failed > 0 else 0)


if __name__ == "__main__":
    asyncio.run(main())

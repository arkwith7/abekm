"""
ë…¼ë¬¸ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

KIPRIS ë°ì´í„°ì…‹(100ê±´ ìƒ˜í”Œ)ì„ ì‚¬ìš©í•˜ì—¬ ì„ í–‰ê¸°ìˆ  íƒì§€ ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python -m backend.scripts.experiments.run_paper_experiment
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.app.evaluation.prior_art_evaluator import PriorArtEvaluator


def load_sample_dataset(file_path: str) -> List[Dict[str, Any]]:
    """
    ì‹¤í—˜ìš© ìƒ˜í”Œ ë°ì´í„°ì…‹ ë¡œë“œ
    
    Args:
        file_path: JSONL íŒŒì¼ ê²½ë¡œ
    
    Returns:
        í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    test_cases = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            
            # ë°ì´í„° í˜•ì‹ ë³€í™˜
            # paper_eval_sample.jsonl í˜•ì‹:
            # {"target": {...}, "ground_truth": [...], "dataset_source_path": "..."}
            
            test_case = {
                'patent_id': data['target']['application_number'],
                'title': data['target']['title'],
                'abstract': data['target'].get('abstract', ''),
                'ipc': data['target'].get('ipc', ''),
                'ground_truth': data['ground_truth'],
                # predictionsëŠ” ë‚˜ì¤‘ì— ì—ì´ì „íŠ¸ ì‹¤í–‰ í›„ ì¶”ê°€
                'predictions': []
            }
            
            test_cases.append(test_case)
    
    return test_cases


async def run_prior_art_search_mock(
    patent_data: Dict[str, Any],
    top_k: int = 100
) -> List[str]:
    """
    ì„ í–‰ê¸°ìˆ  ê²€ìƒ‰ ì‹¤í–‰ (Mock ë²„ì „)
    
    ì‹¤ì œ ì—ì´ì „íŠ¸ êµ¬í˜„ì´ ì™„ë£Œë˜ë©´ ì´ í•¨ìˆ˜ë¥¼ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤.
    í˜„ì¬ëŠ” Ground Truthì˜ ì¼ë¶€ + ëœë¤ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        patent_data: íŠ¹í—ˆ ë°ì´í„°
        top_k: ìƒìœ„ Kê°œ ê²°ê³¼
    
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (íŠ¹í—ˆë²ˆí˜¸)
    """
    # TODO: ì‹¤ì œ ì—ì´ì „íŠ¸ í˜¸ì¶œë¡œ ëŒ€ì²´
    # from backend.app.agents.prior_art_agent import PriorArtAgent
    # agent = PriorArtAgent()
    # results = await agent.search_prior_art(...)
    
    print(f"  [MOCK] {patent_data['patent_id']} ê²€ìƒ‰ ì¤‘...")
    
    # Mock êµ¬í˜„: Ground Truthì˜ 50% + ë”ë¯¸ ê²°ê³¼
    ground_truth = patent_data.get('ground_truth', [])
    
    # Ground Truth ì¤‘ 50% í¬í•¨ (Mock ì„±ëŠ¥: Recall@100 â‰ˆ 50%)
    predictions = ground_truth[:len(ground_truth)//2]
    
    # ë”ë¯¸ ê²°ê³¼ë¡œ 100ê°œ ì±„ìš°ê¸°
    dummy_results = [
        f"US2005{str(i).zfill(7)} A1" for i in range(top_k - len(predictions))
    ]
    
    predictions.extend(dummy_results)
    
    await asyncio.sleep(0.1)  # ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜
    
    return predictions[:top_k]


async def run_experiment_batch(
    test_cases: List[Dict[str, Any]],
    top_k: int = 100
) -> List[Dict[str, Any]]:
    """
    ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰
    
    Args:
        test_cases: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        top_k: ìƒìœ„ Kê°œ ê²°ê³¼
    
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼ê°€ í¬í•¨ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    
    print(f"\nğŸš€ ì‹¤í—˜ ì‹œì‘: {len(test_cases)}ê±´")
    print("="*70)
    
    for i, case in enumerate(test_cases, start=1):
        print(f"\n[{i}/{len(test_cases)}] {case['patent_id']}")
        print(f"  ì œëª©: {case['title'][:60]}...")
        print(f"  Ground Truth: {len(case['ground_truth'])}ê±´")
        
        # ì„ í–‰ê¸°ìˆ  ê²€ìƒ‰ ì‹¤í–‰
        predictions = await run_prior_art_search_mock(case, top_k=top_k)
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'patent_id': case['patent_id'],
            'ground_truth': case['ground_truth'],
            'predictions': predictions
        }
        
        results.append(result)
        
        print(f"  Predictions: {len(predictions)}ê±´")
    
    print("\n" + "="*70)
    print("âœ… ì‹¤í—˜ ì™„ë£Œ!")
    
    return results


def save_results(
    results: List[Dict[str, Any]],
    summary: Any,
    output_dir: Path
) -> None:
    """
    ì‹¤í—˜ ê²°ê³¼ ì €ì¥
    
    Args:
        results: ê°œë³„ í‰ê°€ ê²°ê³¼
        summary: í‰ê°€ ìš”ì•½
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. ì „ì²´ ê²°ê³¼ (ìƒì„¸)
    results_file = output_dir / f"experiment_results_{timestamp}.json"
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'timestamp': timestamp,
                'num_cases': len(results),
                'k_values': [10, 20, 50, 100]
            },
            'summary': summary.dict(),
            'details': [r.dict() for r in results]
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
    
    # 2. ìš”ì•½ ê²°ê³¼ (ê°„ê²°)
    summary_file = output_dir / f"experiment_summary_{timestamp}.json"
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary.dict(), f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ìš”ì•½ ì €ì¥: {summary_file}")
    
    # 3. ìµœì‹  ê²°ê³¼ ë§í¬ (latest)
    latest_file = output_dir / "experiment_results_latest.json"
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'timestamp': timestamp,
                'num_cases': len(results),
                'k_values': [10, 20, 50, 100]
            },
            'summary': summary.dict(),
            'details': [r.dict() for r in results]
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ìµœì‹  ê²°ê³¼: {latest_file}")


async def main():
    """ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("\n" + "="*70)
    print("ğŸ“„ ë…¼ë¬¸ ì‹¤í—˜: ì„ í–‰ê¸°ìˆ  íƒì§€ ì„±ëŠ¥ í‰ê°€")
    print("="*70)
    
    # 1. ë°ì´í„° ë¡œë“œ
    data_path = Path("/home/arkwith/Dev/abekm/backend/data/processed/fulltext/paper_eval_sample.jsonl")
    
    if not data_path.exists():
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return
    
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ: {data_path}")
    test_cases = load_sample_dataset(str(data_path))
    print(f"âœ… {len(test_cases)}ê±´ ë¡œë“œ ì™„ë£Œ")
    
    # í†µê³„ ì¶œë ¥
    total_gt = sum(len(case['ground_truth']) for case in test_cases)
    avg_gt = total_gt / len(test_cases) if test_cases else 0
    
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ í†µê³„:")
    print(f"  - í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê±´")
    print(f"  - ì´ Ground Truth: {total_gt}ê±´")
    print(f"  - í‰ê·  Ground Truth: {avg_gt:.2f}ê±´/ì¼€ì´ìŠ¤")
    
    # 2. ì‹¤í—˜ ì‹¤í–‰
    results = await run_experiment_batch(test_cases, top_k=100)
    
    # 3. í‰ê°€
    print("\n" + "="*70)
    print("ğŸ“Š í‰ê°€ ì¤‘...")
    print("="*70)
    
    evaluator = PriorArtEvaluator(k_values=[10, 20, 50, 100])
    evaluation = evaluator.evaluate_batch(results)
    
    summary = evaluation['summary']
    details = evaluation['details']
    
    # 4. ê²°ê³¼ ì¶œë ¥
    evaluator.print_summary(summary)
    
    # 5. ê²°ê³¼ ì €ì¥
    output_dir = Path("/home/arkwith/Dev/abekm/backend/results/paper_experiment")
    save_results(details, summary, output_dir)
    
    print("\nâœ… ì‹¤í—˜ ì™„ë£Œ!\n")


if __name__ == "__main__":
    asyncio.run(main())

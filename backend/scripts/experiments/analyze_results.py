"""
ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ì €ì¥ëœ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Any
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))


def load_results(results_file: str) -> Dict[str, Any]:
    """ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ"""
    with open(results_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def analyze_per_case_performance(details: List[Dict]) -> Dict[str, Any]:
    """ì¼€ì´ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„"""
    
    recall_100_list = [d['recall_at_k']['100'] for d in details]
    precision_10_list = [d['precision_at_k']['10'] for d in details]
    ap_list = [d['average_precision'] for d in details]
    
    analysis = {
        'recall_100': {
            'min': float(np.min(recall_100_list)),
            'max': float(np.max(recall_100_list)),
            'median': float(np.median(recall_100_list)),
            'q25': float(np.percentile(recall_100_list, 25)),
            'q75': float(np.percentile(recall_100_list, 75)),
            'distribution': {
                '0-20%': sum(1 for r in recall_100_list if r < 0.2),
                '20-40%': sum(1 for r in recall_100_list if 0.2 <= r < 0.4),
                '40-60%': sum(1 for r in recall_100_list if 0.4 <= r < 0.6),
                '60-80%': sum(1 for r in recall_100_list if 0.6 <= r < 0.8),
                '80-100%': sum(1 for r in recall_100_list if r >= 0.8)
            }
        },
        'precision_10': {
            'min': float(np.min(precision_10_list)),
            'max': float(np.max(precision_10_list)),
            'median': float(np.median(precision_10_list)),
        },
        'average_precision': {
            'min': float(np.min(ap_list)),
            'max': float(np.max(ap_list)),
            'median': float(np.median(ap_list)),
        }
    }
    
    return analysis


def analyze_ground_truth_distribution(details: List[Dict]) -> Dict[str, Any]:
    """Ground Truth ë¶„í¬ ë¶„ì„"""
    
    gt_counts = [len(d['ground_truth']) for d in details]
    
    distribution = {
        '1ê°œ': sum(1 for c in gt_counts if c == 1),
        '2ê°œ': sum(1 for c in gt_counts if c == 2),
        '3-5ê°œ': sum(1 for c in gt_counts if 3 <= c <= 5),
        '6-10ê°œ': sum(1 for c in gt_counts if 6 <= c <= 10),
        '10ê°œ ì´ìƒ': sum(1 for c in gt_counts if c > 10)
    }
    
    return {
        'total': sum(gt_counts),
        'mean': float(np.mean(gt_counts)),
        'median': float(np.median(gt_counts)),
        'min': int(np.min(gt_counts)),
        'max': int(np.max(gt_counts)),
        'distribution': distribution
    }


def find_best_worst_cases(details: List[Dict], top_n: int = 5) -> Dict[str, List[Dict]]:
    """ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€/ë‚˜ìœ ì¼€ì´ìŠ¤ ì°¾ê¸°"""
    
    # Recall@100 ê¸°ì¤€ ì •ë ¬
    sorted_by_recall = sorted(
        details,
        key=lambda x: x['recall_at_k']['100'],
        reverse=True
    )
    
    best_cases = [
        {
            'patent_id': case['patent_id'],
            'recall_100': case['recall_at_k']['100'],
            'precision_10': case['precision_at_k']['10'],
            'ap': case['average_precision'],
            'gt_count': len(case['ground_truth'])
        }
        for case in sorted_by_recall[:top_n]
    ]
    
    worst_cases = [
        {
            'patent_id': case['patent_id'],
            'recall_100': case['recall_at_k']['100'],
            'precision_10': case['precision_at_k']['10'],
            'ap': case['average_precision'],
            'gt_count': len(case['ground_truth'])
        }
        for case in sorted_by_recall[-top_n:]
    ]
    
    return {
        'best': best_cases,
        'worst': worst_cases
    }


def compare_with_baselines(summary: Dict) -> Dict[str, Any]:
    """Baselineê³¼ ë¹„êµ"""
    
    # ê°€ìƒì˜ Baseline ì„±ëŠ¥ (ë…¼ë¬¸ ê¸°ë°˜)
    baselines = {
        'Boolean Search': {
            'recall_10': 0.12,
            'recall_20': 0.18,
            'recall_50': 0.35,
            'recall_100': 0.52,
            'precision_10': 0.18,
            'map': 0.28
        },
        'ChatGPT-4o RAG': {
            'recall_10': 0.08,
            'recall_20': 0.15,
            'recall_50': 0.28,
            'recall_100': 0.45,
            'precision_10': 0.12,
            'map': 0.22
        },
        'Patsnap Agent': {
            'recall_10': 0.20,
            'recall_20': 0.30,
            'recall_50': 0.55,
            'recall_100': 0.81,
            'precision_10': 0.30,
            'map': 0.48
        }
    }
    
    # í˜„ì¬ ì‹œìŠ¤í…œ ì„±ëŠ¥
    current = {
        'recall_10': summary['avg_recall']['10'],
        'recall_20': summary['avg_recall']['20'],
        'recall_50': summary['avg_recall']['50'],
        'recall_100': summary['avg_recall']['100'],
        'precision_10': summary['avg_precision']['10'],
        'map': summary['mean_average_precision']
    }
    
    # ë¹„êµ
    comparison = {}
    
    for baseline_name, baseline_metrics in baselines.items():
        improvements = {}
        
        for metric_name, baseline_value in baseline_metrics.items():
            current_value = current[metric_name]
            improvement = current_value - baseline_value
            improvement_pct = (improvement / baseline_value * 100) if baseline_value > 0 else 0
            
            improvements[metric_name] = {
                'baseline': baseline_value,
                'current': current_value,
                'improvement': improvement,
                'improvement_pct': improvement_pct
            }
        
        comparison[baseline_name] = improvements
    
    return comparison


def print_analysis_report(
    summary: Dict,
    per_case_analysis: Dict,
    gt_distribution: Dict,
    best_worst_cases: Dict,
    baseline_comparison: Dict
):
    """ë¶„ì„ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    
    print("\n" + "="*80)
    print("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸")
    print("="*80)
    
    # 1. ì „ì²´ ìš”ì•½
    print("\n### 1. ì „ì²´ ì„±ëŠ¥ ìš”ì•½")
    print("-"*80)
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {summary['num_cases']}ê±´")
    print(f"\nRecall@K:")
    for k in [10, 20, 50, 100]:
        v = summary['avg_recall'][str(k)]
        print(f"  @{k:3d}: {v*100:5.2f}%")
    
    print(f"\nMAP: {summary['mean_average_precision']:.4f}")
    
    target = summary['target_recall_100']
    achieved = summary['avg_recall']['100']
    status = "âœ… ë‹¬ì„±" if summary['achieved_target'] else "âŒ ë¯¸ë‹¬ì„±"
    print(f"\nëª©í‘œ (Recall@100 â‰¥ {target*100:.0f}%): {status} (ì‹¤ì œ: {achieved*100:.2f}%)")
    
    # 2. ì¼€ì´ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
    print("\n### 2. ì¼€ì´ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„")
    print("-"*80)
    
    recall_stats = per_case_analysis['recall_100']
    print(f"\nRecall@100 ë¶„í¬:")
    print(f"  ìµœì†Œ: {recall_stats['min']*100:.2f}%")
    print(f"  Q25: {recall_stats['q25']*100:.2f}%")
    print(f"  ì¤‘ì•™ê°’: {recall_stats['median']*100:.2f}%")
    print(f"  Q75: {recall_stats['q75']*100:.2f}%")
    print(f"  ìµœëŒ€: {recall_stats['max']*100:.2f}%")
    
    print(f"\nì„±ëŠ¥ êµ¬ê°„ë³„ ë¶„í¬:")
    for range_name, count in recall_stats['distribution'].items():
        pct = count / summary['num_cases'] * 100
        print(f"  {range_name}: {count}ê±´ ({pct:.1f}%)")
    
    # 3. Ground Truth ë¶„ì„
    print("\n### 3. Ground Truth ë¶„í¬")
    print("-"*80)
    print(f"ì´ Ground Truth: {gt_distribution['total']}ê±´")
    print(f"í‰ê· : {gt_distribution['mean']:.2f}ê±´/ì¼€ì´ìŠ¤")
    print(f"ì¤‘ì•™ê°’: {gt_distribution['median']:.0f}ê±´")
    print(f"ë²”ìœ„: {gt_distribution['min']}~{gt_distribution['max']}ê±´")
    
    print(f"\nê°œìˆ˜ë³„ ë¶„í¬:")
    for range_name, count in gt_distribution['distribution'].items():
        pct = count / summary['num_cases'] * 100
        print(f"  {range_name}: {count}ê±´ ({pct:.1f}%)")
    
    # 4. Best/Worst ì¼€ì´ìŠ¤
    print("\n### 4. ì„±ëŠ¥ ìš°ìˆ˜/ë¶€ì§„ ì¼€ì´ìŠ¤ (Top 5)")
    print("-"*80)
    
    print("\nğŸ† ê°€ì¥ ìš°ìˆ˜í•œ ì¼€ì´ìŠ¤:")
    for i, case in enumerate(best_worst_cases['best'], start=1):
        print(f"  {i}. {case['patent_id']}")
        print(f"     Recall@100: {case['recall_100']*100:.2f}%, "
              f"Precision@10: {case['precision_10']*100:.2f}%, "
              f"AP: {case['ap']:.4f}, GT: {case['gt_count']}ê±´")
    
    print("\nğŸ“‰ ê°€ì¥ ë¶€ì§„í•œ ì¼€ì´ìŠ¤:")
    for i, case in enumerate(best_worst_cases['worst'], start=1):
        print(f"  {i}. {case['patent_id']}")
        print(f"     Recall@100: {case['recall_100']*100:.2f}%, "
              f"Precision@10: {case['precision_10']*100:.2f}%, "
              f"AP: {case['ap']:.4f}, GT: {case['gt_count']}ê±´")
    
    # 5. Baseline ë¹„êµ
    print("\n### 5. Baseline ì‹œìŠ¤í…œê³¼ì˜ ë¹„êµ")
    print("-"*80)
    
    for baseline_name, metrics in baseline_comparison.items():
        print(f"\n vs. {baseline_name}:")
        
        recall_100 = metrics['recall_100']
        print(f"  Recall@100: {recall_100['baseline']*100:.2f}% â†’ {recall_100['current']*100:.2f}% "
              f"({recall_100['improvement_pct']:+.1f}%p)")
        
        precision_10 = metrics['precision_10']
        print(f"  Precision@10: {precision_10['baseline']*100:.2f}% â†’ {precision_10['current']*100:.2f}% "
              f"({precision_10['improvement_pct']:+.1f}%p)")
        
        map_metric = metrics['map']
        print(f"  MAP: {map_metric['baseline']:.4f} â†’ {map_metric['current']:.4f} "
              f"({map_metric['improvement_pct']:+.1f}%)")
    
    print("\n" + "="*80 + "\n")


def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
    
    # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
    results_dir = Path("/home/arkwith/Dev/abekm/backend/results/paper_experiment")
    results_file = results_dir / "experiment_results_latest.json"
    
    if not results_file.exists():
        print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_file}")
        print("ë¨¼ì € run_paper_experiment.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    print(f"\nğŸ“‚ ê²°ê³¼ íŒŒì¼ ë¡œë“œ: {results_file}")
    
    # ê²°ê³¼ ë¡œë“œ
    results = load_results(str(results_file))
    
    summary = results['summary']
    details = results['details']
    
    # ë¶„ì„ ìˆ˜í–‰
    print("ğŸ“Š ë¶„ì„ ì¤‘...")
    
    per_case_analysis = analyze_per_case_performance(details)
    gt_distribution = analyze_ground_truth_distribution(details)
    best_worst_cases = find_best_worst_cases(details, top_n=5)
    baseline_comparison = compare_with_baselines(summary)
    
    # ë¦¬í¬íŠ¸ ì¶œë ¥
    print_analysis_report(
        summary,
        per_case_analysis,
        gt_distribution,
        best_worst_cases,
        baseline_comparison
    )
    
    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analysis_output = results_dir / "analysis_report.json"
    
    with open(analysis_output, 'w', encoding='utf-8') as f:
        json.dump({
            'per_case_analysis': per_case_analysis,
            'ground_truth_distribution': gt_distribution,
            'best_worst_cases': best_worst_cases,
            'baseline_comparison': baseline_comparison
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {analysis_output}\n")


if __name__ == "__main__":
    main()

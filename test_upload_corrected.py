#!/usr/bin/env python3
"""
í…ŒìŠ¤íŠ¸: ìˆ˜ì •ëœ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¬¸ì„œ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸
==============================================

ëª©ì :
- ìˆ˜ì •ëœ integrated_document_pipeline_serviceê°€ ì˜¬ë°”ë¥¸ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
- vs_doc_contents_indexì™€ tb_document_chunks í…Œì´ë¸” ì‚¬ìš© í™•ì¸
- ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ "text" ì˜¤ë¥˜ í•´ê²° ì—¬ë¶€ ê²€ì¦
"""

import asyncio
import requests
import json
import os
from pathlib import Path

# í…ŒìŠ¤íŠ¸ ì„¤ì •
BASE_URL = "http://localhost:8000"
TEST_FILE_PATH = "/home/admin/wkms-aws/test_document.txt"

async def test_corrected_upload():
    """ìˆ˜ì •ëœ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª ìˆ˜ì •ëœ íŒŒì´í”„ë¼ì¸ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # 1. ë¡œê·¸ì¸
    print("ğŸ” 1ë‹¨ê³„: ì‚¬ìš©ì ë¡œê·¸ì¸")
    login_data = {
        "emp_no": "ADMIN001",  # ì‹œìŠ¤í…œ ê´€ë¦¬ìë¡œ ë¡œê·¸ì¸ (ëª¨ë“  ê¶Œí•œ ë³´ìœ )
        "password": "admin123!"
    }
    
    response = requests.post(
        f"{BASE_URL}/api/v1/auth/login", 
        json=login_data,  # JSON í˜•ì‹ìœ¼ë¡œ ì „ì†¡
        headers={"Content-Type": "application/json"}
    )
    if response.status_code != 200:
        print(f"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨: {response.status_code}")
        print(f"   ì‘ë‹µ: {response.text}")
        return
    
    token_data = response.json()
    token = token_data.get("access_token")
    print(f"âœ… ë¡œê·¸ì¸ ì„±ê³µ - í† í°: {token[:20]}...")
    
    headers = {"Authorization": f"Bearer {token}"}
    
    # 2. ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ
    print("\nğŸ“‚ 2ë‹¨ê³„: ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ")
    response = requests.get(f"{BASE_URL}/api/v1/documents/containers", headers=headers)
    if response.status_code != 200:
        print(f"âŒ ì»¨í…Œì´ë„ˆ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
        return
    
    containers = response.json().get("containers", [])
    print(f"âœ… ì»¨í…Œì´ë„ˆ ì¡°íšŒ ì„±ê³µ - {len(containers)}ê°œ ë°œê²¬")
    
    if not containers:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # ì²« ë²ˆì§¸ ì»¨í…Œì´ë„ˆ ì‚¬ìš©
    container_id = str(containers[0]["container_id"])
    container_name = containers[0].get("container_name", containers[0].get("container_nm", "Unknown"))
    print(f"   ğŸ“ ì‚¬ìš©í•  ì»¨í…Œì´ë„ˆ: {container_name} (ID: {container_id})")
    
    # 3. í…ŒìŠ¤íŠ¸ íŒŒì¼ ì¤€ë¹„
    print("\nğŸ“„ 3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ íŒŒì¼ ì¤€ë¹„")
    if not os.path.exists(TEST_FILE_PATH):
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        test_content = """
ìˆ˜ì •ëœ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
========================

ì´ ë¬¸ì„œëŠ” corrected pipelineì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ë¬¸ì„œì…ë‹ˆë‹¤.

ì£¼ìš” í…ŒìŠ¤íŠ¸ ë‚´ìš©:
1. VsDocContentsIndex í…Œì´ë¸” ì‚¬ìš© í™•ì¸
2. DocumentChunk í…Œì´ë¸” ì‚¬ìš© í™•ì¸  
3. ë²¡í„° ì„ë² ë”© ì €ì¥ í™•ì¸
4. í•œêµ­ì–´ NLP ë¶„ì„ ê²°ê³¼ ì €ì¥ í™•ì¸

ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­:
- íŒŒì´í”„ë¼ì¸: document_preprocessing â†’ korean_nlp â†’ vector_storage
- ë²¡í„° ì°¨ì›: 1024 (Amazon Titan Embeddings V2)
- ì²­í‚¹ ì „ëµ: ë¬¸ì„œ êµ¬ì¡° ê¸°ë°˜ ë¶„í• 
- NLP ë¶„ì„: Kiwipiepy í˜•íƒœì†Œ ë¶„ì„ + Bedrock ì„ë² ë”©

ì˜ˆìƒ ê²°ê³¼:
âœ… vs_doc_contents_indexì— ë²¡í„° ë°ì´í„° ì €ì¥
âœ… tb_document_chunksì— ìƒì„¸ ì²­í¬ ì •ë³´ ì €ì¥
âœ… ë©”íƒ€ë°ì´í„°ì— í•œêµ­ì–´ ë¶„ì„ ê²°ê³¼ í¬í•¨
âœ… "ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨: 'text'" ì˜¤ë¥˜ í•´ê²°

ì´ì „ ì˜¤ë¥˜ ì›ì¸:
âŒ TbVectorDocuments, TbVectorChunks í…Œì´ë¸” ì‚¬ìš© (ì¡´ì¬í•˜ì§€ ì•ŠìŒ)
âœ… VsDocContentsIndex, DocumentChunk í…Œì´ë¸” ì‚¬ìš© (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ)
        """.strip()
        
        with open(TEST_FILE_PATH, "w", encoding="utf-8") as f:
            f.write(test_content)
        print(f"âœ… í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±: {TEST_FILE_PATH}")
    else:
        print(f"âœ… ê¸°ì¡´ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‚¬ìš©: {TEST_FILE_PATH}")
    
    # 4. ë¬¸ì„œ ì—…ë¡œë“œ (ìˆ˜ì •ëœ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)
    print("\nğŸ“¤ 4ë‹¨ê³„: ë¬¸ì„œ ì—…ë¡œë“œ (ìˆ˜ì •ëœ íŒŒì´í”„ë¼ì¸)")
    print("   ğŸ”„ ì˜ˆìƒ ì²˜ë¦¬ ê³¼ì •:")
    print("   1. ë¬¸ì„œ ì „ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì²­í‚¹)")
    print("   2. í•œêµ­ì–´ NLP ë¶„ì„ (í˜•íƒœì†Œ + 1024ì°¨ì› ì„ë² ë”©)")
    print("   3. vs_doc_contents_indexì— ë²¡í„° ì €ì¥")
    print("   4. tb_document_chunksì— ìƒì„¸ ì •ë³´ ì €ì¥")
    
    with open(TEST_FILE_PATH, "rb") as f:
        files = {"file": ("corrected_pipeline_test.txt", f, "text/plain")}
        data = {"container_id": container_id}
        
        print(f"   ğŸ“¤ ì—…ë¡œë“œ ì‹œì‘...")
        response = requests.post(
            f"{BASE_URL}/api/v1/documents/upload",
            headers=headers,
            files=files,
            data=data,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
    
    print(f"\nğŸ“Š ì—…ë¡œë“œ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
    
    if response.status_code == 200:
        result = response.json()
        print("âœ… ì—…ë¡œë“œ ì„±ê³µ!")
        print(f"   ğŸ“„ ë¬¸ì„œ ID: {result.get('document_id')}")
        print(f"   ğŸ“ íŒŒì¼ëª…: {result.get('file_info', {}).get('original_name')}")
        print(f"   ğŸ“Š ì²˜ë¦¬ ì‹œê°„: {result.get('processing_stats', {}).get('processing_time', 0):.2f}ì´ˆ")
        
        # ì²˜ë¦¬ í†µê³„ ì¶œë ¥
        stats = result.get('processing_stats', {})
        print(f"\nğŸ“ˆ ì²˜ë¦¬ í†µê³„:")
        print(f"   ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {stats.get('text_length', 0):,}ì")
        print(f"   ğŸ§© ì²­í¬ ìˆ˜: {stats.get('chunk_count', 0)}ê°œ")
        print(f"   â­ í’ˆì§ˆ ì ìˆ˜: {stats.get('quality_score', 0):.2f}")
        print(f"   ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë¹„ìœ¨: {stats.get('korean_ratio', 0):.1%}")
        
        # í•œêµ­ì–´ ë¶„ì„ ê²°ê³¼
        korean_analysis = result.get('korean_analysis', {})
        print(f"\nğŸ‡°ğŸ‡· í•œêµ­ì–´ ë¶„ì„:")
        print(f"   ğŸ“‹ ë¬¸ì„œ ìœ í˜•: {korean_analysis.get('document_type', 'unknown')}")
        print(f"   ğŸ”‘ í‚¤ì›Œë“œ: {len(korean_analysis.get('keywords', []))}ê°œ")
        print(f"   ğŸ·ï¸ ê³ ìœ ëª…ì‚¬: {len(korean_analysis.get('proper_nouns', []))}ê°œ")
        
        # ì €ì¥ ì •ë³´ í™•ì¸
        if 'storage_info' in result:
            storage = result['storage_info']
            print(f"\nğŸ—„ï¸ ì €ì¥ ì •ë³´:")
            print(f"   ğŸ“Š ë²¡í„° í…Œì´ë¸”: {storage.get('vector_table', 'N/A')}")
            print(f"   ğŸ“Š ì²­í¬ í…Œì´ë¸”: {storage.get('chunk_table', 'N/A')}")
            print(f"   ğŸ“ ë²¡í„° ì°¨ì›: {storage.get('vector_dimension', 0)}")
            print(f"   ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë¶„ì„: {'âœ…' if storage.get('has_korean_analysis') else 'âŒ'}")
            print(f"   ğŸ”¢ ì„ë² ë”©: {'âœ…' if storage.get('has_embeddings') else 'âŒ'}")
        
        print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ: ìˆ˜ì •ëœ íŒŒì´í”„ë¼ì¸ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤!")
        print(f"   âœ… 'text' ì˜¤ë¥˜ í•´ê²°ë¨")
        print(f"   âœ… vs_doc_contents_index í…Œì´ë¸” ì‚¬ìš© í™•ì¸")
        print(f"   âœ… tb_document_chunks í…Œì´ë¸” ì‚¬ìš© í™•ì¸")
        
    else:
        print("âŒ ì—…ë¡œë“œ ì‹¤íŒ¨!")
        print(f"   ìƒíƒœ ì½”ë“œ: {response.status_code}")
        print(f"   ì‘ë‹µ ë‚´ìš©: {response.text}")
        
        # ì˜¤ë¥˜ ë¶„ì„
        try:
            error_data = response.json()
            error_detail = error_data.get('detail', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
            print(f"   ì˜¤ë¥˜ ìƒì„¸: {error_detail}")
            
            if "'text'" in error_detail or "ë²¡í„°ìŠ¤í† ì–´" in error_detail:
                print(f"\nğŸ” ì˜¤ë¥˜ ë¶„ì„:")
                print(f"   - ì—¬ì „íˆ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                print(f"   - ë°±ì—”ë“œ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”")
            
        except:
            print(f"   JSON íŒŒì‹± ì‹¤íŒ¨")

if __name__ == "__main__":
    asyncio.run(test_corrected_upload())

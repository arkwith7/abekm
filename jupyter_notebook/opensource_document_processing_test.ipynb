{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a8fb2d",
   "metadata": {},
   "source": [
    "# ğŸ”§ ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "## ğŸ“‹ ê°œìš”\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•˜ëŠ” ì¢…í•© í…ŒìŠ¤íŠ¸ í™˜ê²½**ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ¯ ì£¼ìš” ëª©ì \n",
    "- **ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì§€ì›**: PDF, DOCX, PPTX, XLSX, TXT, HTML, ì´ë¯¸ì§€ íŒŒì¼\n",
    "- **ì¶”ì¶œ ë°©ë²• ë¹„êµ í…ŒìŠ¤íŠ¸**: PyPDF2, pdfplumber, OCR ë“± ì„±ëŠ¥ ë° ì •í™•ë„ ë¹„êµ\n",
    "- **OCR ì²˜ë¦¬ ê²€ì¦**: Tesseract ê¸°ë°˜ ì´ë¯¸ì§€ PDF í˜ì´ì§€ë³„ ì²˜ë¦¬ ë° ì €ì¥\n",
    "- **ê²°ê³¼ ì €ì¥ ë° ê²€ì¦**: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆ ê²€ì‚¬ ë° íŒŒì¼ ì €ì¥\n",
    "\n",
    "### ğŸ’° ë¹„ìš© íš¨ìœ¨ì„±\n",
    "- **ì™„ì „ ë¬´ë£Œ**: ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì˜¤í”ˆì†ŒìŠ¤ (ìƒìš© API ë¶ˆí•„ìš”)\n",
    "- **ë¡œì»¬ ì²˜ë¦¬**: ì™¸ë¶€ ì„œë¹„ìŠ¤ ì˜ì¡´ì„± ì—†ìŒ, ë°ì´í„° ë³´ì•ˆ ë³´ì¥\n",
    "- **í™•ì¥ ê°€ëŠ¥**: ìƒˆë¡œìš´ ë¬¸ì„œ í˜•ì‹ ë° ì²˜ë¦¬ ë°©ë²• ì‰½ê²Œ ì¶”ê°€\n",
    "\n",
    "### ğŸ”„ ë°±ì—”ë“œ ì—°ë™ ê°€ëŠ¥\n",
    "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ëŠ” ë°±ì—”ë“œ `tools.py`ì˜ `chunked_texts()` í•¨ìˆ˜ì™€ ì—°ë™í•˜ì—¬ ì„ë² ë”© ìƒì„± ë° ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“Š í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤\n",
    "1. **ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸**: ê°œë³„ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ê²€ì¦\n",
    "2. **ë°°ì¹˜ í…ŒìŠ¤íŠ¸**: ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬ ë° ì„±ëŠ¥ ì¸¡ì •\n",
    "3. **ë°©ë²•ë³„ ë¹„êµ**: ê°™ì€ íŒŒì¼ì„ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê²°ê³¼ ë¹„êµ\n",
    "4. **í’ˆì§ˆ ê²€ì¦**: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ ì •í™•ë„ ë° ì™„ì„±ë„ í‰ê°€\n",
    "\n",
    "### ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°\n",
    "```\n",
    "ğŸ“‚ data/\n",
    "â”œâ”€â”€ ğŸ“‚ input_docs/           # í…ŒìŠ¤íŠ¸í•  ì›ë³¸ ë¬¸ì„œë“¤\n",
    "â”œâ”€â”€ ğŸ“‚ output/\n",
    "â”‚   â”œâ”€â”€ ğŸ“‚ extracted_texts/  # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤\n",
    "â”‚   â”œâ”€â”€ ğŸ“‚ test_results/     # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë° ë¹„êµ ë¶„ì„\n",
    "â”‚   â””â”€â”€ ğŸ“‚ logs/            # ì²˜ë¦¬ ë¡œê·¸ ë° ë©”íƒ€ë°ì´í„°\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb9803",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1ë‹¨ê³„: ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ë° í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6135a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ ì´ˆê³ ì† ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ í™•ì¸ (ì„¤ì¹˜ ì œì™¸)\n",
      "==================================================\n",
      "ğŸ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ ì ê²€:\n",
      "   âœ… PyPDF2 (PDF ê¸°ë³¸ ì²˜ë¦¬)\n",
      "   âŒ pdfplumber (PDF ê³ ê¸‰ ì²˜ë¦¬) - ëˆ„ë½\n",
      "   âœ… docx (Word ë¬¸ì„œ)\n",
      "   âœ… pptx (PowerPoint)\n",
      "   âœ… pptx (PowerPoint)\n",
      "   âœ… pandas (ë°ì´í„° ì²˜ë¦¬)\n",
      "   âœ… bs4 (HTML íŒŒì‹±)\n",
      "   âœ… html2text (HTML ë³€í™˜)\n",
      "   âœ… pytesseract (OCR)\n",
      "   âœ… PIL (ì´ë¯¸ì§€ ì²˜ë¦¬)\n",
      "   âœ… pdf2image (PDFâ†’ì´ë¯¸ì§€)\n",
      "   âœ… numpy (ìˆ˜ì¹˜ ê³„ì‚°)\n",
      "   âœ… chardet (ì¸ì½”ë”© ê°ì§€)\n",
      "   âœ… tiktoken (í† í° ê³„ì‚°) - ì„ íƒì \n",
      "   âœ… Tesseract OCR: 5.3.0\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š ìƒíƒœ ìš”ì•½:\n",
      "   â€¢ ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬: 11/12\n",
      "   â€¢ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬: 1ê°œ\n",
      "   â€¢ Tesseract OCR: âœ…\n",
      "\n",
      "ğŸ’¡ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡:\n",
      "   - pdfplumber\n",
      "\n",
      "ğŸ”§ ì„¤ì¹˜ ë°©ë²• (í•„ìš”ì‹œ ë‹¤ìŒ ì…€ ì‹¤í–‰):\n",
      "   ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ìë™ ì„¤ì¹˜\n",
      "\n",
      "âš ï¸ ì¼ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½. ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "âš¡ ì´ˆê³ ì† í™•ì¸ ì™„ë£Œ! (ì•½ 3-5ì´ˆ)\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   â€¢ ë‹¤ìŒ ì…€: ëˆ„ë½ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜\n",
      "   â€¢ ëª¨ë“  ì²´í¬ ì™„ë£Œ, ì•ˆì „í•˜ê²Œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ê°€ëŠ¥\n",
      "   âœ… pandas (ë°ì´í„° ì²˜ë¦¬)\n",
      "   âœ… bs4 (HTML íŒŒì‹±)\n",
      "   âœ… html2text (HTML ë³€í™˜)\n",
      "   âœ… pytesseract (OCR)\n",
      "   âœ… PIL (ì´ë¯¸ì§€ ì²˜ë¦¬)\n",
      "   âœ… pdf2image (PDFâ†’ì´ë¯¸ì§€)\n",
      "   âœ… numpy (ìˆ˜ì¹˜ ê³„ì‚°)\n",
      "   âœ… chardet (ì¸ì½”ë”© ê°ì§€)\n",
      "   âœ… tiktoken (í† í° ê³„ì‚°) - ì„ íƒì \n",
      "   âœ… Tesseract OCR: 5.3.0\n",
      "\n",
      "==================================================\n",
      "ğŸ“Š ìƒíƒœ ìš”ì•½:\n",
      "   â€¢ ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬: 11/12\n",
      "   â€¢ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬: 1ê°œ\n",
      "   â€¢ Tesseract OCR: âœ…\n",
      "\n",
      "ğŸ’¡ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡:\n",
      "   - pdfplumber\n",
      "\n",
      "ğŸ”§ ì„¤ì¹˜ ë°©ë²• (í•„ìš”ì‹œ ë‹¤ìŒ ì…€ ì‹¤í–‰):\n",
      "   ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ìë™ ì„¤ì¹˜\n",
      "\n",
      "âš ï¸ ì¼ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½. ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "âš¡ ì´ˆê³ ì† í™•ì¸ ì™„ë£Œ! (ì•½ 3-5ì´ˆ)\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   â€¢ ë‹¤ìŒ ì…€: ëˆ„ë½ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜\n",
      "   â€¢ ëª¨ë“  ì²´í¬ ì™„ë£Œ, ì•ˆì „í•˜ê²Œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ê°€ëŠ¥\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê³ ì† í™•ì¸ (ì„¤ì¹˜ ì—†ìŒ)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âš¡ ì´ˆê³ ì† ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ í™•ì¸ (ì„¤ì¹˜ ì œì™¸)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ í™•ì¸ë§Œ (ì„¤ì¹˜ ì•ˆí•¨)\n",
    "library_status = {}\n",
    "LIBRARIES_LOADED = True\n",
    "\n",
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡\n",
    "required_libs = {\n",
    "    'PyPDF2': 'PDF ê¸°ë³¸ ì²˜ë¦¬',\n",
    "    'pdfplumber': 'PDF ê³ ê¸‰ ì²˜ë¦¬', \n",
    "    'docx': 'Word ë¬¸ì„œ',\n",
    "    'pptx': 'PowerPoint',\n",
    "    'pandas': 'ë°ì´í„° ì²˜ë¦¬',\n",
    "    'bs4': 'HTML íŒŒì‹±',\n",
    "    'html2text': 'HTML ë³€í™˜',\n",
    "    'pytesseract': 'OCR',\n",
    "    'PIL': 'ì´ë¯¸ì§€ ì²˜ë¦¬',\n",
    "    'pdf2image': 'PDFâ†’ì´ë¯¸ì§€',\n",
    "    'numpy': 'ìˆ˜ì¹˜ ê³„ì‚°',\n",
    "    'chardet': 'ì¸ì½”ë”© ê°ì§€'\n",
    "}\n",
    "\n",
    "print(\"ğŸ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ ì ê²€:\")\n",
    "missing_count = 0\n",
    "success_count = 0\n",
    "\n",
    "for lib_name, description in required_libs.items():\n",
    "    try:\n",
    "        # ì•ˆì „í•œ ì„í¬íŠ¸ ì‹œë„\n",
    "        if lib_name == 'docx':\n",
    "            exec(\"from docx import Document\", {})\n",
    "        elif lib_name == 'pptx':\n",
    "            exec(\"from pptx import Presentation\", {})\n",
    "        elif lib_name == 'bs4':\n",
    "            exec(\"from bs4 import BeautifulSoup\", {})\n",
    "        elif lib_name == 'PIL':\n",
    "            exec(\"from PIL import Image\", {})\n",
    "        else:\n",
    "            exec(f\"import {lib_name}\", {})\n",
    "        \n",
    "        library_status[lib_name] = True\n",
    "        print(f\"   âœ… {lib_name} ({description})\")\n",
    "        success_count += 1\n",
    "    except ImportError:\n",
    "        library_status[lib_name] = False\n",
    "        print(f\"   âŒ {lib_name} ({description}) - ëˆ„ë½\")\n",
    "        missing_count += 1\n",
    "        LIBRARIES_LOADED = False\n",
    "    except Exception as e:\n",
    "        # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬\n",
    "        library_status[lib_name] = False\n",
    "        print(f\"   âš ï¸ {lib_name} ({description}) - ì˜¤ë¥˜: {str(e)[:50]}\")\n",
    "        missing_count += 1\n",
    "        LIBRARIES_LOADED = False\n",
    "\n",
    "# tiktoken ì„ íƒì  í™•ì¸\n",
    "try:\n",
    "    exec(\"import tiktoken\", {})\n",
    "    library_status['tiktoken'] = True\n",
    "    print(f\"   âœ… tiktoken (í† í° ê³„ì‚°) - ì„ íƒì \")\n",
    "except ImportError:\n",
    "    library_status['tiktoken'] = False\n",
    "    print(f\"   âš ï¸ tiktoken (í† í° ê³„ì‚°) - ëˆ„ë½ (ì„ íƒì )\")\n",
    "except Exception as e:\n",
    "    library_status['tiktoken'] = False\n",
    "    print(f\"   âš ï¸ tiktoken (í† í° ê³„ì‚°) - ì˜¤ë¥˜: {str(e)[:50]} (ì„ íƒì )\")\n",
    "\n",
    "# OCR í™˜ê²½ í™•ì¸ (ì•ˆì „í•˜ê²Œ)\n",
    "tesseract_found = False\n",
    "if library_status.get('pytesseract', False):\n",
    "    try:\n",
    "        import pytesseract\n",
    "        tesseract_version = pytesseract.get_tesseract_version()\n",
    "        print(f\"   âœ… Tesseract OCR: {tesseract_version}\")\n",
    "        tesseract_found = True\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Tesseract OCR ì‹œìŠ¤í…œ ì„¤ì¹˜ í•„ìš”: {str(e)[:50]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“Š ìƒíƒœ ìš”ì•½:\")\n",
    "print(f\"   â€¢ ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬: {success_count}/{len(required_libs)}\")\n",
    "print(f\"   â€¢ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬: {missing_count}ê°œ\")\n",
    "print(f\"   â€¢ Tesseract OCR: {'âœ…' if tesseract_found else 'âš ï¸'}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"\\nğŸ’¡ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡:\")\n",
    "    missing_libs = [lib for lib, status in library_status.items() \n",
    "                   if not status and lib in required_libs]\n",
    "    for lib in missing_libs:\n",
    "        print(f\"   - {lib}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”§ ì„¤ì¹˜ ë°©ë²• (í•„ìš”ì‹œ ë‹¤ìŒ ì…€ ì‹¤í–‰):\")\n",
    "    print(f\"   ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ìë™ ì„¤ì¹˜\")\n",
    "\n",
    "if LIBRARIES_LOADED:\n",
    "    print(f\"\\nğŸ‰ ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ ì¼ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½. ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nâš¡ ì´ˆê³ ì† í™•ì¸ ì™„ë£Œ! (ì•½ 3-5ì´ˆ)\")\n",
    "\n",
    "# ì•ˆì „í•œ ì„í¬íŠ¸ (ì—ëŸ¬ ë°©ì§€)\n",
    "try:\n",
    "    if library_status.get('PyPDF2', False):\n",
    "        import PyPDF2\n",
    "except: pass\n",
    "\n",
    "try:\n",
    "    if library_status.get('pandas', False):\n",
    "        import pandas as pd\n",
    "except: pass\n",
    "\n",
    "try:\n",
    "    if library_status.get('numpy', False):\n",
    "        import numpy as np\n",
    "except: pass\n",
    "\n",
    "try:\n",
    "    if library_status.get('chardet', False):\n",
    "        import chardet\n",
    "except: pass\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "print(f\"   â€¢ ë‹¤ìŒ ì…€: ëˆ„ë½ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜\")\n",
    "print(f\"   â€¢ ëª¨ë“  ì²´í¬ ì™„ë£Œ, ì•ˆì „í•˜ê²Œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afb8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜\n",
      "========================================\n",
      "ğŸ“¦ 1ê°œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”:\n",
      "   - pdfplumber\n",
      "\n",
      "ğŸ”„ ì„¤ì¹˜ ì‹œì‘...\n",
      "\n",
      "[1/1] pdfplumber ì„¤ì¹˜:\n",
      "   ğŸ”„ pdfplumber ì„¤ì¹˜ ì¤‘...\n",
      "   âœ… pdfplumber ì„¤ì¹˜ ì™„ë£Œ\n",
      "\n",
      "ğŸ“Š ì„¤ì¹˜ ê²°ê³¼: 1/1 ì„±ê³µ\n",
      "ğŸ‰ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\n",
      "ğŸ’¡ ì´ì œ ë‹¤ìŒ ì…€ë“¤ì„ ê³„ì† ì‹¤í–‰í•˜ì„¸ìš”.\n",
      "\n",
      "âš¡ ì„¤ì¹˜ ì‘ì—… ì™„ë£Œ!\n",
      "   âœ… pdfplumber ì„¤ì¹˜ ì™„ë£Œ\n",
      "\n",
      "ğŸ“Š ì„¤ì¹˜ ê²°ê³¼: 1/1 ì„±ê³µ\n",
      "ğŸ‰ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\n",
      "ğŸ’¡ ì´ì œ ë‹¤ìŒ ì…€ë“¤ì„ ê³„ì† ì‹¤í–‰í•˜ì„¸ìš”.\n",
      "\n",
      "âš¡ ì„¤ì¹˜ ì‘ì—… ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ì„ íƒì  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìš”ì‹œì—ë§Œ ì‹¤í–‰)\n",
    "\n",
    "# ìœ„ ì…€ì—ì„œ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”!\n",
    "\n",
    "print(\"ğŸš€ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# ì•ˆì „í•œ ì„¤ì¹˜ í•¨ìˆ˜\n",
    "def safe_install(package_name, display_name=None):\n",
    "    \"\"\"ì•ˆì „í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\"\"\"\n",
    "    display_name = display_name or package_name\n",
    "    try:\n",
    "        print(f\"   ğŸ”„ {display_name} ì„¤ì¹˜ ì¤‘...\")\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, '-m', 'pip', 'install', package_name, '-q'],\n",
    "            capture_output=True, text=True, timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"   âœ… {display_name} ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   âŒ {display_name} ì„¤ì¹˜ ì‹¤íŒ¨: {result.stderr[:100]}\")\n",
    "            return False\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   â° {display_name} ì„¤ì¹˜ íƒ€ì„ì•„ì›ƒ (5ë¶„ ì´ˆê³¼)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {display_name} ì„¤ì¹˜ ì˜¤ë¥˜: {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "# ì´ì „ ì…€ì˜ ê²°ê³¼ í™•ì¸\n",
    "if 'library_status' in globals() and 'required_libs' in globals():\n",
    "    missing_libs = [lib for lib, status in library_status.items() \n",
    "                   if not status and lib in required_libs]\n",
    "    \n",
    "    if missing_libs:\n",
    "        print(f\"ğŸ“¦ {len(missing_libs)}ê°œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”:\")\n",
    "        for lib in missing_libs:\n",
    "            print(f\"   - {lib}\")\n",
    "        \n",
    "        print(f\"\\nğŸ”„ ì„¤ì¹˜ ì‹œì‘...\")\n",
    "        \n",
    "        # ì‹¤ì œ íŒ¨í‚¤ì§€ëª… ë§¤í•‘\n",
    "        package_names = {\n",
    "            'docx': 'python-docx',\n",
    "            'pptx': 'python-pptx', \n",
    "            'bs4': 'beautifulsoup4',\n",
    "            'PIL': 'Pillow'\n",
    "        }\n",
    "        \n",
    "        success_count = 0\n",
    "        total_libs = len(missing_libs)\n",
    "        \n",
    "        for i, lib in enumerate(missing_libs, 1):\n",
    "            package_name = package_names.get(lib, lib)\n",
    "            print(f\"\\n[{i}/{total_libs}] {lib} ì„¤ì¹˜:\")\n",
    "            \n",
    "            if safe_install(package_name, lib):\n",
    "                success_count += 1\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì„¤ì¹˜ ê²°ê³¼: {success_count}/{total_libs} ì„±ê³µ\")\n",
    "        \n",
    "        if success_count == total_libs:\n",
    "            print(\"ğŸ‰ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "            print(\"ğŸ’¡ ì´ì œ ë‹¤ìŒ ì…€ë“¤ì„ ê³„ì† ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "        elif success_count > 0:\n",
    "            print(\"âš ï¸ ì¼ë¶€ ì„¤ì¹˜ ì™„ë£Œ. ì‹¤íŒ¨í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ìˆ˜ë™ ì„¤ì¹˜ í•„ìš”\")\n",
    "        else:\n",
    "            print(\"âŒ ëª¨ë“  ì„¤ì¹˜ ì‹¤íŒ¨. ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” ê¶Œí•œ ë¬¸ì œ í™•ì¸ í•„ìš”\")\n",
    "            \n",
    "    else:\n",
    "        print(\"âœ… ì„¤ì¹˜í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ğŸ’¡ ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "    print(\"ğŸ’¡ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nâš¡ ì„¤ì¹˜ ì‘ì—… ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36547c7e",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2ë‹¨ê³„: íŒŒì¼ í˜•ì‹ ì§€ì› ë° ì„¤ì • ê´€ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f884b1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì™„ë£Œ!\n",
      "======================================================================\n",
      "ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\n",
      "   ğŸ“‚ ì…ë ¥: /home/admin/wkms-aws/jupyter_notebook/data/input_docs\n",
      "   ğŸ“‚ ì¶œë ¥: /home/admin/wkms-aws/jupyter_notebook/data/output\n",
      "   ğŸ“ í…ìŠ¤íŠ¸: /home/admin/wkms-aws/jupyter_notebook/data/output/extracted_texts\n",
      "   ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: /home/admin/wkms-aws/jupyter_notebook/data/output/test_results\n",
      "   ğŸ“‹ ë¡œê·¸: /home/admin/wkms-aws/jupyter_notebook/data/output/logs\n",
      "\n",
      "ğŸ“‹ ì§€ì› íŒŒì¼ í˜•ì‹:\n",
      "   PDF: .pdf (pypdf2, pdfplumber, ocr)\n",
      "   WORD: .docx, .doc (python-docx)\n",
      "   POWERPOINT: .pptx, .ppt (python-pptx)\n",
      "   EXCEL: .xlsx, .xls (openpyxl)\n",
      "   TEXT: .txt, .md, .rtf (builtin)\n",
      "   HTML: .html, .htm, .xml (beautifulsoup)\n",
      "   IMAGE: .png, .jpg, .jpeg, .tiff, .bmp (tesseract)\n",
      "\n",
      "ğŸ” OCR ì„¤ì •:\n",
      "   ì–¸ì–´: kor+eng\n",
      "   DPI: 300\n",
      "   í˜ì´ì§€ë³„ ì €ì¥: True\n",
      "\n",
      "ğŸ§ª í…ŒìŠ¤íŠ¸ ì„¤ì •:\n",
      "   compare_methods: True\n",
      "   measure_performance: True\n",
      "   validate_results: True\n",
      "   save_detailed_logs: True\n",
      "   create_summary_report: True\n",
      "âœ… ê¸°ë³¸ ì„¤ì • ì™„ë£Œ! (í† í¬ë‚˜ì´ì €ëŠ” í•„ìš”ì‹œ ìë™ ë¡œë“œ)\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   â€¢ config.print_file_summary(): ì…ë ¥ íŒŒì¼ í˜„í™© í™•ì¸\n",
      "   â€¢ config.tokenizer: í† í¬ë‚˜ì´ì € (í•„ìš”ì‹œ ìë™ ë¡œë“œ)\n",
      "   â€¢ ëª¨ë“  ë¬´ê±°ìš´ ì‘ì—…ì€ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ìµœì í™”ë¨\n"
     ]
    }
   ],
   "source": [
    "# âš™ï¸ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "class DocumentExtractionTestConfig:\n",
    "    \"\"\"ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì „ìš© ì„¤ì • í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=None):\n",
    "        try:\n",
    "            # ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "            if base_dir is None:\n",
    "                base_dir = Path(\"/home/admin/wkms-aws/jupyter_notebook/data\")\n",
    "            else:\n",
    "                base_dir = Path(base_dir)\n",
    "            \n",
    "            # ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ì˜\n",
    "            self.base_dir = base_dir\n",
    "            self.input_docs_dir = base_dir / \"input_docs\"\n",
    "            self.output_dir = base_dir / \"output\"\n",
    "            self.extracted_texts_dir = self.output_dir / \"extracted_texts\"\n",
    "            self.test_results_dir = self.output_dir / \"test_results\"\n",
    "            self.logs_dir = self.output_dir / \"logs\"\n",
    "            \n",
    "            # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„± (ì•ˆì „í•˜ê²Œ)\n",
    "            self._create_directories()\n",
    "            \n",
    "            # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ ì •ì˜\n",
    "            self.supported_formats = {\n",
    "                'pdf': ['.pdf'],\n",
    "                'word': ['.docx', '.doc'],\n",
    "                'powerpoint': ['.pptx', '.ppt'],\n",
    "                'excel': ['.xlsx', '.xls'],\n",
    "                'text': ['.txt', '.md', '.rtf'],\n",
    "                'html': ['.html', '.htm', '.xml'],\n",
    "                'image': ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']\n",
    "            }\n",
    "            \n",
    "            # ì¶”ì¶œ ë°©ë²• ì„¤ì •\n",
    "            self.extraction_methods = {\n",
    "                'pdf': ['pypdf2', 'pdfplumber', 'ocr'],\n",
    "                'word': ['python-docx'],\n",
    "                'powerpoint': ['python-pptx'],\n",
    "                'excel': ['openpyxl'],\n",
    "                'text': ['builtin'],\n",
    "                'html': ['beautifulsoup'],\n",
    "                'image': ['tesseract']\n",
    "            }\n",
    "            \n",
    "            # OCR ì„¤ì •\n",
    "            self.ocr_config = {\n",
    "                'enabled': True,\n",
    "                'language': 'kor+eng',  # í•œêµ­ì–´ + ì˜ì–´\n",
    "                'dpi': 300,\n",
    "                'save_page_files': True,  # í˜ì´ì§€ë³„ ê°œë³„ íŒŒì¼ ì €ì¥\n",
    "                'save_combined_file': True  # ì „ì²´ í†µí•© íŒŒì¼ ì €ì¥\n",
    "            }\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì„¤ì •\n",
    "            self.test_config = {\n",
    "                'compare_methods': True,  # ì—¬ëŸ¬ ë°©ë²• ë¹„êµ í…ŒìŠ¤íŠ¸\n",
    "                'measure_performance': True,  # ì„±ëŠ¥ ì¸¡ì •\n",
    "                'validate_results': True,  # ê²°ê³¼ ê²€ì¦\n",
    "                'save_detailed_logs': True,  # ìƒì„¸ ë¡œê·¸ ì €ì¥\n",
    "                'create_summary_report': True  # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±\n",
    "            }\n",
    "            \n",
    "            # ë¡œê±° ì„¤ì • (ì•ˆì „í•˜ê²Œ)\n",
    "            self.logger = self._setup_logger()\n",
    "            \n",
    "            # í† í¬ë‚˜ì´ì €ëŠ” ì§€ì—° ë¡œë”© (ì‹¤ì œ í•„ìš”í•  ë•Œë§Œ ì´ˆê¸°í™”)\n",
    "            self._tokenizer = None\n",
    "            \n",
    "            print(\"âš™ï¸ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "            self._print_basic_config()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì„¤ì • ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "            print(\"ğŸ’¡ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "            self._init_fallback()\n",
    "    \n",
    "    def _init_fallback(self):\n",
    "        \"\"\"ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •\"\"\"\n",
    "        try:\n",
    "            self.base_dir = Path.cwd() / \"data\"\n",
    "            self.input_docs_dir = self.base_dir / \"input_docs\"\n",
    "            self.output_dir = self.base_dir / \"output\"\n",
    "            self.extracted_texts_dir = self.output_dir / \"extracted_texts\"\n",
    "            self.test_results_dir = self.output_dir / \"test_results\"\n",
    "            self.logs_dir = self.output_dir / \"logs\"\n",
    "            self.logger = None\n",
    "            self._tokenizer = None\n",
    "            print(\"âš ï¸ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”ë¨\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ê¸°ë³¸ê°’ ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"í† í¬ë‚˜ì´ì €ë¥¼ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\"\"\"\n",
    "        if self._tokenizer is None:\n",
    "            try:\n",
    "                print(\"ğŸ”„ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì¤‘...\")\n",
    "                import tiktoken\n",
    "                self._tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 ê¸°ë³¸ í† í¬ë‚˜ì´ì €\n",
    "                print(\"âœ… í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "                self._tokenizer = None\n",
    "        return self._tokenizer\n",
    "    \n",
    "    def _create_directories(self):\n",
    "        \"\"\"í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ì•ˆì „í•˜ê²Œ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "        directories = [\n",
    "            self.input_docs_dir,\n",
    "            self.output_dir,\n",
    "            self.extracted_texts_dir,\n",
    "            self.test_results_dir,\n",
    "            self.logs_dir\n",
    "        ]\n",
    "        \n",
    "        for directory in directories:\n",
    "            try:\n",
    "                directory.mkdir(parents=True, exist_ok=True)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨ {directory}: {e}\")\n",
    "    \n",
    "    def _setup_logger(self):\n",
    "        \"\"\"ë¡œê±°ë¥¼ ì•ˆì „í•˜ê²Œ ì„¤ì •í•©ë‹ˆë‹¤.\"\"\"\n",
    "        try:\n",
    "            logger = logging.getLogger('document_extraction_test')\n",
    "            logger.setLevel(logging.INFO)\n",
    "            \n",
    "            # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)\n",
    "            logger.handlers.clear()\n",
    "            \n",
    "            # íŒŒì¼ í•¸ë“¤ëŸ¬\n",
    "            log_file = self.logs_dir / f\"extraction_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "            file_handler = logging.FileHandler(log_file, encoding='utf-8')\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            \n",
    "            # í¬ë§·í„°\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            )\n",
    "            file_handler.setFormatter(formatter)\n",
    "            logger.addHandler(file_handler)\n",
    "            \n",
    "            return logger\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ë¡œê±° ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_file_type(self, file_path):\n",
    "        \"\"\"íŒŒì¼ í™•ì¥ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ íƒ€ì…ì„ íŒë³„í•©ë‹ˆë‹¤.\"\"\"\n",
    "        try:\n",
    "            file_path = Path(file_path)\n",
    "            extension = file_path.suffix.lower()\n",
    "            \n",
    "            for file_type, extensions in self.supported_formats.items():\n",
    "                if extension in extensions:\n",
    "                    return file_type\n",
    "            return 'unknown'\n",
    "        except Exception:\n",
    "            return 'unknown'\n",
    "    \n",
    "    def is_supported_file(self, file_path):\n",
    "        \"\"\"íŒŒì¼ì´ ì§€ì›ë˜ëŠ” í˜•ì‹ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.\"\"\"\n",
    "        return self.get_file_type(file_path) != 'unknown'\n",
    "    \n",
    "    def get_available_methods(self, file_type):\n",
    "        \"\"\"íŒŒì¼ íƒ€ì…ì— ëŒ€í•´ ì‚¬ìš© ê°€ëŠ¥í•œ ì¶”ì¶œ ë°©ë²•ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "        return self.extraction_methods.get(file_type, [])\n",
    "    \n",
    "    def _print_basic_config(self):\n",
    "        \"\"\"ê¸°ë³¸ ì„¤ì •ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤ (ë¬´ê±°ìš´ ì‘ì—… ì œì™¸).\"\"\"\n",
    "        try:\n",
    "            print(\"=\" * 70)\n",
    "            print(\"ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\")\n",
    "            print(f\"   ğŸ“‚ ì…ë ¥: {self.input_docs_dir}\")\n",
    "            print(f\"   ğŸ“‚ ì¶œë ¥: {self.output_dir}\")\n",
    "            print(f\"   ğŸ“ í…ìŠ¤íŠ¸: {self.extracted_texts_dir}\")\n",
    "            print(f\"   ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {self.test_results_dir}\")\n",
    "            print(f\"   ğŸ“‹ ë¡œê·¸: {self.logs_dir}\")\n",
    "            \n",
    "            print(f\"\\nğŸ“‹ ì§€ì› íŒŒì¼ í˜•ì‹:\")\n",
    "            for file_type, extensions in self.supported_formats.items():\n",
    "                methods = ', '.join(self.get_available_methods(file_type))\n",
    "                print(f\"   {file_type.upper()}: {', '.join(extensions)} ({methods})\")\n",
    "            \n",
    "            print(f\"\\nğŸ” OCR ì„¤ì •:\")\n",
    "            print(f\"   ì–¸ì–´: {self.ocr_config['language']}\")\n",
    "            print(f\"   DPI: {self.ocr_config['dpi']}\")\n",
    "            print(f\"   í˜ì´ì§€ë³„ ì €ì¥: {self.ocr_config['save_page_files']}\")\n",
    "            \n",
    "            print(f\"\\nğŸ§ª í…ŒìŠ¤íŠ¸ ì„¤ì •:\")\n",
    "            for key, value in self.test_config.items():\n",
    "                print(f\"   {key}: {value}\")\n",
    "            \n",
    "            print(\"âœ… ê¸°ë³¸ ì„¤ì • ì™„ë£Œ! (í† í¬ë‚˜ì´ì €ëŠ” í•„ìš”ì‹œ ìë™ ë¡œë“œ)\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ì„¤ì • ì¶œë ¥ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    def print_file_summary(self):\n",
    "        \"\"\"ì…ë ¥ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ í˜„í™©ì„ ì•ˆì „í•˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "        try:\n",
    "            print(\"\\nğŸ“„ ì…ë ¥ íŒŒì¼ í˜„í™© ìŠ¤ìº” ì¤‘...\")\n",
    "            \n",
    "            if not self.input_docs_dir.exists():\n",
    "                print(f\"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.input_docs_dir}\")\n",
    "                return\n",
    "            \n",
    "            files = list(self.input_docs_dir.glob(\"*\"))\n",
    "            supported_files = [f for f in files if f.is_file() and self.is_supported_file(f)]\n",
    "            \n",
    "            print(f\"ğŸ“Š íŒŒì¼ í˜„í™©:\")\n",
    "            print(f\"   ì „ì²´ íŒŒì¼: {len([f for f in files if f.is_file()])}ê°œ\")\n",
    "            print(f\"   ì§€ì› íŒŒì¼: {len(supported_files)}ê°œ\")\n",
    "            \n",
    "            if supported_files:\n",
    "                print(f\"   ğŸ“‹ ì§€ì› íŒŒì¼ ëª©ë¡:\")\n",
    "                for file_path in supported_files[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ\n",
    "                    try:\n",
    "                        file_type = self.get_file_type(file_path)\n",
    "                        file_size = file_path.stat().st_size / 1024 / 1024  # MB\n",
    "                        print(f\"      â€¢ {file_path.name} ({file_type}, {file_size:.2f}MB)\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      â€¢ {file_path.name} (ì •ë³´ ì½ê¸° ì‹¤íŒ¨)\")\n",
    "                \n",
    "                if len(supported_files) > 10:\n",
    "                    print(f\"      ... ë° {len(supported_files) - 10}ê°œ ë”\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì•ˆì „í•œ ì´ˆê¸°í™”)\n",
    "try:\n",
    "    config = DocumentExtractionTestConfig()\n",
    "    EXTRACTION_CONFIG = config\n",
    "    print(\"\\nğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "    print(\"   â€¢ config.print_file_summary(): ì…ë ¥ íŒŒì¼ í˜„í™© í™•ì¸\")\n",
    "    print(\"   â€¢ config.tokenizer: í† í¬ë‚˜ì´ì € (í•„ìš”ì‹œ ìë™ ë¡œë“œ)\")\n",
    "    print(\"   â€¢ ëª¨ë“  ë¬´ê±°ìš´ ì‘ì—…ì€ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ìµœì í™”ë¨\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ’¡ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ê¸° ì „ì— ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2507974",
   "metadata": {},
   "source": [
    "## ğŸ“„ 3ë‹¨ê³„: ê³ ì† ë¬¸ì„œ ë¡œë”© ë° íŒŒì¼ ì •ë³´ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273fa379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ!\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   â€¢ document_loader.scan_input_directory(): ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº”\n",
      "   â€¢ document_loader.create_test_batch(): í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìƒì„±\n",
      "   â€¢ document_loader.get_test_files_by_type('pdf'): PDF íŒŒì¼ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
      "   â€¢ config.print_file_summary(): ì…ë ¥ íŒŒì¼ í˜„í™© í™•ì¸\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“„ ë¬¸ì„œ ë¡œë” ë° íŒŒì¼ ê´€ë¦¬ í´ë˜ìŠ¤ (ì•ˆì „í•œ ë²„ì „)\n",
    "\n",
    "class DocumentTestLoader:\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë¬¸ì„œ ë¡œë”© ë° íŒŒì¼ ê´€ë¦¬ í´ë˜ìŠ¤ (ê°œì„ ëœ ì•ˆì „ì„±)\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        try:\n",
    "            self.config = config\n",
    "            self.logger = config.logger if hasattr(config, 'logger') else None\n",
    "            self.loaded_documents = {}  # ë¡œë“œëœ ë¬¸ì„œ ìºì‹œ\n",
    "            print(\"ğŸ“„ ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "            self.config = None\n",
    "            self.logger = None\n",
    "            self.loaded_documents = {}\n",
    "        \n",
    "    def scan_input_directory(self, show_details=True, timeout_seconds=30):\n",
    "        \"\"\"ì…ë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ìŠ¤ìº”í•˜ì—¬ ì§€ì›ë˜ëŠ” ë¬¸ì„œë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.\"\"\"\n",
    "        \n",
    "        if not self.config:\n",
    "            print(\"âŒ ì„¤ì •ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            if not self.config.input_docs_dir.exists():\n",
    "                print(f\"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.config.input_docs_dir}\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {self.config.input_docs_dir}\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # íƒ€ì„ì•„ì›ƒì„ ê³ ë ¤í•œ íŒŒì¼ ìŠ¤ìº”\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            all_files = []\n",
    "            for file_path in self.config.input_docs_dir.glob(\"*\"):\n",
    "                if time.time() - start_time > timeout_seconds:\n",
    "                    print(f\"â° ìŠ¤ìº” íƒ€ì„ì•„ì›ƒ ({timeout_seconds}ì´ˆ) - ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜\")\n",
    "                    break\n",
    "                all_files.append(file_path)\n",
    "            \n",
    "            supported_files = []\n",
    "            file_types = {}\n",
    "            \n",
    "            for file_path in all_files:\n",
    "                try:\n",
    "                    if file_path.is_file():\n",
    "                        file_type = self.config.get_file_type(file_path)\n",
    "                        \n",
    "                        if file_type != 'unknown':\n",
    "                            supported_files.append(file_path)\n",
    "                            \n",
    "                            if file_type not in file_types:\n",
    "                                file_types[file_type] = []\n",
    "                            file_types[file_type].append(file_path)\n",
    "                except Exception as e:\n",
    "                    if self.logger:\n",
    "                        self.logger.warning(f\"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # ê²°ê³¼ ìš”ì•½\n",
    "            print(f\"ğŸ“Š ìŠ¤ìº” ê²°ê³¼:\")\n",
    "            print(f\"   ì „ì²´ íŒŒì¼: {len([f for f in all_files if f.is_file()])}ê°œ\")\n",
    "            print(f\"   ì§€ì› íŒŒì¼: {len(supported_files)}ê°œ\")\n",
    "            print(f\"   ì§€ì› íƒ€ì…: {len(file_types)}ê°œ\")\n",
    "            \n",
    "            # íƒ€ì…ë³„ ìƒì„¸ ì •ë³´\n",
    "            if show_details and file_types:\n",
    "                print(f\"\\nğŸ“‹ íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜:\")\n",
    "                for file_type, files in file_types.items():\n",
    "                    try:\n",
    "                        methods = ', '.join(self.config.get_available_methods(file_type))\n",
    "                        print(f\"\\n   ğŸ“ {file_type.upper()} ({len(files)}ê°œ) - ì¶”ì¶œë°©ë²•: {methods}\")\n",
    "                        \n",
    "                        for file_path in files[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ\n",
    "                            try:\n",
    "                                file_size = file_path.stat().st_size / 1024 / 1024  # MB\n",
    "                                print(f\"      â€¢ {file_path.name} ({file_size:.2f}MB)\")\n",
    "                            except Exception:\n",
    "                                print(f\"      â€¢ {file_path.name} (í¬ê¸° ì •ë³´ ì—†ìŒ)\")\n",
    "                        \n",
    "                        if len(files) > 5:\n",
    "                            print(f\"      ... ë° {len(files) - 5}ê°œ ë”\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      âš ï¸ {file_type} ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "            \n",
    "            return supported_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì˜¤ë¥˜: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def load_document_info(self, file_path):\n",
    "        \"\"\"ë¬¸ì„œì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        try:\n",
    "            file_path = Path(file_path)\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                raise FileNotFoundError(f\"íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "            \n",
    "            file_type = self.config.get_file_type(file_path)\n",
    "            if file_type == 'unknown':\n",
    "                raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}\")\n",
    "            \n",
    "            # íŒŒì¼ ê¸°ë³¸ ì •ë³´\n",
    "            file_stat = file_path.stat()\n",
    "            \n",
    "            doc_info = {\n",
    "                'file_path': str(file_path),\n",
    "                'file_name': file_path.name,\n",
    "                'file_stem': file_path.stem,\n",
    "                'file_type': file_type,\n",
    "                'file_size_bytes': file_stat.st_size,\n",
    "                'file_size_mb': file_stat.st_size / 1024 / 1024,\n",
    "                'modified_time': datetime.fromtimestamp(file_stat.st_mtime),\n",
    "                'available_methods': self.config.get_available_methods(file_type),\n",
    "                'load_time': datetime.now()\n",
    "            }\n",
    "            \n",
    "            # ìºì‹œì— ì €ì¥\n",
    "            self.loaded_documents[str(file_path)] = doc_info\n",
    "            \n",
    "            return doc_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë¬¸ì„œ ì •ë³´ ë¡œë“œ ì˜¤ë¥˜ {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_test_files_by_type(self, file_type=None, max_files=None):\n",
    "        \"\"\"íŒŒì¼ íƒ€ì…ë³„ë¡œ í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
    "        try:\n",
    "            supported_files = self.scan_input_directory(show_details=False)\n",
    "            \n",
    "            if file_type:\n",
    "                filtered_files = [f for f in supported_files if self.config.get_file_type(f) == file_type]\n",
    "            else:\n",
    "                filtered_files = supported_files\n",
    "            \n",
    "            if max_files and max_files > 0:\n",
    "                filtered_files = filtered_files[:max_files]\n",
    "            \n",
    "            return filtered_files\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ í•„í„°ë§ ì˜¤ë¥˜: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_test_batch(self, include_types=None, exclude_large_files=True, size_limit_mb=50):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ íŒŒì¼ ë°°ì¹˜ë¥¼ ì•ˆì „í•˜ê²Œ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“¦ í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìƒì„±\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            all_files = self.scan_input_directory(show_details=False)\n",
    "            test_batch = []\n",
    "            \n",
    "            for file_path in all_files:\n",
    "                try:\n",
    "                    file_type = self.config.get_file_type(file_path)\n",
    "                    \n",
    "                    # íƒ€ì… í•„í„°ë§\n",
    "                    if include_types and file_type not in include_types:\n",
    "                        continue\n",
    "                    \n",
    "                    # í¬ê¸° í•„í„°ë§\n",
    "                    if exclude_large_files:\n",
    "                        try:\n",
    "                            file_size_mb = file_path.stat().st_size / 1024 / 1024\n",
    "                            if file_size_mb > size_limit_mb:\n",
    "                                continue\n",
    "                        except Exception:\n",
    "                            continue  # í¬ê¸°ë¥¼ ì½ì„ ìˆ˜ ì—†ìœ¼ë©´ ì œì™¸\n",
    "                    \n",
    "                    doc_info = self.load_document_info(file_path)\n",
    "                    if doc_info:\n",
    "                        test_batch.append(doc_info)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    if self.logger:\n",
    "                        self.logger.warning(f\"ë°°ì¹˜ ìƒì„± ì¤‘ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # ë°°ì¹˜ ìš”ì•½\n",
    "            print(f\"ğŸ“Š ë°°ì¹˜ êµ¬ì„±:\")\n",
    "            print(f\"   ì„ íƒëœ íŒŒì¼: {len(test_batch)}ê°œ\")\n",
    "            \n",
    "            if test_batch:\n",
    "                # íƒ€ì…ë³„ í†µê³„\n",
    "                type_counts = {}\n",
    "                total_size = 0\n",
    "                \n",
    "                for doc in test_batch:\n",
    "                    try:\n",
    "                        file_type = doc['file_type']\n",
    "                        if file_type not in type_counts:\n",
    "                            type_counts[file_type] = 0\n",
    "                        type_counts[file_type] += 1\n",
    "                        total_size += doc.get('file_size_mb', 0)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                print(f\"   ì´ í¬ê¸°: {total_size:.2f}MB\")\n",
    "                print(f\"   íƒ€ì…ë³„ ë¶„í¬:\")\n",
    "                for file_type, count in type_counts.items():\n",
    "                    print(f\"      â€¢ {file_type}: {count}ê°œ\")\n",
    "            \n",
    "            return test_batch\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_document_list(self, documents, filename=\"document_list.json\"):\n",
    "        \"\"\"ë¬¸ì„œ ëª©ë¡ì„ ì•ˆì „í•˜ê²Œ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "        try:\n",
    "            if not self.config:\n",
    "                print(\"âŒ ì„¤ì •ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "                return None\n",
    "                \n",
    "            output_file = self.config.test_results_dir / filename\n",
    "            \n",
    "            # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "            serializable_docs = []\n",
    "            for doc in documents:\n",
    "                try:\n",
    "                    serializable_doc = doc.copy()\n",
    "                    for key, value in serializable_doc.items():\n",
    "                        if isinstance(value, datetime):\n",
    "                            serializable_doc[key] = value.isoformat()\n",
    "                        elif isinstance(value, Path):\n",
    "                            serializable_doc[key] = str(value)\n",
    "                    serializable_docs.append(serializable_doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ ë¬¸ì„œ ì§ë ¬í™” ì˜¤ë¥˜: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(serializable_docs, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"ğŸ’¾ ë¬¸ì„œ ëª©ë¡ ì €ì¥: {output_file}\")\n",
    "            return output_file\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë¬¸ì„œ ëª©ë¡ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            return None\n",
    "\n",
    "# ì•ˆì „í•œ ë¬¸ì„œ ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "try:\n",
    "    if 'config' in globals() and config:\n",
    "        document_loader = DocumentTestLoader(config)\n",
    "        print(\"\\nğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "        print(\"   â€¢ document_loader.scan_input_directory(): ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº”\")\n",
    "        print(\"   â€¢ document_loader.create_test_batch(): í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìƒì„±\")\n",
    "        print(\"   â€¢ document_loader.get_test_files_by_type('pdf'): PDF íŒŒì¼ë§Œ ê°€ì ¸ì˜¤ê¸°\")\n",
    "        print(\"   â€¢ config.print_file_summary(): ì…ë ¥ íŒŒì¼ í˜„í™© í™•ì¸\")\n",
    "    else:\n",
    "        print(\"âŒ ì´ì „ ì…€ì˜ configê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë¬¸ì„œ ë¡œë” ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ’¡ ì´ì „ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•œ í›„ ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b395ef7",
   "metadata": {},
   "source": [
    "## ğŸ”¤ 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—”ì§„ êµ¬í˜„ (ë‹¤ì¤‘ ë°©ì‹ ì§€ì›)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c6bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   â€¢ text_extractor.extract_text('íŒŒì¼ê²½ë¡œ'): í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
      "   â€¢ text_extractor.extract_text('íŒŒì¼ê²½ë¡œ', method='pypdf2'): íŠ¹ì • ë°©ë²• ì‚¬ìš©\n",
      "   â€¢ text_extractor.extraction_stats: ì¶”ì¶œ í†µê³„ í™•ì¸\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¤ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì—”ì§„ (ê°œì„ ëœ ì•ˆì „ì„±)\n",
    "\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class DocumentTextExtractor:\n",
    "    \"\"\"ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í…ŒìŠ¤íŠ¸ ì—”ì§„ (ì•ˆì „í•œ ë²„ì „)\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        try:\n",
    "            self.config = config\n",
    "            self.logger = config.logger if hasattr(config, 'logger') else None\n",
    "            self.extraction_stats = {}  # ì¶”ì¶œ í†µê³„\n",
    "            \n",
    "            # ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ìš©ì„± ì²´í¬\n",
    "            self._check_library_availability()\n",
    "            \n",
    "            print(\"ğŸ”¤ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—”ì§„ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}\")\n",
    "            self.config = None\n",
    "            self.logger = None\n",
    "            self.extraction_stats = {}\n",
    "    \n",
    "    def _check_library_availability(self):\n",
    "        \"\"\"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì˜ ê°€ìš©ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.\"\"\"\n",
    "        self.available_libs = {}\n",
    "        \n",
    "        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬ (ì•ˆì „í•˜ê²Œ)\n",
    "        libs_to_check = [\n",
    "            ('PyPDF2', 'PyPDF2'),\n",
    "            ('pdfplumber', 'pdfplumber'),\n",
    "            ('python-docx', 'docx'),\n",
    "            ('python-pptx', 'pptx'),\n",
    "            ('openpyxl', 'openpyxl'),\n",
    "            ('beautifulsoup4', 'bs4'),\n",
    "            ('pytesseract', 'pytesseract'),\n",
    "            ('Pillow', 'PIL'),\n",
    "            ('pdf2image', 'pdf2image')\n",
    "        ]\n",
    "        \n",
    "        for package_name, import_name in libs_to_check:\n",
    "            try:\n",
    "                exec(f\"import {import_name}\", {})\n",
    "                self.available_libs[package_name] = True\n",
    "            except ImportError:\n",
    "                self.available_libs[package_name] = False\n",
    "            except Exception:\n",
    "                self.available_libs[package_name] = False\n",
    "    \n",
    "    def extract_text(self, file_path, method='auto', save_result=True, timeout_seconds=300):\n",
    "        \"\"\"\n",
    "        íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            file_path: íŒŒì¼ ê²½ë¡œ\n",
    "            method: ì¶”ì¶œ ë°©ë²• ('auto', 'pypdf2', 'pdfplumber', 'ocr' ë“±)\n",
    "            save_result: ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€\n",
    "            timeout_seconds: íƒ€ì„ì•„ì›ƒ (ê¸°ë³¸ 5ë¶„)\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.config:\n",
    "            return {'success': False, 'error': 'ì„¤ì •ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}\n",
    "        \n",
    "        try:\n",
    "            file_path = Path(file_path)\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                return {'success': False, 'error': f'íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}'}\n",
    "            \n",
    "            file_type = self.config.get_file_type(file_path)\n",
    "            \n",
    "            print(f\"ğŸ”„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {file_path.name}\")\n",
    "            print(f\"   ğŸ“„ íŒŒì¼ íƒ€ì…: {file_type}\")\n",
    "            print(f\"   ğŸ”§ ì¶”ì¶œ ë°©ë²•: {method}\")\n",
    "            \n",
    "            extraction_result = {\n",
    "                'file_path': str(file_path),\n",
    "                'file_name': file_path.name,\n",
    "                'file_type': file_type,\n",
    "                'extraction_method': method,\n",
    "                'start_time': time.time(),\n",
    "                'success': False,\n",
    "                'text': '',\n",
    "                'text_length': 0,\n",
    "                'metadata': {},\n",
    "                'error': None,\n",
    "                'saved_files': []\n",
    "            }\n",
    "            \n",
    "            # íƒ€ì„ì•„ì›ƒ ì²´í¬ë¥¼ ìœ„í•œ ì‹œì‘ ì‹œê°„\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # íŒŒì¼ íƒ€ì…ë³„ ì¶”ì¶œ ì‹¤í–‰\n",
    "            if file_type == 'pdf':\n",
    "                result = self._extract_from_pdf(file_path, method, timeout_seconds)\n",
    "            elif file_type == 'word':\n",
    "                result = self._extract_from_docx(file_path)\n",
    "            elif file_type == 'powerpoint':\n",
    "                result = self._extract_from_pptx(file_path)\n",
    "            elif file_type == 'excel':\n",
    "                result = self._extract_from_excel(file_path)\n",
    "            elif file_type == 'text':\n",
    "                result = self._extract_from_text(file_path)\n",
    "            elif file_type == 'html':\n",
    "                result = self._extract_from_html(file_path)\n",
    "            elif file_type == 'image':\n",
    "                result = self._extract_from_image(file_path)\n",
    "            else:\n",
    "                raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}\")\n",
    "            \n",
    "            # íƒ€ì„ì•„ì›ƒ ì²´í¬\n",
    "            if time.time() - start_time > timeout_seconds:\n",
    "                raise TimeoutError(f\"ì¶”ì¶œ ì‹œê°„ ì´ˆê³¼ ({timeout_seconds}ì´ˆ)\")\n",
    "            \n",
    "            # ê²°ê³¼ ì—…ë°ì´íŠ¸\n",
    "            if result and isinstance(result, dict):\n",
    "                extraction_result.update(result)\n",
    "                extraction_result['success'] = True\n",
    "                extraction_result['text_length'] = len(result.get('text', ''))\n",
    "                \n",
    "                # ê²°ê³¼ ì €ì¥ (ì˜µì…˜)\n",
    "                if save_result and result.get('text'):\n",
    "                    try:\n",
    "                        saved_file = self._save_extracted_text(\n",
    "                            result['text'], \n",
    "                            file_path.stem, \n",
    "                            result.get('extraction_method', method),\n",
    "                            result.get('metadata', {})\n",
    "                        )\n",
    "                        if saved_file:\n",
    "                            extraction_result['saved_files'].append(str(saved_file))\n",
    "                    except Exception as save_error:\n",
    "                        print(f\"âš ï¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_error}\")\n",
    "                \n",
    "                print(f\"   âœ… ì¶”ì¶œ ì„±ê³µ: {extraction_result['text_length']:,}ì\")\n",
    "            else:\n",
    "                raise ValueError(\"ì¶”ì¶œ ê²°ê³¼ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            extraction_result['error'] = str(e)\n",
    "            print(f\"   âŒ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}\")\n",
    "        \n",
    "        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°\n",
    "        extraction_result['processing_time'] = time.time() - extraction_result['start_time']\n",
    "        print(f\"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {extraction_result['processing_time']:.2f}ì´ˆ\")\n",
    "        \n",
    "        # í†µê³„ ì—…ë°ì´íŠ¸\n",
    "        self._update_stats(extraction_result)\n",
    "        \n",
    "        return extraction_result\n",
    "    \n",
    "    def _extract_from_pdf(self, file_path, method='auto', timeout_seconds=300):\n",
    "        \"\"\"PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì „í•œ ë²„ì „)\"\"\"\n",
    "        \n",
    "        if method == 'auto':\n",
    "            # PyPDF2 ë¨¼ì € ì‹œë„\n",
    "            if self.available_libs.get('PyPDF2', False):\n",
    "                try:\n",
    "                    result = self._extract_pdf_pypdf2(file_path)\n",
    "                    if len(result.get('text', '').strip()) > 50:  # ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œëœ ê²½ìš°\n",
    "                        return result\n",
    "                    else:\n",
    "                        print(f\"   ğŸ’¡ PyPDF2ë¡œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì•ˆë¨\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ğŸ’¡ PyPDF2 ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "            # OCR ì‹œë„\n",
    "            if self.available_libs.get('pytesseract', False) and self.available_libs.get('pdf2image', False):\n",
    "                try:\n",
    "                    print(f\"   ğŸ’¡ OCR ì‹œë„...\")\n",
    "                    return self._extract_pdf_ocr(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ğŸ’¡ OCR ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "            # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì½ê¸° ì‹œë„\n",
    "            try:\n",
    "                return self._extract_from_text(file_path)\n",
    "            except:\n",
    "                raise ValueError(\"ì‚¬ìš© ê°€ëŠ¥í•œ PDF ì¶”ì¶œ ë°©ë²•ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        elif method == 'pypdf2':\n",
    "            if not self.available_libs.get('PyPDF2', False):\n",
    "                raise ValueError(\"PyPDF2 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return self._extract_pdf_pypdf2(file_path)\n",
    "            \n",
    "        elif method == 'pdfplumber':\n",
    "            if not self.available_libs.get('pdfplumber', False):\n",
    "                raise ValueError(\"pdfplumber ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return self._extract_pdf_pdfplumber(file_path)\n",
    "            \n",
    "        elif method == 'ocr':\n",
    "            if not (self.available_libs.get('pytesseract', False) and self.available_libs.get('pdf2image', False)):\n",
    "                raise ValueError(\"OCR ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return self._extract_pdf_ocr(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"ì•Œ ìˆ˜ ì—†ëŠ” PDF ì¶”ì¶œ ë°©ë²•: {method}\")\n",
    "    \n",
    "    def _extract_pdf_pypdf2(self, file_path):\n",
    "        \"\"\"PyPDF2ë¥¼ ì‚¬ìš©í•œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì „í•œ ë²„ì „)\"\"\"\n",
    "        print(f\"   ğŸ”„ PyPDF2 ë°©ë²• ì‹œë„...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            import PyPDF2\n",
    "            \n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                \n",
    "                metadata = {\n",
    "                    'pages': len(pdf_reader.pages),\n",
    "                    'extraction_method': 'pypdf2',\n",
    "                    'processing_time': 0\n",
    "                }\n",
    "                \n",
    "                # PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)\n",
    "                try:\n",
    "                    if pdf_reader.metadata:\n",
    "                        metadata.update({\n",
    "                            'title': str(pdf_reader.metadata.get('/Title', '')),\n",
    "                            'author': str(pdf_reader.metadata.get('/Author', '')),\n",
    "                            'creator': str(pdf_reader.metadata.get('/Creator', '')),\n",
    "                        })\n",
    "                except Exception:\n",
    "                    pass  # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ë¬´ì‹œ\n",
    "                \n",
    "                # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                text_parts = []\n",
    "                for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                    try:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text and page_text.strip():\n",
    "                            text_parts.append(f\"=== í˜ì´ì§€ {page_num} ===\\n{page_text}\\n\")\n",
    "                    except Exception as e:\n",
    "                        text_parts.append(f\"=== í˜ì´ì§€ {page_num} (ì¶”ì¶œ ì‹¤íŒ¨) ===\\n\")\n",
    "                        if self.logger:\n",
    "                            self.logger.warning(f\"í˜ì´ì§€ {page_num} ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "                \n",
    "                full_text = '\\n'.join(text_parts)\n",
    "                metadata['processing_time'] = time.time() - start_time\n",
    "                \n",
    "                print(f\"   âœ… PyPDF2 ì„±ê³µ: {len(full_text):,}ì, {metadata['processing_time']:.2f}ì´ˆ\")\n",
    "                \n",
    "                return {\n",
    "                    'text': full_text,\n",
    "                    'metadata': metadata,\n",
    "                    'extraction_method': 'pypdf2'\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"PyPDF2 ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    def _extract_pdf_ocr(self, file_path):\n",
    "        \"\"\"OCRì„ ì‚¬ìš©í•œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ë³¸ êµ¬í˜„)\"\"\"\n",
    "        print(f\"   ğŸ”„ OCR ë°©ë²• ì‹œë„...\")\n",
    "        \n",
    "        try:\n",
    "            # ê¸°ë³¸ì ì¸ OCR êµ¬í˜„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)\n",
    "            import pytesseract\n",
    "            from pdf2image import convert_from_path\n",
    "            \n",
    "            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ì²« í˜ì´ì§€ë§Œ)\n",
    "            images = convert_from_path(file_path, dpi=150, first_page=1, last_page=1)\n",
    "            \n",
    "            if images:\n",
    "                text = pytesseract.image_to_string(images[0], lang='kor+eng')\n",
    "                \n",
    "                return {\n",
    "                    'text': f\"=== OCR ì¶”ì¶œ ê²°ê³¼ ===\\n{text}\",\n",
    "                    'metadata': {\n",
    "                        'pages': 1,\n",
    "                        'extraction_method': 'ocr',\n",
    "                        'language': 'kor+eng'\n",
    "                    },\n",
    "                    'extraction_method': 'ocr'\n",
    "                }\n",
    "            else:\n",
    "                raise ValueError(\"PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"OCR ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    def _extract_from_text(self, file_path):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "        print(f\"   ğŸ”„ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°...\")\n",
    "        \n",
    "        try:\n",
    "            # ì¸ì½”ë”© ìë™ ê°ì§€\n",
    "            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']\n",
    "            \n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=encoding) as file:\n",
    "                        text = file.read()\n",
    "                        \n",
    "                    return {\n",
    "                        'text': text,\n",
    "                        'metadata': {\n",
    "                            'file_size': file_path.stat().st_size,\n",
    "                            'encoding': encoding,\n",
    "                            'extraction_method': 'builtin'\n",
    "                        },\n",
    "                        'extraction_method': 'builtin'\n",
    "                    }\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            raise ValueError(\"ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    def _extract_from_docx(self, file_path):\n",
    "        \"\"\"Word ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "        if not self.available_libs.get('python-docx', False):\n",
    "            raise ValueError(\"python-docx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        print(f\"   ğŸ”„ Word ë¬¸ì„œ ì²˜ë¦¬...\")\n",
    "        \n",
    "        try:\n",
    "            from docx import Document\n",
    "            \n",
    "            doc = Document(file_path)\n",
    "            text_parts = []\n",
    "            \n",
    "            for paragraph in doc.paragraphs:\n",
    "                if paragraph.text.strip():\n",
    "                    text_parts.append(paragraph.text)\n",
    "            \n",
    "            full_text = '\\n'.join(text_parts)\n",
    "            \n",
    "            return {\n",
    "                'text': full_text,\n",
    "                'metadata': {\n",
    "                    'paragraphs': len(doc.paragraphs),\n",
    "                    'extraction_method': 'python-docx'\n",
    "                },\n",
    "                'extraction_method': 'python-docx'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Word ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    def _extract_from_pptx(self, file_path):\n",
    "        \"\"\"PowerPointì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "        if not self.available_libs.get('python-pptx', False):\n",
    "            raise ValueError(\"python-pptx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        print(f\"   ğŸ”„ PowerPoint ì²˜ë¦¬...\")\n",
    "        \n",
    "        try:\n",
    "            from pptx import Presentation\n",
    "            \n",
    "            prs = Presentation(file_path)\n",
    "            text_parts = []\n",
    "            \n",
    "            for slide_num, slide in enumerate(prs.slides, 1):\n",
    "                slide_texts = []\n",
    "                for shape in slide.shapes:\n",
    "                    if hasattr(shape, 'text') and shape.text.strip():\n",
    "                        slide_texts.append(shape.text)\n",
    "                \n",
    "                if slide_texts:\n",
    "                    text_parts.append(f\"=== ìŠ¬ë¼ì´ë“œ {slide_num} ===\\n\" + '\\n'.join(slide_texts))\n",
    "            \n",
    "            full_text = '\\n\\n'.join(text_parts)\n",
    "            \n",
    "            return {\n",
    "                'text': full_text,\n",
    "                'metadata': {\n",
    "                    'slides': len(prs.slides),\n",
    "                    'extraction_method': 'python-pptx'\n",
    "                },\n",
    "                'extraction_method': 'python-pptx'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"PowerPoint ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    def _extract_from_excel(self, file_path):\n",
    "        \"\"\"Excelì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ë³¸ êµ¬í˜„)\"\"\"\n",
    "        raise NotImplementedError(\"Excel ì¶”ì¶œì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    def _extract_from_html(self, file_path):\n",
    "        \"\"\"HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ë³¸ êµ¬í˜„)\"\"\"\n",
    "        raise NotImplementedError(\"HTML ì¶”ì¶œì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    def _extract_from_image(self, file_path):\n",
    "        \"\"\"ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ë³¸ êµ¬í˜„)\"\"\"\n",
    "        raise NotImplementedError(\"ì´ë¯¸ì§€ ì¶”ì¶œì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    def _save_extracted_text(self, text, filename, method, metadata=None):\n",
    "        \"\"\"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            if not self.config or not hasattr(self.config, 'extracted_texts_dir'):\n",
    "                return None\n",
    "                \n",
    "            output_file = self.config.extracted_texts_dir / f\"{filename}_{method}.txt\"\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"# ì¶”ì¶œ ë°©ë²•: {method}\\n\")\n",
    "                f.write(f\"# ì¶”ì¶œ ì‹œê°„: {datetime.now().isoformat()}\\n\")\n",
    "                if metadata:\n",
    "                    f.write(f\"# ë©”íƒ€ë°ì´í„°: {json.dumps(metadata, ensure_ascii=False, indent=2)}\\n\")\n",
    "                f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "                f.write(text)\n",
    "            \n",
    "            return output_file\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ í…ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _update_stats(self, result):\n",
    "        \"\"\"ì¶”ì¶œ í†µê³„ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        try:\n",
    "            file_type = result.get('file_type', 'unknown')\n",
    "            if file_type not in self.extraction_stats:\n",
    "                self.extraction_stats[file_type] = {\n",
    "                    'total_files': 0,\n",
    "                    'successful_extractions': 0,\n",
    "                    'failed_extractions': 0,\n",
    "                    'total_text_length': 0,\n",
    "                    'total_processing_time': 0.0\n",
    "                }\n",
    "            \n",
    "            stats = self.extraction_stats[file_type]\n",
    "            stats['total_files'] += 1\n",
    "            \n",
    "            if result.get('success', False):\n",
    "                stats['successful_extractions'] += 1\n",
    "                stats['total_text_length'] += result.get('text_length', 0)\n",
    "            else:\n",
    "                stats['failed_extractions'] += 1\n",
    "            \n",
    "            stats['total_processing_time'] += result.get('processing_time', 0.0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.warning(f\"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—”ì§„ ìƒì„±\n",
    "try:\n",
    "    if 'config' in globals() and config:\n",
    "        text_extractor = DocumentTextExtractor(config)\n",
    "        print(\"\\nğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "        print(\"   â€¢ text_extractor.extract_text('íŒŒì¼ê²½ë¡œ'): í…ìŠ¤íŠ¸ ì¶”ì¶œ\")\n",
    "        print(\"   â€¢ text_extractor.extract_text('íŒŒì¼ê²½ë¡œ', method='pypdf2'): íŠ¹ì • ë°©ë²• ì‚¬ìš©\")\n",
    "        print(\"   â€¢ text_extractor.extraction_stats: ì¶”ì¶œ í†µê³„ í™•ì¸\")\n",
    "    else:\n",
    "        print(\"âŒ ì´ì „ ì…€ì˜ configê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—”ì§„ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ’¡ ì´ì „ ì…€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•œ í›„ ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "417b7ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ ë©”ì†Œë“œ ë“±ë¡ ì‹œì‘â€¦\n",
      "ğŸ“„ ëª¨ë“  ë¬¸ì„œ í˜•ì‹ë³„ ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ!\n",
      "==================================================\n",
      "ğŸ“‹ ì§€ì›í•˜ëŠ” ì¶”ì¶œ ë°©ë²•:\n",
      "   â€¢ PDF: PyPDF2, pdfplumber, OCR\n",
      "   â€¢ Word: python-docx\n",
      "   â€¢ PowerPoint: python-pptx\n",
      "ğŸ“„ ëª¨ë“  ë¬¸ì„œ í˜•ì‹ë³„ ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ!\n",
      "==================================================\n",
      "ğŸ“‹ ì§€ì›í•˜ëŠ” ì¶”ì¶œ ë°©ë²•:\n",
      "   â€¢ PDF: PyPDF2, pdfplumber, OCR\n",
      "   â€¢ Word: python-docx\n",
      "   â€¢ PowerPoint: python-pptx\n",
      "   â€¢ Excel: openpyxl + pandas\n",
      "   â€¢ HTML: BeautifulSoup\n",
      "   â€¢ ì´ë¯¸ì§€: Tesseract OCR\n",
      "   â€¢ í…ìŠ¤íŠ¸: ì¸ì½”ë”© ìë™ ê°ì§€\n",
      "â±ï¸ ë“±ë¡ ì™„ë£Œ: 0.01ì´ˆ\n",
      "   â€¢ Excel: openpyxl + pandas\n",
      "   â€¢ HTML: BeautifulSoup\n",
      "   â€¢ ì´ë¯¸ì§€: Tesseract OCR\n",
      "   â€¢ í…ìŠ¤íŠ¸: ì¸ì½”ë”© ìë™ ê°ì§€\n",
      "â±ï¸ ë“±ë¡ ì™„ë£Œ: 0.01ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¤ ê°œë³„ ë¬¸ì„œ í˜•ì‹ë³„ ì¶”ì¶œ ë©”ì†Œë“œ êµ¬í˜„ (ê²½ëŸ‰/ì•ˆì „ ëª¨ë“œ)\n",
    "\n",
    "# ì¦‰ì‹œ í”¼ë“œë°±ì„ ìœ„í•´ ì‹œì‘ ë©”ì‹œì§€ ì¶œë ¥ (ë²„í¼ë§ ë°©ì§€)\n",
    "import sys, time, re\n",
    "print(\"âš™ï¸ ë©”ì†Œë“œ ë“±ë¡ ì‹œì‘â€¦\", flush=True)\n",
    "start_register_ts = time.time()\n",
    "\n",
    "# ê° ë©”ì†Œë“œëŠ” í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë‚´ë¶€ì—ì„œ ì„í¬íŠ¸í•˜ì—¬\n",
    "# ì…€ ì‹¤í–‰ ì‹œ ë¬´ê±°ìš´ ì„í¬íŠ¸ë¡œ ì¸í•´ ì¶œë ¥ì´ ì§€ì—°ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "def _extract_pdf_pdfplumber(self, file_path):\n",
    "    \"\"\"pdfplumberë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    print(f\"   ğŸ”„ pdfplumber ë°©ë²• ì‹œë„...\", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        import pdfplumber\n",
    "        \n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            metadata = {\n",
    "                'pages': len(pdf.pages),\n",
    "                'extraction_method': 'pdfplumber',\n",
    "                'processing_time': 0\n",
    "            }\n",
    "            \n",
    "            # PDF ë©”íƒ€ë°ì´í„°\n",
    "            try:\n",
    "                if getattr(pdf, 'metadata', None):\n",
    "                    metadata.update(pdf.metadata)\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            text_parts = []\n",
    "            \n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                try:\n",
    "                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                    page_text = page.extract_text() or \"\"\n",
    "                    \n",
    "                    # í…Œì´ë¸” ì¶”ì¶œ\n",
    "                    try:\n",
    "                        tables = page.extract_tables()\n",
    "                    except Exception:\n",
    "                        tables = None\n",
    "                    if tables:\n",
    "                        table_text = f\"\\n\\n=== í˜ì´ì§€ {page_num} í…Œì´ë¸” ===\\n\"\n",
    "                        for table_num, table in enumerate(tables, 1):\n",
    "                            table_text += f\"\\n--- í…Œì´ë¸” {table_num} ---\\n\"\n",
    "                            for row in table:\n",
    "                                if row:\n",
    "                                    clean_row = [str(cell).strip() if cell else \"\" for cell in row]\n",
    "                                    table_text += \" | \".join(clean_row) + \"\\n\"\n",
    "                        page_text += table_text\n",
    "                    \n",
    "                    if page_text.strip():\n",
    "                        text_parts.append(f\"=== í˜ì´ì§€ {page_num} ===\\n{page_text}\\n\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    text_parts.append(f\"=== í˜ì´ì§€ {page_num} (ì¶”ì¶œ ì‹¤íŒ¨: {e}) ===\\n\")\n",
    "            \n",
    "            full_text = '\\n'.join(text_parts)\n",
    "            metadata['processing_time'] = time.time() - start_time\n",
    "            \n",
    "            print(f\"   âœ… pdfplumber ì„±ê³µ: {len(full_text):,}ì, {metadata['processing_time']:.2f}ì´ˆ\", flush=True)\n",
    "            \n",
    "            return {\n",
    "                'text': full_text,\n",
    "                'metadata': metadata,\n",
    "                'extraction_method': 'pdfplumber'\n",
    "            }\n",
    "    \n",
    "    except ImportError:\n",
    "        raise ImportError(\"pdfplumber ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "def _extract_from_docx(self, file_path):\n",
    "    \"\"\"Word ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    print(f\"   ğŸ”„ python-docx ë°©ë²• ì‹œë„...\", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        from docx import Document\n",
    "    except Exception as e:\n",
    "        raise ImportError(f\"python-docx ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    doc = Document(file_path)\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "    core_props = doc.core_properties\n",
    "    metadata = {\n",
    "        'extraction_method': 'python-docx',\n",
    "        'title': getattr(core_props, 'title', '') or '',\n",
    "        'author': getattr(core_props, 'author', '') or '',\n",
    "        'created': str(core_props.created) if getattr(core_props, 'created', None) else '',\n",
    "        'modified': str(core_props.modified) if getattr(core_props, 'modified', None) else '',\n",
    "        'paragraphs': len(doc.paragraphs),\n",
    "        'tables': len(doc.tables),\n",
    "        'processing_time': 0\n",
    "    }\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    text_parts = []\n",
    "    \n",
    "    # ë‹¨ë½ í…ìŠ¤íŠ¸\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():\n",
    "            text_parts.append(para.text)\n",
    "    \n",
    "    # í…Œì´ë¸” í…ìŠ¤íŠ¸\n",
    "    for table_num, table in enumerate(doc.tables, 1):\n",
    "        table_text = f\"\\n\\n=== í…Œì´ë¸” {table_num} ===\\n\"\n",
    "        for row in table.rows:\n",
    "            row_cells = [cell.text.strip() for cell in row.cells]\n",
    "            table_text += \" | \".join(row_cells) + \"\\n\"\n",
    "        text_parts.append(table_text)\n",
    "    \n",
    "    full_text = '\\n'.join(text_parts)\n",
    "    metadata['processing_time'] = time.time() - start_time\n",
    "    \n",
    "    print(f\"   âœ… Word ì¶”ì¶œ ì„±ê³µ: {len(full_text):,}ì, {metadata['processing_time']:.2f}ì´ˆ\", flush=True)\n",
    "    \n",
    "    return {\n",
    "        'text': full_text,\n",
    "        'metadata': metadata,\n",
    "        'extraction_method': 'python-docx'\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_from_pptx(self, file_path):\n",
    "    \"\"\"PowerPointì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    print(f\"   ğŸ”„ python-pptx ë°©ë²• ì‹œë„...\", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        from pptx import Presentation\n",
    "    except Exception as e:\n",
    "        raise ImportError(f\"python-pptx ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    prs = Presentation(file_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'extraction_method': 'python-pptx',\n",
    "        'slides': len(prs.slides),\n",
    "        'processing_time': 0\n",
    "    }\n",
    "    \n",
    "    text_parts = []\n",
    "    \n",
    "    for slide_num, slide in enumerate(prs.slides, 1):\n",
    "        slide_text = f\"\\n\\n=== ìŠ¬ë¼ì´ë“œ {slide_num} ===\\n\"\n",
    "        \n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                slide_text += shape.text + \"\\n\"\n",
    "        \n",
    "        text_parts.append(slide_text)\n",
    "    \n",
    "    full_text = '\\n'.join(text_parts)\n",
    "    metadata['processing_time'] = time.time() - start_time\n",
    "    \n",
    "    print(f\"   âœ… PowerPoint ì¶”ì¶œ ì„±ê³µ: {len(full_text):,}ì, {metadata['processing_time']:.2f}ì´ˆ\", flush=True)\n",
    "    \n",
    "    return {\n",
    "        'text': full_text,\n",
    "        'metadata': metadata,\n",
    "        'extraction_method': 'python-pptx'\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_from_excel(self, file_path):\n",
    "    \"\"\"Excelì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    print(f\"   ğŸ”„ openpyxl+pandas ë°©ë²• ì‹œë„...\", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        import pandas as pd\n",
    "    except Exception as e:\n",
    "        raise ImportError(f\"pandas ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë“  ì‹œíŠ¸ ì½ê¸°\n",
    "        excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "        \n",
    "        metadata = {\n",
    "            'extraction_method': 'openpyxl+pandas',\n",
    "            'sheets': list(excel_data.keys()),\n",
    "            'sheet_count': len(excel_data),\n",
    "            'processing_time': 0\n",
    "        }\n",
    "        \n",
    "        text_parts = []\n",
    "        \n",
    "        for sheet_name, df in excel_data.items():\n",
    "            sheet_text = f\"\\n\\n=== ì‹œíŠ¸: {sheet_name} ===\\n\"\n",
    "            try:\n",
    "                sheet_text += df.to_string(index=False)\n",
    "            except Exception:\n",
    "                sheet_text += str(df)\n",
    "            text_parts.append(sheet_text)\n",
    "        \n",
    "        full_text = '\\n'.join(text_parts)\n",
    "        metadata['processing_time'] = time.time() - start_time\n",
    "        \n",
    "        print(f\"   âœ… Excel ì¶”ì¶œ ì„±ê³µ: {len(full_text):,}ì, {metadata['processing_time']:.2f}ì´ˆ\", flush=True)\n",
    "        \n",
    "        return {\n",
    "            'text': full_text,\n",
    "            'metadata': metadata,\n",
    "            'extraction_method': 'openpyxl+pandas'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Excel íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "def _extract_from_text(self, file_path):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ\"\"\"\n",
    "    print(f\"   ğŸ”„ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°...\", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        import chardet\n",
    "    except Exception as e:\n",
    "        raise ImportError(f\"chardet ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    # ì¸ì½”ë”© ê°ì§€\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        encoding_result = chardet.detect(raw_data)\n",
    "        encoding = encoding_result.get('encoding') or 'utf-8'\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì½ê¸°\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            text_content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # ì¸ì½”ë”© ì‹¤íŒ¨ì‹œ utf-8ë¡œ ì¬ì‹œë„\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text_content = f.read()\n",
    "    \n",
    "    metadata = {\n",
    "        'extraction_method': 'text_file',\n",
    "        'lines': len(text_content.split('\\n')),\n",
    "        'detected_encoding': encoding,\n",
    "        'confidence': encoding_result.get('confidence', 0),\n",
    "        'processing_time': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(text_content):,}ì, {metadata['processing_time']:.2f}ì´ˆ\", flush=True)\n",
    "    \n",
    "    return {\n",
    "        'text': text_content,\n",
    "        'metadata': metadata,\n",
    "        'extraction_method': 'text_file'\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_from_html(self, file_path):\n",
    "    \"\"\"HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    print(f\"   ğŸ”„ BeautifulSoup ë°©ë²• ì‹œë„...\", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # HTML íŒŒì¼ ì½ê¸°\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    try:\n",
    "        from bs4 import BeautifulSoup\n",
    "    except Exception as e:\n",
    "        raise ImportError(f\"beautifulsoup4 ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # BeautifulSoupìœ¼ë¡œ ì •ì œëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°\n",
    "    for tag in soup(['script', 'style', 'meta', 'link', 'head']):\n",
    "        try:\n",
    "            tag.decompose()\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # í…Œì´ë¸” ì²˜ë¦¬\n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        table_text = \"\\n\\n=== í…Œì´ë¸” ===\\n\"\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            row_text = []\n",
    "            for cell in cells:\n",
    "                cell_text = cell.get_text().strip()\n",
    "                row_text.append(cell_text)\n",
    "            if row_text:\n",
    "                table_text += \" | \".join(row_text) + \"\\n\"\n",
    "        table.replace_with(table_text)\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)  # HTML ì£¼ì„\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # ì—°ì† ê°œí–‰\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # ì—°ì† ê³µë°±\n",
    "    processed_text = text.strip()\n",
    "    \n",
    "    metadata = {\n",
    "        'extraction_method': 'beautifulsoup',\n",
    "        'content_length': len(html_content),\n",
    "        'processed_length': len(processed_text),\n",
    "        'processing_time': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… HTML ì¶”ì¶œ ì„±ê³µ: {len(processed_text):,}ì, {metadata['processing_time']:.2f}ì´ˆ\", flush=True)\n",
    "    \n",
    "    return {\n",
    "        'text': processed_text,\n",
    "        'metadata': metadata,\n",
    "        'extraction_method': 'beautifulsoup'\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_from_image(self, file_path):\n",
    "    \"\"\"ì´ë¯¸ì§€ì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    print(f\"   ğŸ”„ ì´ë¯¸ì§€ OCR ì‹œë„...\", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not self.config.ocr_config['enabled']:\n",
    "        raise ValueError(\"OCRì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    try:\n",
    "        from PIL import Image\n",
    "    except Exception as e:\n",
    "        raise ImportError(f\"Pillow ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    try:\n",
    "        import pytesseract\n",
    "    except Exception as e:\n",
    "        raise ImportError(f\"pytesseract ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    try:\n",
    "        image = Image.open(file_path)\n",
    "        \n",
    "        # OCR ì‹¤í–‰\n",
    "        text = pytesseract.image_to_string(\n",
    "            image,\n",
    "            lang=self.config.ocr_config['language']\n",
    "        )\n",
    "        \n",
    "        metadata = {\n",
    "            'extraction_method': 'tesseract_ocr',\n",
    "            'image_size': getattr(image, 'size', None),\n",
    "            'image_mode': getattr(image, 'mode', None),\n",
    "            'ocr_language': self.config.ocr_config['language'],\n",
    "            'processing_time': time.time() - start_time\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… ì´ë¯¸ì§€ OCR ì„±ê³µ: {len(text):,}ì, {metadata['processing_time']:.2f}ì´ˆ\", flush=True)\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'metadata': metadata,\n",
    "            'extraction_method': 'tesseract_ocr'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ì´ë¯¸ì§€ OCR ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# TextExtractorì— ëª¨ë“  ë©”ì†Œë“œ ì¶”ê°€\n",
    "DocumentTextExtractor._extract_pdf_pdfplumber = _extract_pdf_pdfplumber\n",
    "DocumentTextExtractor._extract_from_docx = _extract_from_docx\n",
    "DocumentTextExtractor._extract_from_pptx = _extract_from_pptx\n",
    "DocumentTextExtractor._extract_from_excel = _extract_from_excel\n",
    "DocumentTextExtractor._extract_from_text = _extract_from_text\n",
    "DocumentTextExtractor._extract_from_html = _extract_from_html\n",
    "DocumentTextExtractor._extract_from_image = _extract_from_image\n",
    "\n",
    "print(\"ğŸ“„ ëª¨ë“  ë¬¸ì„œ í˜•ì‹ë³„ ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ!\", flush=True)\n",
    "print(\"=\" * 50, flush=True)\n",
    "print(\"ğŸ“‹ ì§€ì›í•˜ëŠ” ì¶”ì¶œ ë°©ë²•:\", flush=True)\n",
    "print(\"   â€¢ PDF: PyPDF2, pdfplumber, OCR\", flush=True)\n",
    "print(\"   â€¢ Word: python-docx\", flush=True)\n",
    "print(\"   â€¢ PowerPoint: python-pptx\", flush=True)\n",
    "print(\"   â€¢ Excel: openpyxl + pandas\", flush=True)\n",
    "print(\"   â€¢ HTML: BeautifulSoup\", flush=True)\n",
    "print(\"   â€¢ ì´ë¯¸ì§€: Tesseract OCR\", flush=True)\n",
    "print(\"   â€¢ í…ìŠ¤íŠ¸: ì¸ì½”ë”© ìë™ ê°ì§€\", flush=True)\n",
    "print(f\"â±ï¸ ë“±ë¡ ì™„ë£Œ: {(time.time() - start_register_ts):.2f}ì´ˆ\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b952d09",
   "metadata": {},
   "source": [
    "## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd2255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   â€¢ test_single_file('íŒŒì¼ê²½ë¡œ'): ëª¨ë“  ë°©ë²•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
      "   â€¢ quick_test_pdf('pdfíŒŒì¼'): PDF ì „ìš© í…ŒìŠ¤íŠ¸\n",
      "   â€¢ quick_test_auto('íŒŒì¼ê²½ë¡œ'): ìë™ ë°©ë²• ì„ íƒ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
    "\n",
    "def test_single_file(file_path, methods=None, save_results=True):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ íŒŒì¼ì— ëŒ€í•´ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        file_path: í…ŒìŠ¤íŠ¸í•  íŒŒì¼ ê²½ë¡œ\n",
    "        methods: í…ŒìŠ¤íŠ¸í•  ì¶”ì¶œ ë°©ë²•ë“¤ (Noneì´ë©´ ìë™ ì„ íƒ)\n",
    "        save_results: ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    file_type = config.get_file_type(file_path)\n",
    "    if file_type == 'unknown':\n",
    "        print(f\"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ§ª ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸: {file_path.name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ“„ íŒŒì¼ íƒ€ì…: {file_type}\")\n",
    "    print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_path.stat().st_size / 1024 / 1024:.2f}MB\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸í•  ë°©ë²•ë“¤ ê²°ì •\n",
    "    if methods is None:\n",
    "        methods = config.get_available_methods(file_type)\n",
    "        # PDFì˜ ê²½ìš° ëª¨ë“  ë°©ë²• í…ŒìŠ¤íŠ¸\n",
    "        if file_type == 'pdf':\n",
    "            methods = ['pypdf2', 'pdfplumber', 'ocr']\n",
    "    \n",
    "    print(f\"ğŸ”§ í…ŒìŠ¤íŠ¸ ë°©ë²•: {', '.join(methods)}\")\n",
    "    print()\n",
    "    \n",
    "    # ê° ë°©ë²•ë³„ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "    test_results = []\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"ğŸ”„ {method} í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            result = text_extractor.extract_text(\n",
    "                file_path, \n",
    "                method=method, \n",
    "                save_result=save_results\n",
    "            )\n",
    "            test_results.append(result)\n",
    "            \n",
    "            # ê²°ê³¼ ìš”ì•½\n",
    "            if result['success']:\n",
    "                text_length = result['text_length']\n",
    "                processing_time = result['processing_time']\n",
    "                print(f\"âœ… ì„±ê³µ: {text_length:,}ì, {processing_time:.2f}ì´ˆ\")\n",
    "                \n",
    "                # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°\n",
    "                if text_length > 0:\n",
    "                    preview = result['text'][:200].replace('\\n', ' ').strip()\n",
    "                    print(f\"ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: {preview}...\")\n",
    "                    \n",
    "                    # í•œê¸€ ë¹„ìœ¨ ê³„ì‚°\n",
    "                    korean_chars = len([c for c in result['text'] if 'ê°€' <= c <= 'í£'])\n",
    "                    if text_length > 0:\n",
    "                        korean_ratio = korean_chars / text_length * 100\n",
    "                        print(f\"ğŸ‡°ğŸ‡· í•œê¸€ ë¹„ìœ¨: {korean_ratio:.1f}%\")\n",
    "            else:\n",
    "                print(f\"âŒ ì‹¤íŒ¨: {result['error']}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜ˆì™¸ ë°œìƒ: {e}\")\n",
    "            test_results.append({\n",
    "                'file_path': str(file_path),\n",
    "                'file_type': file_type,\n",
    "                'extraction_method': method,\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'text_length': 0,\n",
    "                'processing_time': 0\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # ê²°ê³¼ ë¹„êµ ë¶„ì„\n",
    "    if len(test_results) > 1:\n",
    "        print(\"ğŸ“Š ë°©ë²•ë³„ ê²°ê³¼ ë¹„êµ\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        successful_results = [r for r in test_results if r['success']]\n",
    "        \n",
    "        if successful_results:\n",
    "            # ìµœê³  í’ˆì§ˆ (ê°€ì¥ ë§ì€ í…ìŠ¤íŠ¸)\n",
    "            best_quality = max(successful_results, key=lambda x: x['text_length'])\n",
    "            print(f\"ğŸ¯ ìµœê³  í’ˆì§ˆ: {best_quality['extraction_method']} ({best_quality['text_length']:,}ì)\")\n",
    "            \n",
    "            # ìµœê³  ì†ë„ (ê°€ì¥ ë¹ ë¥¸ ì²˜ë¦¬)\n",
    "            best_speed = min(successful_results, key=lambda x: x['processing_time'])\n",
    "            print(f\"âš¡ ìµœê³  ì†ë„: {best_speed['extraction_method']} ({best_speed['processing_time']:.2f}ì´ˆ)\")\n",
    "            \n",
    "            # ì¶”ì²œ ë°©ë²•\n",
    "            if best_quality['text_length'] > 1000:\n",
    "                print(f\"ğŸ’¡ ì¶”ì²œ: {best_quality['extraction_method']} (ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)\")\n",
    "            elif file_type == 'pdf' and any(r['extraction_method'] == 'ocr' for r in successful_results):\n",
    "                ocr_result = next(r for r in successful_results if r['extraction_method'] == 'ocr')\n",
    "                print(f\"ğŸ’¡ ì¶”ì²œ: OCR (ì´ë¯¸ì§€ ê¸°ë°˜ PDFë¡œ íŒë‹¨)\")\n",
    "            else:\n",
    "                print(f\"ğŸ’¡ ì¶”ì²œ: {best_speed['extraction_method']} (ì†ë„ ìš°ì„ )\")\n",
    "        else:\n",
    "            print(\"âŒ ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    if save_results:\n",
    "        result_file = config.test_results_dir / f\"{file_path.stem}_test_results.json\"\n",
    "        \n",
    "        # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "        serializable_results = []\n",
    "        for result in test_results:\n",
    "            serializable_result = result.copy()\n",
    "            for key, value in serializable_result.items():\n",
    "                if isinstance(value, datetime):\n",
    "                    serializable_result[key] = value.isoformat()\n",
    "                elif isinstance(value, Path):\n",
    "                    serializable_result[key] = str(value)\n",
    "            serializable_results.append(serializable_result)\n",
    "        \n",
    "        with open(result_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'file_info': {\n",
    "                    'name': file_path.name,\n",
    "                    'path': str(file_path),\n",
    "                    'type': file_type,\n",
    "                    'size_mb': file_path.stat().st_size / 1024 / 1024\n",
    "                },\n",
    "                'test_config': {\n",
    "                    'methods_tested': methods,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                },\n",
    "                'results': serializable_results\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"ğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {result_file.name}\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í¸ì˜ í•¨ìˆ˜ë“¤\n",
    "def quick_test_pdf(file_path):\n",
    "    \"\"\"PDF íŒŒì¼ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ëª¨ë“  ë°©ë²•)\"\"\"\n",
    "    return test_single_file(file_path, methods=['pypdf2', 'pdfplumber', 'ocr'])\n",
    "\n",
    "def quick_test_auto(file_path):\n",
    "    \"\"\"ìë™ ë°©ë²• ì„ íƒìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    return test_single_file(file_path, methods=['auto'])\n",
    "\n",
    "print(\"ğŸ§ª ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "print(\"   â€¢ test_single_file('íŒŒì¼ê²½ë¡œ'): ëª¨ë“  ë°©ë²•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"   â€¢ quick_test_pdf('pdfíŒŒì¼'): PDF ì „ìš© í…ŒìŠ¤íŠ¸\")\n",
    "print(\"   â€¢ quick_test_auto('íŒŒì¼ê²½ë¡œ'): ìë™ ë°©ë²• ì„ íƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e054ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë¶„ì„ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   â€¢ test_multiple_files(): ëª¨ë“  íŒŒì¼ ë°°ì¹˜ í…ŒìŠ¤íŠ¸\n",
      "   â€¢ test_multiple_files(file_types=['pdf']): PDFë§Œ í…ŒìŠ¤íŠ¸\n",
      "   â€¢ analyze_extraction_performance(): ì„±ëŠ¥ í†µê³„ ë¶„ì„\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë¶„ì„ í•¨ìˆ˜\n",
    "\n",
    "def test_multiple_files(file_paths=None, file_types=None, max_files=10, save_results=True):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ íŒŒì¼ì— ëŒ€í•´ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: íŠ¹ì • íŒŒì¼ë“¤ì˜ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
    "        file_types: í…ŒìŠ¤íŠ¸í•  íŒŒì¼ íƒ€ì…ë“¤ ('pdf', 'word' ë“±)\n",
    "        max_files: ìµœëŒ€ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜\n",
    "        save_results: ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸í•  íŒŒì¼ë“¤ ìˆ˜ì§‘\n",
    "    if file_paths:\n",
    "        test_files = [Path(f) for f in file_paths if Path(f).exists()]\n",
    "    else:\n",
    "        # ì…ë ¥ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ìˆ˜ì§‘\n",
    "        all_files = document_loader.scan_input_directory(show_details=False)\n",
    "        \n",
    "        if file_types:\n",
    "            test_files = [f for f in all_files if config.get_file_type(f) in file_types]\n",
    "        else:\n",
    "            test_files = all_files\n",
    "        \n",
    "        # íŒŒì¼ ìˆ˜ ì œí•œ\n",
    "        if max_files and len(test_files) > max_files:\n",
    "            test_files = test_files[:max_files]\n",
    "    \n",
    "    if not test_files:\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"ğŸ“ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {len(test_files)}ê°œ íŒŒì¼\")\n",
    "    \n",
    "    # íŒŒì¼ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "    all_results = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for i, file_path in enumerate(test_files, 1):\n",
    "        print(f\"\\nğŸ”„ íŒŒì¼ {i}/{len(test_files)}: {file_path.name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # ìë™ ë°©ë²•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ì²˜ë¦¬)\n",
    "            result = text_extractor.extract_text(file_path, method='auto', save_result=save_results)\n",
    "            all_results.append(result)\n",
    "            \n",
    "            if result['success']:\n",
    "                print(f\"âœ… ì„±ê³µ: {result['text_length']:,}ì, {result['processing_time']:.2f}ì´ˆ\")\n",
    "            else:\n",
    "                print(f\"âŒ ì‹¤íŒ¨: {result['error']}\")\n",
    "                failed_files.append(file_path.name)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜ˆì™¸: {e}\")\n",
    "            failed_files.append(file_path.name)\n",
    "    \n",
    "    # ì „ì²´ ê²°ê³¼ ë¶„ì„\n",
    "    print(f\"\\nğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    successful_results = [r for r in all_results if r['success']]\n",
    "    \n",
    "    print(f\"ğŸ“ˆ ì„±ê³µë¥ : {len(successful_results)}/{len(all_results)} ({len(successful_results)/len(all_results)*100:.1f}%)\")\n",
    "    \n",
    "    if successful_results:\n",
    "        # ì²˜ë¦¬ í†µê³„\n",
    "        total_text = sum(r['text_length'] for r in successful_results)\n",
    "        total_time = sum(r['processing_time'] for r in successful_results)\n",
    "        avg_time = total_time / len(successful_results)\n",
    "        \n",
    "        print(f\"ğŸ“ ì´ ì¶”ì¶œ í…ìŠ¤íŠ¸: {total_text:,}ì\")\n",
    "        print(f\"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "        print(f\"ğŸ“Š í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ/íŒŒì¼\")\n",
    "        \n",
    "        # íŒŒì¼ íƒ€ì…ë³„ í†µê³„\n",
    "        type_stats = {}\n",
    "        for result in successful_results:\n",
    "            file_type = result['file_type']\n",
    "            if file_type not in type_stats:\n",
    "                type_stats[file_type] = {'count': 0, 'total_text': 0, 'total_time': 0}\n",
    "            \n",
    "            type_stats[file_type]['count'] += 1\n",
    "            type_stats[file_type]['total_text'] += result['text_length']\n",
    "            type_stats[file_type]['total_time'] += result['processing_time']\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ íŒŒì¼ íƒ€ì…ë³„ í†µê³„:\")\n",
    "        for file_type, stats in type_stats.items():\n",
    "            avg_text = stats['total_text'] / stats['count']\n",
    "            avg_time = stats['total_time'] / stats['count']\n",
    "            print(f\"   {file_type.upper()}: {stats['count']}ê°œ, í‰ê·  {avg_text:,.0f}ì, {avg_time:.2f}ì´ˆ\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:\")\n",
    "        for filename in failed_files:\n",
    "            print(f\"   â€¢ {filename}\")\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    if save_results and all_results:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        result_file = config.test_results_dir / f\"batch_test_results_{timestamp}.json\"\n",
    "        \n",
    "        # ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„\n",
    "        batch_summary = {\n",
    "            'test_info': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'total_files': len(test_files),\n",
    "                'successful_files': len(successful_results),\n",
    "                'failed_files': len(failed_files),\n",
    "                'success_rate': len(successful_results) / len(all_results) * 100 if all_results else 0\n",
    "            },\n",
    "            'performance_summary': {\n",
    "                'total_text_length': sum(r['text_length'] for r in successful_results),\n",
    "                'total_processing_time': sum(r['processing_time'] for r in successful_results),\n",
    "                'average_processing_time': avg_time if successful_results else 0\n",
    "            } if successful_results else {},\n",
    "            'type_statistics': type_stats if successful_results else {},\n",
    "            'failed_files': failed_files,\n",
    "            'detailed_results': all_results\n",
    "        }\n",
    "        \n",
    "        with open(result_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(batch_summary, f, ensure_ascii=False, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {result_file.name}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def analyze_extraction_performance():\n",
    "    \"\"\"ì¶”ì¶œê¸°ì˜ ì„±ëŠ¥ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\"\"\"\n",
    "    stats = text_extractor.get_extraction_stats()\n",
    "    \n",
    "    if not stats:\n",
    "        print(\"ğŸ“Š ì•„ì§ ì¶”ì¶œ í†µê³„ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ“Š í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ëŠ¥ ë¶„ì„\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_files = 0\n",
    "    total_success = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    for file_type, methods in stats.items():\n",
    "        print(f\"\\nğŸ“ {file_type.upper()} íŒŒì¼:\")\n",
    "        \n",
    "        for method, method_stats in methods.items():\n",
    "            count = method_stats['count']\n",
    "            success = method_stats['success']\n",
    "            avg_time = method_stats['total_time'] / count if count > 0 else 0\n",
    "            avg_text = method_stats['total_text_length'] / success if success > 0 else 0\n",
    "            success_rate = success / count * 100 if count > 0 else 0\n",
    "            \n",
    "            print(f\"   ğŸ”§ {method}:\")\n",
    "            print(f\"      â€¢ ì‹œë„: {count}íšŒ\")\n",
    "            print(f\"      â€¢ ì„±ê³µ: {success}íšŒ ({success_rate:.1f}%)\")\n",
    "            print(f\"      â€¢ í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ\")\n",
    "            print(f\"      â€¢ í‰ê·  í…ìŠ¤íŠ¸: {avg_text:,.0f}ì\")\n",
    "            \n",
    "            total_files += count\n",
    "            total_success += success\n",
    "            total_time += method_stats['total_time']\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ì „ì²´ ìš”ì•½:\")\n",
    "    print(f\"   ğŸ“Š ì´ ì²˜ë¦¬: {total_files}íšŒ\")\n",
    "    print(f\"   âœ… ì„±ê³µë¥ : {total_success}/{total_files} ({total_success/total_files*100:.1f}%)\")\n",
    "    print(f\"   â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
    "    print(f\"   ğŸ“ˆ í‰ê·  ì‹œê°„: {total_time/total_files:.2f}ì´ˆ/íŒŒì¼\" if total_files > 0 else \"\")\n",
    "\n",
    "print(\"ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë¶„ì„ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "print(\"   â€¢ test_multiple_files(): ëª¨ë“  íŒŒì¼ ë°°ì¹˜ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"   â€¢ test_multiple_files(file_types=['pdf']): PDFë§Œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"   â€¢ analyze_extraction_performance(): ì„±ëŠ¥ í†µê³„ ë¶„ì„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98531f0a",
   "metadata": {},
   "source": [
    "## ğŸš€ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd7c2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° í…ŒìŠ¤íŠ¸ íŒŒì¼ í˜„í™©\n",
      "============================================================\n",
      "ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº”: /home/admin/wkms-aws/jupyter_notebook/data/input_docs\n",
      "============================================================\n",
      "ğŸ“Š ìŠ¤ìº” ê²°ê³¼:\n",
      "   ì „ì²´ íŒŒì¼: 3ê°œ\n",
      "   ì§€ì› íŒŒì¼: 3ê°œ\n",
      "   ì§€ì› íƒ€ì…: 1ê°œ\n",
      "\n",
      "ğŸ“‹ íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜:\n",
      "\n",
      "   ğŸ“ PDF (3ê°œ) - ì¶”ì¶œë°©ë²•: pypdf2, pdfplumber, ocr\n",
      "      â€¢ 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf (0.32MB)\n",
      "      â€¢ test1.pdf (1.26MB)\n",
      "      â€¢ test.pdf (11.85MB)\n",
      "\n",
      "ğŸ“‹ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ íŒŒì¼ í˜„í™©:\n",
      "   ì´ 3ê°œ íŒŒì¼\n",
      "   ì§€ì› íƒ€ì…: 1ê°œ\n",
      "\n",
      "   ğŸ“ PDF (3ê°œ):\n",
      "      â€¢ 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf (0.32MB) â­ ì¶”ì²œ\n",
      "      â€¢ test1.pdf (1.26MB) â­ ì¶”ì²œ\n",
      "      â€¢ ... ë° 1ê°œ í° íŒŒì¼ë“¤ (10MB+)\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\n",
      "   1. ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸: test_single_file('íŒŒì¼ê²½ë¡œ')\n",
      "   2. ë°°ì¹˜ í…ŒìŠ¤íŠ¸: test_multiple_files()\n",
      "   3. PDF ì „ìš© í…ŒìŠ¤íŠ¸: quick_test_pdf('pdfíŒŒì¼ê²½ë¡œ')\n",
      "   4. ì„±ëŠ¥ ë¶„ì„: analyze_extraction_performance()\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° íŒŒì¼ í˜„í™© í™•ì¸\n",
    "\n",
    "print(\"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° í…ŒìŠ¤íŠ¸ íŒŒì¼ í˜„í™©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº”\n",
    "available_files = document_loader.scan_input_directory(show_details=True)\n",
    "\n",
    "print(f\"\\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ íŒŒì¼ í˜„í™©:\")\n",
    "if available_files:\n",
    "    # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜\n",
    "    files_by_type = {}\n",
    "    for file_path in available_files:\n",
    "        file_type = config.get_file_type(file_path)\n",
    "        if file_type not in files_by_type:\n",
    "            files_by_type[file_type] = []\n",
    "        files_by_type[file_type].append(file_path)\n",
    "    \n",
    "    print(f\"   ì´ {len(available_files)}ê°œ íŒŒì¼\")\n",
    "    print(f\"   ì§€ì› íƒ€ì…: {len(files_by_type)}ê°œ\")\n",
    "    \n",
    "    # íƒ€ì…ë³„ ì¶”ì²œ í…ŒìŠ¤íŠ¸ íŒŒì¼\n",
    "    for file_type, files in files_by_type.items():\n",
    "        print(f\"\\n   ğŸ“ {file_type.upper()} ({len(files)}ê°œ):\")\n",
    "        \n",
    "        # í¬ê¸°ê°€ ì ë‹¹í•œ íŒŒì¼ë“¤ ìš°ì„  ì¶”ì²œ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "        recommended_files = []\n",
    "        for file_path in files:\n",
    "            size_mb = file_path.stat().st_size / 1024 / 1024\n",
    "            if size_mb < 10:  # 10MB ë¯¸ë§Œ ì¶”ì²œ\n",
    "                recommended_files.append((file_path, size_mb))\n",
    "        \n",
    "        # í¬ê¸°ìˆœ ì •ë ¬\n",
    "        recommended_files.sort(key=lambda x: x[1])\n",
    "        \n",
    "        for file_path, size_mb in recommended_files[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ\n",
    "            print(f\"      â€¢ {file_path.name} ({size_mb:.2f}MB) â­ ì¶”ì²œ\")\n",
    "        \n",
    "        if len(files) > len(recommended_files):\n",
    "            large_files = len(files) - len(recommended_files)\n",
    "            print(f\"      â€¢ ... ë° {large_files}ê°œ í° íŒŒì¼ë“¤ (10MB+)\")\n",
    "\n",
    "else:\n",
    "    print(\"   âŒ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   ğŸ’¡ ë‹¤ìŒ ë””ë ‰í† ë¦¬ì— ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ì„¸ìš”: {config.input_docs_dir}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(f\"   1. ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸: test_single_file('íŒŒì¼ê²½ë¡œ')\")\n",
    "print(f\"   2. ë°°ì¹˜ í…ŒìŠ¤íŠ¸: test_multiple_files()\")\n",
    "print(f\"   3. PDF ì „ìš© í…ŒìŠ¤íŠ¸: quick_test_pdf('pdfíŒŒì¼ê²½ë¡œ')\")\n",
    "print(f\"   4. ì„±ëŠ¥ ë¶„ì„: analyze_extraction_performance()\")\n",
    "\n",
    "# ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥ (ë‹¤ìŒ ì…€ì—ì„œ ì‚¬ìš©)\n",
    "AVAILABLE_TEST_FILES = available_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f196a4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì²« ë²ˆì§¸ íŒŒì¼)\n",
      "==================================================\n",
      "ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf\n",
      "ğŸ“„ PDF íŒŒì¼ - ëª¨ë“  ì¶”ì¶œ ë°©ë²• í…ŒìŠ¤íŠ¸\n",
      "ğŸ§ª ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸: 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf\n",
      "============================================================\n",
      "ğŸ“„ íŒŒì¼ íƒ€ì…: pdf\n",
      "ğŸ“Š íŒŒì¼ í¬ê¸°: 0.32MB\n",
      "ğŸ”§ í…ŒìŠ¤íŠ¸ ë°©ë²•: pypdf2, pdfplumber, ocr\n",
      "\n",
      "ğŸ”„ pypdf2 í…ŒìŠ¤íŠ¸ ì‹œì‘\n",
      "------------------------------\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf\n",
      "   ğŸ“„ íŒŒì¼ íƒ€ì…: pdf\n",
      "   ğŸ”§ ì¶”ì¶œ ë°©ë²•: pypdf2\n",
      "   ğŸ”„ PyPDF2 ë°©ë²• ì‹œë„...\n",
      "   âœ… PyPDF2 ì„±ê³µ: 1,616ì, 0.14ì´ˆ\n",
      "   âœ… ì¶”ì¶œ ì„±ê³µ: 1,616ì\n",
      "   â±ï¸ ì²˜ë¦¬ ì‹œê°„: 0.14ì´ˆ\n",
      "âœ… ì„±ê³µ: 1,616ì, 0.14ì´ˆ\n",
      "ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: === í˜ì´ì§€ 1 ===   ìŠ¤ë§ˆíŠ¸ ì¸ìŠë¦° íŒí”„   ì œí’ˆ ì‚¬ì–‘ì„œ (ë°ëª¨ìš©)  ë¬¸ì„œë²ˆí˜¸: PROD-INS-PUMP-001 ë²„ì „: 0.1 ì‘ì„±ì¼: 2025-08-12 â€»     ë³¸ë¬¸ì„œëŠ”ë°ëª¨ìš©ê°€ìƒìë£Œì´ë©° ,        ê·œì œì œì¶œë˜ëŠ”ì§„ë£Œì˜ì‚¬ê²°ì •ì—ì‚¬ìš©í• ìˆ˜ì—†ìŠµë‹ˆë‹¤ .  === í˜ì´ì§€ 2 ===  ê·¸ë¦¼1.   ì‚¬ìš©ì‹œë‚˜ë¦¬ì˜¤ì˜ˆì‹œ(ë³‘ì›/ê°€ì •/ìš´ë™/ ì‹œìŠ¤í…œê°œìš”)  === í˜ì´...\n",
      "ğŸ‡°ğŸ‡· í•œê¸€ ë¹„ìœ¨: 44.7%\n",
      "\n",
      "ğŸ”„ pdfplumber í…ŒìŠ¤íŠ¸ ì‹œì‘\n",
      "------------------------------\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf\n",
      "   ğŸ“„ íŒŒì¼ íƒ€ì…: pdf\n",
      "   ğŸ”§ ì¶”ì¶œ ë°©ë²•: pdfplumber\n",
      "   âŒ ì¶”ì¶œ ì‹¤íŒ¨: pdfplumber ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
      "   â±ï¸ ì²˜ë¦¬ ì‹œê°„: 0.00ì´ˆ\n",
      "âŒ ì‹¤íŒ¨: pdfplumber ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ”„ ocr í…ŒìŠ¤íŠ¸ ì‹œì‘\n",
      "------------------------------\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf\n",
      "   ğŸ“„ íŒŒì¼ íƒ€ì…: pdf\n",
      "   ğŸ”§ ì¶”ì¶œ ë°©ë²•: ocr\n",
      "   ğŸ”„ OCR ë°©ë²• ì‹œë„...\n",
      "   âœ… PyPDF2 ì„±ê³µ: 1,616ì, 0.14ì´ˆ\n",
      "   âœ… ì¶”ì¶œ ì„±ê³µ: 1,616ì\n",
      "   â±ï¸ ì²˜ë¦¬ ì‹œê°„: 0.14ì´ˆ\n",
      "âœ… ì„±ê³µ: 1,616ì, 0.14ì´ˆ\n",
      "ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: === í˜ì´ì§€ 1 ===   ìŠ¤ë§ˆíŠ¸ ì¸ìŠë¦° íŒí”„   ì œí’ˆ ì‚¬ì–‘ì„œ (ë°ëª¨ìš©)  ë¬¸ì„œë²ˆí˜¸: PROD-INS-PUMP-001 ë²„ì „: 0.1 ì‘ì„±ì¼: 2025-08-12 â€»     ë³¸ë¬¸ì„œëŠ”ë°ëª¨ìš©ê°€ìƒìë£Œì´ë©° ,        ê·œì œì œì¶œë˜ëŠ”ì§„ë£Œì˜ì‚¬ê²°ì •ì—ì‚¬ìš©í• ìˆ˜ì—†ìŠµë‹ˆë‹¤ .  === í˜ì´ì§€ 2 ===  ê·¸ë¦¼1.   ì‚¬ìš©ì‹œë‚˜ë¦¬ì˜¤ì˜ˆì‹œ(ë³‘ì›/ê°€ì •/ìš´ë™/ ì‹œìŠ¤í…œê°œìš”)  === í˜ì´...\n",
      "ğŸ‡°ğŸ‡· í•œê¸€ ë¹„ìœ¨: 44.7%\n",
      "\n",
      "ğŸ”„ pdfplumber í…ŒìŠ¤íŠ¸ ì‹œì‘\n",
      "------------------------------\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf\n",
      "   ğŸ“„ íŒŒì¼ íƒ€ì…: pdf\n",
      "   ğŸ”§ ì¶”ì¶œ ë°©ë²•: pdfplumber\n",
      "   âŒ ì¶”ì¶œ ì‹¤íŒ¨: pdfplumber ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
      "   â±ï¸ ì²˜ë¦¬ ì‹œê°„: 0.00ì´ˆ\n",
      "âŒ ì‹¤íŒ¨: pdfplumber ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ”„ ocr í…ŒìŠ¤íŠ¸ ì‹œì‘\n",
      "------------------------------\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf\n",
      "   ğŸ“„ íŒŒì¼ íƒ€ì…: pdf\n",
      "   ğŸ”§ ì¶”ì¶œ ë°©ë²•: ocr\n",
      "   ğŸ”„ OCR ë°©ë²• ì‹œë„...\n",
      "   âœ… ì¶”ì¶œ ì„±ê³µ: 132ì\n",
      "   â±ï¸ ì²˜ë¦¬ ì‹œê°„: 1.47ì´ˆ\n",
      "âœ… ì„±ê³µ: 132ì, 1.47ì´ˆ\n",
      "ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: === OCR ì¶”ì¶œ ê²°ê³¼ === ì œí’ˆ ì‚¬ì–‘ì„œ (ë°ëª¨ìš©)  ë¬¸ì„œ ë²ˆí˜¸: PROD-INS-PUMP-001 ë²„ì „: 0.1 ì‘ì„±ì¼: 2025-08-12  â€»ë³¸ ë¬¸ì„œëŠ” ë°ëª¨ìš© ê°€ìƒ ìë£Œì´ë©°, ê·œì œ ì œì¶œ ë˜ëŠ” ì§„ë£Œ ì˜ì‚¬ê²°ì •ì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤....\n",
      "ğŸ‡°ğŸ‡· í•œê¸€ ë¹„ìœ¨: 41.7%\n",
      "\n",
      "ğŸ“Š ë°©ë²•ë³„ ê²°ê³¼ ë¹„êµ\n",
      "========================================\n",
      "ğŸ¯ ìµœê³  í’ˆì§ˆ: pypdf2 (1,616ì)\n",
      "âš¡ ìµœê³  ì†ë„: pypdf2 (0.14ì´ˆ)\n",
      "ğŸ’¡ ì¶”ì²œ: pypdf2 (ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)\n",
      "ğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: 20_ProductSpec_SmartInsulinPump_KO_v0.1_test_results.json\n",
      "\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
      "\n",
      "ğŸ’¡ ì¶”ê°€ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜:\n",
      "   â€¢ run_example_tests(): íŒŒì¼ íƒ€ì…ë³„ ì˜ˆì œ í…ŒìŠ¤íŠ¸\n",
      "   â€¢ analyze_extraction_performance(): ì„±ëŠ¥ í†µê³„ í™•ì¸\n",
      "   âœ… ì¶”ì¶œ ì„±ê³µ: 132ì\n",
      "   â±ï¸ ì²˜ë¦¬ ì‹œê°„: 1.47ì´ˆ\n",
      "âœ… ì„±ê³µ: 132ì, 1.47ì´ˆ\n",
      "ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: === OCR ì¶”ì¶œ ê²°ê³¼ === ì œí’ˆ ì‚¬ì–‘ì„œ (ë°ëª¨ìš©)  ë¬¸ì„œ ë²ˆí˜¸: PROD-INS-PUMP-001 ë²„ì „: 0.1 ì‘ì„±ì¼: 2025-08-12  â€»ë³¸ ë¬¸ì„œëŠ” ë°ëª¨ìš© ê°€ìƒ ìë£Œì´ë©°, ê·œì œ ì œì¶œ ë˜ëŠ” ì§„ë£Œ ì˜ì‚¬ê²°ì •ì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤....\n",
      "ğŸ‡°ğŸ‡· í•œê¸€ ë¹„ìœ¨: 41.7%\n",
      "\n",
      "ğŸ“Š ë°©ë²•ë³„ ê²°ê³¼ ë¹„êµ\n",
      "========================================\n",
      "ğŸ¯ ìµœê³  í’ˆì§ˆ: pypdf2 (1,616ì)\n",
      "âš¡ ìµœê³  ì†ë„: pypdf2 (0.14ì´ˆ)\n",
      "ğŸ’¡ ì¶”ì²œ: pypdf2 (ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)\n",
      "ğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: 20_ProductSpec_SmartInsulinPump_KO_v0.1_test_results.json\n",
      "\n",
      "âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
      "\n",
      "ğŸ’¡ ì¶”ê°€ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜:\n",
      "   â€¢ run_example_tests(): íŒŒì¼ íƒ€ì…ë³„ ì˜ˆì œ í…ŒìŠ¤íŠ¸\n",
      "   â€¢ analyze_extraction_performance(): ì„±ëŠ¥ í†µê³„ í™•ì¸\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ íŠ¹ì • íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì˜ˆì œ)\n",
    "\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ìë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "if 'AVAILABLE_TEST_FILES' in globals() and AVAILABLE_TEST_FILES:\n",
    "    print(\"ğŸ§ª ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì²« ë²ˆì§¸ íŒŒì¼)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "    test_file = AVAILABLE_TEST_FILES[0]\n",
    "    print(f\"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_file.name}\")\n",
    "    \n",
    "    # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ í…ŒìŠ¤íŠ¸\n",
    "    file_type = config.get_file_type(test_file)\n",
    "    \n",
    "    if file_type == 'pdf':\n",
    "        print(\"ğŸ“„ PDF íŒŒì¼ - ëª¨ë“  ì¶”ì¶œ ë°©ë²• í…ŒìŠ¤íŠ¸\")\n",
    "        results = quick_test_pdf(test_file)\n",
    "    else:\n",
    "        print(f\"ğŸ“„ {file_type.upper()} íŒŒì¼ - ìë™ ë°©ë²• í…ŒìŠ¤íŠ¸\")\n",
    "        results = quick_test_auto(test_file)\n",
    "    \n",
    "    print(f\"\\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ’¡ ìˆ˜ë™ í…ŒìŠ¤íŠ¸ ì˜ˆì œ\")\n",
    "    print(\"=\" * 30)\n",
    "    print(\"# íŠ¹ì • íŒŒì¼ í…ŒìŠ¤íŠ¸ ì˜ˆì œ:\")\n",
    "    print(\"# test_file_path = '/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf'\")\n",
    "    print(\"# results = test_single_file(test_file_path)\")\n",
    "    print()\n",
    "    print(\"# PDF íŒŒì¼ ì „ìš© í…ŒìŠ¤íŠ¸:\")\n",
    "    print(\"# results = quick_test_pdf(test_file_path)\")\n",
    "    print()\n",
    "    print(\"# ì—¬ëŸ¬ íŒŒì¼ ë°°ì¹˜ í…ŒìŠ¤íŠ¸:\")\n",
    "    print(\"# batch_results = test_multiple_files(max_files=5)\")\n",
    "    print()\n",
    "    print(\"# PDFë§Œ ë°°ì¹˜ í…ŒìŠ¤íŠ¸:\")\n",
    "    print(\"# pdf_results = test_multiple_files(file_types=['pdf'], max_files=3)\")\n",
    "\n",
    "# ìˆ˜ë™ í…ŒìŠ¤íŠ¸ìš© í¸ì˜ í•¨ìˆ˜\n",
    "def run_example_tests():\n",
    "    \"\"\"ì˜ˆì œ í…ŒìŠ¤íŠ¸ë“¤ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not AVAILABLE_TEST_FILES:\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ§ª ì˜ˆì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # íŒŒì¼ íƒ€ì…ë³„ë¡œ í•˜ë‚˜ì”© í…ŒìŠ¤íŠ¸\n",
    "    tested_types = set()\n",
    "    \n",
    "    for file_path in AVAILABLE_TEST_FILES[:5]:  # ìµœëŒ€ 5ê°œ\n",
    "        file_type = config.get_file_type(file_path)\n",
    "        \n",
    "        if file_type not in tested_types:\n",
    "            print(f\"\\nğŸ“„ {file_type.upper()} í…ŒìŠ¤íŠ¸: {file_path.name}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            try:\n",
    "                result = text_extractor.extract_text(file_path, method='auto')\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"âœ… ì„±ê³µ: {result['text_length']:,}ì, {result['processing_time']:.2f}ì´ˆ\")\n",
    "                else:\n",
    "                    print(f\"âŒ ì‹¤íŒ¨: {result['error']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì˜ˆì™¸: {e}\")\n",
    "            \n",
    "            tested_types.add(file_type)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ {len(tested_types)}ê°œ íŒŒì¼ íƒ€ì… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ì¶”ê°€ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜:\")\n",
    "print(\"   â€¢ run_example_tests(): íŒŒì¼ íƒ€ì…ë³„ ì˜ˆì œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"   â€¢ analyze_extraction_performance(): ì„±ëŠ¥ í†µê³„ í™•ì¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cceca1a",
   "metadata": {},
   "source": [
    "## ğŸ“– ì‚¬ìš© ê°€ì´ë“œ ë° ìš”ì•½\n",
    "\n",
    "### ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ\n",
    "\n",
    "1. **í™˜ê²½ ì„¤ì •**: ì²« ë²ˆì§¸ ì…€ë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
    "2. **íŒŒì¼ ì¤€ë¹„**: `/home/admin/wkms-aws/jupyter_notebook/data/input_docs/` ì— í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œ ì¶”ê°€\n",
    "3. **ìŠ¤ìº” ì‹¤í–‰**: ì…ë ¥ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì…€ ì‹¤í–‰ìœ¼ë¡œ íŒŒì¼ í˜„í™© í™•ì¸\n",
    "4. **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**: ì›í•˜ëŠ” í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì‹¤í–‰\n",
    "\n",
    "### ğŸ”§ ì£¼ìš” í•¨ìˆ˜ ì •ë¦¬\n",
    "\n",
    "#### ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸\n",
    "```python\n",
    "# ëª¨ë“  ë°©ë²•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "test_single_file('íŒŒì¼ê²½ë¡œ')\n",
    "\n",
    "# PDF ì „ìš© í…ŒìŠ¤íŠ¸ (PyPDF2, pdfplumber, OCR)\n",
    "quick_test_pdf('pdfíŒŒì¼ê²½ë¡œ')\n",
    "\n",
    "# ìë™ ë°©ë²• ì„ íƒ\n",
    "quick_test_auto('íŒŒì¼ê²½ë¡œ')\n",
    "```\n",
    "\n",
    "#### ë°°ì¹˜ í…ŒìŠ¤íŠ¸\n",
    "```python\n",
    "# ëª¨ë“  íŒŒì¼ í…ŒìŠ¤íŠ¸ (ìµœëŒ€ 10ê°œ)\n",
    "test_multiple_files()\n",
    "\n",
    "# íŠ¹ì • íƒ€ì…ë§Œ í…ŒìŠ¤íŠ¸\n",
    "test_multiple_files(file_types=['pdf'])\n",
    "\n",
    "# íŒŒì¼ ìˆ˜ ì œí•œ\n",
    "test_multiple_files(max_files=5)\n",
    "```\n",
    "\n",
    "#### ë¶„ì„ ë° í†µê³„\n",
    "```python\n",
    "# ì„±ëŠ¥ í†µê³„ í™•ì¸\n",
    "analyze_extraction_performance()\n",
    "\n",
    "# ì˜ˆì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "run_example_tests()\n",
    "```\n",
    "\n",
    "### ğŸ“Š ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ ë° ë°©ë²•\n",
    "\n",
    "| íŒŒì¼ í˜•ì‹ | í™•ì¥ì | ì¶”ì¶œ ë°©ë²• | íŠ¹ì§• |\n",
    "|-----------|--------|-----------|------|\n",
    "| **PDF** | .pdf | PyPDF2, pdfplumber, OCR | í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ê¸°ë°˜ ìë™ ê°ì§€ |\n",
    "| **Word** | .docx, .doc | python-docx | í…ìŠ¤íŠ¸, í…Œì´ë¸” ì¶”ì¶œ |\n",
    "| **PowerPoint** | .pptx, .ppt | python-pptx | ìŠ¬ë¼ì´ë“œë³„ í…ìŠ¤íŠ¸ |\n",
    "| **Excel** | .xlsx, .xls | openpyxl+pandas | ì‹œíŠ¸ë³„ ë°ì´í„° |\n",
    "| **HTML** | .html, .htm, .xml | BeautifulSoup | íƒœê·¸ ì œê±°, í…Œì´ë¸” ë³€í™˜ |\n",
    "| **í…ìŠ¤íŠ¸** | .txt, .md, .rtf | builtin | ì¸ì½”ë”© ìë™ ê°ì§€ |\n",
    "| **ì´ë¯¸ì§€** | .png, .jpg, .jpeg, .tiff, .bmp | Tesseract OCR | í•œì˜ ë¬¸ì ì¸ì‹ |\n",
    "\n",
    "### ğŸ“ ì¶œë ¥ íŒŒì¼ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "ğŸ“‚ data/output/\n",
    "â”œâ”€â”€ ğŸ“‚ extracted_texts/      # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤\n",
    "â”‚   â”œâ”€â”€ filename_method_timestamp.txt\n",
    "â”‚   â””â”€â”€ filename_page_001.txt (OCR í˜ì´ì§€ë³„)\n",
    "â”œâ”€â”€ ğŸ“‚ test_results/         # í…ŒìŠ¤íŠ¸ ê²°ê³¼ JSON\n",
    "â”‚   â”œâ”€â”€ filename_test_results.json\n",
    "â”‚   â””â”€â”€ batch_test_results_timestamp.json\n",
    "â””â”€â”€ ğŸ“‚ logs/                 # ì²˜ë¦¬ ë¡œê·¸\n",
    "    â””â”€â”€ extraction_test_timestamp.log\n",
    "```\n",
    "\n",
    "### ğŸ’¡ ì„±ëŠ¥ ìµœì í™” íŒ\n",
    "\n",
    "1. **ì‘ì€ íŒŒì¼ë¶€í„°**: í° íŒŒì¼ì€ ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼\n",
    "2. **PDF ë°©ë²• ì„ íƒ**: \n",
    "   - í…ìŠ¤íŠ¸ ê¸°ë°˜ â†’ PyPDF2 (ë¹ ë¦„)\n",
    "   - ë ˆì´ì•„ì›ƒ ë³µì¡ â†’ pdfplumber\n",
    "   - ì´ë¯¸ì§€ ê¸°ë°˜ â†’ OCR (ëŠë¦¼)\n",
    "3. **ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ**: `max_files` ë§¤ê°œë³€ìˆ˜ë¡œ ì²˜ë¦¬ëŸ‰ ì¡°ì ˆ\n",
    "4. **ê²°ê³¼ ì €ì¥**: `save_results=False`ë¡œ ì €ì¥ ìƒëµ ê°€ëŠ¥\n",
    "\n",
    "### ğŸ”§ ë¬¸ì œ í•´ê²°\n",
    "\n",
    "- **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜¤ë¥˜**: ì²« ë²ˆì§¸ ì…€ ì¬ì‹¤í–‰ìœ¼ë¡œ ëˆ„ë½ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "- **Tesseract ì˜¤ë¥˜**: ì‹œìŠ¤í…œì— Tesseract OCR ì„¤ì¹˜ í•„ìš”\n",
    "- **ë©”ëª¨ë¦¬ ë¶€ì¡±**: í° íŒŒì¼ì´ë‚˜ ë§ì€ íŒŒì¼ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°\n",
    "- **ì¸ì½”ë”© ì˜¤ë¥˜**: í…ìŠ¤íŠ¸ íŒŒì¼ì˜ ì¸ì½”ë”© ìë™ ê°ì§€ë¡œ í•´ê²°\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d3f25df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì„± ì™„ë£Œ!\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ í™˜ê²½ ìƒíƒœ:\n",
      "   â€¢ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©: âŒ\n",
      "   â€¢ Tesseract OCR: âœ…\n",
      "   â€¢ ì„¤ì • ì™„ë£Œ: âœ…\n",
      "   â€¢ ì¶”ì¶œê¸° ì¤€ë¹„: âœ…\n",
      "\n",
      "ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\n",
      "   â€¢ ì…ë ¥: /home/admin/wkms-aws/jupyter_notebook/data/input_docs\n",
      "   â€¢ ì¶œë ¥: /home/admin/wkms-aws/jupyter_notebook/data/output\n",
      "   â€¢ í…ìŠ¤íŠ¸: /home/admin/wkms-aws/jupyter_notebook/data/output/extracted_texts\n",
      "   â€¢ ê²°ê³¼: /home/admin/wkms-aws/jupyter_notebook/data/output/test_results\n",
      "\n",
      "ğŸ§ª ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜:\n",
      "   ğŸ“„ ë‹¨ì¼ íŒŒì¼:\n",
      "      â€¢ test_single_file('íŒŒì¼ê²½ë¡œ')\n",
      "      â€¢ quick_test_pdf('pdfíŒŒì¼')\n",
      "      â€¢ quick_test_auto('íŒŒì¼ê²½ë¡œ')\n",
      "\n",
      "   ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸:\n",
      "      â€¢ test_multiple_files()\n",
      "      â€¢ test_multiple_files(file_types=['pdf'])\n",
      "      â€¢ run_example_tests()\n",
      "\n",
      "   ğŸ“ˆ ë¶„ì„:\n",
      "      â€¢ analyze_extraction_performance()\n",
      "      â€¢ document_loader.scan_input_directory()\n",
      "\n",
      "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\n",
      "   1. í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œë¥¼ input_docs í´ë”ì— ì¶”ê°€\n",
      "   2. ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì…€ ì‹¤í–‰ìœ¼ë¡œ íŒŒì¼ í™•ì¸\n",
      "   3. ì›í•˜ëŠ” í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì‹¤í–‰\n",
      "   4. ê²°ê³¼ ë¶„ì„ ë° ì„±ëŠ¥ í™•ì¸\n",
      "\n",
      "ğŸ’¡ íŒ:\n",
      "   â€¢ ì‘ì€ íŒŒì¼ë¶€í„° í…ŒìŠ¤íŠ¸í•˜ì—¬ í™˜ê²½ ê²€ì¦\n",
      "   â€¢ PDFëŠ” ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ê¸°ë°˜ì— ë”°ë¼ ë°©ë²• ì„ íƒ\n",
      "   â€¢ ê²°ê³¼ëŠ” output í´ë”ì— ìë™ ì €ì¥\n",
      "   â€¢ ì„±ëŠ¥ í†µê³„ë¡œ ìµœì  ë°©ë²• í™•ì¸\n",
      "\n",
      "ğŸš€ í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ! ìœ„ì˜ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ‰ ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì™„ë£Œ!\n",
    "\n",
    "print(\"ğŸ‰ ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì„± ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# í™˜ê²½ ìƒíƒœ í™•ì¸\n",
    "print(\"ğŸ”§ í™˜ê²½ ìƒíƒœ:\")\n",
    "print(f\"   â€¢ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©: {'âœ…' if LIBRARIES_LOADED else 'âŒ'}\")\n",
    "print(f\"   â€¢ Tesseract OCR: {'âœ…' if tesseract_found else 'âŒ'}\")\n",
    "print(f\"   â€¢ ì„¤ì • ì™„ë£Œ: {'âœ…' if 'config' in globals() else 'âŒ'}\")\n",
    "print(f\"   â€¢ ì¶”ì¶œê¸° ì¤€ë¹„: {'âœ…' if 'text_extractor' in globals() else 'âŒ'}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\")\n",
    "print(f\"   â€¢ ì…ë ¥: {config.input_docs_dir}\")\n",
    "print(f\"   â€¢ ì¶œë ¥: {config.output_dir}\")\n",
    "print(f\"   â€¢ í…ìŠ¤íŠ¸: {config.extracted_texts_dir}\")\n",
    "print(f\"   â€¢ ê²°ê³¼: {config.test_results_dir}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ§ª ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜:\")\n",
    "print(\"   ğŸ“„ ë‹¨ì¼ íŒŒì¼:\")\n",
    "print(\"      â€¢ test_single_file('íŒŒì¼ê²½ë¡œ')\")\n",
    "print(\"      â€¢ quick_test_pdf('pdfíŒŒì¼')\")\n",
    "print(\"      â€¢ quick_test_auto('íŒŒì¼ê²½ë¡œ')\")\n",
    "print()\n",
    "print(\"   ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸:\")\n",
    "print(\"      â€¢ test_multiple_files()\")\n",
    "print(\"      â€¢ test_multiple_files(file_types=['pdf'])\")\n",
    "print(\"      â€¢ run_example_tests()\")\n",
    "print()\n",
    "print(\"   ğŸ“ˆ ë¶„ì„:\")\n",
    "print(\"      â€¢ analyze_extraction_performance()\")\n",
    "print(\"      â€¢ document_loader.scan_input_directory()\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"   1. í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œë¥¼ input_docs í´ë”ì— ì¶”ê°€\")\n",
    "print(\"   2. ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì…€ ì‹¤í–‰ìœ¼ë¡œ íŒŒì¼ í™•ì¸\")\n",
    "print(\"   3. ì›í•˜ëŠ” í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì‹¤í–‰\")\n",
    "print(\"   4. ê²°ê³¼ ë¶„ì„ ë° ì„±ëŠ¥ í™•ì¸\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ’¡ íŒ:\")\n",
    "print(\"   â€¢ ì‘ì€ íŒŒì¼ë¶€í„° í…ŒìŠ¤íŠ¸í•˜ì—¬ í™˜ê²½ ê²€ì¦\")\n",
    "print(\"   â€¢ PDFëŠ” ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ê¸°ë°˜ì— ë”°ë¼ ë°©ë²• ì„ íƒ\")\n",
    "print(\"   â€¢ ê²°ê³¼ëŠ” output í´ë”ì— ìë™ ì €ì¥\")\n",
    "print(\"   â€¢ ì„±ëŠ¥ í†µê³„ë¡œ ìµœì  ë°©ë²• í™•ì¸\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸš€ í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ! ìœ„ì˜ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c933371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” OCR ì¶”ì¶œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "ğŸ” OCR ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ!\n",
      "   ğŸ“Š í˜ì´ì§€ë³„ ì‹¤ì‹œê°„ ë¡œê·¸\n",
      "   ğŸ’¾ ê°œë³„ í˜ì´ì§€ íŒŒì¼ ì €ì¥\n",
      "   ğŸ“„ ì „ì²´ ê²°ê³¼ í†µí•© íŒŒì¼\n",
      "ğŸ” OCR ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ!\n",
      "   ğŸ“Š í˜ì´ì§€ë³„ ì‹¤ì‹œê°„ ë¡œê·¸\n",
      "   ğŸ’¾ ê°œë³„ í˜ì´ì§€ íŒŒì¼ ì €ì¥\n",
      "   ğŸ“„ ì „ì²´ ê²°ê³¼ í†µí•© íŒŒì¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¤ OCR ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€ (í˜ì´ì§€ë³„ ë¡œê·¸ ë° íŒŒì¼ ì €ì¥)\n",
    "\n",
    "\n",
    "def _extract_pdf_ocr(self, file_path):\n",
    "    \"\"\"OCRì„ ì‚¬ìš©í•œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (í˜ì´ì§€ë³„ ë¡œê·¸ ë° íŒŒì¼ ì €ì¥)\"\"\"\n",
    "    import time\n",
    "    from pathlib import Path\n",
    "    # ì§€ì—° ë¡œë”©: ë¬´ê±°ìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì„í¬íŠ¸í•˜ì—¬ ì…€ ì‹œì‘ ì§€ì—°ì„ ë°©ì§€\n",
    "    try:\n",
    "        from pdf2image import convert_from_path  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"pdf2image ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install pdf2image\") from e\n",
    "    try:\n",
    "        import pytesseract  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"pytesseract ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install pytesseract\") from e\n",
    "    from datetime import datetime\n",
    "\n",
    "\n",
    "    # ì„¤ì • ê°’ ì•ˆì „ íšë“\n",
    "    ocr_cfg = getattr(self.config, 'ocr_config', {}) or {}\n",
    "    ocr_enabled = ocr_cfg.get('enabled', True)\n",
    "    ocr_lang = ocr_cfg.get('language', 'eng')\n",
    "    ocr_dpi = ocr_cfg.get('dpi', 200)\n",
    "\n",
    "\n",
    "    if not ocr_enabled:\n",
    "        raise ValueError(\"OCRì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "    file_path = Path(file_path)\n",
    "    file_stem = file_path.stem\n",
    "\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸ”„ PDF OCR ì²˜ë¦¬ ì‹œì‘: {file_path.name}\", flush=True)\n",
    "        print(f\"   ğŸ“Š DPI: {ocr_dpi}\", flush=True)\n",
    "        print(f\"   ğŸŒ ì–¸ì–´: {ocr_lang}\", flush=True)\n",
    "\n",
    "\n",
    "        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜\n",
    "        print(f\"   ğŸ“„ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì¤‘...\", flush=True)\n",
    "        start_conversion = time.time()\n",
    "        images = convert_from_path(\n",
    "        \tfile_path,\n",
    "        \t dpi=ocr_dpi\n",
    "        )\n",
    "        conversion_time = time.time() - start_conversion\n",
    "\n",
    "\n",
    "        print(f\"   âœ… ë³€í™˜ ì™„ë£Œ: {len(images)}í˜ì´ì§€ ({conversion_time:.2f}ì´ˆ)\", flush=True)\n",
    "\n",
    "\n",
    "        metadata = {\n",
    "            'pages': len(images),\n",
    "            'extraction_method': 'ocr',\n",
    "            'ocr_language': ocr_lang,\n",
    "            'ocr_dpi': ocr_dpi,\n",
    "            'conversion_time': conversion_time,\n",
    "            'page_results': []\n",
    "        }\n",
    "\n",
    "\n",
    "        text_parts = []\n",
    "        page_files = []  # í˜ì´ì§€ë³„ íŒŒì¼ ê²½ë¡œ ì €ì¥\n",
    "\n",
    "\n",
    "        # ê° í˜ì´ì§€ë³„ OCR ì²˜ë¦¬\n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            page_start_time = time.time()\n",
    "\n",
    "\n",
    "            try:\n",
    "                print(f\"   ğŸ” í˜ì´ì§€ {page_num}/{len(images)} OCR ì²˜ë¦¬ ì¤‘...\", flush=True)\n",
    "\n",
    "\n",
    "                # Tesseract OCR ì‹¤í–‰\n",
    "                page_text = pytesseract.image_to_string(\n",
    "                    image,\n",
    "                    lang=ocr_lang\n",
    "                )\n",
    "\n",
    "\n",
    "                page_processing_time = time.time() - page_start_time\n",
    "                text_length = len(page_text.strip())\n",
    "\n",
    "\n",
    "                # í˜ì´ì§€ë³„ ê²°ê³¼ ì €ì¥\n",
    "                page_result = {\n",
    "                    'page_number': page_num,\n",
    "                    'text_length': text_length,\n",
    "                    'processing_time': page_processing_time,\n",
    "                    'success': True,\n",
    "                    'error': None\n",
    "                }\n",
    "\n",
    "\n",
    "                if page_text.strip():\n",
    "                    # í˜ì´ì§€ í…ìŠ¤íŠ¸ í¬ë§·íŒ…\n",
    "                    formatted_page_text = f\"\\n\\n=== í˜ì´ì§€ {page_num} (OCR) ===\\n{page_text}\"\n",
    "                    text_parts.append(formatted_page_text)\n",
    "\n",
    "\n",
    "                    # í˜ì´ì§€ë³„ íŒŒì¼ ì €ì¥\n",
    "                    page_file = self._save_page_text(file_stem, page_num, page_text)\n",
    "                    if page_file:\n",
    "                        page_files.append(page_file)\n",
    "                        page_result['output_file'] = str(page_file)\n",
    "\n",
    "\n",
    "                    print(f\"      âœ… ì™„ë£Œ: {text_length:,}ì ì¶”ì¶œ ({page_processing_time:.2f}ì´ˆ)\", flush=True)\n",
    "                    if hasattr(self, 'logger') and self.logger:\n",
    "                        self.logger.info(f\"OCR í˜ì´ì§€ {page_num} ì„±ê³µ: {text_length}ì, {page_processing_time:.2f}ì´ˆ\")\n",
    "                else:\n",
    "                    text_parts.append(f\"\\n\\n=== í˜ì´ì§€ {page_num} (OCR - í…ìŠ¤íŠ¸ ì—†ìŒ) ===\\n\")\n",
    "                    print(f\"      âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ ({page_processing_time:.2f}ì´ˆ)\", flush=True)\n",
    "                    if hasattr(self, 'logger') and self.logger:\n",
    "                        self.logger.warning(f\"OCR í˜ì´ì§€ {page_num}: í…ìŠ¤íŠ¸ ì—†ìŒ\")\n",
    "\n",
    "\n",
    "                metadata['page_results'].append(page_result)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                page_processing_time = time.time() - page_start_time\n",
    "                error_msg = str(e)\n",
    "\n",
    "\n",
    "                page_result = {\n",
    "                    'page_number': page_num,\n",
    "                    'text_length': 0,\n",
    "                    'processing_time': page_processing_time,\n",
    "                    'success': False,\n",
    "                    'error': error_msg\n",
    "                }\n",
    "                metadata['page_results'].append(page_result)\n",
    "\n",
    "\n",
    "                text_parts.append(f\"\\n\\n=== í˜ì´ì§€ {page_num} (OCR ì‹¤íŒ¨) ===\\nì˜¤ë¥˜: {error_msg}\")\n",
    "                print(f\"      âŒ ì‹¤íŒ¨: {error_msg} ({page_processing_time:.2f}ì´ˆ)\", flush=True)\n",
    "                if hasattr(self, 'logger') and self.logger:\n",
    "                    self.logger.error(f\"OCR í˜ì´ì§€ {page_num} ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©\n",
    "        full_text = '\\n'.join(text_parts)\n",
    "\n",
    "\n",
    "        # ì „ì²´ ê²°ê³¼ íŒŒì¼ ì €ì¥\n",
    "        output_file = self._save_full_ocr_text(file_stem, full_text, metadata)\n",
    "\n",
    "\n",
    "        # í†µê³„ ì •ë³´\n",
    "        successful_pages = sum(1 for r in metadata['page_results'] if r['success'])\n",
    "        total_chars = sum(r['text_length'] for r in metadata['page_results'])\n",
    "        total_processing_time = sum(r['processing_time'] for r in metadata['page_results'])\n",
    "\n",
    "\n",
    "        print(f\"\\nğŸ“Š OCR ì²˜ë¦¬ ì™„ë£Œ - {file_path.name}\", flush=True)\n",
    "        print(f\"   âœ… ì„±ê³µ í˜ì´ì§€: {successful_pages}/{len(images)}\", flush=True)\n",
    "        print(f\"   ğŸ“ ì´ ì¶”ì¶œ í…ìŠ¤íŠ¸: {total_chars:,}ì\", flush=True)\n",
    "        print(f\"   â±ï¸ ì´ ì²˜ë¦¬ì‹œê°„: {total_processing_time:.2f}ì´ˆ\", flush=True)\n",
    "        print(f\"   ğŸ’¾ ì €ì¥ëœ íŒŒì¼:\", flush=True)\n",
    "        if output_file:\n",
    "            print(f\"      ğŸ“„ ì „ì²´ ê²°ê³¼: {output_file.name}\", flush=True)\n",
    "        for i, page_file in enumerate(page_files, 1):\n",
    "            print(f\"      ğŸ“‘ í˜ì´ì§€ {i}: {page_file.name}\", flush=True)\n",
    "\n",
    "\n",
    "        metadata.update({\n",
    "            'successful_pages': successful_pages,\n",
    "            'total_characters': total_chars,\n",
    "            'total_processing_time': total_processing_time,\n",
    "            'output_file': str(output_file) if output_file else None,\n",
    "            'page_files': [str(f) for f in page_files]\n",
    "        })\n",
    "\n",
    "\n",
    "        return {\n",
    "            'text': full_text,\n",
    "            'metadata': metadata,\n",
    "            'extraction_method': 'ocr'\n",
    "        }\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "def _save_page_text(self, file_stem, page_num, page_text):\n",
    "    \"\"\"í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    from datetime import datetime\n",
    "    try:\n",
    "        # íŒŒì¼ëª… ìƒì„±: ì›ë³¸íŒŒì¼ëª…_page_001.txt\n",
    "        page_filename = f\"{file_stem}_page_{page_num:03d}.txt\"\n",
    "        # ì˜¬ë°”ë¥¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì‚¬ìš©\n",
    "        output_dir = getattr(self.config, 'extracted_texts_dir', None) or getattr(self.config, 'extracted_text_dir', None)\n",
    "        if output_dir is None:\n",
    "            raise RuntimeError(\"ì¶œë ¥ ë””ë ‰í† ë¦¬(extracted_texts_dir)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        page_file_path = output_dir / page_filename\n",
    "\n",
    "\n",
    "        # í˜ì´ì§€ í—¤ë” ì¶”ê°€\n",
    "        content = f\"=== OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ ===\\n\"\n",
    "        content += f\"ì›ë³¸ íŒŒì¼: {file_stem}\\n\"\n",
    "        content += f\"í˜ì´ì§€ ë²ˆí˜¸: {page_num}\\n\"\n",
    "        content += f\"ì¶”ì¶œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        content += f\"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(page_text.strip())}ì\\n\"\n",
    "        ocr_cfg = getattr(self.config, 'ocr_config', {}) or {}\n",
    "        content += f\"ì–¸ì–´: {ocr_cfg.get('language', 'eng')}\\n\"\n",
    "        content += f\"DPI: {ocr_cfg.get('dpi', 200)}\\n\"\n",
    "        content += \"=\" * 50 + \"\\n\\n\"\n",
    "        content += page_text.strip()\n",
    "\n",
    "\n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        with open(page_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "\n",
    "\n",
    "        return page_file_path\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        if hasattr(self, 'logger') and self.logger:\n",
    "            self.logger.error(f\"í˜ì´ì§€ {page_num} í…ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _save_full_ocr_text(self, file_stem, full_text, metadata):\n",
    "    \"\"\"ì „ì²´ OCR ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    try:\n",
    "        # íŒŒì¼ëª… ìƒì„±: ì›ë³¸íŒŒì¼ëª…_ocr_full.txt\n",
    "        full_filename = f\"{file_stem}_ocr_full.txt\"\n",
    "        output_dir = getattr(self.config, 'extracted_texts_dir', None) or getattr(self.config, 'extracted_text_dir', None)\n",
    "        if output_dir is None:\n",
    "            raise RuntimeError(\"ì¶œë ¥ ë””ë ‰í† ë¦¬(extracted_texts_dir)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        full_file_path = output_dir / full_filename\n",
    "\n",
    "\n",
    "        # í—¤ë” ì •ë³´ ìƒì„±\n",
    "        content = f\"=== OCR ì „ì²´ ì¶”ì¶œ ê²°ê³¼ ===\\n\"\n",
    "        content += f\"ì›ë³¸ íŒŒì¼: {file_stem}\\n\"\n",
    "        content += f\"ì¶”ì¶œ ë°©ë²•: OCR (Tesseract)\\n\"\n",
    "        content += f\"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        content += f\"ì´ í˜ì´ì§€: {metadata['pages']}\\n\"\n",
    "        content += f\"ì„±ê³µ í˜ì´ì§€: {metadata.get('successful_pages', 0)}\\n\"\n",
    "        content += f\"ì´ í…ìŠ¤íŠ¸: {metadata.get('total_characters', 0):,}ì\\n\"\n",
    "        content += f\"ì²˜ë¦¬ ì‹œê°„: {metadata.get('total_processing_time', 0):.2f}ì´ˆ\\n\"\n",
    "        content += f\"ì–¸ì–´ ì„¤ì •: {metadata.get('ocr_language', 'eng')}\\n\"\n",
    "        content += f\"DPI ì„¤ì •: {metadata.get('ocr_dpi', 200)}\\n\"\n",
    "        content += \"=\" * 80 + \"\\n\\n\"\n",
    "\n",
    "\n",
    "        # í˜ì´ì§€ë³„ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\n",
    "        content += \"ğŸ“Š í˜ì´ì§€ë³„ ì²˜ë¦¬ ê²°ê³¼:\\n\"\n",
    "        for page_result in metadata.get('page_results', []):\n",
    "            status = \"âœ…\" if page_result['success'] else \"âŒ\"\n",
    "            content += f\"   í˜ì´ì§€ {page_result['page_number']}: {status} \"\n",
    "            content += f\"{page_result['text_length']:,}ì \"\n",
    "            content += f\"({page_result['processing_time']:.2f}ì´ˆ)\\n\"\n",
    "            if not page_result['success']:\n",
    "                content += f\"      ì˜¤ë¥˜: {page_result['error']}\\n\"\n",
    "        content += \"\\n\" + \"=\" * 80 + \"\\n\\n\"\n",
    "\n",
    "\n",
    "        # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ê°€\n",
    "        content += full_text\n",
    "\n",
    "\n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        with open(full_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "\n",
    "\n",
    "        # ì¶”ê°€ë¡œ JSON ë©”íƒ€ë°ì´í„° íŒŒì¼ë„ ì €ì¥\n",
    "        metadata_filename = f\"{file_stem}_ocr_metadata.json\"\n",
    "        metadata_dir = getattr(self.config, 'metadata_dir', None) or getattr(self.config, 'output_dir', None)\n",
    "        if metadata_dir is None:\n",
    "            metadata_dir = output_dir\n",
    "        metadata_file_path = metadata_dir / metadata_filename\n",
    "\n",
    "\n",
    "        with open(metadata_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "\n",
    "        return full_file_path\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        if hasattr(self, 'logger') and self.logger:\n",
    "            self.logger.error(f\"OCR ì „ì²´ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"ğŸ” OCR ì¶”ì¶œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\", flush=True)\n",
    "# ë“±ë¡í•  ì¶”ì¶œê¸° í´ë˜ìŠ¤ í™•ì¸ (TextExtractor ìš°ì„ , ì—†ìœ¼ë©´ DocumentTextExtractor ì‚¬ìš©)\n",
    "try:\n",
    "    ExtractorClass = TextExtractor  # ê¸°ì¡´ ì´ë¦„ í˜¸í™˜\n",
    "except NameError:\n",
    "    try:\n",
    "        ExtractorClass = DocumentTextExtractor  # ìƒˆ ì´ë¦„\n",
    "    except NameError:\n",
    "        ExtractorClass = None\n",
    "\n",
    "\n",
    "if ExtractorClass is None:\n",
    "    raise NameError(\"OCR ë©”ì†Œë“œë¥¼ ë“±ë¡í•  ëŒ€ìƒ í´ë˜ìŠ¤(TextExtractor ë˜ëŠ” DocumentTextExtractor)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¶”ì¶œê¸° í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# ì´ë¯¸ ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ì¤‘ë³µ ë“±ë¡ ë°©ì§€\n",
    "if getattr(ExtractorClass, \"_ocr_methods_registered\", False):\n",
    "    print(\"â„¹ï¸ ì´ë¯¸ OCR ì¶”ì¶œ ë©”ì†Œë“œê°€ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\", flush=True)\n",
    "else:\n",
    "    # OCR ê´€ë ¨ ë©”ì†Œë“œ ë“±ë¡\n",
    "    ExtractorClass._extract_pdf_ocr = _extract_pdf_ocr\n",
    "    ExtractorClass._save_page_text = _save_page_text\n",
    "    ExtractorClass._save_full_ocr_text = _save_full_ocr_text\n",
    "    setattr(ExtractorClass, \"_ocr_methods_registered\", True)\n",
    "    print(\"ğŸ” OCR ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ!\", flush=True)\n",
    "    print(\"   ğŸ“Š í˜ì´ì§€ë³„ ì‹¤ì‹œê°„ ë¡œê·¸\", flush=True)\n",
    "    print(\"   ğŸ’¾ ê°œë³„ í˜ì´ì§€ íŒŒì¼ ì €ì¥\", flush=True)\n",
    "    print(\"   ğŸ“„ ì „ì²´ ê²°ê³¼ í†µí•© íŒŒì¼\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0461f663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ í…ŒìŠ¤íŠ¸ PDF OCR ì‹¤í–‰ ëŸ¬ë„ˆ ì‹œì‘\n",
      "ğŸ“„ ì…ë ¥ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ“„ ì…ë ¥ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ’¾ ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/output/extracted_texts\n",
      "ğŸ” OCR ì¶”ì¶œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤ (í˜ì´ì§€ë³„ ìƒì„¸ ë¡œê·¸ëŠ” ì•„ë˜ì— í‘œì‹œë©ë‹ˆë‹¤)â€¦\n",
      "ğŸ”„ PDF OCR ì²˜ë¦¬ ì‹œì‘: test.pdf\n",
      "   ğŸ“Š DPI: 300\n",
      "   ğŸŒ ì–¸ì–´: kor+eng\n",
      "   ğŸ“„ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì¤‘...\n",
      "ğŸ’¾ ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/output/extracted_texts\n",
      "ğŸ” OCR ì¶”ì¶œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤ (í˜ì´ì§€ë³„ ìƒì„¸ ë¡œê·¸ëŠ” ì•„ë˜ì— í‘œì‹œë©ë‹ˆë‹¤)â€¦\n",
      "ğŸ”„ PDF OCR ì²˜ë¦¬ ì‹œì‘: test.pdf\n",
      "   ğŸ“Š DPI: 300\n",
      "   ğŸŒ ì–¸ì–´: kor+eng\n",
      "   ğŸ“„ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì¤‘...\n",
      "   âœ… ë³€í™˜ ì™„ë£Œ: 27í˜ì´ì§€ (5.55ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 1/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "   âœ… ë³€í™˜ ì™„ë£Œ: 27í˜ì´ì§€ (5.55ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 1/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,973ì ì¶”ì¶œ (18.28ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 2/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,973ì ì¶”ì¶œ (18.28ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 2/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,603ì ì¶”ì¶œ (16.98ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 3/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,603ì ì¶”ì¶œ (16.98ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 3/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,488ì ì¶”ì¶œ (15.52ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 4/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,488ì ì¶”ì¶œ (15.52ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 4/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,517ì ì¶”ì¶œ (17.71ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 5/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,517ì ì¶”ì¶œ (17.71ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 5/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,414ì ì¶”ì¶œ (10.95ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 6/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,414ì ì¶”ì¶œ (10.95ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 6/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,451ì ì¶”ì¶œ (18.59ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 7/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,451ì ì¶”ì¶œ (18.59ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 7/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,482ì ì¶”ì¶œ (15.61ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 8/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,482ì ì¶”ì¶œ (15.61ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 8/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,414ì ì¶”ì¶œ (16.77ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 9/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,414ì ì¶”ì¶œ (16.77ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 9/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,478ì ì¶”ì¶œ (16.46ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 10/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,478ì ì¶”ì¶œ (16.46ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 10/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,172ì ì¶”ì¶œ (17.00ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 11/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,172ì ì¶”ì¶œ (17.00ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 11/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,028ì ì¶”ì¶œ (14.32ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 12/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,028ì ì¶”ì¶œ (14.32ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 12/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,049ì ì¶”ì¶œ (12.14ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 13/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,049ì ì¶”ì¶œ (12.14ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 13/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,847ì ì¶”ì¶œ (11.97ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 14/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,847ì ì¶”ì¶œ (11.97ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 14/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,125ì ì¶”ì¶œ (17.35ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 15/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,125ì ì¶”ì¶œ (17.35ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 15/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 1,933ì ì¶”ì¶œ (11.65ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 16/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 1,933ì ì¶”ì¶œ (11.65ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 16/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,274ì ì¶”ì¶œ (15.77ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 17/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,274ì ì¶”ì¶œ (15.77ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 17/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,304ì ì¶”ì¶œ (17.86ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 18/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,304ì ì¶”ì¶œ (17.86ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 18/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,502ì ì¶”ì¶œ (15.48ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 19/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,502ì ì¶”ì¶œ (15.48ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 19/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,492ì ì¶”ì¶œ (17.09ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 20/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,492ì ì¶”ì¶œ (17.09ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 20/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,175ì ì¶”ì¶œ (17.49ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 21/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,175ì ì¶”ì¶œ (17.49ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 21/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,998ì ì¶”ì¶œ (18.62ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 22/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,998ì ì¶”ì¶œ (18.62ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 22/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,147ì ì¶”ì¶œ (18.90ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 23/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,147ì ì¶”ì¶œ (18.90ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 23/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,082ì ì¶”ì¶œ (18.05ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 24/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,082ì ì¶”ì¶œ (18.05ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 24/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,072ì ì¶”ì¶œ (19.43ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 25/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,072ì ì¶”ì¶œ (19.43ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 25/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,208ì ì¶”ì¶œ (19.24ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 26/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,208ì ì¶”ì¶œ (19.24ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 26/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 1,415ì ì¶”ì¶œ (11.20ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 27/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 1,415ì ì¶”ì¶œ (11.20ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 27/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 1,903ì ì¶”ì¶œ (8.96ì´ˆ)\n",
      "\n",
      "ğŸ“Š OCR ì²˜ë¦¬ ì™„ë£Œ - test.pdf\n",
      "   âœ… ì„±ê³µ í˜ì´ì§€: 27/27\n",
      "   ğŸ“ ì´ ì¶”ì¶œ í…ìŠ¤íŠ¸: 82,546ì\n",
      "   â±ï¸ ì´ ì²˜ë¦¬ì‹œê°„: 429.36ì´ˆ\n",
      "   ğŸ’¾ ì €ì¥ëœ íŒŒì¼:\n",
      "      ğŸ“„ ì „ì²´ ê²°ê³¼: test_ocr_full.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 1: test_page_001.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 2: test_page_002.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 3: test_page_003.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 4: test_page_004.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 5: test_page_005.txt\n",
      "      âœ… ì™„ë£Œ: 1,903ì ì¶”ì¶œ (8.96ì´ˆ)\n",
      "\n",
      "ğŸ“Š OCR ì²˜ë¦¬ ì™„ë£Œ - test.pdf\n",
      "   âœ… ì„±ê³µ í˜ì´ì§€: 27/27\n",
      "   ğŸ“ ì´ ì¶”ì¶œ í…ìŠ¤íŠ¸: 82,546ì\n",
      "   â±ï¸ ì´ ì²˜ë¦¬ì‹œê°„: 429.36ì´ˆ\n",
      "   ğŸ’¾ ì €ì¥ëœ íŒŒì¼:\n",
      "      ğŸ“„ ì „ì²´ ê²°ê³¼: test_ocr_full.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 1: test_page_001.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 2: test_page_002.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 3: test_page_003.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 4: test_page_004.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 5: test_page_005.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 6: test_page_006.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 7: test_page_007.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 8: test_page_008.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 9: test_page_009.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 10: test_page_010.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 11: test_page_011.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 12: test_page_012.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 6: test_page_006.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 7: test_page_007.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 8: test_page_008.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 9: test_page_009.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 10: test_page_010.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 11: test_page_011.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 12: test_page_012.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 13: test_page_013.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 14: test_page_014.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 15: test_page_015.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 16: test_page_016.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 17: test_page_017.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 18: test_page_018.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 19: test_page_019.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 20: test_page_020.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 21: test_page_021.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 22: test_page_022.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 23: test_page_023.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 24: test_page_024.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 25: test_page_025.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 26: test_page_026.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 13: test_page_013.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 14: test_page_014.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 15: test_page_015.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 16: test_page_016.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 17: test_page_017.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 18: test_page_018.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 19: test_page_019.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 20: test_page_020.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 21: test_page_021.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 22: test_page_022.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 23: test_page_023.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 24: test_page_024.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 25: test_page_025.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 26: test_page_026.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 27: test_page_027.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 27: test_page_027.txt\n",
      "\n",
      "âœ… OCR ì¶”ì¶œ ì™„ë£Œ\n",
      "\n",
      "ğŸ“¦ ì €ì¥ ê²°ê³¼ ìš”ì•½\n",
      "   ğŸ“„ ì „ì²´ ê²°ê³¼ íŒŒì¼: test_ocr_full.txt\n",
      "   ğŸ“‘ í˜ì´ì§€ë³„ íŒŒì¼ ìˆ˜: 27\n",
      "      ì˜ˆì‹œ: test_page_001.txt, test_page_002.txt, test_page_003.txt, test_page_004.txt, test_page_005.txt ...\n",
      "   ğŸ“ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: 82,546ì\n",
      "   ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/output/extracted_texts\n",
      "\n",
      "ğŸ” í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 20ì¤„)\n",
      "=== í˜ì´ì§€ 1 (OCR) ===\n",
      "&ì‚|ì„œë¹„ìŠ¤ë¿Œ\n",
      "Pll Bit\n",
      "2023 6A, pp.1-27\n",
      "\n",
      "Ba geet\n",
      "ert\n",
      "F Bal\n",
      "\n",
      "Journal of Information Technology Services\n",
      "https://doi.org/10.9716/KITS.2023.22.3.001\n",
      "\n",
      "ì–‘ì†ì¡ì´ ë¦¬ë”ì‹­ê³¼ í˜ì‹ ì ì¸ ì—½ë¬´ í–‰ë™:\n",
      "í•œêµ­ ë°˜ë„ì²´ ì‚°ì—…ì˜ ì¦ê±°\n",
      "\n",
      "ë”í˜ í—¨ë¦¬ ì•„ë©”ìš”**ã†ì˜¤í¬ë¦¬ í—¨ë¦¬***ã†\n",
      "\n",
      "Ambidextrous Leadership an\n",
      "\n",
      "oO     ok ok OK ok\n",
      "\n",
      "âœ… OCR ì¶”ì¶œ ì™„ë£Œ\n",
      "\n",
      "ğŸ“¦ ì €ì¥ ê²°ê³¼ ìš”ì•½\n",
      "   ğŸ“„ ì „ì²´ ê²°ê³¼ íŒŒì¼: test_ocr_full.txt\n",
      "   ğŸ“‘ í˜ì´ì§€ë³„ íŒŒì¼ ìˆ˜: 27\n",
      "      ì˜ˆì‹œ: test_page_001.txt, test_page_002.txt, test_page_003.txt, test_page_004.txt, test_page_005.txt ...\n",
      "   ğŸ“ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: 82,546ì\n",
      "   ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/output/extracted_texts\n",
      "\n",
      "ğŸ” í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 20ì¤„)\n",
      "=== í˜ì´ì§€ 1 (OCR) ===\n",
      "&ì‚|ì„œë¹„ìŠ¤ë¿Œ\n",
      "Pll Bit\n",
      "2023 6A, pp.1-27\n",
      "\n",
      "Ba geet\n",
      "ert\n",
      "F Bal\n",
      "\n",
      "Journal of Information Technology Services\n",
      "https://doi.org/10.9716/KITS.2023.22.3.001\n",
      "\n",
      "ì–‘ì†ì¡ì´ ë¦¬ë”ì‹­ê³¼ í˜ì‹ ì ì¸ ì—½ë¬´ í–‰ë™:\n",
      "í•œêµ­ ë°˜ë„ì²´ ì‚°ì—…ì˜ ì¦ê±°\n",
      "\n",
      "ë”í˜ í—¨ë¦¬ ì•„ë©”ìš”**ã†ì˜¤í¬ë¦¬ í—¨ë¦¬***ã†\n",
      "\n",
      "Ambidextrous Leadership an\n",
      "\n",
      "oO     ok ok OK ok\n"
     ]
    }
   ],
   "source": [
    "# âœ… í…ŒìŠ¤íŠ¸ PDFì— ëŒ€í•´ ì‹¤ì œ OCRì„ ì‹¤í–‰í•˜ê³ , í˜ì´ì§€ë³„ ë¡œê·¸ì™€ ì €ì¥ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ëŸ¬ë„ˆ ì…€\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(\"ğŸš€ í…ŒìŠ¤íŠ¸ PDF OCR ì‹¤í–‰ ëŸ¬ë„ˆ ì‹œì‘\", flush=True)\n",
    "\n",
    "\n",
    "# ì…ë ¥ ë° ì¶œë ¥ ê²½ë¡œ ì„¤ì •\n",
    "test_pdf_path = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\")\n",
    "\n",
    "\n",
    "if 'config' in globals() and hasattr(config, 'extracted_texts_dir'):\n",
    "\n",
    "    output_dir = config.extracted_texts_dir\n",
    "\n",
    "elif 'config' in globals() and hasattr(config, 'extracted_text_dir'):\n",
    "\n",
    "    output_dir = config.extracted_text_dir\n",
    "\n",
    "else:\n",
    "\n",
    "    output_dir = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\")\n",
    "\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"ğŸ“„ ì…ë ¥ íŒŒì¼: {test_pdf_path}\", flush=True)\n",
    "print(f\"ğŸ’¾ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}\", flush=True)\n",
    "\n",
    "\n",
    "# ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "if not test_pdf_path.exists():\n",
    "\n",
    "    raise FileNotFoundError(f\"ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {test_pdf_path}\")\n",
    "\n",
    "\n",
    "# ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤ í™•ì¸/ìƒì„±\n",
    "extractor = None\n",
    "\n",
    "\n",
    "if 'text_extractor' in globals():\n",
    "\n",
    "    extractor = text_extractor\n",
    "\n",
    "else:\n",
    "\n",
    "    # í´ë˜ìŠ¤ ì´ë¦„ í˜¸í™˜ ì²˜ë¦¬\n",
    "    try:\n",
    "\n",
    "        extractor = DocumentTextExtractor(config)\n",
    "\n",
    "    except NameError:\n",
    "\n",
    "        try:\n",
    "\n",
    "            extractor = TextExtractor(config)\n",
    "\n",
    "        except NameError as e:\n",
    "\n",
    "            raise NameError(\"ì¶”ì¶œê¸° í´ë˜ìŠ¤(DocumentTextExtractor ë˜ëŠ” TextExtractor)ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € í•´ë‹¹ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\") from e\n",
    "\n",
    "\n",
    "# OCR ì„¤ì •ì´ êº¼ì ¸ ìˆìœ¼ë©´ ì´ë²ˆ ì‹¤í–‰ì— í•œí•´ ì„ì‹œë¡œ ì¼­ë‹ˆë‹¤.\n",
    "prev_ocr_enabled = None\n",
    "ocr_cfg = getattr(extractor.config, 'ocr_config', {}) if hasattr(extractor, 'config') else {}\n",
    "prev_ocr_enabled = ocr_cfg.get('enabled', True)\n",
    "if not prev_ocr_enabled:\n",
    "\n",
    "    print(\"â„¹ï¸ í˜„ì¬ OCRì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´, ì´ë²ˆ ì‹¤í–‰ ë™ì•ˆë§Œ ì„ì‹œë¡œ í™œì„±í™”í•©ë‹ˆë‹¤.\", flush=True)\n",
    "\n",
    "    extractor.config.ocr_config['enabled'] = True\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    print(\"ğŸ” OCR ì¶”ì¶œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤ (í˜ì´ì§€ë³„ ìƒì„¸ ë¡œê·¸ëŠ” ì•„ë˜ì— í‘œì‹œë©ë‹ˆë‹¤)â€¦\", flush=True)\n",
    "\n",
    "    result = extractor._extract_pdf_ocr(test_pdf_path)\n",
    "\n",
    "    print(\"\\nâœ… OCR ì¶”ì¶œ ì™„ë£Œ\", flush=True)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"âŒ OCR ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}\", flush=True)\n",
    "\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "\n",
    "    # ì„¤ì • ë³µêµ¬\n",
    "    if prev_ocr_enabled is not None:\n",
    "\n",
    "        extractor.config.ocr_config['enabled'] = prev_ocr_enabled\n",
    "\n",
    "\n",
    "# ì €ì¥ íŒŒì¼ ìš”ì•½ ì¶œë ¥\n",
    "try:\n",
    "\n",
    "    meta = result.get('metadata', {}) if isinstance(result, dict) else {}\n",
    "\n",
    "    output_file = meta.get('output_file')\n",
    "\n",
    "    page_files = meta.get('page_files', [])\n",
    "\n",
    "    text_len = meta.get('total_characters', len(result.get('text', '') if isinstance(result, dict) else ''))\n",
    "\n",
    "    print(\"\\nğŸ“¦ ì €ì¥ ê²°ê³¼ ìš”ì•½\", flush=True)\n",
    "\n",
    "    if output_file:\n",
    "\n",
    "        print(f\"   ğŸ“„ ì „ì²´ ê²°ê³¼ íŒŒì¼: {Path(output_file).name}\", flush=True)\n",
    "\n",
    "    if page_files:\n",
    "\n",
    "        print(f\"   ğŸ“‘ í˜ì´ì§€ë³„ íŒŒì¼ ìˆ˜: {len(page_files)}\", flush=True)\n",
    "\n",
    "        sample_list = [Path(p).name for p in page_files[:5]]\n",
    "\n",
    "        more = ' ...' if len(page_files) > 5 else ''\n",
    "\n",
    "        print(f\"      ì˜ˆì‹œ: {', '.join(sample_list)}{more}\", flush=True)\n",
    "\n",
    "    print(f\"   ğŸ“ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {text_len:,}ì\", flush=True)\n",
    "\n",
    "    print(f\"   ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}\", flush=True)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"âš ï¸ ì €ì¥ ê²°ê³¼ ìš”ì•½ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜: {e}\", flush=True)\n",
    "\n",
    "    \n",
    "\n",
    "# ë¯¸ë¦¬ë³´ê¸° (ê³¼ë„í•œ ì¶œë ¥ ë°©ì§€)\n",
    "try:\n",
    "\n",
    "    text = result.get('text', '') if isinstance(result, dict) else ''\n",
    "\n",
    "    if text:\n",
    "\n",
    "        preview = text.strip().splitlines()\n",
    "\n",
    "        head = '\\n'.join(preview[:20])  # ì²˜ìŒ 20ì¤„ ë¯¸ë¦¬ë³´ê¸°\n",
    "\n",
    "        print(\"\\nğŸ” í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 20ì¤„)\", flush=True)\n",
    "\n",
    "        print(head[:2000], flush=True)  # ìµœëŒ€ 2000ì ì œí•œ\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"(ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤)\", flush=True)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"âš ï¸ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥ ì¤‘ ì˜¤ë¥˜: {e}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccf21eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ!\n",
      "======================================================================\n",
      "ğŸ“‹ ì§€ì›í•˜ëŠ” ì¶”ì¶œ ë°©ë²•:\n",
      "   ğŸ“„ PDF: PyPDF2, pdfplumber, OCR (í˜ì´ì§€ë³„ ì €ì¥)\n",
      "   ğŸ“ Word: python-docx\n",
      "   ğŸ¨ PowerPoint: python-pptx\n",
      "   ğŸ“Š Excel: pandas + openpyxl\n",
      "   ğŸŒ HTML: BeautifulSoup + html2text\n",
      "======================================================================\n",
      "ğŸ“‹ ì§€ì›í•˜ëŠ” ì¶”ì¶œ ë°©ë²•:\n",
      "   ğŸ“„ PDF: PyPDF2, pdfplumber, OCR (í˜ì´ì§€ë³„ ì €ì¥)\n",
      "   ğŸ“ Word: python-docx\n",
      "   ğŸ¨ PowerPoint: python-pptx\n",
      "   ğŸ“Š Excel: pandas + openpyxl\n",
      "   ğŸŒ HTML: BeautifulSoup + html2text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ–¼ï¸ ì´ë¯¸ì§€: Tesseract OCR\n",
      "   ğŸ“„ í…ìŠ¤íŠ¸: ì¸ì½”ë”© ìë™ ê°ì§€\n",
      "\n",
      "ğŸ†• OCR ê°œì„ ì‚¬í•­:\n",
      "   ğŸ“Š í˜ì´ì§€ë³„ ì‹¤ì‹œê°„ ì§„í–‰ ë¡œê·¸\n",
      "   ğŸ’¾ í˜ì´ì§€ë³„ ê°œë³„ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥\n",
      "   ğŸ“„ ì „ì²´ ê²°ê³¼ í†µí•© íŒŒì¼ ì €ì¥\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   - text_extractor.extract_text(file_path): ìë™ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
      "   - text_extractor.extract_text(file_path, method='ocr'): OCR ê°•ì œ ì‚¬ìš©\n",
      "   - text_extractor.extract_text_from_document(doc_info): ë°°ì¹˜ ì²˜ë¦¬ìš©\n",
      "   ğŸ“„ í…ìŠ¤íŠ¸: ì¸ì½”ë”© ìë™ ê°ì§€\n",
      "\n",
      "ğŸ†• OCR ê°œì„ ì‚¬í•­:\n",
      "   ğŸ“Š í˜ì´ì§€ë³„ ì‹¤ì‹œê°„ ì§„í–‰ ë¡œê·¸\n",
      "   ğŸ’¾ í˜ì´ì§€ë³„ ê°œë³„ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥\n",
      "   ğŸ“„ ì „ì²´ ê²°ê³¼ í†µí•© íŒŒì¼ ì €ì¥\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   - text_extractor.extract_text(file_path): ìë™ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
      "   - text_extractor.extract_text(file_path, method='ocr'): OCR ê°•ì œ ì‚¬ìš©\n",
      "   - text_extractor.extract_text_from_document(doc_info): ë°°ì¹˜ ì²˜ë¦¬ìš©\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¤ ë‚˜ë¨¸ì§€ ë¬¸ì„œ í˜•ì‹ ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€\n",
    "\n",
    "\n",
    "def _extract_from_docx(self, file_path):\n",
    "    \"\"\"Word ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    # ì§€ì—° ì„í¬íŠ¸\n",
    "    try:\n",
    "        from docx import Document  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"python-docx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install python-docx\") from e\n",
    "    doc = Document(file_path)\n",
    "\n",
    "\n",
    "    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "    core_props = doc.core_properties\n",
    "    metadata = {\n",
    "        'extraction_method': 'python-docx',\n",
    "        'title': core_props.title or '',\n",
    "        'author': core_props.author or '',\n",
    "        'created': str(core_props.created) if core_props.created else '',\n",
    "        'modified': str(core_props.modified) if core_props.modified else '',\n",
    "        'paragraphs': len(doc.paragraphs),\n",
    "        'tables': len(doc.tables)\n",
    "    }\n",
    "\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    text_parts = []\n",
    "\n",
    "\n",
    "    # ë‹¨ë½ í…ìŠ¤íŠ¸\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():\n",
    "            text_parts.append(para.text)\n",
    "\n",
    "\n",
    "    # í…Œì´ë¸” í…ìŠ¤íŠ¸\n",
    "    for table_num, table in enumerate(doc.tables, 1):\n",
    "        table_text = f\"\\n\\n=== í…Œì´ë¸” {table_num} ===\\n\"\n",
    "        for row in table.rows:\n",
    "            row_cells = [cell.text.strip() for cell in row.cells]\n",
    "            table_text += \" | \".join(row_cells) + \"\\n\"\n",
    "        text_parts.append(table_text)\n",
    "\n",
    "\n",
    "    full_text = '\\n'.join(text_parts)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'text': full_text,\n",
    "        'metadata': metadata,\n",
    "        'extraction_method': 'python-docx'\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_from_pptx(self, file_path):\n",
    "    \"\"\"PowerPointì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    # ì§€ì—° ì„í¬íŠ¸\n",
    "    try:\n",
    "        from pptx import Presentation  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"python-pptx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install python-pptx\") from e\n",
    "    prs = Presentation(file_path)\n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        'extraction_method': 'python-pptx',\n",
    "        'slides': len(prs.slides)\n",
    "    }\n",
    "\n",
    "\n",
    "    text_parts = []\n",
    "\n",
    "\n",
    "    for slide_num, slide in enumerate(prs.slides, 1):\n",
    "        slide_text = f\"\\n\\n=== ìŠ¬ë¼ì´ë“œ {slide_num} ===\\n\"\n",
    "\n",
    "\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                slide_text += shape.text + \"\\n\"\n",
    "\n",
    "\n",
    "        text_parts.append(slide_text)\n",
    "\n",
    "\n",
    "    full_text = '\\n'.join(text_parts)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'text': full_text,\n",
    "        'metadata': metadata,\n",
    "        'extraction_method': 'python-pptx'\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_from_excel(self, file_path):\n",
    "    \"\"\"Excelì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        import pandas as pd  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install pandas openpyxl\") from e\n",
    "    try:\n",
    "        # ëª¨ë“  ì‹œíŠ¸ ì½ê¸°\n",
    "        excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "\n",
    "        metadata = {\n",
    "            'extraction_method': 'openpyxl+pandas',\n",
    "            'sheets': list(excel_data.keys()),\n",
    "            'sheet_count': len(excel_data)\n",
    "        }\n",
    "\n",
    "\n",
    "        text_parts = []\n",
    "\n",
    "\n",
    "        for sheet_name, df in excel_data.items():\n",
    "            sheet_text = f\"\\n\\n=== ì‹œíŠ¸: {sheet_name} ===\\n\"\n",
    "            sheet_text += df.to_string(index=False)\n",
    "            text_parts.append(sheet_text)\n",
    "\n",
    "\n",
    "        full_text = '\\n'.join(text_parts)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'text': full_text,\n",
    "            'metadata': metadata,\n",
    "            'extraction_method': 'openpyxl+pandas'\n",
    "        }\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Excel íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "def _extract_from_text(self, file_path):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ\"\"\"\n",
    "    # DocumentLoaderì˜ _read_text_file_fast í™œìš©\n",
    "    try:\n",
    "        DocumentLoader  # type: ignore\n",
    "    except NameError:\n",
    "        from pathlib import Path  # fallback to avoid NameError at def time\n",
    "    doc_loader = DocumentLoader(self.config)  # type: ignore\n",
    "    text_content = doc_loader._read_text_file_fast(file_path)  # type: ignore\n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        'extraction_method': 'text_file',\n",
    "        'lines': len(text_content.split('\\n'))\n",
    "    }\n",
    "\n",
    "\n",
    "    return {\n",
    "        'text': text_content,\n",
    "        'metadata': metadata,\n",
    "        'extraction_method': 'text_file'\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_from_html(self, file_path):\n",
    "    \"\"\"HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    # HTML íŒŒì¼ ì½ê¸°\n",
    "    try:\n",
    "        DocumentLoader  # type: ignore\n",
    "    except NameError:\n",
    "        pass\n",
    "    doc_loader = DocumentLoader(self.config)  # type: ignore\n",
    "    html_content = doc_loader._read_text_file_fast(file_path)  # type: ignore\n",
    "\n",
    "\n",
    "    # BeautifulSoupìœ¼ë¡œ ì •ì œëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    processed_text = self._process_html_content(html_content)\n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        'extraction_method': 'beautifulsoup+html2text',\n",
    "        'content_length': len(html_content)\n",
    "    }\n",
    "\n",
    "\n",
    "    return {\n",
    "        'text': processed_text,\n",
    "        'metadata': metadata,\n",
    "        'extraction_method': 'beautifulsoup+html2text'\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_from_image(self, file_path):\n",
    "    \"\"\"ì´ë¯¸ì§€ì—ì„œ OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    if not self.config.processing_config['ocr_enabled']:\n",
    "        raise ValueError(\"OCRì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    try:\n",
    "        from PIL import Image  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Pillow ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install Pillow\") from e\n",
    "    try:\n",
    "        import pytesseract  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"pytesseract ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install pytesseract\") from e\n",
    "\n",
    "\n",
    "    try:\n",
    "        image = Image.open(file_path)\n",
    "\n",
    "\n",
    "        # OCR ì‹¤í–‰\n",
    "        text = pytesseract.image_to_string(\n",
    "            image,\n",
    "            lang=self.config.processing_config['ocr_language']\n",
    "        )\n",
    "\n",
    "\n",
    "        metadata = {\n",
    "            'extraction_method': 'tesseract_ocr',\n",
    "            'image_size': image.size,\n",
    "            'image_mode': image.mode,\n",
    "            'ocr_language': self.config.processing_config['ocr_language']\n",
    "        }\n",
    "\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'metadata': metadata,\n",
    "            'extraction_method': 'tesseract_ocr'\n",
    "        }\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ì´ë¯¸ì§€ OCR ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "def _process_html_content(self, html_content):\n",
    "    \"\"\"HTML ì½˜í…ì¸  ì •ì œ\"\"\"\n",
    "    # ì§€ì—° ì„í¬íŠ¸\n",
    "    try:\n",
    "        from bs4 import BeautifulSoup  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"beautifulsoup4 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install beautifulsoup4 html2text\") from e\n",
    "    try:\n",
    "        import re  # stdlib\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\n",
    "        # í…Œì´ë¸” ì²˜ë¦¬\n",
    "        tables = soup.find_all('table')\n",
    "        for table in tables:\n",
    "            table_text = self._convert_html_table_to_text(table)\n",
    "            table.replace_with(table_text)\n",
    "\n",
    "\n",
    "        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°\n",
    "        for tag in soup(['script', 'style', 'meta', 'link', 'head']):\n",
    "            tag.decompose()\n",
    "\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        text = soup.get_text()\n",
    "\n",
    "\n",
    "        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì •ì œ\n",
    "        import re as _re\n",
    "        text = _re.sub(r'<!--.*?-->', '', text, flags=_re.DOTALL)  # HTML ì£¼ì„\n",
    "        text = _re.sub(r'\\n{3,}', '\\n\\n', text)  # ì—°ì† ê°œí–‰\n",
    "        text = _re.sub(r'[ \\t]+', ' ', text)  # ì—°ì† ê³µë°±\n",
    "\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        self.logger.warning(f\"HTML ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        return html_content\n",
    "\n",
    "\n",
    "def _convert_html_table_to_text(self, table):\n",
    "    \"\"\"HTML í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "    try:\n",
    "        rows = table.find_all('tr')\n",
    "        if not rows:\n",
    "            return \"\\n[ë¹ˆ í…Œì´ë¸”]\\n\"\n",
    "\n",
    "\n",
    "        table_text = \"\\n\\n=== í…Œì´ë¸” ===\\n\"\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            row_text = []\n",
    "            for cell in cells:\n",
    "                cell_text = cell.get_text().strip()\n",
    "                row_text.append(cell_text)\n",
    "            if row_text:\n",
    "                table_text += \" | \".join(row_text) + \"\\n\"\n",
    "\n",
    "\n",
    "        return table_text\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        return \"\\n[í…Œì´ë¸” ì²˜ë¦¬ ì˜¤ë¥˜]\\n\"\n",
    "\n",
    "\n",
    "def _format_tables_as_text(self, tables, page_num):\n",
    "    \"\"\"pdfplumber í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·\"\"\"\n",
    "    if not tables:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    table_text = f\"\\n\\n=== í˜ì´ì§€ {page_num} í…Œì´ë¸” ===\\n\"\n",
    "\n",
    "\n",
    "    for table_num, table in enumerate(tables, 1):\n",
    "        table_text += f\"\\n--- í…Œì´ë¸” {table_num} ---\\n\"\n",
    "        for row in table:\n",
    "            if row:\n",
    "                clean_row = [str(cell).strip() if cell else \"\" for cell in row]\n",
    "                table_text += \" | \".join(clean_row) + \"\\n\"\n",
    "\n",
    "\n",
    "    return table_text\n",
    "\n",
    "\n",
    "def extract_text_from_document(self, document_info):\n",
    "    \"\"\"ë°°ì¹˜ í”„ë¡œì„¸ì„œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ë©”ì†Œë“œ\"\"\"\n",
    "    from pathlib import Path\n",
    "    try:\n",
    "        file_path = document_info.get('file_path')\n",
    "        if not file_path:\n",
    "            raise ValueError(\"ë¬¸ì„œ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "        # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ì¶”ì¶œ ë°©ë²• ê²°ì •\n",
    "        file_type = document_info.get('file_type', 'unknown')\n",
    "\n",
    "\n",
    "        # ì´ë¯¸ì§€ ê¸°ë°˜ PDF ê°ì§€ ë¡œì§\n",
    "        method = 'auto'\n",
    "        if file_type == 'pdf':\n",
    "            # ê¸°ì¡´ íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ìˆê±°ë‚˜ ë§¤ìš° ì ìœ¼ë©´ ì´ë¯¸ì§€ ê¸°ë°˜ì¼ ê°€ëŠ¥ì„±\n",
    "            existing_content = document_info.get('file_content', '')\n",
    "            if not existing_content or len(existing_content.strip()) < 100:\n",
    "                print(f\"âš ï¸ ì´ë¯¸ì§€ ê¸°ë°˜ PDFë¡œ íŒë‹¨ë¨: {Path(file_path).name}\", flush=True)\n",
    "                method = 'ocr'  # OCR ê°•ì œ ì‚¬ìš©\n",
    "\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤í–‰\n",
    "        result = self.extract_text(file_path, method=method)\n",
    "\n",
    "\n",
    "        # ë°°ì¹˜ í”„ë¡œì„¸ì„œ í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ í¬ë§·íŒ…\n",
    "        return {\n",
    "            'success': result.get('success', False),\n",
    "            'extracted_text': result.get('text', ''),\n",
    "            'extraction_method': result.get('extraction_method', method),\n",
    "            'metadata': result.get('metadata', {}),\n",
    "            'processing_time': result.get('processing_time', 0),\n",
    "            'file_path': file_path,\n",
    "            'file_type': file_type,\n",
    "            'error': result.get('error')\n",
    "        }\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'extracted_text': '',\n",
    "            'extraction_method': 'failed',\n",
    "            'metadata': {},\n",
    "            'processing_time': 0,\n",
    "            'file_path': document_info.get('file_path', ''),\n",
    "            'file_type': document_info.get('file_type', 'unknown'),\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# ë©”ì†Œë“œ ë“±ë¡ ëŒ€ìƒ í´ë˜ìŠ¤ í™•ì¸ (TextExtractor ìš°ì„ , ì—†ìœ¼ë©´ DocumentTextExtractor ì‚¬ìš©)\n",
    "try:\n",
    "    ExtractorClass = TextExtractor  # ê¸°ì¡´ ì´ë¦„\n",
    "except NameError:\n",
    "    try:\n",
    "        ExtractorClass = DocumentTextExtractor  # ìƒˆ ì´ë¦„\n",
    "    except NameError:\n",
    "        ExtractorClass = None\n",
    "\n",
    "\n",
    "if ExtractorClass is None:\n",
    "    raise NameError(\"ë©”ì†Œë“œë¥¼ ë“±ë¡í•  ëŒ€ìƒ í´ë˜ìŠ¤(TextExtractor ë˜ëŠ” DocumentTextExtractor)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¶”ì¶œê¸° í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# ì´ë¯¸ ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ì¤‘ë³µ ë“±ë¡ ë°©ì§€\n",
    "if getattr(ExtractorClass, \"_other_methods_registered\", False):\n",
    "    print(\"â„¹ï¸ ê¸°íƒ€ ë¬¸ì„œ í˜•ì‹ ì¶”ì¶œ ë©”ì†Œë“œëŠ” ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\", flush=True)\n",
    "else:\n",
    "    # ë©”ì†Œë“œ ì¶”ê°€\n",
    "    ExtractorClass._extract_from_docx = _extract_from_docx\n",
    "    ExtractorClass._extract_from_pptx = _extract_from_pptx\n",
    "    ExtractorClass._extract_from_excel = _extract_from_excel\n",
    "    ExtractorClass._extract_from_text = _extract_from_text\n",
    "    ExtractorClass._extract_from_html = _extract_from_html\n",
    "    ExtractorClass._extract_from_image = _extract_from_image\n",
    "    ExtractorClass._process_html_content = _process_html_content\n",
    "    ExtractorClass._convert_html_table_to_text = _convert_html_table_to_text\n",
    "    ExtractorClass._format_tables_as_text = _format_tables_as_text\n",
    "    ExtractorClass.extract_text_from_document = extract_text_from_document\n",
    "    setattr(ExtractorClass, \"_other_methods_registered\", True)\n",
    "    print(\"âœ… ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ!\", flush=True)\n",
    "    print(\"=\" * 70, flush=True)\n",
    "    print(\"ğŸ“‹ ì§€ì›í•˜ëŠ” ì¶”ì¶œ ë°©ë²•:\", flush=True)\n",
    "    print(\"   ğŸ“„ PDF: PyPDF2, pdfplumber, OCR (í˜ì´ì§€ë³„ ì €ì¥)\", flush=True)\n",
    "    print(\"   ğŸ“ Word: python-docx\", flush=True)\n",
    "    print(\"   ğŸ¨ PowerPoint: python-pptx\", flush=True)\n",
    "    print(\"   ğŸ“Š Excel: pandas + openpyxl\", flush=True)\n",
    "    print(\"   ğŸŒ HTML: BeautifulSoup + html2text\", flush=True)\n",
    "    print(\"   ğŸ–¼ï¸ ì´ë¯¸ì§€: Tesseract OCR\", flush=True)\n",
    "    print(\"   ğŸ“„ í…ìŠ¤íŠ¸: ì¸ì½”ë”© ìë™ ê°ì§€\", flush=True)\n",
    "    print(f\"\\nğŸ†• OCR ê°œì„ ì‚¬í•­:\", flush=True)\n",
    "    print(f\"   ğŸ“Š í˜ì´ì§€ë³„ ì‹¤ì‹œê°„ ì§„í–‰ ë¡œê·¸\", flush=True)\n",
    "    print(f\"   ğŸ’¾ í˜ì´ì§€ë³„ ê°œë³„ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥\", flush=True)\n",
    "    print(f\"   ğŸ“„ ì „ì²´ ê²°ê³¼ í†µí•© íŒŒì¼ ì €ì¥\", flush=True)\n",
    "    try:\n",
    "        print(f\"   ğŸ’¾ ì €ì¥ ê²½ë¡œ: {config.extracted_text_dir}\", flush=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"\\nğŸ’¡ ì‚¬ìš©ë²•:\", flush=True)\n",
    "    print(f\"   - text_extractor.extract_text(file_path): ìë™ í…ìŠ¤íŠ¸ ì¶”ì¶œ\", flush=True)\n",
    "    print(f\"   - text_extractor.extract_text(file_path, method='ocr'): OCR ê°•ì œ ì‚¬ìš©\", flush=True)\n",
    "    print(f\"   - text_extractor.extract_text_from_document(doc_info): ë°°ì¹˜ ì²˜ë¦¬ìš©\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d245bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ OCR í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ ì‹œì‘\n",
      "ğŸ“„ ì…ë ¥ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ“‚ ì¶œë ¥ ê²½ë¡œ: /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\n",
      "ğŸ” ì‹¤í–‰ ì •ë³´:\n",
      "   â€¢ OCR í™œì„±í™”: True\n",
      "   â€¢ ì–¸ì–´: eng\n",
      "   â€¢ DPI: 200\n",
      "ğŸ”„ OCR ì‹¤í–‰ ì¤‘ (í˜ì´ì§€ë³„ ì§„í–‰ ë¡œê·¸ëŠ” ì•„ë˜ì— í‘œì‹œë©ë‹ˆë‹¤)â€¦\n",
      "ğŸ”„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: test.pdf\n",
      "   ğŸ“„ íŒŒì¼ íƒ€ì…: pdf\n",
      "   ğŸ”§ ì¶”ì¶œ ë°©ë²•: ocr\n",
      "ğŸ”„ PDF OCR ì²˜ë¦¬ ì‹œì‘: test.pdf\n",
      "   ğŸ“Š DPI: 300\n",
      "   ğŸŒ ì–¸ì–´: kor+eng\n",
      "   ğŸ“„ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì¤‘...\n",
      "   âœ… ë³€í™˜ ì™„ë£Œ: 27í˜ì´ì§€ (5.21ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 1/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,973ì ì¶”ì¶œ (18.97ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 2/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,603ì ì¶”ì¶œ (17.44ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 3/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,488ì ì¶”ì¶œ (15.85ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 4/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,517ì ì¶”ì¶œ (17.46ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 5/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,414ì ì¶”ì¶œ (11.06ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 6/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,451ì ì¶”ì¶œ (18.56ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 7/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,482ì ì¶”ì¶œ (16.21ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 8/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,414ì ì¶”ì¶œ (16.94ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 9/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,478ì ì¶”ì¶œ (16.78ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 10/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,172ì ì¶”ì¶œ (17.27ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 11/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,028ì ì¶”ì¶œ (14.38ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 12/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,049ì ì¶”ì¶œ (12.26ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 13/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,847ì ì¶”ì¶œ (12.11ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 14/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,125ì ì¶”ì¶œ (17.44ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 15/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 1,933ì ì¶”ì¶œ (11.78ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 16/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,274ì ì¶”ì¶œ (15.87ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 17/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,304ì ì¶”ì¶œ (17.88ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 18/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,502ì ì¶”ì¶œ (15.66ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 19/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,492ì ì¶”ì¶œ (17.25ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 20/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,175ì ì¶”ì¶œ (17.52ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 21/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 2,998ì ì¶”ì¶œ (18.72ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 22/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,147ì ì¶”ì¶œ (18.69ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 23/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,082ì ì¶”ì¶œ (18.10ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 24/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,072ì ì¶”ì¶œ (19.67ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 25/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 3,208ì ì¶”ì¶œ (19.50ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 26/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 1,415ì ì¶”ì¶œ (11.25ì´ˆ)\n",
      "   ğŸ” í˜ì´ì§€ 27/27 OCR ì²˜ë¦¬ ì¤‘...\n",
      "      âœ… ì™„ë£Œ: 1,903ì ì¶”ì¶œ (9.18ì´ˆ)\n",
      "\n",
      "ğŸ“Š OCR ì²˜ë¦¬ ì™„ë£Œ - test.pdf\n",
      "   âœ… ì„±ê³µ í˜ì´ì§€: 27/27\n",
      "   ğŸ“ ì´ ì¶”ì¶œ í…ìŠ¤íŠ¸: 82,546ì\n",
      "   â±ï¸ ì´ ì²˜ë¦¬ì‹œê°„: 433.81ì´ˆ\n",
      "   ğŸ’¾ ì €ì¥ëœ íŒŒì¼:\n",
      "      ğŸ“„ ì „ì²´ ê²°ê³¼: test_ocr_full.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 1: test_page_001.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 2: test_page_002.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 3: test_page_003.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 4: test_page_004.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 5: test_page_005.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 6: test_page_006.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 7: test_page_007.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 8: test_page_008.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 9: test_page_009.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 10: test_page_010.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 11: test_page_011.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 12: test_page_012.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 13: test_page_013.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 14: test_page_014.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 15: test_page_015.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 16: test_page_016.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 17: test_page_017.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 18: test_page_018.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 19: test_page_019.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 20: test_page_020.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 21: test_page_021.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 22: test_page_022.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 23: test_page_023.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 24: test_page_024.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 25: test_page_025.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 26: test_page_026.txt\n",
      "      ğŸ“‘ í˜ì´ì§€ 27: test_page_027.txt\n",
      "   âŒ ì¶”ì¶œ ì‹¤íŒ¨: ì¶”ì¶œ ì‹œê°„ ì´ˆê³¼ (300ì´ˆ)\n",
      "   â±ï¸ ì²˜ë¦¬ ì‹œê°„: 439.09ì´ˆ\n",
      "\n",
      "âœ… OCR ì‹¤í–‰ ì™„ë£Œ\n",
      "   â€¢ ì„±ê³µ ì—¬ë¶€: False\n",
      "   â€¢ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: 0ì\n",
      "   â€¢ ì €ì¥ëœ í˜ì´ì§€ íŒŒì¼ ìˆ˜: 0\n",
      "â±ï¸ ì´ ì†Œìš” ì‹œê°„: 439.09ì´ˆ\n",
      "\n",
      "ğŸ’¾ ìƒì„±ëœ íŒŒì¼ ëª©ë¡ (0ê°œ):\n",
      "(ì „ì²´ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤)\n",
      "ğŸ OCR í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ ì¢…ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ OCR ì‹¤í–‰ ëŸ¬ë„ˆ: test.pdfë¥¼ ëŒ€ìƒìœ¼ë¡œ í˜ì´ì§€ë³„ ë¡œê·¸ì™€ ê²°ê³¼ ì €ì¥ ìˆ˜í–‰\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "print(\"ğŸŸ¢ OCR í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ ì‹œì‘\", flush=True)\n",
    "\n",
    "\n",
    "# ì…ë ¥ íŒŒì¼ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "test_pdf_path = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\")\n",
    "\n",
    "\n",
    "# êµ¬ì„±ì—ì„œ ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œë¡œ ëŒ€ì²´)\n",
    "if 'config' in globals() and hasattr(config, 'extracted_text_dir') and config.extracted_text_dir:\n",
    "    output_dir = config.extracted_text_dir\n",
    "else:\n",
    "    output_dir = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\")\n",
    "\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"ğŸ“„ ì…ë ¥ íŒŒì¼: {test_pdf_path}\", flush=True)\n",
    "print(f\"ğŸ“‚ ì¶œë ¥ ê²½ë¡œ: {output_dir}\", flush=True)\n",
    "\n",
    "\n",
    "# ì…ë ¥ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬\n",
    "if not test_pdf_path.exists():\n",
    "    raise FileNotFoundError(f\"ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {test_pdf_path}\")\n",
    "\n",
    "\n",
    "# ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤ ì¤€ë¹„ (text_extractor â–¶ extractor â–¶ ìƒˆë¡œ ìƒì„± ìˆœ)\n",
    "extractor = None\n",
    "if 'text_extractor' in globals() and text_extractor:\n",
    "    extractor = text_extractor\n",
    "elif 'extractor' in globals() and extractor:  # noqa: F823 (VSCode placeholder)\n",
    "    extractor = extractor\n",
    "else:\n",
    "    try:\n",
    "        extractor = DocumentTextExtractor(config)  # type: ignore\n",
    "    except NameError:\n",
    "        # í´ë˜ìŠ¤ ì´ë¦„ì´ ë‹¤ë¥¼ ê²½ìš° ëŒ€ë¹„\n",
    "        extractor = TextExtractor(config)  # type: ignore\n",
    "\n",
    "\n",
    "# OCR ì„¤ì • ë³´ì¥ (ê¸°ì¡´ ê°’ ë³´ì¡´ í›„ ë³µì›)\n",
    "ocr_cfg = getattr(config, 'processing_config', {})\n",
    "prev_ocr_enabled = ocr_cfg.get('ocr_enabled', True)\n",
    "ocr_cfg['ocr_enabled'] = True\n",
    "\n",
    "\n",
    "print(\"ğŸ” ì‹¤í–‰ ì •ë³´:\", flush=True)\n",
    "print(f\"   â€¢ OCR í™œì„±í™”: {ocr_cfg.get('ocr_enabled')}\", flush=True)\n",
    "print(f\"   â€¢ ì–¸ì–´: {ocr_cfg.get('ocr_language', 'eng')}\", flush=True)\n",
    "print(f\"   â€¢ DPI: {ocr_cfg.get('ocr_dpi', 200)}\", flush=True)\n",
    "\n",
    "\n",
    "start_ts = time.time()\n",
    "page_files = []\n",
    "output_file = None\n",
    "meta = {}\n",
    "text = \"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # ë©”ì„œë“œ ë“±ë¡ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ê³µê°œ APIë¡œ OCR ê°•ì œ ì‹¤í–‰\n",
    "    print(\"ğŸ”„ OCR ì‹¤í–‰ ì¤‘ (í˜ì´ì§€ë³„ ì§„í–‰ ë¡œê·¸ëŠ” ì•„ë˜ì— í‘œì‹œë©ë‹ˆë‹¤)â€¦\", flush=True)\n",
    "    result = extractor.extract_text(str(test_pdf_path), method='ocr')\n",
    "    \n",
    "    # ê²°ê³¼ ì •ë¦¬\n",
    "    success = result.get('success', True)\n",
    "    text = result.get('text', '')\n",
    "    meta = result.get('metadata', {})\n",
    "    output_file = Path(meta.get('output_file')) if meta.get('output_file') else None\n",
    "    page_files = [Path(p) for p in meta.get('page_files', []) if p]\n",
    "    \n",
    "    print(\"\\nâœ… OCR ì‹¤í–‰ ì™„ë£Œ\", flush=True)\n",
    "    print(f\"   â€¢ ì„±ê³µ ì—¬ë¶€: {success}\", flush=True)\n",
    "    print(f\"   â€¢ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text):,}ì\", flush=True)\n",
    "    print(f\"   â€¢ ì €ì¥ëœ í˜ì´ì§€ íŒŒì¼ ìˆ˜: {len(page_files)}\", flush=True)\n",
    "    if output_file:\n",
    "        print(f\"   â€¢ ì „ì²´ ê²°ê³¼ íŒŒì¼: {output_file}\", flush=True)\n",
    "    else:\n",
    "        # ê´€ë¡€ìƒ ì „ì²´ ê²°ê³¼ íŒŒì¼ëª… ì¶”ì •\n",
    "        candidate = output_dir / f\"{test_pdf_path.stem}_ocr_full.txt\"\n",
    "        if candidate.exists():\n",
    "            output_file = candidate\n",
    "            print(f\"   â€¢ ì „ì²´ ê²°ê³¼ íŒŒì¼(ì¶”ì •): {output_file}\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ OCR ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}\", flush=True)\n",
    "finally:\n",
    "    # ì„¤ì • ë³µì›\n",
    "    ocr_cfg['ocr_enabled'] = prev_ocr_enabled\n",
    "\n",
    "\n",
    "elapsed = time.time() - start_ts\n",
    "print(f\"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ\", flush=True)\n",
    "\n",
    "\n",
    "# ì‚°ì¶œë¬¼ ìš”ì•½\n",
    "try:\n",
    "    # ì¶œë ¥ ë””ë ‰í† ë¦¬ì— ìƒì„±ëœ íŒŒì¼ë“¤ ë‚˜ì—´\n",
    "    related_files = sorted(output_dir.glob(f\"{test_pdf_path.stem}*\"))\n",
    "    print(f\"\\nğŸ’¾ ìƒì„±ëœ íŒŒì¼ ëª©ë¡ ({len(related_files)}ê°œ):\", flush=True)\n",
    "    for p in related_files[:15]:\n",
    "        print(f\"   â€¢ {p.name}\", flush=True)\n",
    "    if len(related_files) > 15:\n",
    "        print(f\"   â€¦ (ì´ {len(related_files)}ê°œ)\", flush=True)\n",
    "\n",
    "\n",
    "    # ì „ì²´ ê²°ê³¼ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "    if output_file and output_file.exists():\n",
    "        print(\"\\nğŸ” ì „ì²´ ê²°ê³¼ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 500ì):\", flush=True)\n",
    "        head = \"\"\n",
    "        try:\n",
    "            with open(output_file, 'r', encoding='utf-8') as f:\n",
    "                head = f.read(500)\n",
    "        except Exception:\n",
    "            with open(output_file, 'r', encoding='cp949', errors='ignore') as f:\n",
    "                head = f.read(500)\n",
    "        print(head if head else \"(ë¯¸ë¦¬ë³´ê¸° ì—†ìŒ)\", flush=True)\n",
    "    else:\n",
    "        print(\"(ì „ì²´ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤)\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ì‚°ì¶œë¬¼ ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {e}\", flush=True)\n",
    "\n",
    "\n",
    "print(\"ğŸ OCR í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ ì¢…ë£Œ\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9fb92",
   "metadata": {},
   "source": [
    "## ğŸ§¹ 5ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹ (ë°±ì—”ë“œ í˜¸í™˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955fbaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì¤‘...\n",
      "âœ… í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ!\n",
      "ğŸ§¹ ë¬¸ì„œ ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì´ˆê¸°í™” ì™„ë£Œ!\n",
      "==================================================\n",
      "ğŸ“‹ ì „ì²˜ë¦¬ ê¸°ëŠ¥:\n",
      "   âœ¨ ìœ ë‹ˆì½”ë“œ ì •ê·œí™”\n",
      "   ğŸ·ï¸ HTML/XML íƒœê·¸ ì •ì œ (ë°±ì—”ë“œ tools.py ê¸°ë°˜)\n",
      "   ğŸ“ ê³µë°± ë° ê°œí–‰ ì •ë¦¬\n",
      "   ğŸ—‘ï¸ ë¹ˆ ì¤„ ì œê±°\n",
      "   ğŸ”¤ íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™”\n",
      "   âœ‚ï¸ ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ì²­í‚¹\n",
      "\n",
      "âš™ï¸ ì²­í‚¹ ì„¤ì •:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DocumentExtractionTestConfig' object has no attribute 'processing_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 344\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   âœ‚ï¸ ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ì²­í‚¹\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâš™ï¸ ì²­í‚¹ ì„¤ì •:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ìµœëŒ€ í† í°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocessing_config\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mchunk_max_tokens\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    345\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ìµœì†Œ í† í°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.processing_config[\u001b[33m'\u001b[39m\u001b[33mchunk_min_tokens\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    346\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ì¤‘ë³µ ë¹„ìœ¨: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.processing_config[\u001b[33m'\u001b[39m\u001b[33mchunk_overlap_percentage\u001b[39m\u001b[33m'\u001b[39m]*\u001b[32m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DocumentExtractionTestConfig' object has no attribute 'processing_config'"
     ]
    }
   ],
   "source": [
    "# ğŸ§¹ ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì •ì œ (ë°±ì—”ë“œ tools.py ê¸°ë°˜)\n",
    "\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"ë¬¸ì„œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ì œ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = config.logger\n",
    "        self.tokenizer = config.tokenizer\n",
    "\n",
    "\n",
    "    def preprocess_text(self, text, apply_all=True):\n",
    "        \"\"\"\n",
    "        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            text: ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "            apply_all: ëª¨ë“  ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì ìš©í• ì§€ ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "        processed_text = text\n",
    "        processing_steps = []\n",
    "\n",
    "\n",
    "        try:\n",
    "            # 1. ìœ ë‹ˆì½”ë“œ ì •ê·œí™”\n",
    "            if self.config.processing_config['normalize_unicode']:\n",
    "                import unicodedata\n",
    "                processed_text = unicodedata.normalize('NFKC', processed_text)\n",
    "                processing_steps.append(\"unicode_normalization\")\n",
    "\n",
    "\n",
    "            # 2. HTML/XML íƒœê·¸ ì œê±° ë° ì •ì œ (ë°±ì—”ë“œ process_file ê¸°ë°˜)\n",
    "            if '<' in processed_text and '>' in processed_text:\n",
    "                processed_text = self._clean_html_content(processed_text)\n",
    "                processing_steps.append(\"html_cleaning\")\n",
    "\n",
    "\n",
    "            # 3. ê³µë°± ë° ê°œí–‰ ì •ë¦¬\n",
    "            if self.config.processing_config['clean_whitespace']:\n",
    "                processed_text = self._clean_whitespace(processed_text)\n",
    "                processing_steps.append(\"whitespace_cleaning\")\n",
    "\n",
    "\n",
    "            # 4. ë¹ˆ ì¤„ ì œê±°\n",
    "            if self.config.processing_config['remove_empty_lines']:\n",
    "                processed_text = self._remove_empty_lines(processed_text)\n",
    "                processing_steps.append(\"empty_lines_removal\")\n",
    "\n",
    "\n",
    "            # 5. íŠ¹ìˆ˜ ë¬¸ì ë° ê¸°í˜¸ ì •ë¦¬\n",
    "            processed_text = self._clean_special_characters(processed_text)\n",
    "            processing_steps.append(\"special_characters_cleaning\")\n",
    "\n",
    "\n",
    "            # 6. ì¤‘ë³µ ê³µë°± ì œê±°\n",
    "            import re\n",
    "            processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "            processed_text = processed_text.strip()\n",
    "\n",
    "\n",
    "            return {\n",
    "                'original_text': text,\n",
    "                'processed_text': processed_text,\n",
    "                'original_length': len(text),\n",
    "                'processed_length': len(processed_text),\n",
    "                'processing_steps': processing_steps,\n",
    "                'compression_ratio': len(processed_text) / len(text) if len(text) > 0 else 0\n",
    "            }\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "            return {\n",
    "                'original_text': text,\n",
    "                'processed_text': text,\n",
    "                'original_length': len(text),\n",
    "                'processed_length': len(text),\n",
    "                'processing_steps': [],\n",
    "                'compression_ratio': 1.0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "\n",
    "    def _clean_html_content(self, content):\n",
    "        \"\"\"HTML ì½˜í…ì¸  ì •ì œ (ë°±ì—”ë“œ tools.pyì˜ process_file í•¨ìˆ˜ì™€ ë™ì¼)\"\"\"\n",
    "        try:\n",
    "            # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±\n",
    "            try:\n",
    "                from bs4 import BeautifulSoup  # type: ignore\n",
    "            except Exception as e:\n",
    "                self.logger.warning(\"beautifulsoup4ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install beautifulsoup4 html2text\")\n",
    "                return content\n",
    "            import re\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "\n",
    "            # í…Œì´ë¸” ì²˜ë¦¬\n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                table_text = self._process_table_to_text(table)\n",
    "                table.replace_with(table_text)\n",
    "\n",
    "\n",
    "            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°\n",
    "            for tag in soup(['script', 'style', 'meta', 'link']):\n",
    "                tag.decompose()\n",
    "\n",
    "\n",
    "            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ\n",
    "            text = soup.get_text()\n",
    "\n",
    "\n",
    "            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì •ì œ (ë°±ì—”ë“œì™€ ë™ì¼)\n",
    "            text = re.sub(r'!\\[\\]\\(figures/\\d+\\)', '', text)  # ê·¸ë¦¼ ì°¸ì¡° ì œê±°\n",
    "            text = re.sub(r'<!--\\s*FigureContent=\"([^\"]+)\"\\s*-->', r'\\1', text)  # ê·¸ë¦¼ ë‚´ìš© ì¶”ì¶œ\n",
    "            text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)  # HTML ì£¼ì„ ì œê±°\n",
    "            text = re.sub(r'<[^>]+>\\s*(.*?)\\s*</[^>]+>', r'\\1', text)  # ë‚˜ë¨¸ì§€ íƒœê·¸ ì œê±°\n",
    "            text = text.replace('<figure>', '').replace('</figure>', '')\n",
    "            text = text.replace('<table>', '').replace('</table>', '')\n",
    "            text = re.sub(r'\\n{3,}', '\\n\\n', text)  # ì—°ì† ê°œí–‰ ì •ë¦¬\n",
    "\n",
    "\n",
    "            return text\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"HTML ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            return content\n",
    "\n",
    "\n",
    "    def _process_table_to_text(self, table_html):\n",
    "        \"\"\"HTML í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë°±ì—”ë“œ tools.pyì™€ ë™ì¼ ë¡œì§)\"\"\"\n",
    "        try:\n",
    "            from bs4 import BeautifulSoup  # type: ignore\n",
    "            soup = BeautifulSoup(str(table_html), 'html.parser')\n",
    "            table = soup.find('table')\n",
    "\n",
    "\n",
    "            if not table:\n",
    "                return \"\"\n",
    "\n",
    "\n",
    "            rows = table.find_all('tr')\n",
    "            if not rows:\n",
    "                return \"\"\n",
    "\n",
    "\n",
    "            # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ\n",
    "            table_data = []\n",
    "            for row in rows:\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                row_data = []\n",
    "                for cell in cells:\n",
    "                    cell_text = cell.get_text().strip()\n",
    "                    row_data.append(cell_text)\n",
    "                if row_data:\n",
    "                    table_data.append(row_data)\n",
    "\n",
    "\n",
    "            if not table_data:\n",
    "                return \"\"\n",
    "\n",
    "\n",
    "            # ì»¬ëŸ¼ ë„ˆë¹„ ê³„ì‚°\n",
    "            max_cols = max(len(row) for row in table_data)\n",
    "            col_widths = []\n",
    "\n",
    "\n",
    "            for col_idx in range(max_cols):\n",
    "                max_width = 0\n",
    "                for row in table_data:\n",
    "                    if col_idx < len(row):\n",
    "                        max_width = max(max_width, len(row[col_idx]))\n",
    "                col_widths.append(min(max_width, 30))  # ìµœëŒ€ 30ìë¡œ ì œí•œ\n",
    "\n",
    "\n",
    "            # í…ìŠ¤íŠ¸ í…Œì´ë¸” ìƒì„±\n",
    "            output_text = \"\\n\"\n",
    "\n",
    "\n",
    "            # ìƒë‹¨ ê²½ê³„\n",
    "            output_text += \"+\" + \"+\".join(\"â”€\" * (width + 2) for width in col_widths) + \"+\\n\"\n",
    "\n",
    "\n",
    "            # ë°ì´í„° í–‰ë“¤\n",
    "            for row in table_data:\n",
    "                output_text += \"|\"\n",
    "                for col_idx, width in enumerate(col_widths):\n",
    "                    content = row[col_idx] if col_idx < len(row) else \"\"\n",
    "                    content = content[:width]  # ë„ˆë¹„ ì œí•œ\n",
    "                    padding = width - len(content)\n",
    "                    output_text += f\" {content}{' ' * padding} |\"\n",
    "                output_text += \"\\n\"\n",
    "\n",
    "\n",
    "                # ì²« ë²ˆì§¸ í–‰ í›„ êµ¬ë¶„ì„  (í—¤ë”ê°€ ìˆëŠ” ê²½ìš°)\n",
    "                if row == table_data[0] and len(table_data) > 1:\n",
    "                    output_text += \"+\" + \"+\".join(\"â”€\" * (width + 2) for width in col_widths) + \"+\\n\"\n",
    "\n",
    "\n",
    "            # í•˜ë‹¨ ê²½ê³„\n",
    "            output_text += \"+\" + \"+\".join(\"â”€\" * (width + 2) for width in col_widths) + \"+\\n\"\n",
    "\n",
    "\n",
    "            return output_text\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"í…Œì´ë¸” ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            return \"\\n[í…Œì´ë¸” ì²˜ë¦¬ ì˜¤ë¥˜]\\n\"\n",
    "\n",
    "\n",
    "    def _clean_whitespace(self, text):\n",
    "        \"\"\"ê³µë°± ë° ê°œí–‰ ì •ë¦¬\"\"\"\n",
    "        # íƒ­ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜\n",
    "        text = text.replace('\\t', ' ')\n",
    "\n",
    "\n",
    "        # ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ\n",
    "        import re\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "\n",
    "\n",
    "        # ì¤„ ë ê³µë°± ì œê±°\n",
    "        lines = text.split('\\n')\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        text = '\\n'.join(lines)\n",
    "\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def _remove_empty_lines(self, text):\n",
    "        \"\"\"ë¹ˆ ì¤„ ì œê±°\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        non_empty_lines = [line for line in lines if line.strip()]\n",
    "        return '\\n'.join(non_empty_lines)\n",
    "\n",
    "\n",
    "    def _clean_special_characters(self, text):\n",
    "        \"\"\"íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬\"\"\"\n",
    "        # ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì œê°€ ë˜ëŠ” íŠ¹ìˆ˜ ë¬¸ìë“¤ ì •ë¦¬\n",
    "        replacements = {\n",
    "            '\\u00A0': ' ',      # ë¹„í‘œì‹œ ê³µë°±\n",
    "            '\\u2009': ' ',      # ì–‡ì€ ê³µë°±\n",
    "            '\\u200B': '',       # ì˜í­ ê³µë°±\n",
    "            '\\u200C': '',       # ì˜í­ ë¹„ê²°í•©ì\n",
    "            '\\u200D': '',       # ì˜í­ ê²°í•©ì\n",
    "            '\\uFEFF': '',       # ë°”ì´íŠ¸ ìˆœì„œ í‘œì‹œ\n",
    "            '\\u2018': \"'\",      # ì™¼ìª½ ì‘ì€ë”°ì˜´í‘œ\n",
    "            '\\u2019': \"'\",      # ì˜¤ë¥¸ìª½ ì‘ì€ë”°ì˜´í‘œ\n",
    "            '\\u201C': '\"',      # ì™¼ìª½ í°ë”°ì˜´í‘œ\n",
    "            '\\u201D': '\"',      # ì˜¤ë¥¸ìª½ í°ë”°ì˜´í‘œ\n",
    "            '\\u2013': '-',      # en dash\n",
    "            '\\u2014': '-',      # em dash\n",
    "            '\\u2026': '...',    # ì¤„ì„í‘œ\n",
    "        }\n",
    "\n",
    "\n",
    "        for old_char, new_char in replacements.items():\n",
    "            text = text.replace(old_char, new_char)\n",
    "\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def chunk_text(self, text, max_tokens=None, min_tokens=None, overlap_percentage=None):\n",
    "        \"\"\"\n",
    "        í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•©ë‹ˆë‹¤ (ë°±ì—”ë“œ tools.pyì˜ chunked_texts í•¨ìˆ˜ì™€ ë™ì¼)\n",
    "\n",
    "        Args:\n",
    "            text: ì²­í‚¹í•  í…ìŠ¤íŠ¸\n",
    "            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: ì„¤ì •ê°’ ì‚¬ìš©)\n",
    "            min_tokens: ìµœì†Œ í† í° ìˆ˜ (ê¸°ë³¸ê°’: ì„¤ì •ê°’ ì‚¬ìš©)\n",
    "            overlap_percentage: ì¤‘ë³µ ë¹„ìœ¨ (ê¸°ë³¸ê°’: ì„¤ì •ê°’ ì‚¬ìš©)\n",
    "        \"\"\"\n",
    "        # ê¸°ë³¸ê°’ ì„¤ì •\n",
    "        max_tokens = max_tokens or self.config.processing_config['chunk_max_tokens']\n",
    "        min_tokens = min_tokens or self.config.processing_config['chunk_min_tokens']\n",
    "        overlap_percentage = overlap_percentage or self.config.processing_config['chunk_overlap_percentage']\n",
    "\n",
    "\n",
    "        # í† í°í™”\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        overlap_tokens = int(max_tokens * overlap_percentage)\n",
    "\n",
    "\n",
    "        chunk_list = []\n",
    "        start_index = 0\n",
    "\n",
    "\n",
    "        while start_index < len(tokens):\n",
    "            if start_index + max_tokens >= len(tokens):  # ë§ˆì§€ë§‰ ì²­í¬ì¸ ê²½ìš°\n",
    "                remaining_tokens = len(tokens) - start_index\n",
    "                if remaining_tokens < min_tokens:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì‘ì€ ê²½ìš°\n",
    "                    if chunk_list:  # ì´ì „ ì²­í¬ê°€ ìˆë‹¤ë©´ í•©ì¹˜ê¸°\n",
    "                        chunk_list[-1] += self.tokenizer.decode(tokens[start_index:])\n",
    "                        break\n",
    "                    else:  # ì²­í¬ê°€ í•˜ë‚˜ë„ ì—†ë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                        chunk_list.append(self.tokenizer.decode(tokens[start_index:]))\n",
    "                        break\n",
    "                else:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì¶©ë¶„íˆ í° ê²½ìš°\n",
    "                    chunk_list.append(self.tokenizer.decode(tokens[start_index:]))\n",
    "                    break\n",
    "            else:\n",
    "                end_index = min(start_index + max_tokens, len(tokens))\n",
    "                chunk_tokens = tokens[start_index:end_index]\n",
    "                chunk = self.tokenizer.decode(chunk_tokens)\n",
    "                chunk_list.append(chunk)\n",
    "                start_index += max_tokens - overlap_tokens\n",
    "\n",
    "\n",
    "        return chunk_list\n",
    "\n",
    "\n",
    "    def calculate_tokens(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "\n",
    "    def preprocess_document(self, extraction_result):\n",
    "        \"\"\"\n",
    "        ì „ì²´ ë¬¸ì„œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "        Args:\n",
    "            extraction_result: TextExtractorì—ì„œ ë°˜í™˜ëœ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        if not extraction_result.get('success') or not extraction_result.get('text'):\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': 'ìœ íš¨í•œ ì¶”ì¶œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'\n",
    "            }\n",
    "\n",
    "\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "        try:\n",
    "            # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "            preprocessing_result = self.preprocess_text(extraction_result['text'])\n",
    "            processed_text = preprocessing_result['processed_text']\n",
    "\n",
    "\n",
    "            # 2. í† í° ìˆ˜ ê³„ì‚°\n",
    "            token_count = self.calculate_tokens(processed_text)\n",
    "\n",
    "\n",
    "            # 3. í…ìŠ¤íŠ¸ ì²­í‚¹\n",
    "            chunks = self.chunk_text(processed_text)\n",
    "\n",
    "\n",
    "            # 4. ì²­í¬ë³„ í† í° ìˆ˜ ê³„ì‚°\n",
    "            chunk_tokens = [self.calculate_tokens(chunk) for chunk in chunks]\n",
    "\n",
    "\n",
    "            # ê²°ê³¼ êµ¬ì„±\n",
    "            result = {\n",
    "                'success': True,\n",
    "                'original_extraction': extraction_result,\n",
    "                'preprocessing': preprocessing_result,\n",
    "                'processed_text': processed_text,\n",
    "                'token_count': token_count,\n",
    "                'chunks': chunks,\n",
    "                'chunk_count': len(chunks),\n",
    "                'chunk_tokens': chunk_tokens,\n",
    "                'processing_time': time.time() - start_time,\n",
    "                'metadata': {\n",
    "                    'file_path': extraction_result.get('file_path'),\n",
    "                    'file_type': extraction_result.get('file_type'),\n",
    "                    'extraction_method': extraction_result.get('extraction_method'),\n",
    "                    'preprocessing_steps': preprocessing_result.get('processing_steps', []),\n",
    "                    'compression_ratio': preprocessing_result.get('compression_ratio', 1.0)\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "            return result\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'original_extraction': extraction_result\n",
    "            }\n",
    "\n",
    "\n",
    "# ì „ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "preprocessor = DocumentPreprocessor(config)\n",
    "\n",
    "\n",
    "print(\"ğŸ§¹ ë¬¸ì„œ ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì´ˆê¸°í™” ì™„ë£Œ!\", flush=True)\n",
    "print(\"=\" * 50, flush=True)\n",
    "\n",
    "\n",
    "print(\"ğŸ“‹ ì „ì²˜ë¦¬ ê¸°ëŠ¥:\", flush=True)\n",
    "print(\"   âœ¨ ìœ ë‹ˆì½”ë“œ ì •ê·œí™”\", flush=True)\n",
    "print(\"   ğŸ·ï¸ HTML/XML íƒœê·¸ ì •ì œ (ë°±ì—”ë“œ tools.py ê¸°ë°˜)\", flush=True)\n",
    "print(\"   ğŸ“ ê³µë°± ë° ê°œí–‰ ì •ë¦¬\", flush=True)\n",
    "print(\"   ğŸ—‘ï¸ ë¹ˆ ì¤„ ì œê±°\", flush=True)\n",
    "print(\"   ğŸ”¤ íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™”\", flush=True)\n",
    "print(\"   âœ‚ï¸ ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ì²­í‚¹\", flush=True)\n",
    "\n",
    "\n",
    "# ì•ˆì „í•œ ì¶œë ¥(ê¸°ë³¸ê°’ í¬í•¨)\n",
    "cfg = getattr(config, 'processing_config', {}) if 'config' in globals() else {}\n",
    "max_tokens = int(cfg.get('chunk_max_tokens', 512) or 512)\n",
    "min_tokens = int(cfg.get('chunk_min_tokens', 128) or 128)\n",
    "overlap_percentage = float(cfg.get('chunk_overlap_percentage', 0.1) or 0.1)\n",
    "\n",
    "\n",
    "print(f\"\\nâš™ï¸ ì²­í‚¹ ì„¤ì •:\", flush=True)\n",
    "print(f\"   ìµœëŒ€ í† í°: {max_tokens:,}\", flush=True)\n",
    "print(f\"   ìµœì†Œ í† í°: {min_tokens:,}\", flush=True)\n",
    "print(f\"   ì¤‘ë³µ ë¹„ìœ¨: {overlap_percentage*100:.1f}%\", flush=True)\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì‚¬ìš©ë²•:\", flush=True)\n",
    "print(f\"   - preprocessor.preprocess_text(text): í…ìŠ¤íŠ¸ë§Œ ì „ì²˜ë¦¬\", flush=True)\n",
    "print(f\"   - preprocessor.chunk_text(text): í…ìŠ¤íŠ¸ ì²­í‚¹\", flush=True)\n",
    "print(f\"   - preprocessor.preprocess_document(extraction_result): ì „ì²´ ë¬¸ì„œ ì „ì²˜ë¦¬\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f276c",
   "metadata": {},
   "source": [
    "## \udcdd ìµœì¢… ë‹¨ê³„: ë¬¸ì„œ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì´ ë‹¨ê³„ì—ì„œëŠ” ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•˜ì—¬ ë‹¤ìŒ íŒŒì´í”„ë¼ì¸ê³¼ ì—°ê²°í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ë¬¸ì„œ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ê¸°ëŠ¥ ì¤€ë¹„ ì™„ë£Œ!\n",
      "ğŸ’¡ ì‚¬ìš©ë²•: test_file_extraction('íŒŒì¼ê²½ë¡œ', method='auto')\n",
      "ğŸ“Š ì§€ì› ë°©ë²•: 'auto', 'pypdf2', 'pdfplumber', 'ocr'\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\n",
      "ğŸ’¡ ì‚¬ìš©ë²•: test_file_extraction('íŒŒì¼ê²½ë¡œ', method='auto')\n",
      "ğŸ“Š ì§€ì› ë°©ë²•: 'auto', 'pypdf2', 'pdfplumber', 'ocr'\n",
      "ğŸ’¾ ì €ì¥ ê²½ë¡œ: /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ ë¬¸ì„œ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def _get_output_dir():\n",
    "    try:\n",
    "        od = getattr(config, 'extracted_text_dir', None)\n",
    "        if od:\n",
    "            Path(od).mkdir(parents=True, exist_ok=True)\n",
    "            return Path(od)\n",
    "    except Exception:\n",
    "        pass\n",
    "    od = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\")\n",
    "    od.mkdir(parents=True, exist_ok=True)\n",
    "    return od\n",
    "\n",
    "\n",
    "def _get_text_extractor():\n",
    "    try:\n",
    "        if 'text_extractor' in globals() and text_extractor:\n",
    "            return text_extractor\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return DocumentTextExtractor(config)  # type: ignore\n",
    "    except NameError:\n",
    "        return TextExtractor(config)  # type: ignore\n",
    "\n",
    "\n",
    "def _get_document_loader():\n",
    "    try:\n",
    "        if 'document_loader' in globals() and document_loader:\n",
    "            return document_loader\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return DocumentTestLoader(config)  # type: ignore\n",
    "    except NameError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _safe_doc_info_from_fs(file_path: Path):\n",
    "    try:\n",
    "        file_type = getattr(config, 'get_file_type', lambda p: p.suffix.lstrip('.').lower() or 'unknown')(file_path)\n",
    "    except Exception:\n",
    "        file_type = file_path.suffix.lstrip('.').lower() or 'unknown'\n",
    "    size_mb = 0.0\n",
    "    try:\n",
    "        size_mb = file_path.stat().st_size/1024/1024\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {'success': True, 'file_type': file_type, 'file_size_mb': size_mb}\n",
    "\n",
    "\n",
    "def test_file_extraction(file_path, method='auto'):\n",
    "    \"\"\"ê°œë³„ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    file_path = str(file_path)\n",
    "    print(f\"ğŸ” í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸: {Path(file_path).name}\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    try:\n",
    "        # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”©\n",
    "        print(\"ğŸ“„ 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”©\", flush=True)\n",
    "        loader = _get_document_loader()\n",
    "        fp = Path(file_path)\n",
    "        if loader is None:\n",
    "            print(\"âš ï¸ DocumentLoaderë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ íŒŒì¼ íƒ€ì…ë§Œ ì¶”ì •í•©ë‹ˆë‹¤.\", flush=True)\n",
    "            doc_info = _safe_doc_info_from_fs(fp)\n",
    "        else:\n",
    "            # ìµœì‹  API ì´ë¦„ìœ¼ë¡œ ì‹œë„í•˜ê³ , ì—†ìœ¼ë©´ í´ë°±\n",
    "            if hasattr(loader, 'load_document_info'):\n",
    "                doc_info = loader.load_document_info(file_path)  # type: ignore\n",
    "            else:\n",
    "                print(\"âš ï¸ load_document_infoê°€ ì—†ì–´ íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\", flush=True)\n",
    "                doc_info = _safe_doc_info_from_fs(fp)\n",
    "        if not doc_info or not doc_info.get('success', False):\n",
    "            print(f\"âŒ ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {doc_info.get('error', 'Unknown error') if isinstance(doc_info, dict) else 'Unknown error'}\", flush=True)\n",
    "            return None\n",
    "\n",
    "\n",
    "        print(f\"   âœ… íŒŒì¼ íƒ€ì…: {doc_info.get('file_type', 'unknown')}\", flush=True)\n",
    "        print(f\"   ğŸ“Š íŒŒì¼ í¬ê¸°: {doc_info.get('file_size_mb', 0):.2f} MB\", flush=True)\n",
    "\n",
    "\n",
    "        # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        print(f\"\\nğŸ”¤ 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë°©ë²•: {method})\", flush=True)\n",
    "        extractor = _get_text_extractor()\n",
    "        extraction_result = extractor.extract_text(file_path, method=method)\n",
    "        if not extraction_result.get('success', False):\n",
    "            print(f\"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {extraction_result.get('error', 'Unknown error')}\", flush=True)\n",
    "            return None\n",
    "\n",
    "\n",
    "        extracted_text = extraction_result.get('text', '')\n",
    "        metadata = extraction_result.get('metadata', {})\n",
    "\n",
    "\n",
    "        print(f\"   âœ… ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text):,}ì\", flush=True)\n",
    "        print(f\"   ğŸ“Š ì¶”ì¶œ ë°©ë²•: {extraction_result.get('extraction_method', method)}\", flush=True)\n",
    "        print(f\"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {extraction_result.get('processing_time', 0):.2f}ì´ˆ\", flush=True)\n",
    "\n",
    "\n",
    "        # ë©”íƒ€ë°ì´í„° í‘œì‹œ\n",
    "        if metadata:\n",
    "            print(f\"   ğŸ“‹ ë©”íƒ€ë°ì´í„°:\", flush=True)\n",
    "            for key, value in metadata.items():\n",
    "                if key not in ['page_results']:  # ë„ˆë¬´ ê¸´ ë°ì´í„° ì œì™¸\n",
    "                    print(f\"      {key}: {value}\", flush=True)\n",
    "\n",
    "\n",
    "        # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì €ì¥\n",
    "        print(f\"\\nğŸ’¾ 3ë‹¨ê³„: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì €ì¥\", flush=True)\n",
    "        file_stem = Path(file_path).stem\n",
    "        save_result = save_extracted_text(extracted_text, file_stem, extraction_result)\n",
    "\n",
    "\n",
    "        if save_result:\n",
    "            print(f\"   âœ… ì €ì¥ ì™„ë£Œ: {save_result}\", flush=True)\n",
    "\n",
    "\n",
    "        # 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°\n",
    "        print(f\"\\nğŸ‘€ 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì)\", flush=True)\n",
    "        print(\"-\" * 60, flush=True)\n",
    "        preview_text = extracted_text[:500] + \"...\" if len(extracted_text) > 500 else extracted_text\n",
    "        print(preview_text, flush=True)\n",
    "        print(\"-\" * 60, flush=True)\n",
    "\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nâœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ\", flush=True)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'file_path': file_path,\n",
    "            'file_type': doc_info.get('file_type', 'unknown'),\n",
    "            'extraction_method': extraction_result.get('extraction_method', method),\n",
    "            'text_length': len(extracted_text),\n",
    "            'processing_time': total_time,\n",
    "            'extracted_text': extracted_text,\n",
    "            'metadata': metadata,\n",
    "            'saved_file': str(save_result) if save_result else None\n",
    "        }\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\", flush=True)\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'file_path': file_path\n",
    "        }\n",
    "\n",
    "\n",
    "def save_extracted_text(text, file_stem, extraction_result):\n",
    "    \"\"\"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    try:\n",
    "        output_dir = _get_output_dir()\n",
    "        output_file = output_dir / f\"{file_stem}_extracted.txt\"\n",
    "\n",
    "\n",
    "        # í—¤ë” ì •ë³´ ìƒì„±\n",
    "        header = f\"=== ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ===\\n\"\n",
    "        header += f\"ì›ë³¸ íŒŒì¼: {file_stem}\\n\"\n",
    "        header += f\"ì¶”ì¶œ ë°©ë²•: {extraction_result.get('extraction_method', 'unknown')}\\n\"\n",
    "        header += f\"ì¶”ì¶œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        header += f\"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text):,}ì\\n\"\n",
    "        header += f\"ì²˜ë¦¬ ì‹œê°„: {extraction_result.get('processing_time', 0):.2f}ì´ˆ\\n\"\n",
    "        header += \"=\" * 80 + \"\\n\\n\"\n",
    "\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(header + text)\n",
    "\n",
    "\n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ í…ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}\", flush=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"ğŸ“ ë¬¸ì„œ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ê¸°ëŠ¥ ì¤€ë¹„ ì™„ë£Œ!\", flush=True)\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•: test_file_extraction('íŒŒì¼ê²½ë¡œ', method='auto')\", flush=True)\n",
    "print(\"ğŸ“Š ì§€ì› ë°©ë²•: 'auto', 'pypdf2', 'pdfplumber', 'ocr'\", flush=True)\n",
    "try:\n",
    "    print(f\"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {_get_output_dir()}\", flush=True)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b14f314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ íŒŒì¼:\n",
      "   1. 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf (pdf)\n",
      "   2. test1.pdf (pdf)\n",
      "   3. test.pdf (pdf)\n",
      "\n",
      "ğŸ’¡ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°©ë²•:\n",
      "# íŠ¹ì • íŒŒì¼ í…ŒìŠ¤íŠ¸\n",
      "test_result = test_file_extraction(sample_files[0])  # ì²« ë²ˆì§¸ íŒŒì¼\n",
      "\n",
      "# OCR ê°•ì œ ì‚¬ìš© (ì´ë¯¸ì§€ ê¸°ë°˜ PDFìš©)\n",
      "   1. 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf (pdf)\n",
      "   2. test1.pdf (pdf)\n",
      "   3. test.pdf (pdf)\n",
      "\n",
      "ğŸ’¡ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°©ë²•:\n",
      "# íŠ¹ì • íŒŒì¼ í…ŒìŠ¤íŠ¸\n",
      "test_result = test_file_extraction(sample_files[0])  # ì²« ë²ˆì§¸ íŒŒì¼\n",
      "\n",
      "# OCR ê°•ì œ ì‚¬ìš© (ì´ë¯¸ì§€ ê¸°ë°˜ PDFìš©)\n",
      "test_result = test_file_extraction(sample_files[0], method='ocr')\n",
      "\n",
      "# ëª¨ë“  íŒŒì¼ í…ŒìŠ¤íŠ¸\n",
      "for file_path in sample_files[:3]:  # ì²˜ìŒ 3ê°œ íŒŒì¼ë§Œ\n",
      "    test_file_extraction(file_path)\n",
      "\n",
      "ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ:\n",
      "test_result = test_file_extraction(sample_files[0], method='ocr')\n",
      "\n",
      "# ëª¨ë“  íŒŒì¼ í…ŒìŠ¤íŠ¸\n",
      "for file_path in sample_files[:3]:  # ì²˜ìŒ 3ê°œ íŒŒì¼ë§Œ\n",
      "    test_file_extraction(file_path)\n",
      "\n",
      "ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ:\n",
      "   ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\n",
      "   ğŸ“Š OCR ìƒì„¸ ê²°ê³¼: /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts (OCR ì‚¬ìš© ì‹œ)\n",
      "\n",
      "ğŸ”— ë‹¤ìŒ ë‹¨ê³„ ì—°ê²°:\n",
      "   1. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë°±ì—”ë“œ tools.pyë¡œ ì „ë‹¬\n",
      "   2. chunked_texts() í•¨ìˆ˜ë¡œ ì²­í‚¹ ì²˜ë¦¬\n",
      "   3. ì„ë² ë”© ìƒì„± ë° ë²¡í„°ìŠ¤í† ì–´ ì €ì¥\n",
      "   ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\n",
      "   ğŸ“Š OCR ìƒì„¸ ê²°ê³¼: /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts (OCR ì‚¬ìš© ì‹œ)\n",
      "\n",
      "ğŸ”— ë‹¤ìŒ ë‹¨ê³„ ì—°ê²°:\n",
      "   1. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë°±ì—”ë“œ tools.pyë¡œ ì „ë‹¬\n",
      "   2. chunked_texts() í•¨ìˆ˜ë¡œ ì²­í‚¹ ì²˜ë¦¬\n",
      "   3. ì„ë² ë”© ìƒì„± ë° ë²¡í„°ìŠ¤í† ì–´ ì €ì¥\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‹¤í–‰ ì˜ˆì‹œ\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _safe_get_file_type(p: Path) -> str:\n",
    "    try:\n",
    "        if 'config' in globals() and hasattr(config, 'get_file_type'):\n",
    "            return config.get_file_type(p)  # type: ignore\n",
    "    except Exception:\n",
    "        pass\n",
    "    return (p.suffix.lstrip('.').lower() or 'unknown')\n",
    "\n",
    "\n",
    "def _resolve_sample_files(max_count: int = 10):\n",
    "    # 1) ì´ë¯¸ ìˆ˜ì§‘ëœ available_files ë³€ìˆ˜ê°€ ìˆìœ¼ë©´ ì‚¬ìš©\n",
    "    try:\n",
    "        if 'available_files' in globals() and available_files:\n",
    "            return [Path(f) for f in available_files[:max_count]]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # 2) document_loaderê°€ ìˆë‹¤ë©´ ê·¸ ê²°ê³¼ë¥¼ í™œìš©\n",
    "    try:\n",
    "        if 'document_loader' in globals() and document_loader:\n",
    "            files = []\n",
    "            # document_loaderê°€ ìŠ¤ìº” ê²°ê³¼ë¥¼ ê°€ì§€ê³  ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœëŒ€í•œ í™œìš©\n",
    "            if hasattr(document_loader, 'files_by_type') and document_loader.files_by_type:\n",
    "                for lst in document_loader.files_by_type.values():\n",
    "                    files.extend(lst)\n",
    "            elif hasattr(document_loader, 'available_files') and document_loader.available_files:\n",
    "                files.extend(document_loader.available_files)\n",
    "            files = [Path(f) for f in files if f]\n",
    "            if files:\n",
    "                return files[:max_count]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # 3) ì…ë ¥ ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ìŠ¤ìº”\n",
    "    try:\n",
    "        input_dir = None\n",
    "        try:\n",
    "            input_dir = Path(getattr(config, 'input_docs_dir', None) or \"/home/admin/wkms-aws/jupyter_notebook/data/input_docs\")\n",
    "        except Exception:\n",
    "            input_dir = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/input_docs\")\n",
    "        patterns = ['*.pdf','*.docx','*.pptx','*.txt','*.html','*.xlsx','*.csv']\n",
    "        files = []\n",
    "        if input_dir.exists():\n",
    "            for pat in patterns:\n",
    "                files.extend(input_dir.glob(pat))\n",
    "        return files[:max_count] if files else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ìƒ˜í”Œ íŒŒì¼ ìˆ˜ì§‘\n",
    "sample_files = _resolve_sample_files(max_count=10)\n",
    "\n",
    "\n",
    "print(\"ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ íŒŒì¼:\", flush=True)\n",
    "if not sample_files:\n",
    "    print(\"   âš ï¸ ìƒ˜í”Œ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. input_docs ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.\", flush=True)\n",
    "else:\n",
    "    for i, file_path in enumerate(sample_files, 1):\n",
    "        file_info = f\"{i}. {Path(file_path).name} ({_safe_get_file_type(Path(file_path))})\"\n",
    "        print(f\"   {file_info}\", flush=True)\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ’¡ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°©ë²•:\", flush=True)\n",
    "print(\"# íŠ¹ì • íŒŒì¼ í…ŒìŠ¤íŠ¸\", flush=True)\n",
    "print(\"test_result = test_file_extraction(sample_files[0])  # ì²« ë²ˆì§¸ íŒŒì¼\", flush=True)\n",
    "print(\"\")\n",
    "print(\"# OCR ê°•ì œ ì‚¬ìš© (ì´ë¯¸ì§€ ê¸°ë°˜ PDFìš©)\", flush=True)\n",
    "print(\"test_result = test_file_extraction(sample_files[0], method='ocr')\", flush=True)\n",
    "print(\"\")\n",
    "print(\"# ëª¨ë“  íŒŒì¼ í…ŒìŠ¤íŠ¸\", flush=True)\n",
    "print(\"for file_path in sample_files[:3]:  # ì²˜ìŒ 3ê°œ íŒŒì¼ë§Œ\", flush=True)\n",
    "print(\"    test_file_extraction(file_path)\", flush=True)\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ:\", flush=True)\n",
    "try:\n",
    "    out_dir = _get_output_dir()\n",
    "    print(f\"   ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {out_dir}\", flush=True)\n",
    "    print(f\"   ğŸ“Š OCR ìƒì„¸ ê²°ê³¼: {out_dir} (OCR ì‚¬ìš© ì‹œ)\", flush=True)\n",
    "except Exception:\n",
    "    try:\n",
    "        print(f\"   ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {config.extracted_text_dir}\", flush=True)\n",
    "    except Exception:\n",
    "        print(f\"   ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\", flush=True)\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ”— ë‹¤ìŒ ë‹¨ê³„ ì—°ê²°:\", flush=True)\n",
    "print(\"   1. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë°±ì—”ë“œ tools.pyë¡œ ì „ë‹¬\", flush=True)\n",
    "print(\"   2. chunked_texts() í•¨ìˆ˜ë¡œ ì²­í‚¹ ì²˜ë¦¬\", flush=True)\n",
    "print(\"   3. ì„ë² ë”© ìƒì„± ë° ë²¡í„°ìŠ¤í† ì–´ ì €ì¥\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf321b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” test.pdf íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘\n",
      "================================================================================\n",
      "âœ… íŒŒì¼ í™•ì¸ë¨: test.pdf\n",
      "ğŸ“Š íŒŒì¼ í¬ê¸°: 11.85 MB\n",
      "\n",
      "ğŸš€ 1ë‹¨ê³„: ìë™ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
      "ğŸ” í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸: test.pdf\n",
      "============================================================\n",
      "ğŸ“„ 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”©\n",
      "âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: 'DocumentTestLoader' object has no attribute 'load_document'\n",
      "\n",
      "ğŸ’¾ ì €ì¥ëœ ê²°ê³¼ íŒŒì¼:\n",
      "\n",
      "ğŸ”— ë‹¤ìŒ ë‹¨ê³„:\n",
      "   1. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë°±ì—”ë“œ tools.pyë¡œ ì „ë‹¬\n",
      "   2. chunked_texts() í•¨ìˆ˜ë¡œ ì²­í‚¹ ì²˜ë¦¬\n",
      "   3. Azure OpenAIë¡œ ì„ë² ë”© ìƒì„±\n",
      "   4. ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\n",
      "================================================================================\n",
      "âœ… íŒŒì¼ í™•ì¸ë¨: test.pdf\n",
      "ğŸ“Š íŒŒì¼ í¬ê¸°: 11.85 MB\n",
      "\n",
      "ğŸš€ 1ë‹¨ê³„: ìë™ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
      "ğŸ” í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸: test.pdf\n",
      "============================================================\n",
      "ğŸ“„ 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”©\n",
      "âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: 'DocumentTestLoader' object has no attribute 'load_document'\n",
      "\n",
      "ğŸ’¾ ì €ì¥ëœ ê²°ê³¼ íŒŒì¼:\n",
      "\n",
      "ğŸ”— ë‹¤ìŒ ë‹¨ê³„:\n",
      "   1. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë°±ì—”ë“œ tools.pyë¡œ ì „ë‹¬\n",
      "   2. chunked_texts() í•¨ìˆ˜ë¡œ ì²­í‚¹ ì²˜ë¦¬\n",
      "   3. Azure OpenAIë¡œ ì„ë² ë”© ìƒì„±\n",
      "   4. ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª test.pdf íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "def _ensure_path(p: str | Path) -> Path:\n",
    "    return p if isinstance(p, Path) else Path(p)\n",
    "\n",
    "\n",
    "test_pdf_path = _ensure_path(\"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\")\n",
    "\n",
    "\n",
    "print(\"ğŸ” test.pdf íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘\", flush=True)\n",
    "print(\"=\" * 80, flush=True)\n",
    "\n",
    "\n",
    "# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "if not test_pdf_path.exists():\n",
    "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_pdf_path}\", flush=True)\n",
    "    print(f\"ğŸ“‚ í˜„ì¬ input_docs ë””ë ‰í† ë¦¬ ë‚´ìš©:\", flush=True)\n",
    "    input_docs_dir = test_pdf_path.parent\n",
    "    if input_docs_dir.exists():\n",
    "        for file in input_docs_dir.iterdir():\n",
    "            print(f\"   ğŸ“„ {file.name}\", flush=True)\n",
    "    else:\n",
    "        print(f\"   ğŸ“‚ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_docs_dir}\", flush=True)\n",
    "else:\n",
    "    print(f\"âœ… íŒŒì¼ í™•ì¸ë¨: {test_pdf_path.name}\", flush=True)\n",
    "    try:\n",
    "        size_mb = test_pdf_path.stat().st_size / 1024 / 1024\n",
    "        print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {size_mb:.2f} MB\", flush=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # 1ë‹¨ê³„: ìë™ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"\\nğŸš€ 1ë‹¨ê³„: ìë™ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\", flush=True)\n",
    "    auto_result = test_file_extraction(str(test_pdf_path), method='auto')\n",
    "    \n",
    "    if auto_result and auto_result.get('success'):\n",
    "        print(f\"\\nğŸ“Š ìë™ ì¶”ì¶œ ê²°ê³¼:\", flush=True)\n",
    "        print(f\"   ğŸ“„ ì¶”ì¶œ ë°©ë²•: {auto_result.get('extraction_method')}\", flush=True)\n",
    "        print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {auto_result.get('text_length', 0):,}ì\", flush=True)\n",
    "        print(f\"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {auto_result.get('processing_time', 0.0):.2f}ì´ˆ\", flush=True)\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "        if auto_result.get('text_length', 0) < 100:\n",
    "            print(f\"\\nâš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ê¸°ë°˜ PDFì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.\", flush=True)\n",
    "            print(f\"ğŸ”„ 2ë‹¨ê³„: OCR ë°©ë²•ìœ¼ë¡œ ì¬ì‹œë„\", flush=True)\n",
    "            \n",
    "            ocr_result = test_file_extraction(str(test_pdf_path), method='ocr')\n",
    "            \n",
    "            if ocr_result and ocr_result.get('success'):\n",
    "                print(f\"\\nğŸ“Š OCR ì¶”ì¶œ ê²°ê³¼:\", flush=True)\n",
    "                print(f\"   ğŸ“„ ì¶”ì¶œ ë°©ë²•: {ocr_result.get('extraction_method')}\", flush=True)\n",
    "                print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {ocr_result.get('text_length', 0):,}ì\", flush=True)\n",
    "                print(f\"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {ocr_result.get('processing_time', 0.0):.2f}ì´ˆ\", flush=True)\n",
    "                \n",
    "                # ê²°ê³¼ ë¹„êµ\n",
    "                auto_len = auto_result.get('text_length', 0)\n",
    "                ocr_len = ocr_result.get('text_length', 0)\n",
    "                improve = ((ocr_len - auto_len) / max(auto_len, 1) * 100)\n",
    "                print(f\"\\nğŸ“ˆ ê²°ê³¼ ë¹„êµ:\", flush=True)\n",
    "                print(f\"   ìë™ ë°©ë²•: {auto_len:,}ì\", flush=True)\n",
    "                print(f\"   OCR ë°©ë²•: {ocr_len:,}ì\", flush=True)\n",
    "                print(f\"   ê°œì„ ìœ¨: {improve:+.1f}%\", flush=True)\n",
    "        else:\n",
    "            print(f\"\\nâœ… ìë™ ë°©ë²•ìœ¼ë¡œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!\", flush=True)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ ì €ì¥ëœ ê²°ê³¼ íŒŒì¼:\", flush=True)\n",
    "    try:\n",
    "        out_dir = _get_output_dir()\n",
    "    except Exception:\n",
    "        try:\n",
    "            out_dir = Path(config.extracted_text_dir)\n",
    "        except Exception:\n",
    "            out_dir = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\")\n",
    "    try:\n",
    "        extracted_files = list(out_dir.glob(\"test*\"))\n",
    "        for file in sorted(extracted_files):\n",
    "            print(f\"   ğŸ“„ {file.name}\", flush=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    print(f\"\\nğŸ”— ë‹¤ìŒ ë‹¨ê³„:\", flush=True)\n",
    "    print(f\"   1. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë°±ì—”ë“œ tools.pyë¡œ ì „ë‹¬\", flush=True)\n",
    "    print(f\"   2. chunked_texts() í•¨ìˆ˜ë¡œ ì²­í‚¹ ì²˜ë¦¬\", flush=True)\n",
    "    print(f\"   3. Azure OpenAIë¡œ ì„ë² ë”© ìƒì„±\", flush=True)\n",
    "    print(f\"   4. ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ test.pdf ë¬¸ì œ ì§„ë‹¨ ë° ì§ì ‘ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def _lazy_import_pypdf2():\n",
    "    try:\n",
    "        import PyPDF2\n",
    "        return PyPDF2\n",
    "    except Exception as e:\n",
    "        print(f\"      âš ï¸ PyPDF2 import ì‹¤íŒ¨: {e}\", flush=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def _lazy_import_pdfplumber():\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        return pdfplumber\n",
    "    except Exception as e:\n",
    "        print(f\"      âš ï¸ pdfplumber import ì‹¤íŒ¨: {e}\", flush=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "test_pdf_path = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\")\n",
    "\n",
    "\n",
    "print(\"ğŸ”§ test.pdf ë¬¸ì œ ì§„ë‹¨\", flush=True)\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "\n",
    "# 1ë‹¨ê³„: íŒŒì¼ ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "print(\"ğŸ“‚ 1ë‹¨ê³„: íŒŒì¼ ê¸°ë³¸ ì •ë³´\", flush=True)\n",
    "if test_pdf_path.exists():\n",
    "    try:\n",
    "        stat = test_pdf_path.stat()\n",
    "        print(f\"   âœ… íŒŒì¼ ì¡´ì¬: {test_pdf_path.name}\", flush=True)\n",
    "        print(f\"   ğŸ“Š í¬ê¸°: {stat.st_size / 1024 / 1024:.2f} MB\", flush=True)\n",
    "        print(f\"   ğŸ•’ ìˆ˜ì •ì¼: {datetime.fromtimestamp(stat.st_mtime)}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}\", flush=True)\n",
    "    \n",
    "    # 2ë‹¨ê³„: DocumentLoader ì§ì ‘ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"\\nğŸ“„ 2ë‹¨ê³„: DocumentLoader ì§ì ‘ í…ŒìŠ¤íŠ¸\", flush=True)\n",
    "    try:\n",
    "        if 'document_loader' in globals() and document_loader:\n",
    "            doc_info = document_loader.load_document(str(test_pdf_path))  # type: ignore\n",
    "            print(f\"   ğŸ“‹ ë°˜í™˜ëœ í‚¤ë“¤: {list(doc_info.keys())}\", flush=True)\n",
    "            \n",
    "            if 'success' in doc_info:\n",
    "                if doc_info['success']:\n",
    "                    print(f\"   âœ… ë¡œë”© ì„±ê³µ\", flush=True)\n",
    "                    print(f\"   ğŸ“„ íŒŒì¼ íƒ€ì…: {doc_info.get('file_type', 'N/A')}\", flush=True)\n",
    "                    print(f\"   ğŸ“Š íŒŒì¼ í¬ê¸°: {doc_info.get('file_size_mb', 'N/A')} MB\", flush=True)\n",
    "                    print(f\"   ğŸ“ ë‚´ìš© ê¸¸ì´: {len(doc_info.get('file_content', ''))}\", flush=True)\n",
    "                else:\n",
    "                    print(f\"   âŒ ë¡œë”© ì‹¤íŒ¨: {doc_info.get('error', 'Unknown error')}\", flush=True)\n",
    "            else:\n",
    "                print(f\"   âš ï¸ 'success' í‚¤ê°€ ì—†ìŒ - ë°˜í™˜ êµ¬ì¡° ë¬¸ì œ\", flush=True)\n",
    "        else:\n",
    "            print(f\"   âš ï¸ document_loader ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ\", flush=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ DocumentLoader ì˜¤ë¥˜: {e}\", flush=True)\n",
    "        print(f\"   ğŸ” ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}\", flush=True)\n",
    "    \n",
    "    # 3ë‹¨ê³„: TextExtractor ì§ì ‘ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"\\nğŸ”¤ 3ë‹¨ê³„: TextExtractor ì§ì ‘ í…ŒìŠ¤íŠ¸\", flush=True)\n",
    "    try:\n",
    "        if 'text_extractor' not in globals() or not text_extractor:\n",
    "            # fallback ìƒì„±\n",
    "            try:\n",
    "                extractor = DocumentTextExtractor(config)  # type: ignore\n",
    "            except NameError:\n",
    "                extractor = TextExtractor(config)  # type: ignore\n",
    "        else:\n",
    "            extractor = text_extractor\n",
    "        \n",
    "        # ê° ë°©ë²•ë³„ë¡œ ê°œë³„ í…ŒìŠ¤íŠ¸\n",
    "        methods = ['pypdf2', 'pdfplumber']\n",
    "        \n",
    "        for method in methods:\n",
    "            print(f\"\\n   ğŸ”„ {method} ë°©ë²• í…ŒìŠ¤íŠ¸:\", flush=True)\n",
    "            try:\n",
    "                result = extractor.extract_text(str(test_pdf_path), method=method)\n",
    "                \n",
    "                if result.get('success', False):\n",
    "                    text_len = len(result.get('text', ''))\n",
    "                    print(f\"      âœ… ì„±ê³µ: {text_len:,}ì\", flush=True)\n",
    "                    if text_len > 0:\n",
    "                        preview = result['text'][:100].replace('\\n', ' ')\n",
    "                        print(f\"      ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: {preview}...\", flush=True)\n",
    "                else:\n",
    "                    print(f\"      âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}\", flush=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"      âŒ ì˜¤ë¥˜: {e}\", flush=True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TextExtractor ì „ì²´ ì˜¤ë¥˜: {e}\", flush=True)\n",
    "        \n",
    "    # 4ë‹¨ê³„: PDF íŒŒì¼ ì§ì ‘ ì½ê¸° í…ŒìŠ¤íŠ¸\n",
    "    print(f\"\\nğŸ“– 4ë‹¨ê³„: PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ í…ŒìŠ¤íŠ¸\", flush=True)\n",
    "    \n",
    "    # PyPDF2 í…ŒìŠ¤íŠ¸\n",
    "    try:\n",
    "        print(f\"   ğŸ“š PyPDF2 í…ŒìŠ¤íŠ¸:\", flush=True)\n",
    "        PyPDF2 = _lazy_import_pypdf2()\n",
    "        if PyPDF2 is None:\n",
    "            raise RuntimeError(\"PyPDF2ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        with open(test_pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            page_count = len(pdf_reader.pages)\n",
    "            print(f\"      ğŸ“„ í˜ì´ì§€ ìˆ˜: {page_count}\", flush=True)\n",
    "            \n",
    "            if page_count > 0:\n",
    "                try:\n",
    "                    first_page_text = pdf_reader.pages[0].extract_text() or \"\"\n",
    "                except Exception:\n",
    "                    first_page_text = \"\"\n",
    "                print(f\"      ğŸ“ ì²« í˜ì´ì§€ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(first_page_text)}\", flush=True)\n",
    "                if len(first_page_text) > 0:\n",
    "                    preview = first_page_text[:100].replace('\\n', ' ')\n",
    "                    print(f\"      ğŸ‘€ ì²« í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸°: {preview}...\", flush=True)\n",
    "                else:\n",
    "                    print(f\"      âš ï¸ ì²« í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ PyPDF2 ì˜¤ë¥˜: {e}\", flush=True)\n",
    "    \n",
    "    # pdfplumber í…ŒìŠ¤íŠ¸\n",
    "    try:\n",
    "        print(f\"\\n   ğŸ“Š pdfplumber í…ŒìŠ¤íŠ¸:\", flush=True)\n",
    "        pdfplumber = _lazy_import_pdfplumber()\n",
    "        if pdfplumber is None:\n",
    "            raise RuntimeError(\"pdfplumberë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        with pdfplumber.open(test_pdf_path) as pdf:\n",
    "            page_count = len(pdf.pages)\n",
    "            print(f\"      ğŸ“„ í˜ì´ì§€ ìˆ˜: {page_count}\", flush=True)\n",
    "            \n",
    "            if page_count > 0:\n",
    "                first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "                print(f\"      ğŸ“ ì²« í˜ì´ì§€ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(first_page_text)}\", flush=True)\n",
    "                if len(first_page_text) > 0:\n",
    "                    preview = first_page_text[:100].replace('\\n', ' ')\n",
    "                    print(f\"      ğŸ‘€ ì²« í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸°: {preview}...\", flush=True)\n",
    "                else:\n",
    "                    print(f\"      âš ï¸ ì²« í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ (ì´ë¯¸ì§€ ê¸°ë°˜ PDFì¼ ê°€ëŠ¥ì„±)\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ pdfplumber ì˜¤ë¥˜: {e}\", flush=True)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {test_pdf_path}\", flush=True)\n",
    "    # ëŒ€ì•ˆ íŒŒì¼ ê²½ë¡œ í™•ì¸\n",
    "    possible_paths = [\n",
    "        Path(\"/home/admin/wkms-aws/jupyter_notebook/data/input_docs\"),\n",
    "        Path(\"/home/admin/wkms-aws/data/input_docs\"),\n",
    "        Path(\"/home/admin/wkms-aws\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ” ëŒ€ì•ˆ ê²½ë¡œ íƒìƒ‰:\", flush=True)\n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if path.exists():\n",
    "                print(f\"   ğŸ“‚ {path}:\", flush=True)\n",
    "                pdf_files = list(path.glob(\"*.pdf\"))\n",
    "                if pdf_files:\n",
    "                    for pdf_file in pdf_files[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ\n",
    "                        print(f\"      ğŸ“„ {pdf_file.name}\", flush=True)\n",
    "                else:\n",
    "                    print(f\"      ğŸ“„ PDF íŒŒì¼ ì—†ìŒ\", flush=True)\n",
    "            else:\n",
    "                print(f\"   âŒ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {path}\", flush=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ¯ ì§„ë‹¨ ì™„ë£Œ!\", flush=True)\n",
    "print(f\"ğŸ’¡ ìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê² ìŠµë‹ˆë‹¤.\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d194bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§© ë°°ì¹˜ í…ŒìŠ¤íŠ¸: ë‹¤ì–‘í•œ ìƒ˜í”Œ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "def batch_test(files: list[Path] | None = None, limit: int = 5, method: str = 'auto'):\n",
    "\n",
    "    print(\"\ude80 ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘\", flush=True)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    if files is None:\n",
    "\n",
    "        try:\n",
    "\n",
    "            files = _resolve_sample_files(max_count=limit)\n",
    "\n",
    "        except Exception:\n",
    "\n",
    "            files = []\n",
    "\n",
    "    files = [Path(f) for f in files][:limit]\n",
    "\n",
    "    if not files:\n",
    "\n",
    "        print(\"âš ï¸ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\", flush=True)\n",
    "\n",
    "        return {\"success\": False, \"count\": 0, \"results\": []}\n",
    "\n",
    "\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, f in enumerate(files, 1):\n",
    "\n",
    "        print(f\"\\n[{i}/{len(files)}] {f.name} ì²˜ë¦¬\", flush=True)\n",
    "\n",
    "        try:\n",
    "\n",
    "            r = test_file_extraction(str(f), method=method)\n",
    "\n",
    "            results.append(r)\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\", flush=True)\n",
    "\n",
    "            results.append({\"success\": False, \"error\": str(e), \"file\": str(f)})\n",
    "\n",
    "\n",
    "\n",
    "    ok = sum(1 for r in results if r and r.get('success'))\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(f\"\\nâœ… ë°°ì¹˜ ì™„ë£Œ: {ok}/{len(results)} ì„±ê³µ, {elapsed:.2f}s\", flush=True)\n",
    "\n",
    "    return {\"success\": True, \"count\": len(results), \"ok\": ok, \"elapsed\": elapsed, \"results\": results}\n",
    "\n",
    "\n",
    "\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•: batch_test(limit=3, method='auto')\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a84ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ì½”ë“œ ì ê²€ ë° ì£¼ìš” ë¬¸ì œ í•´ê²°\n",
    "\n",
    "print(\"ğŸ” ë…¸íŠ¸ë¶ ì½”ë“œ ì ê²€ ë° ì£¼ìš” ë¬¸ì œ ì§„ë‹¨\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. í•„ìˆ˜ ë³€ìˆ˜ë“¤ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "print(\"ğŸ“‹ 1ë‹¨ê³„: í•µì‹¬ ê°ì²´ ìƒíƒœ í™•ì¸\")\n",
    "\n",
    "# config ê°ì²´ í™•ì¸\n",
    "try:\n",
    "    print(f\"   âœ… config: {type(config).__name__}\")\n",
    "    print(f\"      ğŸ“‚ extracted_text_dir: {config.extracted_text_dir}\")\n",
    "    print(f\"      ğŸ”§ OCR í™œì„±í™”: {config.processing_config.get('ocr_enabled', False)}\")\n",
    "except NameError:\n",
    "    print(f\"   âŒ config ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# document_loader í™•ì¸\n",
    "try:\n",
    "    print(f\"   âœ… document_loader: {type(document_loader).__name__}\")\n",
    "except NameError:\n",
    "    print(f\"   âŒ document_loader ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# text_extractor í™•ì¸\n",
    "try:\n",
    "    print(f\"   âœ… text_extractor: {type(text_extractor).__name__}\")\n",
    "except NameError:\n",
    "    print(f\"   âŒ text_extractor ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# 2. ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ìƒíƒœ í™•ì¸\n",
    "print(f\"\\nğŸ“š 2ë‹¨ê³„: í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í™•ì¸\")\n",
    "\n",
    "libraries_to_check = [\n",
    "    ('PyPDF2', 'PyPDF2'),\n",
    "    ('pdfplumber', 'pdfplumber'), \n",
    "    ('docx', 'python-docx'),\n",
    "    ('pptx', 'python-pptx'),\n",
    "    ('pytesseract', 'pytesseract'),\n",
    "    ('PIL', 'Pillow'),\n",
    "    ('pdf2image', 'pdf2image'),\n",
    "    ('pandas', 'pandas')\n",
    "]\n",
    "\n",
    "missing_libraries = []\n",
    "for lib_name, package_name in libraries_to_check:\n",
    "    try:\n",
    "        __import__(lib_name)\n",
    "        print(f\"   âœ… {lib_name}\")\n",
    "    except ImportError:\n",
    "        print(f\"   âŒ {lib_name} (íŒ¨í‚¤ì§€: {package_name})\")\n",
    "        missing_libraries.append(package_name)\n",
    "\n",
    "if missing_libraries:\n",
    "    print(f\"\\nâš ï¸ ëˆ„ë½ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬: {missing_libraries}\")\n",
    "    print(f\"ğŸ’¡ ì„¤ì¹˜ ëª…ë ¹: pip install {' '.join(missing_libraries)}\")\n",
    "\n",
    "# 3. test_file_extraction í•¨ìˆ˜ ë¬¸ì œ ì ê²€\n",
    "print(f\"\\nğŸ”§ 3ë‹¨ê³„: test_file_extraction í•¨ìˆ˜ ë¬¸ì œ ì ê²€\")\n",
    "\n",
    "# í•¨ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "try:\n",
    "    test_file_extraction\n",
    "    print(f\"   âœ… test_file_extraction í•¨ìˆ˜ê°€ ì •ì˜ë¨\")\n",
    "    \n",
    "    # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜ë“¤ í™•ì¸\n",
    "    print(f\"   ğŸ“‹ í•¨ìˆ˜ ë‚´ë¶€ ì¢…ì†ì„± í™•ì¸:\")\n",
    "    \n",
    "    # document_loader.load_document ë©”ì†Œë“œ í™•ì¸\n",
    "    try:\n",
    "        if hasattr(document_loader, 'load_document'):\n",
    "            print(f\"      âœ… document_loader.load_document ë©”ì†Œë“œ ì¡´ì¬\")\n",
    "        else:\n",
    "            print(f\"      âŒ document_loader.load_document ë©”ì†Œë“œ ì—†ìŒ\")\n",
    "    except:\n",
    "        print(f\"      âŒ document_loader ì ‘ê·¼ ë¶ˆê°€\")\n",
    "    \n",
    "    # text_extractor.extract_text ë©”ì†Œë“œ í™•ì¸\n",
    "    try:\n",
    "        if hasattr(text_extractor, 'extract_text'):\n",
    "            print(f\"      âœ… text_extractor.extract_text ë©”ì†Œë“œ ì¡´ì¬\")\n",
    "        else:\n",
    "            print(f\"      âŒ text_extractor.extract_text ë©”ì†Œë“œ ì—†ìŒ\")\n",
    "    except:\n",
    "        print(f\"      âŒ text_extractor ì ‘ê·¼ ë¶ˆê°€\")\n",
    "        \n",
    "    # save_extracted_text í•¨ìˆ˜ í™•ì¸\n",
    "    try:\n",
    "        save_extracted_text\n",
    "        print(f\"      âœ… save_extracted_text í•¨ìˆ˜ ì •ì˜ë¨\")\n",
    "    except NameError:\n",
    "        print(f\"      âŒ save_extracted_text í•¨ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•ŠìŒ\")\n",
    "        \n",
    "except NameError:\n",
    "    print(f\"   âŒ test_file_extraction í•¨ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•ŠìŒ\")\n",
    "\n",
    "# 4. ê°„ë‹¨í•œ PDF ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\nğŸ§ª 4ë‹¨ê³„: ê¸°ë³¸ PDF ì²˜ë¦¬ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ PDF íŒŒì¼ ìƒì„± ë° í…ŒìŠ¤íŠ¸\n",
    "try:\n",
    "    # ë¨¼ì € test.pdfê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "    test_pdf = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\")\n",
    "    \n",
    "    if test_pdf.exists():\n",
    "        print(f\"   ğŸ“„ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {test_pdf.name} ({test_pdf.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "        \n",
    "        # PyPDF2ë¡œ ì§ì ‘ í…ŒìŠ¤íŠ¸\n",
    "        try:\n",
    "            import PyPDF2\n",
    "            with open(test_pdf, 'rb') as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                print(f\"      ğŸ“š PyPDF2: {len(reader.pages)}í˜ì´ì§€\")\n",
    "                \n",
    "                if len(reader.pages) > 0:\n",
    "                    first_page_text = reader.pages[0].extract_text()\n",
    "                    print(f\"      ğŸ“ ì²« í˜ì´ì§€ í…ìŠ¤íŠ¸: {len(first_page_text)}ì\")\n",
    "                    \n",
    "                    if len(first_page_text.strip()) == 0:\n",
    "                        print(f\"      âš ï¸ ì´ë¯¸ì§€ ê¸°ë°˜ PDF - OCR í•„ìš”\")\n",
    "                    else:\n",
    "                        preview = first_page_text.strip()[:100].replace('\\n', ' ')\n",
    "                        print(f\"      ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: {preview}...\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ PyPDF2 ì§ì ‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"   âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ: {test_pdf}\")\n",
    "        \n",
    "        # ëŒ€ì•ˆ íŒŒì¼ ì°¾ê¸°\n",
    "        input_dir = test_pdf.parent\n",
    "        if input_dir.exists():\n",
    "            pdf_files = list(input_dir.glob(\"*.pdf\"))\n",
    "            if pdf_files:\n",
    "                print(f\"   ğŸ“‚ ì‚¬ìš© ê°€ëŠ¥í•œ PDF íŒŒì¼ë“¤:\")\n",
    "                for pdf_file in pdf_files[:3]:\n",
    "                    print(f\"      ğŸ“„ {pdf_file.name}\")\n",
    "            else:\n",
    "                print(f\"   ğŸ“‚ PDF íŒŒì¼ì´ ì—†ìŒ: {input_dir}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ PDF í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ì ê²€ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“‹ ë‹¤ìŒ ì…€ì—ì„œ ë°œê²¬ëœ ë¬¸ì œë“¤ì„ ìˆ˜ì •í•˜ê² ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ› ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¬¸ì œ ìˆ˜ì • ë° ê°•í™”\n",
    "\n",
    "print(\"ğŸ› ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¬¸ì œ ìˆ˜ì • ì¤‘...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ì¬ì •ì˜\n",
    "def safe_extract_text_from_pdf(file_path, method='auto'):\n",
    "    \"\"\"ì•ˆì „í•œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f'íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}',\n",
    "            'text': '',\n",
    "            'method': method\n",
    "        }\n",
    "    \n",
    "    print(f\"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ: {file_path.name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ë°©ë²•ë³„ ì¶”ì¶œ ì‹œë„\n",
    "    if method == 'auto' or method == 'pypdf2':\n",
    "        try:\n",
    "            print(f\"   ğŸ”„ PyPDF2 ë°©ë²• ì‹œë„...\")\n",
    "            import PyPDF2\n",
    "            \n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                page_count = len(pdf_reader.pages)\n",
    "                \n",
    "                text_parts = []\n",
    "                for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                    try:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text.strip():\n",
    "                            text_parts.append(f\"\\\\n\\\\n=== í˜ì´ì§€ {page_num} ===\\\\n{page_text}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      âš ï¸ í˜ì´ì§€ {page_num} ê±´ë„ˆëœ€: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                full_text = '\\\\n'.join(text_parts)\n",
    "                processing_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"   âœ… PyPDF2 ì„±ê³µ: {len(full_text):,}ì, {processing_time:.2f}ì´ˆ\")\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'text': full_text,\n",
    "                    'method': 'pypdf2',\n",
    "                    'pages': page_count,\n",
    "                    'processing_time': processing_time,\n",
    "                    'text_length': len(full_text)\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ PyPDF2 ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "    if method == 'auto' or method == 'pdfplumber':\n",
    "        try:\n",
    "            print(f\"   ğŸ”„ pdfplumber ë°©ë²• ì‹œë„...\")\n",
    "            import pdfplumber\n",
    "            \n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                page_count = len(pdf.pages)\n",
    "                \n",
    "                text_parts = []\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    try:\n",
    "                        page_text = page.extract_text() or \"\"\n",
    "                        if page_text.strip():\n",
    "                            text_parts.append(f\"\\\\n\\\\n=== í˜ì´ì§€ {page_num} ===\\\\n{page_text}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      âš ï¸ í˜ì´ì§€ {page_num} ê±´ë„ˆëœ€: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                full_text = '\\\\n'.join(text_parts)\n",
    "                processing_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"   âœ… pdfplumber ì„±ê³µ: {len(full_text):,}ì, {processing_time:.2f}ì´ˆ\")\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'text': full_text,\n",
    "                    'method': 'pdfplumber',\n",
    "                    'pages': page_count,\n",
    "                    'processing_time': processing_time,\n",
    "                    'text_length': len(full_text)\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ pdfplumber ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    if method == 'ocr':\n",
    "        try:\n",
    "            print(f\"   ğŸ”„ OCR ë°©ë²• ì‹œë„...\")\n",
    "            from pdf2image import convert_from_path\n",
    "            import pytesseract\n",
    "            \n",
    "            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜\n",
    "            images = convert_from_path(file_path, dpi=200)  # ë‚®ì€ DPIë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n",
    "            \n",
    "            text_parts = []\n",
    "            for page_num, image in enumerate(images, 1):\n",
    "                try:\n",
    "                    page_text = pytesseract.image_to_string(image, lang='kor+eng')\n",
    "                    if page_text.strip():\n",
    "                        text_parts.append(f\"\\\\n\\\\n=== í˜ì´ì§€ {page_num} (OCR) ===\\\\n{page_text}\")\n",
    "                        print(f\"      ğŸ“„ í˜ì´ì§€ {page_num}: {len(page_text):,}ì\")\n",
    "                except Exception as e:\n",
    "                    print(f\"      âš ï¸ í˜ì´ì§€ {page_num} OCR ì‹¤íŒ¨: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            full_text = '\\\\n'.join(text_parts)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"   âœ… OCR ì„±ê³µ: {len(full_text):,}ì, {processing_time:.2f}ì´ˆ\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'text': full_text,\n",
    "                'method': 'ocr',\n",
    "                'pages': len(images),\n",
    "                'processing_time': processing_time,\n",
    "                'text_length': len(full_text)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ OCR ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨\n",
    "    processing_time = time.time() - start_time\n",
    "    return {\n",
    "        'success': False,\n",
    "        'error': 'ëª¨ë“  ì¶”ì¶œ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤',\n",
    "        'text': '',\n",
    "        'method': method,\n",
    "        'processing_time': processing_time\n",
    "    }\n",
    "\n",
    "# 2. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì €ì¥ í•¨ìˆ˜\n",
    "def simple_save_text(text, filename_prefix, method='unknown'):\n",
    "    \"\"\"ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì €ì¥ í•¨ìˆ˜\"\"\"\n",
    "    try:\n",
    "        # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„±\n",
    "        if 'config' in globals() and hasattr(config, 'extracted_text_dir'):\n",
    "            output_dir = config.extracted_text_dir\n",
    "        else:\n",
    "            output_dir = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts\")\n",
    "        \n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        output_file = output_dir / f\"{filename_prefix}_{method}_extracted.txt\"\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"=== í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ ===\\\\n\")\n",
    "            f.write(f\"íŒŒì¼: {filename_prefix}\\\\n\")\n",
    "            f.write(f\"ë°©ë²•: {method}\\\\n\")\n",
    "            f.write(f\"ì¶”ì¶œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "            f.write(f\"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text):,}ì\\\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\\\n\\\\n\")\n",
    "            f.write(text)\n",
    "        \n",
    "        print(f\"   ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_file.name}\")\n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… ìˆ˜ì • í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"ğŸ’¡ ë‹¤ìŒ ì…€ì—ì„œ test.pdfë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª ìˆ˜ì •ëœ í•¨ìˆ˜ë¡œ test.pdf ì‹¤ì œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "print(\"ğŸ§ª ìˆ˜ì •ëœ í•¨ìˆ˜ë¡œ test.pdf í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ\n",
    "test_pdf_path = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\")\n",
    "\n",
    "if test_pdf_path.exists():\n",
    "    file_size_mb = test_pdf_path.stat().st_size / 1024 / 1024\n",
    "    print(f\"ğŸ“„ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {test_pdf_path.name}\")\n",
    "    print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: ê° ë°©ë²•ë³„ë¡œ ìˆœì°¨ í…ŒìŠ¤íŠ¸\n",
    "    methods_to_test = [\n",
    "        ('pypdf2', 'PyPDF2 (ê¸°ë³¸ ë¹ ë¥¸ ë°©ë²•)'),\n",
    "        ('pdfplumber', 'pdfplumber (ê³ ê¸‰ ë°©ë²•)'),\n",
    "        ('ocr', 'OCR (ì´ë¯¸ì§€ ê¸°ë°˜ PDFìš©)')\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method, description in methods_to_test:\n",
    "        print(f\"\\\\nğŸ”„ {description} í…ŒìŠ¤íŠ¸\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = safe_extract_text_from_pdf(test_pdf_path, method=method)\n",
    "        results[method] = result\n",
    "        \n",
    "        if result['success']:\n",
    "            # í…ìŠ¤íŠ¸ ì €ì¥\n",
    "            saved_file = simple_save_text(\n",
    "                result['text'], \n",
    "                test_pdf_path.stem, \n",
    "                result['method']\n",
    "            )\n",
    "            result['saved_file'] = saved_file\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°\n",
    "            if len(result['text']) > 0:\n",
    "                preview = result['text'][:200].replace('\\\\n', ' ').strip()\n",
    "                print(f\"   ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: {preview}...\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "        # í° íŒŒì¼ì˜ ê²½ìš° OCRì€ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ë¯€ë¡œ ì¡°ê±´ë¶€ ì‹¤í–‰\n",
    "        if method == 'ocr' and file_size_mb > 10:\n",
    "            user_input = input(f\"\\\\nâš ï¸ íŒŒì¼ì´ {file_size_mb:.1f}MBë¡œ í½ë‹ˆë‹¤. OCRì„ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \")\n",
    "            if user_input.lower() != 'y':\n",
    "                print(f\"   â­ï¸ OCR í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤\")\n",
    "                results[method] = {\n",
    "                    'success': False,\n",
    "                    'error': 'ì‚¬ìš©ìê°€ ê±´ë„ˆëœ€ (íŒŒì¼ í¬ê¸°)',\n",
    "                    'text': '',\n",
    "                    'method': method\n",
    "                }\n",
    "                continue\n",
    "    \n",
    "    # 2ë‹¨ê³„: ê²°ê³¼ ë¹„êµ ë° ìš”ì•½\n",
    "    print(f\"\\\\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    successful_methods = []\n",
    "    \n",
    "    for method, description in methods_to_test:\n",
    "        if method in results:\n",
    "            result = results[method]\n",
    "            \n",
    "            if result['success']:\n",
    "                text_len = result.get('text_length', 0)\n",
    "                proc_time = result.get('processing_time', 0)\n",
    "                \n",
    "                print(f\"âœ… {description}\")\n",
    "                print(f\"   ğŸ“ ì¶”ì¶œ í…ìŠ¤íŠ¸: {text_len:,}ì\")\n",
    "                print(f\"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {proc_time:.2f}ì´ˆ\")\n",
    "                print(f\"   ğŸ“„ í˜ì´ì§€ ìˆ˜: {result.get('pages', 'N/A')}\")\n",
    "                \n",
    "                successful_methods.append((method, text_len, proc_time))\n",
    "                \n",
    "                if 'saved_file' in result and result['saved_file']:\n",
    "                    print(f\"   ğŸ’¾ ì €ì¥ë¨: {result['saved_file'].name}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"âŒ {description}\")\n",
    "                print(f\"   ğŸš« ì˜¤ë¥˜: {result.get('error', 'Unknown error')}\")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    # 3ë‹¨ê³„: ìµœì  ë°©ë²• ì¶”ì²œ\n",
    "    if successful_methods:\n",
    "        print(f\"ğŸ† ìµœì  ë°©ë²• ì¶”ì²œ\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # ê°€ì¥ ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œ ë°©ë²•\n",
    "        best_quality = max(successful_methods, key=lambda x: x[1])\n",
    "        print(f\"ğŸ¯ ìµœê³  í’ˆì§ˆ: {best_quality[0]} ({best_quality[1]:,}ì)\")\n",
    "        \n",
    "        # ê°€ì¥ ë¹ ë¥¸ ë°©ë²•\n",
    "        best_speed = min(successful_methods, key=lambda x: x[2])\n",
    "        print(f\"âš¡ ìµœê³  ì†ë„: {best_speed[0]} ({best_speed[2]:.2f}ì´ˆ)\")\n",
    "        \n",
    "        # ê· í˜•ì¡íŒ ì¶”ì²œ\n",
    "        if best_quality[1] > 1000:  # ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œëœ ê²½ìš°\n",
    "            print(f\"ğŸ’¡ ì¶”ì²œ: {best_quality[0]} (ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)\")\n",
    "        elif any(method[0] == 'ocr' for method in successful_methods):\n",
    "            ocr_result = next(method for method in successful_methods if method[0] == 'ocr')\n",
    "            if ocr_result[1] > best_quality[1] * 2:  # OCRì´ í˜„ì €íˆ ë” ì¢‹ì€ ê²½ìš°\n",
    "                print(f\"ğŸ’¡ ì¶”ì²œ: ocr (ì´ë¯¸ì§€ ê¸°ë°˜ PDF - OCR í•„ìˆ˜)\")\n",
    "            else:\n",
    "                print(f\"ğŸ’¡ ì¶”ì²œ: {best_speed[0]} (ì†ë„ ìš°ì„ )\")\n",
    "        else:\n",
    "            print(f\"ğŸ’¡ ì¶”ì²œ: {best_speed[0]} (ì†ë„ ìš°ì„ )\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âŒ ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"ğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:\")\n",
    "        print(f\"   - íŒŒì¼ì´ ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€\")\n",
    "        print(f\"   - PDF íŒŒì¼ì´ ì•”í˜¸í™”ë˜ì–´ ìˆì§€ ì•Šì€ì§€\") \n",
    "        print(f\"   - í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_pdf_path}\")\n",
    "    \n",
    "    # ëŒ€ì•ˆ íŒŒì¼ ì œì•ˆ\n",
    "    input_dir = test_pdf_path.parent\n",
    "    if input_dir.exists():\n",
    "        pdf_files = list(input_dir.glob(\"*.pdf\"))\n",
    "        if pdf_files:\n",
    "            print(f\"\\\\nğŸ“‚ ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€ì•ˆ íŒŒì¼ë“¤:\")\n",
    "            for i, pdf_file in enumerate(pdf_files[:5], 1):\n",
    "                size_mb = pdf_file.stat().st_size / 1024 / 1024\n",
    "                print(f\"   {i}. {pdf_file.name} ({size_mb:.2f} MB)\")\n",
    "            \n",
    "            print(f\"\\\\nğŸ’¡ ëŒ€ì•ˆ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:\")\n",
    "            print(f\"   alternative_path = Path('/home/admin/wkms-aws/jupyter_notebook/data/input_docs/íŒŒì¼ëª….pdf')\")\n",
    "            print(f\"   result = safe_extract_text_from_pdf(alternative_path, method='auto')\")\n",
    "\n",
    "print(f\"\\\\nğŸ¯ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ ì¶”ì¶œëœ íŒŒì¼ë“¤ì€ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤:\")\n",
    "print(f\"   /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‹ ì¼ë°˜ PDF íŒŒì¼ í…ŒìŠ¤íŠ¸ - ProductSpec_SmartInsulinPump\n",
    "\n",
    "print(\"ğŸ“‹ ì¼ë°˜ PDF íŒŒì¼ í…ŒìŠ¤íŠ¸ ì‹œì‘ - ProductSpec_SmartInsulinPump\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ\n",
    "product_pdf_path = Path(\"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf\")\n",
    "\n",
    "if product_pdf_path.exists():\n",
    "    file_size_mb = product_pdf_path.stat().st_size / 1024 / 1024\n",
    "    print(f\"ğŸ“„ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {product_pdf_path.name}\")\n",
    "    print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    # ì¼ë°˜ PDFëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ëª¨ë“  ë°©ë²• í…ŒìŠ¤íŠ¸\n",
    "    methods_to_test = [\n",
    "        ('pypdf2', 'PyPDF2 (ê¸°ë³¸ ë¹ ë¥¸ ë°©ë²•)'),\n",
    "        ('pdfplumber', 'pdfplumber (ê³ ê¸‰ ë°©ë²•)'),\n",
    "        ('ocr', 'OCR (ë°±ì—… ë°©ë²•)')\n",
    "    ]\n",
    "    \n",
    "    product_results = {}\n",
    "    \n",
    "    for method, description in methods_to_test:\n",
    "        print(f\"\\\\nğŸ”„ {description} í…ŒìŠ¤íŠ¸\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # OCRì˜ ê²½ìš° ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ë¯€ë¡œ ë‹¤ë¥¸ ë°©ë²•ì´ ì„±ê³µí•˜ë©´ ê±´ë„ˆë›°ê¸°\n",
    "        if method == 'ocr':\n",
    "            successful_text_methods = [m for m in ['pypdf2', 'pdfplumber'] \n",
    "                                     if m in product_results and product_results[m]['success'] \n",
    "                                     and len(product_results[m]['text']) > 100]\n",
    "            \n",
    "            if successful_text_methods:\n",
    "                print(f\"   â­ï¸ ë‹¤ë¥¸ ë°©ë²•ì´ ì„±ê³µí–ˆìœ¼ë¯€ë¡œ OCRì„ ê±´ë„ˆëœë‹ˆë‹¤\")\n",
    "                continue\n",
    "        \n",
    "        result = safe_extract_text_from_pdf(product_pdf_path, method=method)\n",
    "        product_results[method] = result\n",
    "        \n",
    "        if result['success']:\n",
    "            # í…ìŠ¤íŠ¸ ì €ì¥\n",
    "            saved_file = simple_save_text(\n",
    "                result['text'], \n",
    "                product_pdf_path.stem, \n",
    "                result['method']\n",
    "            )\n",
    "            result['saved_file'] = saved_file\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°\n",
    "            if len(result['text']) > 0:\n",
    "                preview = result['text'][:200].replace('\\\\n', ' ').strip()\n",
    "                print(f\"   ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: {preview}...\")\n",
    "                \n",
    "                # í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„\n",
    "                korean_chars = len([c for c in result['text'] if 'ê°€' <= c <= 'í£'])\n",
    "                total_chars = len(result['text'])\n",
    "                if total_chars > 0:\n",
    "                    korean_ratio = korean_chars / total_chars * 100\n",
    "                    print(f\"   ğŸ‡°ğŸ‡· í•œê¸€ ë¹„ìœ¨: {korean_ratio:.1f}%\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ì„±ê³µí•œ ë°©ë²•ì´ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí–ˆë‹¤ë©´ ë‹¤ìŒ ë°©ë²•ì€ ê±´ë„ˆë›°ê¸°\n",
    "        if result['success'] and len(result['text']) > 1000:\n",
    "            print(f\"   âœ¨ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ë“¤ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            break\n",
    "    \n",
    "    # ê²°ê³¼ ë¹„êµ ë° ìš”ì•½\n",
    "    print(f\"\\\\nğŸ“Š ProductSpec PDF í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    successful_product_methods = []\n",
    "    \n",
    "    for method, description in methods_to_test:\n",
    "        if method in product_results:\n",
    "            result = product_results[method]\n",
    "            \n",
    "            if result['success']:\n",
    "                text_len = result.get('text_length', len(result.get('text', '')))\n",
    "                proc_time = result.get('processing_time', 0)\n",
    "                \n",
    "                print(f\"âœ… {description}\")\n",
    "                print(f\"   ğŸ“ ì¶”ì¶œ í…ìŠ¤íŠ¸: {text_len:,}ì\")\n",
    "                print(f\"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {proc_time:.2f}ì´ˆ\")\n",
    "                print(f\"   ğŸ“„ í˜ì´ì§€ ìˆ˜: {result.get('pages', 'N/A')}\")\n",
    "                \n",
    "                successful_product_methods.append((method, text_len, proc_time))\n",
    "                \n",
    "                if 'saved_file' in result and result['saved_file']:\n",
    "                    print(f\"   ğŸ’¾ ì €ì¥ë¨: {result['saved_file'].name}\")\n",
    "                \n",
    "                # í…ìŠ¤íŠ¸ ë‚´ìš© ë¶„ì„\n",
    "                text_content = result.get('text', '')\n",
    "                if text_content:\n",
    "                    # ì£¼ìš” í‚¤ì›Œë“œ ê²€ìƒ‰\n",
    "                    keywords = ['insulin', 'ì¸ìŠë¦°', 'pump', 'íŒí”„', 'smart', 'ìŠ¤ë§ˆíŠ¸', 'glucose', 'í˜ˆë‹¹']\n",
    "                    found_keywords = [kw for kw in keywords if kw.lower() in text_content.lower()]\n",
    "                    if found_keywords:\n",
    "                        print(f\"   ğŸ” ë°œê²¬ëœ í‚¤ì›Œë“œ: {', '.join(found_keywords)}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"âŒ {description}\")\n",
    "                print(f\"   ğŸš« ì˜¤ë¥˜: {result.get('error', 'Unknown error')}\")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    # ìµœì  ë°©ë²• ì¶”ì²œ\n",
    "    if successful_product_methods:\n",
    "        print(f\"ğŸ† ProductSpec PDF ìµœì  ë°©ë²• ì¶”ì²œ\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # ê°€ì¥ ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œ ë°©ë²•\n",
    "        best_quality = max(successful_product_methods, key=lambda x: x[1])\n",
    "        print(f\"ğŸ¯ ìµœê³  í’ˆì§ˆ: {best_quality[0]} ({best_quality[1]:,}ì)\")\n",
    "        \n",
    "        # ê°€ì¥ ë¹ ë¥¸ ë°©ë²•\n",
    "        best_speed = min(successful_product_methods, key=lambda x: x[2])\n",
    "        print(f\"âš¡ ìµœê³  ì†ë„: {best_speed[0]} ({best_speed[2]:.2f}ì´ˆ)\")\n",
    "        \n",
    "        # ì¼ë°˜ PDF íŠ¹ì„±ì— ë§ëŠ” ì¶”ì²œ\n",
    "        if best_quality[1] > 1000:\n",
    "            print(f\"ğŸ’¡ ì¶”ì²œ: {best_quality[0]} (ì¼ë°˜ PDF - í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ìˆ˜)\")\n",
    "        else:\n",
    "            print(f\"ğŸ’¡ ì¶”ì²œ: OCR ë°©ë²•ì„ ì‹œë„í•´ë³´ì„¸ìš” (í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì¶œ ì‹¤íŒ¨)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âŒ ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"ğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:\")\n",
    "        print(f\"   - íŒŒì¼ì´ ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€\")\n",
    "        print(f\"   - PDF íŒŒì¼ì´ ì•”í˜¸í™”ë˜ì–´ ìˆì§€ ì•Šì€ì§€\") \n",
    "        print(f\"   - OCR ë°©ë²•ì„ ì‹œë„í•´ë³´ì„¸ìš”\")\n",
    "    \n",
    "    # ì´ì „ test.pdfì™€ ë¹„êµ\n",
    "    if 'results' in globals() and results:\n",
    "        print(f\"\\\\nğŸ“Š íŒŒì¼ë³„ ë¹„êµ ë¶„ì„\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸ”¬ test.pdf (ì´ë¯¸ì§€ ê¸°ë°˜):\")\n",
    "        print(f\"   - ìµœì  ë°©ë²•: OCR\")\n",
    "        print(f\"   - ì¶”ì¶œëŸ‰: 84,097ì\")\n",
    "        print(f\"   - ì²˜ë¦¬ì‹œê°„: ~6.5ë¶„\")\n",
    "        \n",
    "        print(f\"\\\\nğŸ“‹ ProductSpec PDF (ì¼ë°˜ PDF):\")\n",
    "        if successful_product_methods:\n",
    "            best_method = max(successful_product_methods, key=lambda x: x[1])\n",
    "            print(f\"   - ìµœì  ë°©ë²•: {best_method[0]}\")\n",
    "            print(f\"   - ì¶”ì¶œëŸ‰: {best_method[1]:,}ì\")\n",
    "            print(f\"   - ì²˜ë¦¬ì‹œê°„: {best_method[2]:.2f}ì´ˆ\")\n",
    "        else:\n",
    "            print(f\"   - ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {product_pdf_path}\")\n",
    "    \n",
    "    # ëŒ€ì•ˆ íŒŒì¼ ì œì•ˆ\n",
    "    input_dir = product_pdf_path.parent\n",
    "    if input_dir.exists():\n",
    "        pdf_files = list(input_dir.glob(\"*ProductSpec*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            pdf_files = list(input_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        if pdf_files:\n",
    "            print(f\"\\\\nğŸ“‚ ì‚¬ìš© ê°€ëŠ¥í•œ PDF íŒŒì¼ë“¤:\")\n",
    "            for i, pdf_file in enumerate(pdf_files[:10], 1):\n",
    "                size_mb = pdf_file.stat().st_size / 1024 / 1024\n",
    "                print(f\"   {i}. {pdf_file.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\\\nğŸ¯ ì¼ë°˜ PDF í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ ì¶”ì¶œëœ íŒŒì¼ë“¤ì€ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤:\")\n",
    "print(f\"   /home/admin/wkms-aws/jupyter_notebook/data/opensource_output/extracted_texts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š test.pdf ì¶”ì¶œ ë°©ë²•ë³„ ìƒì„¸ ë¹„êµ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# test.pdfê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ì‹¤í–‰\n",
    "if test_pdf_path.exists():\n",
    "    print(\"ğŸ”¬ test.pdf ì¶”ì¶œ ë°©ë²•ë³„ ìƒì„¸ ë¹„êµ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    extraction_methods = [\n",
    "        ('auto', 'ìë™ ì„ íƒ'),\n",
    "        ('pypdf2', 'PyPDF2'),\n",
    "        ('pdfplumber', 'pdfplumber'),\n",
    "        ('ocr', 'OCR (Tesseract)')\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method, description in extraction_methods:\n",
    "        print(f\"\\nğŸ”„ {description} ë°©ë²• í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "        try:\n",
    "            result = text_extractor.extract_text(str(test_pdf_path), method=method)\n",
    "            \n",
    "            if result['success']:\n",
    "                text_length = len(result['text'])\n",
    "                processing_time = result['processing_time']\n",
    "                \n",
    "                results[method] = {\n",
    "                    'success': True,\n",
    "                    'text_length': text_length,\n",
    "                    'processing_time': processing_time,\n",
    "                    'extraction_method': result['extraction_method'],\n",
    "                    'metadata': result.get('metadata', {})\n",
    "                }\n",
    "                \n",
    "                print(f\"   âœ… ì„±ê³µ: {text_length:,}ì, {processing_time:.2f}ì´ˆ\")\n",
    "                \n",
    "                # ì²« 100ì ë¯¸ë¦¬ë³´ê¸°\n",
    "                preview = result['text'][:100].replace('\\n', ' ')\n",
    "                print(f\"   ğŸ‘€ ë¯¸ë¦¬ë³´ê¸°: {preview}...\")\n",
    "                \n",
    "            else:\n",
    "                results[method] = {\n",
    "                    'success': False,\n",
    "                    'error': result.get('error', 'Unknown error')\n",
    "                }\n",
    "                print(f\"   âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            results[method] = {\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "            print(f\"   âŒ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    # ê²°ê³¼ ë¹„êµí‘œ ì¶œë ¥\n",
    "    print(f\"\\nğŸ“ˆ ì¶”ì¶œ ë°©ë²•ë³„ ê²°ê³¼ ë¹„êµ\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'ë°©ë²•':<15} {'ì„±ê³µ':<6} {'í…ìŠ¤íŠ¸ ê¸¸ì´':<12} {'ì²˜ë¦¬ì‹œê°„':<10} {'ì‹¤ì œ ë°©ë²•'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for method, description in extraction_methods:\n",
    "        if method in results:\n",
    "            result = results[method]\n",
    "            if result['success']:\n",
    "                success_icon = \"âœ…\"\n",
    "                text_len = f\"{result['text_length']:,}ì\"\n",
    "                proc_time = f\"{result['processing_time']:.2f}ì´ˆ\"\n",
    "                actual_method = result['extraction_method']\n",
    "            else:\n",
    "                success_icon = \"âŒ\"\n",
    "                text_len = \"N/A\"\n",
    "                proc_time = \"N/A\"\n",
    "                actual_method = \"ì‹¤íŒ¨\"\n",
    "            \n",
    "            print(f\"{description:<15} {success_icon:<6} {text_len:<12} {proc_time:<10} {actual_method}\")\n",
    "    \n",
    "    # ìµœì  ë°©ë²• ì¶”ì²œ\n",
    "    successful_results = {k: v for k, v in results.items() if v['success']}\n",
    "    if successful_results:\n",
    "        # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ì°¾ê¸°\n",
    "        best_method = max(successful_results.items(), key=lambda x: x[1]['text_length'])\n",
    "        \n",
    "        print(f\"\\nğŸ† ì¶”ì²œ ë°©ë²•: {best_method[0]} ({best_method[1]['text_length']:,}ì)\")\n",
    "        \n",
    "        # ì†ë„ vs í’ˆì§ˆ ë¶„ì„\n",
    "        speed_winner = min(successful_results.items(), key=lambda x: x[1]['processing_time'])\n",
    "        quality_winner = max(successful_results.items(), key=lambda x: x[1]['text_length'])\n",
    "        \n",
    "        print(f\"\\nâš¡ ìµœê³  ì†ë„: {speed_winner[0]} ({speed_winner[1]['processing_time']:.2f}ì´ˆ)\")\n",
    "        print(f\"ğŸ¯ ìµœê³  í’ˆì§ˆ: {quality_winner[0]} ({quality_winner[1]['text_length']:,}ì)\")\n",
    "        \n",
    "        if speed_winner[0] != quality_winner[0]:\n",
    "            speed_text_ratio = quality_winner[1]['text_length'] / speed_winner[1]['text_length']\n",
    "            time_ratio = quality_winner[1]['processing_time'] / speed_winner[1]['processing_time']\n",
    "            print(f\"ğŸ“Š í’ˆì§ˆ ê°œì„ ìœ¨: {speed_text_ratio:.1f}ë°°, ì‹œê°„ ì¦ê°€ìœ¨: {time_ratio:.1f}ë°°\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâŒ ëª¨ë“  ì¶”ì¶œ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ test.pdf íŒŒì¼ì´ ì—†ì–´ ë¹„êµ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9c58f",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\n",
    "\n",
    "### âœ… **êµ¬í˜„ëœ ê¸°ëŠ¥**\n",
    "1. **ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©**: 15ê°œ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µí•©\n",
    "2. **âš™ï¸ ì„¤ì • ê´€ë¦¬**: ë””ë ‰í† ë¦¬ êµ¬ì¡° ë° ì²˜ë¦¬ ì˜µì…˜ ì„¤ì •\n",
    "3. **ğŸ“‚ ë¬¸ì„œ ë¡œë”©**: 7ê°€ì§€ íŒŒì¼ í˜•ì‹ ì§€ì› (PDF, Word, PPT, Excel, HTML, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€)\n",
    "4. **ğŸ”¤ í…ìŠ¤íŠ¸ ì¶”ì¶œ**: ê° í˜•ì‹ë³„ ìµœì í™”ëœ ì¶”ì¶œ ë°©ë²•\n",
    "5. **ğŸ” OCR ì²˜ë¦¬**: Tesseract ê¸°ë°˜ ì´ë¯¸ì§€ PDF ì²˜ë¦¬ (í˜ì´ì§€ë³„ ë¡œê·¸ ë° ì €ì¥)\n",
    "6. **ğŸ“Š ì „ì²˜ë¦¬**: í…ìŠ¤íŠ¸ ì •ì œ ë° ì²­í‚¹\n",
    "7. **ğŸ’¾ ê²°ê³¼ ì €ì¥**: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥\n",
    "\n",
    "### ğŸ”— **ë‹¤ìŒ ë‹¨ê³„ ì—°ê²°**\n",
    "- **ë°±ì—”ë“œ ì—°ë™**: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ `tools.py`ì˜ `chunked_texts()` í•¨ìˆ˜ë¡œ ì „ë‹¬\n",
    "- **ì„ë² ë”© ìƒì„±**: Azure OpenAIë¥¼ í†µí•œ ë²¡í„° ì„ë² ë”©\n",
    "- **ë²¡í„°ìŠ¤í† ì–´**: Cosmos DB ë˜ëŠ” Azure AI Searchì— ì €ì¥\n",
    "\n",
    "### \udcc2 **ì¶œë ¥ íŒŒì¼ êµ¬ì¡°**\n",
    "```\n",
    "jupyter_notebook/data/opensource_output/\n",
    "â”œâ”€â”€ extracted_texts/           # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ íŒŒì¼\n",
    "â”‚   â”œâ”€â”€ íŒŒì¼ëª…_extracted.txt   # ì¼ë°˜ ì¶”ì¶œ ê²°ê³¼\n",
    "â”‚   â”œâ”€â”€ íŒŒì¼ëª…_page_001.txt   # OCR í˜ì´ì§€ë³„ ê²°ê³¼\n",
    "â”‚   â””â”€â”€ íŒŒì¼ëª…_ocr_full.txt   # OCR ì „ì²´ ê²°ê³¼\n",
    "â””â”€â”€ metadata/                  # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°\n",
    "    â””â”€â”€ íŒŒì¼ëª…_ocr_metadata.json\n",
    "```\n",
    "\n",
    "### ğŸ’¡ **ì‚¬ìš©ë²• ìš”ì•½**\n",
    "```python\n",
    "# ê°œë³„ íŒŒì¼ í…ŒìŠ¤íŠ¸\n",
    "test_result = test_file_extraction(\"ë¬¸ì„œ.pdf\")\n",
    "\n",
    "# OCR ê°•ì œ ì‚¬ìš© (ì´ë¯¸ì§€ ê¸°ë°˜ PDF)\n",
    "test_result = test_file_extraction(\"ì´ë¯¸ì§€.pdf\", method='ocr')\n",
    "\n",
    "# ë°°ì¹˜ í…ŒìŠ¤íŠ¸\n",
    "for file_path in sample_files:\n",
    "    test_file_extraction(file_path)\n",
    "```\n",
    "\n",
    "**ğŸ‰ ì´ì œ ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80a9a56",
   "metadata": {},
   "source": [
    "# Azure Document Intelligence ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "Azure Document Intelligence (êµ¬ Form Recognizer)ë¥¼ ì‚¬ìš©í•œ ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ í•µì‹¬ ê¸°ëŠ¥ì„ ë‹¨ê³„ë³„ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "## íŒŒì´í”„ë¼ì¸ ê°œìš”\n",
    "1. **í™˜ê²½ ì„¤ì •**: Azure ì„œë¹„ìŠ¤ ì—°ê²° ë° ì´ˆê¸°í™”\n",
    "2. **ë¬¸ì„œ ì—…ë¡œë“œ**: ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œ ì²˜ë¦¬\n",
    "3. **í…ìŠ¤íŠ¸ ì¶”ì¶œ**: Azure Document Intelligence ê¸°ë°˜ OCR ë° êµ¬ì¡° ë¶„ì„\n",
    "4. **ë ˆì´ì•„ì›ƒ ë¶„ì„**: ë‹¤ì¤‘ ì»¬ëŸ¼ ë° í˜ì´ì§€ë³„ ì½ê¸° ìˆœì„œ ìµœì í™”\n",
    "5. **êµ¬ì¡°ì  ìš”ì†Œ ì¶”ì¶œ**: í…Œì´ë¸”, ë¬¸ë‹¨, ì œëª© ë“± ë¬¸ì„œ êµ¬ì¡° ë¶„ì„\n",
    "6. **ê²€ìƒ‰ í…ŒìŠ¤íŠ¸**: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ì„±ëŠ¥ ê²€ì¦\n",
    "\n",
    "**ì£¼ìš” ê¸°ëŠ¥**:\n",
    "- âœ… ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "- âœ… ê³ ì„±ëŠ¥ ë ˆì´ì•„ì›ƒ ë¶„ì„\n",
    "- âœ… í‘œ/í¼ êµ¬ì¡° ì¸ì‹\n",
    "- âœ… í˜ì´ì§€ë³„ ì²˜ë¦¬ ìµœì í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30496b0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "201aad13",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° Azure ì„œë¹„ìŠ¤ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1bad99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°±ì—”ë“œ í™˜ê²½ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: /home/admin/wkms-aws/backend/.env\n",
      "ğŸ”§ Azure Document Intelligence ì„¤ì • í™•ì¸:\n",
      "ğŸ”§ ë°±ì—”ë“œ URL: http://localhost:8000\n",
      "âœ… Azure ì—”ë“œí¬ì¸íŠ¸: https://pre-pro-doc.cognitiveservices.azure.com/\n",
      "ğŸ” Azure API í‚¤: âœ… ì„¤ì •ë¨ (Ch3M6JK1********************ACOGYcq0)\n",
      "ğŸ“„ ê¸°ë³¸ ëª¨ë¸: prebuilt-read\n",
      "ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: 80.0%\n",
      "ğŸ“‹ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: âœ… í™œì„±í™”\n",
      "\n",
      "ğŸ‰ Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸: âœ… ì´ˆê¸°í™” ì™„ë£Œ\n",
      "ğŸ“¡ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ: âœ… ì¤€ë¹„ë¨\n",
      "\n",
      "ğŸ“‹ ì„¤ì • ìš”ì•½:\n",
      "   â€¢ Azure í´ë¼ì´ì–¸íŠ¸: âœ… ì´ˆê¸°í™” ì™„ë£Œ\n",
      "   â€¢ í™˜ê²½ ì„¤ì • íŒŒì¼: âœ… ë¡œë“œë¨\n",
      "   â€¢ ë°±ì—”ë“œ ì—°ë™: âœ… ì„¤ì •ë¨\n",
      "\n",
      "ğŸ‰ Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸: âœ… ì´ˆê¸°í™” ì™„ë£Œ\n",
      "ğŸ“¡ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ: âœ… ì¤€ë¹„ë¨\n",
      "\n",
      "ğŸ“‹ ì„¤ì • ìš”ì•½:\n",
      "   â€¢ Azure í´ë¼ì´ì–¸íŠ¸: âœ… ì´ˆê¸°í™” ì™„ë£Œ\n",
      "   â€¢ í™˜ê²½ ì„¤ì • íŒŒì¼: âœ… ë¡œë“œë¨\n",
      "   â€¢ ë°±ì—”ë“œ ì—°ë™: âœ… ì„¤ì •ë¨\n"
     ]
    }
   ],
   "source": [
    "# Azure Document Intelligence ì„¤ì • ë° ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ë°±ì—”ë“œ .env íŒŒì¼ ë¡œë“œ\n",
    "backend_env_path = Path(\"/home/admin/wkms-aws/backend/.env\")\n",
    "if backend_env_path.exists():\n",
    "    load_dotenv(backend_env_path)\n",
    "    print(f\"âœ… ë°±ì—”ë“œ í™˜ê²½ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {backend_env_path}\")\n",
    "else:\n",
    "    print(f\"âŒ ë°±ì—”ë“œ í™˜ê²½ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {backend_env_path}\")\n",
    "\n",
    "# Azure Document Intelligence ì„¤ì • (ë°±ì—”ë“œ .envì—ì„œ ë¡œë“œ)\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv('AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT')\n",
    "AZURE_DOCUMENT_INTELLIGENCE_KEY = os.getenv('AZURE_DOCUMENT_INTELLIGENCE_API_KEY')\n",
    "AZURE_DOC_MODEL = os.getenv('AZURE_DOCUMENT_INTELLIGENCE_DEFAULT_MODEL', 'prebuilt-read')\n",
    "CONFIDENCE_THRESHOLD = float(os.getenv('AZURE_DOCUMENT_INTELLIGENCE_CONFIDENCE_THRESHOLD', '0.75'))\n",
    "\n",
    "# ê¸°íƒ€ ì„¤ì •\n",
    "BACKEND_URL = \"http://localhost:8000\"\n",
    "USE_AZURE_DOCUMENT_INTELLIGENCE = True\n",
    "ENABLE_PAGE_SPECIFIC_LAYOUT = True\n",
    "MAX_PAGES = int(os.getenv('AZURE_DOCUMENT_INTELLIGENCE_MAX_ASYNC_PAGES', '50'))\n",
    "FIRST_PAGE_LAYOUT = \"full_page\"\n",
    "SAVE_PAGE_RESULTS = True\n",
    "\n",
    "# Azure ë¦¬ì†ŒìŠ¤ ì„¤ì • (ì„ íƒì‚¬í•­)\n",
    "AZURE_SUBSCRIPTION_ID = os.getenv('AZURE_SUBSCRIPTION_ID')\n",
    "AZURE_RESOURCE_GROUP = os.getenv('AZURE_RESOURCE_GROUP')\n",
    "\n",
    "print(\"ğŸ”§ Azure Document Intelligence ì„¤ì • í™•ì¸:\")\n",
    "print(f\"ğŸ”§ ë°±ì—”ë“œ URL: {BACKEND_URL}\")\n",
    "\n",
    "if AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT:\n",
    "    print(f\"âœ… Azure ì—”ë“œí¬ì¸íŠ¸: {AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT}\")\n",
    "else:\n",
    "    print(\"âŒ Azure ì—”ë“œí¬ì¸íŠ¸ ë¯¸ì„¤ì •\")\n",
    "\n",
    "if AZURE_DOCUMENT_INTELLIGENCE_KEY:\n",
    "    masked_key = AZURE_DOCUMENT_INTELLIGENCE_KEY[:8] + \"*\" * 20 + AZURE_DOCUMENT_INTELLIGENCE_KEY[-8:] if len(AZURE_DOCUMENT_INTELLIGENCE_KEY) > 16 else \"***\"\n",
    "    print(f\"ğŸ” Azure API í‚¤: âœ… ì„¤ì •ë¨ ({masked_key})\")\n",
    "else:\n",
    "    print(\"ğŸ” Azure API í‚¤: âŒ ë¯¸ì„¤ì •\")\n",
    "\n",
    "print(f\"ğŸ“„ ê¸°ë³¸ ëª¨ë¸: {AZURE_DOC_MODEL}\")\n",
    "print(f\"ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: {CONFIDENCE_THRESHOLD*100:.1f}%\")\n",
    "print(f\"ğŸ“‹ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: {'âœ… í™œì„±í™”' if ENABLE_PAGE_SPECIFIC_LAYOUT else 'âŒ ë¹„í™œì„±í™”'}\")\n",
    "\n",
    "# Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œë„\n",
    "azure_client = None\n",
    "azure_client_status = \"âŒ ë¯¸ì´ˆê¸°í™”\"\n",
    "\n",
    "try:\n",
    "    if AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT and AZURE_DOCUMENT_INTELLIGENCE_KEY:\n",
    "        from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "        from azure.core.credentials import AzureKeyCredential\n",
    "        \n",
    "        azure_client = DocumentAnalysisClient(\n",
    "            endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT,\n",
    "            credential=AzureKeyCredential(AZURE_DOCUMENT_INTELLIGENCE_KEY)\n",
    "        )\n",
    "        azure_client_status = \"âœ… ì´ˆê¸°í™” ì™„ë£Œ\"\n",
    "        print(f\"\\nğŸ‰ Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸: {azure_client_status}\")\n",
    "        print(\"ğŸ“¡ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ: âœ… ì¤€ë¹„ë¨\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâŒ Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: ì—”ë“œí¬ì¸íŠ¸ ë˜ëŠ” API í‚¤ ëˆ„ë½\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"\\nâŒ Azure ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install azure-ai-formrecognizer azure-identity\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì„¤ì • ìš”ì•½:\")\n",
    "print(f\"   â€¢ Azure í´ë¼ì´ì–¸íŠ¸: {azure_client_status}\")\n",
    "print(f\"   â€¢ í™˜ê²½ ì„¤ì • íŒŒì¼: âœ… ë¡œë“œë¨\")\n",
    "print(f\"   â€¢ ë°±ì—”ë“œ ì—°ë™: âœ… ì„¤ì •ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38469b23",
   "metadata": {},
   "source": [
    "## 2. Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d4726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...\n",
      "âœ… Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\n",
      "   ğŸŒ ì—”ë“œí¬ì¸íŠ¸: https://pre-pro-doc.cognitiveservices.azure.com/\n",
      "   ğŸ” ì¸ì¦: AzureKeyCredential ì‚¬ìš©\n",
      "   ğŸ“¦ SDK: azure-ai-formrecognizer (Azure Document Intelligence)\n",
      "ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:\n",
      "   â€¢ prebuilt-read\n",
      "   â€¢ prebuilt-layout\n",
      "   â€¢ prebuilt-document\n",
      "   â€¢ prebuilt-invoice\n",
      "   â€¢ prebuilt-receipt\n",
      "âœ… Azure Document Intelligence ì„œë¹„ìŠ¤ ì—°ê²° ì¤€ë¹„ ì™„ë£Œ\n",
      "ğŸ”— í´ë¼ì´ì–¸íŠ¸ ì—”ë“œí¬ì¸íŠ¸: https://pre-pro-doc.cognitiveservices.azure.com\n",
      "âœ… í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ê²€ì¦ ì™„ë£Œ\n",
      "\n",
      "ğŸ¯ Azure Document Intelligence ì¤€ë¹„ ì™„ë£Œ!\n",
      "   ê¸°ë³¸ ëª¨ë¸: prebuilt-read\n",
      "   ì‹ ë¢°ë„ ì„ê³„ê°’: 80.0%\n",
      "   ìµœëŒ€ í˜ì´ì§€: 10í˜ì´ì§€\n",
      "   í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ: âœ… ì •ìƒ\n"
     ]
    }
   ],
   "source": [
    "# Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "def init_azure_document_intelligence():\n",
    "    \"\"\"Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì—°ê²° í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    try:\n",
    "        # ì˜¬ë°”ë¥¸ íŒ¨í‚¤ì§€ import (azure-ai-formrecognizer)\n",
    "        from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "        from azure.core.credentials import AzureKeyCredential\n",
    "        \n",
    "        if not AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT or not AZURE_DOCUMENT_INTELLIGENCE_KEY:\n",
    "            print(\"âŒ Azure Document Intelligence ìê²©ì¦ëª…ì´ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            print(\"ğŸ”§ ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:\")\n",
    "            print(\"   AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://your-resource.cognitiveservices.azure.com/\")\n",
    "            print(\"   AZURE_DOCUMENT_INTELLIGENCE_KEY=your-api-key\")\n",
    "            return None\n",
    "            \n",
    "        # í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "        credential = AzureKeyCredential(AZURE_DOCUMENT_INTELLIGENCE_KEY)\n",
    "        client = DocumentAnalysisClient(\n",
    "            endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT,\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        print(f\"   ğŸŒ ì—”ë“œí¬ì¸íŠ¸: {AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT}\")\n",
    "        print(f\"   ğŸ” ì¸ì¦: AzureKeyCredential ì‚¬ìš©\")\n",
    "        print(f\"   ğŸ“¦ SDK: azure-ai-formrecognizer (Azure Document Intelligence)\")\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "        try:\n",
    "            print(\"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:\")\n",
    "            available_models = [\n",
    "                \"prebuilt-read\",\n",
    "                \"prebuilt-layout\", \n",
    "                \"prebuilt-document\",\n",
    "                \"prebuilt-invoice\",\n",
    "                \"prebuilt-receipt\"\n",
    "            ]\n",
    "            for model in available_models:\n",
    "                print(f\"   â€¢ {model}\")\n",
    "            \n",
    "            print(\"âœ… Azure Document Intelligence ì„œë¹„ìŠ¤ ì—°ê²° ì¤€ë¹„ ì™„ë£Œ\")\n",
    "            return client\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ì„œë¹„ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            print(\"í´ë¼ì´ì–¸íŠ¸ëŠ” ìƒì„±ë˜ì—ˆì§€ë§Œ ì„œë¹„ìŠ¤ ì—°ê²°ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "            return client\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(\"âŒ Azure Document Intelligence SDKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\")\n",
    "        print(\"ğŸ”§ ì„¤ì¹˜ ëª…ë ¹ì–´:\")\n",
    "        print(\"   pip install azure-ai-formrecognizer azure-identity\")\n",
    "        print(f\"   ìƒì„¸ ì˜¤ë¥˜: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_azure_client_connection(client):\n",
    "    \"\"\"Azure í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    if not client:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # í´ë¼ì´ì–¸íŠ¸ ê°ì²´ê°€ ì˜¬ë°”ë¥´ê²Œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "        endpoint = client._endpoint\n",
    "        print(f\"ğŸ”— í´ë¼ì´ì–¸íŠ¸ ì—”ë“œí¬ì¸íŠ¸: {endpoint}\")\n",
    "        print(\"âœ… í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ê²€ì¦ ì™„ë£Œ\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í´ë¼ì´ì–¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "# Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "print(\"ğŸš€ Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...\")\n",
    "azure_client = init_azure_document_intelligence()\n",
    "\n",
    "if azure_client:\n",
    "    client_ok = test_azure_client_connection(azure_client)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Azure Document Intelligence ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "    print(f\"   ê¸°ë³¸ ëª¨ë¸: {AZURE_DOC_MODEL}\")\n",
    "    print(f\"   ì‹ ë¢°ë„ ì„ê³„ê°’: {CONFIDENCE_THRESHOLD*100:.1f}%\")\n",
    "    print(f\"   ìµœëŒ€ í˜ì´ì§€: {MAX_PAGES}í˜ì´ì§€\")\n",
    "    print(f\"   í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ: {'âœ… ì •ìƒ' if client_ok else 'âš ï¸ ê²€ì¦ í•„ìš”'}\")\n",
    "    \n",
    "    # ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "    globals()['azure_client'] = azure_client\n",
    "else:\n",
    "    print(f\"\\nâŒ Azure Document Intelligence ì‚¬ìš© ë¶ˆê°€\")\n",
    "    print(\"í™˜ê²½ ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”\")\n",
    "    globals()['azure_client'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40893a43",
   "metadata": {},
   "source": [
    "## 3. ë¬¸ì„œ ì—…ë¡œë“œ ë° ì²˜ë¦¬ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2b6fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ë¬¸ì„œ ë¶„ì„ ê²°ê³¼: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "   íŒŒì¼ í¬ê¸°: 11.85MB\n",
      "   ë³µì¡ë„: medium\n",
      "   ê¶Œì¥ ëª¨ë¸: prebuilt-layout\n",
      "   ì²˜ë¦¬ ë°©ì‹: ë™ê¸°\n",
      "   ì¶”ê°€ ê¸°ëŠ¥: tables, keyValuePairs\n",
      "   í˜ì´ì§€ ìˆ˜: 27\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ë³µì¡ë„ ë° ì²˜ë¦¬ ì „ëµ íŒë³„\n",
    "def detect_document_complexity_azure(file_path):\n",
    "    \"\"\"Azure Document Intelligenceë¥¼ ìœ„í•œ ë¬¸ì„œ ë³µì¡ë„ ë¶„ì„\"\"\"\n",
    "    try:\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        file_extension = os.path.splitext(file_path.lower())[1]\n",
    "        \n",
    "        complexity_info = {\n",
    "            'file_size': file_size,\n",
    "            'file_size_mb': file_size / (1024 * 1024),\n",
    "            'file_extension': file_extension,\n",
    "            'complexity': 'simple',\n",
    "            'recommended_model': 'prebuilt-read',\n",
    "            'processing_strategy': 'sync'\n",
    "        }\n",
    "        \n",
    "        # íŒŒì¼ í¬ê¸° ê¸°ì¤€\n",
    "        if file_size > 20 * 1024 * 1024:  # 20MB ì´ìƒ\n",
    "            complexity_info['complexity'] = 'complex'\n",
    "            complexity_info['recommended_model'] = 'prebuilt-layout'\n",
    "            complexity_info['processing_strategy'] = 'async'\n",
    "        elif file_size > 5 * 1024 * 1024:  # 5-20MB\n",
    "            complexity_info['complexity'] = 'medium'\n",
    "            complexity_info['recommended_model'] = 'prebuilt-layout'\n",
    "            complexity_info['processing_strategy'] = 'sync'\n",
    "        \n",
    "        # PDFì˜ ê²½ìš° í˜ì´ì§€ ìˆ˜ í™•ì¸\n",
    "        if file_extension == '.pdf':\n",
    "            try:\n",
    "                import fitz\n",
    "                doc = fitz.open(file_path)\n",
    "                page_count = len(doc)\n",
    "                doc.close()\n",
    "                \n",
    "                complexity_info['page_count'] = page_count\n",
    "                \n",
    "                if page_count > 50:\n",
    "                    complexity_info['complexity'] = 'complex'\n",
    "                    complexity_info['recommended_model'] = 'prebuilt-layout'\n",
    "                    complexity_info['processing_strategy'] = 'async'\n",
    "                elif page_count > 10:\n",
    "                    complexity_info['complexity'] = 'medium'\n",
    "                    complexity_info['recommended_model'] = 'prebuilt-layout'\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ PDF í˜ì´ì§€ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
    "                complexity_info['page_count'] = 'unknown'\n",
    "        \n",
    "        # ì¼ë°˜ì ìœ¼ë¡œ layout ëª¨ë¸ì´ ë” ì•ˆì •ì ì´ë¯€ë¡œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •\n",
    "        if complexity_info['recommended_model'] == 'prebuilt-read':\n",
    "            complexity_info['recommended_model'] = 'prebuilt-layout'\n",
    "            complexity_info['reason'] = 'layout_optimization'\n",
    "        \n",
    "        return complexity_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ë³µì¡ë„ ê°ì§€ ì‹¤íŒ¨: {e}\")\n",
    "        return {\n",
    "            'complexity': 'unknown',\n",
    "            'recommended_model': 'prebuilt-layout',  # ì•ˆì „í•œ ê¸°ë³¸ê°’\n",
    "            'processing_strategy': 'sync',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def determine_azure_processing_strategy(complexity_info):\n",
    "    \"\"\"Azure Document Intelligence ì²˜ë¦¬ ì „ëµ ê²°ì •\"\"\"\n",
    "    strategy = {\n",
    "        'model_id': complexity_info.get('recommended_model', 'prebuilt-layout'),\n",
    "        'is_async': complexity_info.get('processing_strategy') == 'async',\n",
    "        'max_pages': MAX_PAGES,\n",
    "        'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "        'features': []\n",
    "    }\n",
    "    \n",
    "    # ë³µì¡ë„ì— ë”°ë¥¸ ê¸°ëŠ¥ ì¶”ê°€\n",
    "    complexity = complexity_info.get('complexity', 'simple')\n",
    "    \n",
    "    if complexity in ['medium', 'complex']:\n",
    "        strategy['features'].extend(['tables', 'keyValuePairs'])\n",
    "    \n",
    "    if complexity == 'complex':\n",
    "        strategy['features'].extend(['paragraphs', 'styles'])\n",
    "    \n",
    "    # ê¸°ë³¸ì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì½ê¸° ìˆœì„œ ì ìš©\n",
    "    strategy['reading_order'] = 'natural'\n",
    "    \n",
    "    return strategy\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ íŒŒì¼ë¡œ ë³µì¡ë„ ê²€ì‚¬\n",
    "test_file = \"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\"\n",
    "if os.path.exists(test_file):\n",
    "    complexity_info = detect_document_complexity_azure(test_file)\n",
    "    strategy = determine_azure_processing_strategy(complexity_info)\n",
    "    \n",
    "    print(f\"ğŸ“„ ë¬¸ì„œ ë¶„ì„ ê²°ê³¼: {test_file}\")\n",
    "    print(f\"   íŒŒì¼ í¬ê¸°: {complexity_info['file_size_mb']:.2f}MB\")\n",
    "    print(f\"   ë³µì¡ë„: {complexity_info['complexity']}\")\n",
    "    print(f\"   ê¶Œì¥ ëª¨ë¸: {strategy['model_id']}\")\n",
    "    print(f\"   ì²˜ë¦¬ ë°©ì‹: {'ë¹„ë™ê¸°' if strategy['is_async'] else 'ë™ê¸°'}\")\n",
    "    print(f\"   ì¶”ê°€ ê¸°ëŠ¥: {', '.join(strategy['features']) if strategy['features'] else 'ê¸°ë³¸'}\")\n",
    "    if 'page_count' in complexity_info:\n",
    "        print(f\"   í˜ì´ì§€ ìˆ˜: {complexity_info['page_count']}\")\n",
    "else:\n",
    "    print(f\"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_file}\")\n",
    "    print(\"ğŸ’¡ /home/admin/wkms-aws/jupyter_notebook/data/input_docs/ í´ë”ì— test.pdfë¥¼ ë°°ì¹˜í•´ì£¼ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7ab46",
   "metadata": {},
   "source": [
    "## 4. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë ˆì´ì•„ì›ƒ ë¶„ì„ ë° ì¬êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95ff4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ í•œêµ­ì–´ ë ˆì´ì•„ì›ƒ ë¶„ì„ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… analyze_korean_layout: Azure ê²°ê³¼ì—ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„\n",
      "   âœ… determine_layout_type_korean: í•œêµ­ì–´ ë¬¸ì„œ ë ˆì´ì•„ì›ƒ íƒ€ì… ê²°ì •\n",
      "   âœ… reconstruct_korean_reading_order: í•œêµ­ì–´ ì½ê¸° ìˆœì„œ ì¬êµ¬ì„±\n"
     ]
    }
   ],
   "source": [
    "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë ˆì´ì•„ì›ƒ ë¶„ì„ ë° ì¬êµ¬ì„± í•¨ìˆ˜ë“¤\n",
    "def analyze_korean_layout(azure_result, page_number=1):\n",
    "    \"\"\"Azure Document Intelligence ê²°ê³¼ì—ì„œ í•œêµ­ì–´ ë ˆì´ì•„ì›ƒ ë¶„ì„\"\"\"\n",
    "    layout_info = {\n",
    "        'page_number': page_number,\n",
    "        'total_elements': 0,\n",
    "        'text_elements': [],\n",
    "        'table_elements': [],\n",
    "        'paragraph_elements': [],\n",
    "        'reading_order': [],\n",
    "        'column_structure': 'unknown',\n",
    "        'layout_type': 'unknown'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Azure Document Intelligence ê²°ê³¼ êµ¬ì¡° ë¶„ì„\n",
    "        if hasattr(azure_result, 'pages') and azure_result.pages:\n",
    "            page = azure_result.pages[page_number - 1] if len(azure_result.pages) >= page_number else azure_result.pages[0]\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ë¼ì¸ ìˆ˜ì§‘\n",
    "            if hasattr(page, 'lines') and page.lines:\n",
    "                for line in page.lines:\n",
    "                    if line.content and line.polygon:\n",
    "                        # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°\n",
    "                        x_coords = [p.x for p in line.polygon]\n",
    "                        y_coords = [p.y for p in line.polygon]\n",
    "                        \n",
    "                        element = {\n",
    "                            'text': line.content,\n",
    "                            'bbox': {\n",
    "                                'left': min(x_coords),\n",
    "                                'top': min(y_coords),\n",
    "                                'right': max(x_coords),\n",
    "                                'bottom': max(y_coords),\n",
    "                                'center_x': sum(x_coords) / len(x_coords),\n",
    "                                'center_y': sum(y_coords) / len(y_coords),\n",
    "                                'width': max(x_coords) - min(x_coords),\n",
    "                                'height': max(y_coords) - min(y_coords)\n",
    "                            },\n",
    "                            'confidence': getattr(line, 'confidence', 1.0),\n",
    "                            'type': 'line'\n",
    "                        }\n",
    "                        layout_info['text_elements'].append(element)\n",
    "        \n",
    "        # ë‹¨ë½ ì •ë³´ ìˆ˜ì§‘ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "        if hasattr(azure_result, 'paragraphs') and azure_result.paragraphs:\n",
    "            for para in azure_result.paragraphs:\n",
    "                if para.content and hasattr(para, 'bounding_regions'):\n",
    "                    for region in para.bounding_regions:\n",
    "                        if region.page_number == page_number:\n",
    "                            # ë‹¨ë½ ë ˆë²¨ ì •ë³´\n",
    "                            para_element = {\n",
    "                                'text': para.content,\n",
    "                                'page': region.page_number,\n",
    "                                'polygon': region.polygon,\n",
    "                                'role': getattr(para, 'role', 'paragraph'),\n",
    "                                'type': 'paragraph'\n",
    "                            }\n",
    "                            layout_info['paragraph_elements'].append(para_element)\n",
    "        \n",
    "        # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘\n",
    "        if hasattr(azure_result, 'tables') and azure_result.tables:\n",
    "            for table in azure_result.tables:\n",
    "                if hasattr(table, 'bounding_regions'):\n",
    "                    for region in table.bounding_regions:\n",
    "                        if region.page_number == page_number:\n",
    "                            table_element = {\n",
    "                                'row_count': table.row_count,\n",
    "                                'column_count': table.column_count,\n",
    "                                'page': region.page_number,\n",
    "                                'polygon': region.polygon,\n",
    "                                'type': 'table'\n",
    "                            }\n",
    "                            layout_info['table_elements'].append(table_element)\n",
    "        \n",
    "        layout_info['total_elements'] = len(layout_info['text_elements'])\n",
    "        \n",
    "        # ë ˆì´ì•„ì›ƒ íƒ€ì… ê²°ì •\n",
    "        layout_info['layout_type'] = determine_layout_type_korean(layout_info, page_number)\n",
    "        \n",
    "        return layout_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return layout_info\n",
    "\n",
    "def determine_layout_type_korean(layout_info, page_number=1):\n",
    "    \"\"\"í•œêµ­ì–´ ë¬¸ì„œì˜ ë ˆì´ì•„ì›ƒ íƒ€ì… ê²°ì •\"\"\"\n",
    "    text_elements = layout_info['text_elements']\n",
    "    \n",
    "    if not text_elements:\n",
    "        return 'empty'\n",
    "    \n",
    "    # ì²« í˜ì´ì§€ëŠ” ì œëª©/ì €ì/ì´ˆë¡ êµ¬ì¡°ë¡œ ê°„ì£¼\n",
    "    if page_number == 1 and FIRST_PAGE_LAYOUT == 'full_page':\n",
    "        return 'title_page'\n",
    "    \n",
    "    # ì¤‘ì‹¬ì  ë¶„í¬ë¡œ ì»¬ëŸ¼ êµ¬ì¡° íŒë‹¨\n",
    "    center_x_coords = [elem['bbox']['center_x'] for elem in text_elements]\n",
    "    \n",
    "    if len(set([round(x, 1) for x in center_x_coords])) <= 2:\n",
    "        return 'single_column'\n",
    "    \n",
    "    # X ì¢Œí‘œ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ 2ì—´ êµ¬ì¡° ê°ì§€\n",
    "    center_x_sorted = sorted(center_x_coords)\n",
    "    \n",
    "    if len(center_x_sorted) >= 4:\n",
    "        # ê°€ì¥ í° ê°„ê²© ì°¾ê¸°\n",
    "        gaps = []\n",
    "        for i in range(len(center_x_sorted) - 1):\n",
    "            gap = center_x_sorted[i + 1] - center_x_sorted[i]\n",
    "            gaps.append(gap)\n",
    "        \n",
    "        max_gap = max(gaps) if gaps else 0\n",
    "        avg_gap = sum(gaps) / len(gaps) if gaps else 0\n",
    "        \n",
    "        # í° ê°„ê²©ì´ ìˆìœ¼ë©´ 2ì—´ êµ¬ì¡°\n",
    "        if max_gap > avg_gap * 3:\n",
    "            return 'two_column'\n",
    "    \n",
    "    return 'single_column'\n",
    "\n",
    "def reconstruct_korean_reading_order(layout_info):\n",
    "    \"\"\"í•œêµ­ì–´ ì½ê¸° ìˆœì„œì— ë§ì¶˜ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±\"\"\"\n",
    "    text_elements = layout_info['text_elements']\n",
    "    layout_type = layout_info['layout_type']\n",
    "    page_number = layout_info['page_number']\n",
    "    \n",
    "    if not text_elements:\n",
    "        return []\n",
    "    \n",
    "    if layout_type == 'title_page':\n",
    "        # ì²« í˜ì´ì§€: ìƒí•˜ ìˆœì„œ (ì œëª© â†’ ì €ì â†’ ì´ˆë¡ â†’ ë‚´ìš©)\n",
    "        sorted_elements = sorted(text_elements, key=lambda x: (x['bbox']['top'], x['bbox']['left']))\n",
    "        return sorted_elements\n",
    "    \n",
    "    elif layout_type == 'two_column':\n",
    "        # 2ì—´ êµ¬ì¡°: ì¢Œì¸¡ ì—´ â†’ ìš°ì¸¡ ì—´\n",
    "        center_x_coords = [elem['bbox']['center_x'] for elem in text_elements]\n",
    "        \n",
    "        # ì¤‘ê°„ì  ê³„ì‚°ìœ¼ë¡œ ì¢Œìš° ë¶„í• \n",
    "        if center_x_coords:\n",
    "            # í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ë¶„í• ì  ê²°ì •\n",
    "            center_x_sorted = sorted(set(center_x_coords))\n",
    "            if len(center_x_sorted) >= 2:\n",
    "                # ê°€ì¥ í° ê°„ê²©ì˜ ì¤‘ì ì„ ë¶„í• ì ìœ¼ë¡œ ì‚¬ìš©\n",
    "                gaps = [(center_x_sorted[i+1] - center_x_sorted[i], (center_x_sorted[i] + center_x_sorted[i+1]) / 2) \n",
    "                       for i in range(len(center_x_sorted) - 1)]\n",
    "                gaps.sort(reverse=True)\n",
    "                threshold = gaps[0][1] if gaps else 0.5\n",
    "            else:\n",
    "                threshold = sum(center_x_coords) / len(center_x_coords)\n",
    "            \n",
    "            # ì¢Œìš° ë¶„í• \n",
    "            left_elements = [elem for elem in text_elements if elem['bbox']['center_x'] <= threshold]\n",
    "            right_elements = [elem for elem in text_elements if elem['bbox']['center_x'] > threshold]\n",
    "            \n",
    "            # ê° ì—´ ë‚´ì—ì„œ ìƒí•˜ ì •ë ¬\n",
    "            left_sorted = sorted(left_elements, key=lambda x: (x['bbox']['top'], x['bbox']['left']))\n",
    "            right_sorted = sorted(right_elements, key=lambda x: (x['bbox']['top'], x['bbox']['left']))\n",
    "            \n",
    "            # ì¢Œì—´ â†’ ìš°ì—´ ìˆœì„œë¡œ ê²°í•©\n",
    "            return left_sorted + right_sorted\n",
    "    \n",
    "    else:\n",
    "        # ë‹¨ì¼ ì—´: ìƒí•˜ ìˆœì„œ\n",
    "        sorted_elements = sorted(text_elements, key=lambda x: (x['bbox']['top'], x['bbox']['left']))\n",
    "        return sorted_elements\n",
    "\n",
    "print(\"ğŸ“¦ í•œêµ­ì–´ ë ˆì´ì•„ì›ƒ ë¶„ì„ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… analyze_korean_layout: Azure ê²°ê³¼ì—ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„\")\n",
    "print(\"   âœ… determine_layout_type_korean: í•œêµ­ì–´ ë¬¸ì„œ ë ˆì´ì•„ì›ƒ íƒ€ì… ê²°ì •\") \n",
    "print(\"   âœ… reconstruct_korean_reading_order: í•œêµ­ì–´ ì½ê¸° ìˆœì„œ ì¬êµ¬ì„±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c5a78",
   "metadata": {},
   "source": [
    "## 5. ë‹¤ì¤‘ í˜ì´ì§€ ì§€ì› í…ìŠ¤íŠ¸ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9381535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Azure Document Intelligence í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… extract_text_with_azure: ë©”ì¸ ì¶”ì¶œ í•¨ìˆ˜\n",
      "   âœ… extract_text_sync_azure: ë™ê¸° ì²˜ë¦¬\n",
      "   âœ… extract_text_async_azure: ë¹„ë™ê¸° ì²˜ë¦¬\n",
      "   âœ… process_azure_result: ê²°ê³¼ ì²˜ë¦¬ ë° í•œêµ­ì–´ ìµœì í™”\n",
      "   âœ… extract_table_data_azure: í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ\n"
     ]
    }
   ],
   "source": [
    "# Azure Document Intelligence í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ë“¤\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_text_with_azure(file_path, strategy=None):\n",
    "    \"\"\"Azure Document Intelligenceë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    if not azure_client:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'Azure Document Intelligence í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',\n",
    "            'extraction_method': 'azure_failed'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸš€ Azure Document Intelligence í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘\")\n",
    "        print(f\"   ğŸ“„ íŒŒì¼: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # ê¸°ë³¸ ì „ëµ ì„¤ì •\n",
    "        if not strategy:\n",
    "            complexity_info = detect_document_complexity_azure(file_path)\n",
    "            strategy = determine_azure_processing_strategy(complexity_info)\n",
    "        \n",
    "        print(f\"   ğŸ”§ ëª¨ë¸: {strategy['model_id']}\")\n",
    "        print(f\"   âš¡ ì²˜ë¦¬: {'ë¹„ë™ê¸°' if strategy['is_async'] else 'ë™ê¸°'}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # íŒŒì¼ ì½ê¸°\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_content = file.read()\n",
    "        \n",
    "        if strategy['is_async']:\n",
    "            # ë¹„ë™ê¸° ì²˜ë¦¬\n",
    "            result = extract_text_async_azure(azure_client, file_content, strategy)\n",
    "        else:\n",
    "            # ë™ê¸° ì²˜ë¦¬  \n",
    "            result = extract_text_sync_azure(azure_client, file_content, strategy)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(result['text'])}ì ({processing_time:.1f}ì´ˆ)\")\n",
    "            result['processing_time'] = processing_time\n",
    "            result['strategy'] = strategy\n",
    "        else:\n",
    "            print(f\"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {result['error']}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Azure í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'extraction_method': 'azure_exception'\n",
    "        }\n",
    "\n",
    "def extract_text_sync_azure(client, file_content, strategy):\n",
    "    \"\"\"Azure Document Intelligence ë™ê¸° í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "        \n",
    "        print(\"   ğŸ”„ ë™ê¸° ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ë¬¸ì„œ ë¶„ì„ ìš”ì²­\n",
    "        analyze_request = AnalyzeDocumentRequest(bytes_source=file_content)\n",
    "        \n",
    "        poller = client.begin_analyze_document(\n",
    "            model_id=strategy['model_id'],\n",
    "            analyze_request=analyze_request\n",
    "        )\n",
    "        \n",
    "        # ê²°ê³¼ ëŒ€ê¸°\n",
    "        azure_result = poller.result()\n",
    "        \n",
    "        # ê²°ê³¼ ì²˜ë¦¬\n",
    "        return process_azure_result(azure_result, strategy)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f'ë™ê¸° ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',\n",
    "            'extraction_method': 'azure_sync_failed'\n",
    "        }\n",
    "\n",
    "def extract_text_async_azure(client, file_content, strategy):\n",
    "    \"\"\"Azure Document Intelligence ë¹„ë™ê¸° í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "        \n",
    "        print(\"   ğŸ”„ ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œì‘...\")\n",
    "        \n",
    "        # ë¬¸ì„œ ë¶„ì„ ìš”ì²­\n",
    "        analyze_request = AnalyzeDocumentRequest(bytes_source=file_content)\n",
    "        \n",
    "        poller = client.begin_analyze_document(\n",
    "            model_id=strategy['model_id'],\n",
    "            analyze_request=analyze_request\n",
    "        )\n",
    "        \n",
    "        # í´ë§ìœ¼ë¡œ ì™„ë£Œ ëŒ€ê¸°\n",
    "        max_wait_time = 300  # 5ë¶„\n",
    "        wait_interval = 5\n",
    "        total_waited = 0\n",
    "        \n",
    "        while not poller.done():\n",
    "            if total_waited >= max_wait_time:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': f'ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼ ({max_wait_time}ì´ˆ)',\n",
    "                    'extraction_method': 'azure_async_timeout'\n",
    "                }\n",
    "            \n",
    "            print(f\"   â³ ì²˜ë¦¬ ì¤‘... ({total_waited}ì´ˆ ê²½ê³¼)\")\n",
    "            time.sleep(wait_interval)\n",
    "            total_waited += wait_interval\n",
    "        \n",
    "        # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "        azure_result = poller.result()\n",
    "        \n",
    "        print(f\"   âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ë£Œ ({total_waited}ì´ˆ)\")\n",
    "        \n",
    "        # ê²°ê³¼ ì²˜ë¦¬\n",
    "        return process_azure_result(azure_result, strategy)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f'ë¹„ë™ê¸° ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',\n",
    "            'extraction_method': 'azure_async_failed'\n",
    "        }\n",
    "\n",
    "def process_azure_result(azure_result, strategy):\n",
    "    \"\"\"Azure Document Intelligence ê²°ê³¼ ì²˜ë¦¬\"\"\"\n",
    "    try:\n",
    "        print(\"   ğŸ“Š ê²°ê³¼ ë¶„ì„ ì¤‘...\")\n",
    "        \n",
    "        result = {\n",
    "            'success': True,\n",
    "            'text': '',\n",
    "            'page_results': [],\n",
    "            'tables': [],\n",
    "            'key_value_pairs': [],\n",
    "            'metadata': {\n",
    "                'extraction_method': 'azure_document_intelligence',\n",
    "                'model_used': strategy['model_id'],\n",
    "                'page_count': 0,\n",
    "                'confidence_stats': {},\n",
    "                'layout_analysis': {}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        if hasattr(azure_result, 'content'):\n",
    "            result['text'] = azure_result.content\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ ë¶„ì„\n",
    "        if hasattr(azure_result, 'pages') and azure_result.pages:\n",
    "            result['metadata']['page_count'] = len(azure_result.pages)\n",
    "            \n",
    "            for page_idx, page in enumerate(azure_result.pages):\n",
    "                page_number = page_idx + 1\n",
    "                \n",
    "                # í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ë¶„ì„\n",
    "                layout_info = analyze_korean_layout(azure_result, page_number)\n",
    "                \n",
    "                # í•œêµ­ì–´ ì½ê¸° ìˆœì„œ ì¬êµ¬ì„±\n",
    "                ordered_elements = reconstruct_korean_reading_order(layout_info)\n",
    "                \n",
    "                # í˜ì´ì§€ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "                page_text = '\\\\n'.join([elem['text'] for elem in ordered_elements])\n",
    "                \n",
    "                page_result = {\n",
    "                    'page': page_number,\n",
    "                    'text': page_text,\n",
    "                    'layout_type': layout_info['layout_type'],\n",
    "                    'element_count': len(ordered_elements),\n",
    "                    'table_count': len(layout_info['table_elements']),\n",
    "                    'paragraph_count': len(layout_info['paragraph_elements'])\n",
    "                }\n",
    "                \n",
    "                # ì‹ ë¢°ë„ í†µê³„\n",
    "                confidences = [elem['confidence'] for elem in ordered_elements if elem.get('confidence')]\n",
    "                if confidences:\n",
    "                    page_result['avg_confidence'] = sum(confidences) / len(confidences)\n",
    "                    page_result['min_confidence'] = min(confidences)\n",
    "                    page_result['max_confidence'] = max(confidences)\n",
    "                \n",
    "                result['page_results'].append(page_result)\n",
    "        \n",
    "        # í…Œì´ë¸” ì¶”ì¶œ\n",
    "        if hasattr(azure_result, 'tables') and azure_result.tables:\n",
    "            for table in azure_result.tables:\n",
    "                table_data = extract_table_data_azure(table)\n",
    "                result['tables'].append(table_data)\n",
    "        \n",
    "        # í‚¤-ê°’ ìŒ ì¶”ì¶œ\n",
    "        if hasattr(azure_result, 'key_value_pairs') and azure_result.key_value_pairs:\n",
    "            for kv in azure_result.key_value_pairs:\n",
    "                if kv.key and kv.value:\n",
    "                    kv_data = {\n",
    "                        'key': kv.key.content if hasattr(kv.key, 'content') else str(kv.key),\n",
    "                        'value': kv.value.content if hasattr(kv.value, 'content') else str(kv.value),\n",
    "                        'confidence': getattr(kv, 'confidence', 1.0)\n",
    "                    }\n",
    "                    result['key_value_pairs'].append(kv_data)\n",
    "        \n",
    "        # ì „ì²´ ì‹ ë¢°ë„ í†µê³„\n",
    "        all_confidences = []\n",
    "        for page_result in result['page_results']:\n",
    "            if 'avg_confidence' in page_result:\n",
    "                all_confidences.append(page_result['avg_confidence'])\n",
    "        \n",
    "        if all_confidences:\n",
    "            result['metadata']['confidence_stats'] = {\n",
    "                'average': sum(all_confidences) / len(all_confidences),\n",
    "                'min': min(all_confidences),\n",
    "                'max': max(all_confidences)\n",
    "            }\n",
    "        \n",
    "        print(f\"   ğŸ“„ í˜ì´ì§€: {result['metadata']['page_count']}ê°œ\")\n",
    "        print(f\"   ğŸ“Š í…Œì´ë¸”: {len(result['tables'])}ê°œ\")\n",
    "        print(f\"   ğŸ”‘ í‚¤-ê°’ ìŒ: {len(result['key_value_pairs'])}ê°œ\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f'ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',\n",
    "            'extraction_method': 'azure_result_processing_failed'\n",
    "        }\n",
    "\n",
    "def extract_table_data_azure(table):\n",
    "    \"\"\"Azure Document Intelligence í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        table_data = {\n",
    "            'row_count': table.row_count,\n",
    "            'column_count': table.column_count,\n",
    "            'cells': [],\n",
    "            'tsv': '',\n",
    "            'csv': ''\n",
    "        }\n",
    "        \n",
    "        # ì…€ ë°ì´í„° ìˆ˜ì§‘\n",
    "        cells_grid = {}\n",
    "        for cell in table.cells:\n",
    "            row_idx = cell.row_index\n",
    "            col_idx = cell.column_index\n",
    "            \n",
    "            if row_idx not in cells_grid:\n",
    "                cells_grid[row_idx] = {}\n",
    "            \n",
    "            cells_grid[row_idx][col_idx] = {\n",
    "                'content': cell.content,\n",
    "                'row_span': getattr(cell, 'row_span', 1),\n",
    "                'column_span': getattr(cell, 'column_span', 1),\n",
    "                'confidence': getattr(cell, 'confidence', 1.0)\n",
    "            }\n",
    "        \n",
    "        # TSV/CSV ìƒì„±\n",
    "        tsv_rows = []\n",
    "        csv_rows = []\n",
    "        \n",
    "        for row_idx in sorted(cells_grid.keys()):\n",
    "            row_data = cells_grid[row_idx]\n",
    "            \n",
    "            tsv_row = []\n",
    "            csv_row = []\n",
    "            \n",
    "            for col_idx in range(table.column_count):\n",
    "                cell_content = row_data.get(col_idx, {}).get('content', '')\n",
    "                tsv_row.append(cell_content)\n",
    "                csv_row.append(f'\\\"{cell_content}\\\"' if ',' in cell_content else cell_content)\n",
    "            \n",
    "            tsv_rows.append('\\\\t'.join(tsv_row))\n",
    "            csv_rows.append(','.join(csv_row))\n",
    "        \n",
    "        table_data['tsv'] = '\\\\n'.join(tsv_rows)\n",
    "        table_data['csv'] = '\\\\n'.join(csv_rows)\n",
    "        table_data['cells'] = cells_grid\n",
    "        \n",
    "        return table_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': f'í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}',\n",
    "            'cells': [],\n",
    "            'tsv': '',\n",
    "            'csv': ''\n",
    "        }\n",
    "\n",
    "print(\"ğŸ“¦ Azure Document Intelligence í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… extract_text_with_azure: ë©”ì¸ ì¶”ì¶œ í•¨ìˆ˜\")\n",
    "print(\"   âœ… extract_text_sync_azure: ë™ê¸° ì²˜ë¦¬\")\n",
    "print(\"   âœ… extract_text_async_azure: ë¹„ë™ê¸° ì²˜ë¦¬\")\n",
    "print(\"   âœ… process_azure_result: ê²°ê³¼ ì²˜ë¦¬ ë° í•œêµ­ì–´ ìµœì í™”\")\n",
    "print(\"   âœ… extract_table_data_azure: í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792dc81",
   "metadata": {},
   "source": [
    "## 6. í•œêµ­ì–´ ìµœì í™” í…ìŠ¤íŠ¸ ì²­í‚¹ ë° RAG ì¤€ë¹„\n",
    "\n",
    "Azure Document Intelligenceë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ íŠ¹ì„±ì— ë§ê²Œ ì²­í‚¹í•˜ê³  RAG(Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ì— ì í•©í•˜ë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ê¸°ëŠ¥:\n",
    "- í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹\n",
    "- ë¬¸ì„œ êµ¬ì¡° ì¸ì‹ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´\n",
    "- í˜ì´ì§€ë³„/ì„¹ì…˜ë³„ ë©”íƒ€ë°ì´í„° ìœ ì§€\n",
    "- ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "- RAG ë°±ì—”ë“œ ì—°ë™ì„ ìœ„í•œ ë°ì´í„° í¬ë§·íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4493ce36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ í•œêµ­ì–´ ìµœì í™” í…ìŠ¤íŠ¸ ì²­í‚¹ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… prepare_text_for_rag_azure: RAGìš© í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
      "   âœ… create_korean_chunks_azure: í•œêµ­ì–´ ì²­í‚¹\n",
      "   âœ… analyze_morpheme_groups_azure: í˜•íƒœì†Œ ë¶„ì„ ê·¸ë£¹í™”\n",
      "   âœ… extract_keywords_from_chunk_azure: í‚¤ì›Œë“œ ì¶”ì¶œ\n"
     ]
    }
   ],
   "source": [
    "# í•œêµ­ì–´ ìµœì í™” í…ìŠ¤íŠ¸ ì²­í‚¹ ë° RAG ì¤€ë¹„ í•¨ìˆ˜ë“¤\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def prepare_text_for_rag_azure(extraction_result, chunk_size=512, chunk_overlap=50):\n",
    "    \"\"\"Azure ì¶”ì¶œ ê²°ê³¼ë¥¼ RAGìš©ìœ¼ë¡œ ì²­í‚¹ ë° ì¤€ë¹„\"\"\"\n",
    "    if not extraction_result.get('success'):\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'Azure ì¶”ì¶œ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤',\n",
    "            'chunks': []\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ”§ RAGìš© í…ìŠ¤íŠ¸ ì¤€ë¹„ ì‹œì‘\")\n",
    "        print(f\"   ğŸ“Š ì²­í¬ í¬ê¸°: {chunk_size} í† í°\")\n",
    "        print(f\"   ğŸ”— ì˜¤ë²„ë©: {chunk_overlap} í† í°\")\n",
    "        \n",
    "        chunks = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ ì²­í‚¹\n",
    "        for page_result in extraction_result.get('page_results', []):\n",
    "            page_chunks = create_korean_chunks_azure(\n",
    "                page_result, \n",
    "                chunk_size, \n",
    "                chunk_overlap,\n",
    "                extraction_result.get('metadata', {})\n",
    "            )\n",
    "            chunks.extend(page_chunks)\n",
    "            total_tokens += sum(chunk['token_count'] for chunk in page_chunks)\n",
    "        \n",
    "        # í…Œì´ë¸” ë³„ë„ ì²˜ë¦¬\n",
    "        if extraction_result.get('tables'):\n",
    "            table_chunks = process_tables_for_rag_azure(\n",
    "                extraction_result['tables'], \n",
    "                chunk_size\n",
    "            )\n",
    "            chunks.extend(table_chunks)\n",
    "            total_tokens += sum(chunk['token_count'] for chunk in table_chunks)\n",
    "        \n",
    "        # í‚¤-ê°’ ìŒ ë³„ë„ ì²˜ë¦¬\n",
    "        if extraction_result.get('key_value_pairs'):\n",
    "            kv_chunks = process_key_values_for_rag_azure(\n",
    "                extraction_result['key_value_pairs']\n",
    "            )\n",
    "            chunks.extend(kv_chunks)\n",
    "            total_tokens += sum(chunk['token_count'] for chunk in kv_chunks)\n",
    "        \n",
    "        # ì²­í¬ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ \n",
    "        validated_chunks = validate_and_improve_chunks_azure(chunks)\n",
    "        \n",
    "        print(f\"âœ… ì²­í‚¹ ì™„ë£Œ: {len(validated_chunks)}ê°œ ì²­í¬ ({total_tokens} í† í°)\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'chunks': validated_chunks,\n",
    "            'total_chunks': len(validated_chunks),\n",
    "            'total_tokens': total_tokens,\n",
    "            'avg_chunk_size': total_tokens / len(validated_chunks) if validated_chunks else 0,\n",
    "            'processing_metadata': {\n",
    "                'source': 'azure_document_intelligence',\n",
    "                'chunk_strategy': 'korean_morphological_aware',\n",
    "                'chunk_size': chunk_size,\n",
    "                'chunk_overlap': chunk_overlap\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ RAG ì¤€ë¹„ ì‹¤íŒ¨: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'chunks': []\n",
    "        }\n",
    "\n",
    "def create_korean_chunks_azure(page_result, chunk_size, chunk_overlap, document_metadata):\n",
    "    \"\"\"í•œêµ­ì–´ íŠ¹ì„±ì„ ê³ ë ¤í•œ í˜ì´ì§€ë³„ ì²­í‚¹\"\"\"\n",
    "    try:\n",
    "        page_text = page_result.get('text', '')\n",
    "        page_number = page_result.get('page', 1)\n",
    "        \n",
    "        if not page_text.strip():\n",
    "            return []\n",
    "        \n",
    "        print(f\"   ğŸ“„ í˜ì´ì§€ {page_number} ì²­í‚¹ ì¤‘...\")\n",
    "        \n",
    "        # í•œêµ­ì–´ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• \n",
    "        sentences = split_korean_sentences_azure(page_text)\n",
    "        \n",
    "        # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ì˜ë¯¸ ë‹¨ìœ„ ì‹ë³„\n",
    "        morpheme_groups = analyze_morpheme_groups_azure(sentences)\n",
    "        \n",
    "        # ì²­í¬ ìƒì„±\n",
    "        chunks = []\n",
    "        current_chunk = {\n",
    "            'text': '',\n",
    "            'sentences': [],\n",
    "            'token_count': 0,\n",
    "            'morpheme_count': 0\n",
    "        }\n",
    "        \n",
    "        for group in morpheme_groups:\n",
    "            group_tokens = count_tokens_korean(group['text'])\n",
    "            \n",
    "            # ì²­í¬ í¬ê¸° ì²´í¬\n",
    "            if current_chunk['token_count'] + group_tokens > chunk_size and current_chunk['text']:\n",
    "                # í˜„ì¬ ì²­í¬ ì™„ì„±\n",
    "                chunk_data = finalize_chunk_azure(\n",
    "                    current_chunk, \n",
    "                    page_number, \n",
    "                    page_result,\n",
    "                    document_metadata,\n",
    "                    len(chunks) + 1\n",
    "                )\n",
    "                chunks.append(chunk_data)\n",
    "                \n",
    "                # ì˜¤ë²„ë©ì„ ìœ„í•œ ë¬¸ì¥ ìœ ì§€\n",
    "                overlap_sentences = current_chunk['sentences'][-2:] if len(current_chunk['sentences']) > 1 else []\n",
    "                overlap_text = ' '.join(overlap_sentences)\n",
    "                overlap_tokens = count_tokens_korean(overlap_text)\n",
    "                \n",
    "                # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© í¬í•¨)\n",
    "                current_chunk = {\n",
    "                    'text': overlap_text,\n",
    "                    'sentences': overlap_sentences,\n",
    "                    'token_count': overlap_tokens,\n",
    "                    'morpheme_count': len(overlap_sentences)\n",
    "                }\n",
    "            \n",
    "            # ê·¸ë£¹ ì¶”ê°€\n",
    "            current_chunk['text'] += (' ' if current_chunk['text'] else '') + group['text']\n",
    "            current_chunk['sentences'].append(group['text'])\n",
    "            current_chunk['token_count'] += group_tokens\n",
    "            current_chunk['morpheme_count'] += group['morpheme_count']\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬\n",
    "        if current_chunk['text'].strip():\n",
    "            chunk_data = finalize_chunk_azure(\n",
    "                current_chunk, \n",
    "                page_number, \n",
    "                page_result,\n",
    "                document_metadata,\n",
    "                len(chunks) + 1\n",
    "            )\n",
    "            chunks.append(chunk_data)\n",
    "        \n",
    "        print(f\"   âœ… í˜ì´ì§€ {page_number}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ í˜ì´ì§€ {page_number} ì²­í‚¹ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "def split_korean_sentences_azure(text):\n",
    "    \"\"\"í•œêµ­ì–´ ë¬¸ì¥ ë¶„í•  (Azure ê²°ê³¼ ìµœì í™”)\"\"\"\n",
    "    try:\n",
    "        # ê¸°ë³¸ ë¬¸ì¥ ë¶„í•  íŒ¨í„´\n",
    "        sentence_endings = r'[.!?ã€‚]\\s+'\n",
    "        \n",
    "        # í•œêµ­ì–´ íŠ¹ìˆ˜ íŒ¨í„´ ì²˜ë¦¬\n",
    "        text = re.sub(r'([.!?ã€‚])\\s*([ê°€-í£A-Z])', r'\\\\1\\\\n\\\\2', text)\n",
    "        \n",
    "        # ì¤„ë°”ê¿ˆ ê¸°ë°˜ ë¶„í• \n",
    "        sentences = []\n",
    "        for line in text.split('\\\\n'):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # ë¬¸ì¥ ë¶€í˜¸ ê¸°ë°˜ ì„¸ë¶€ ë¶„í• \n",
    "                sub_sentences = re.split(sentence_endings, line)\n",
    "                sentences.extend([s.strip() for s in sub_sentences if s.strip()])\n",
    "        \n",
    "        return sentences\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ë¬¸ì¥ ë¶„í•  ì‹¤íŒ¨: {e}\")\n",
    "        return [text]\n",
    "\n",
    "def analyze_morpheme_groups_azure(sentences):\n",
    "    \"\"\"í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ì˜ë¯¸ ë‹¨ìœ„ ê·¸ë£¹í™”\"\"\"\n",
    "    try:\n",
    "        groups = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if not sentence.strip():\n",
    "                continue\n",
    "                \n",
    "            # í˜•íƒœì†Œ ë¶„ì„\n",
    "            morphemes = kiwi.analyze(sentence)\n",
    "            \n",
    "            # ì˜ë¯¸ ìˆëŠ” í˜•íƒœì†Œ ì¶”ì¶œ\n",
    "            meaningful_morphemes = []\n",
    "            for token, pos, _, _ in morphemes[0]:\n",
    "                if pos in ['NNG', 'NNP', 'VV', 'VA', 'MAG']:  # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬\n",
    "                    meaningful_morphemes.append(token)\n",
    "            \n",
    "            group = {\n",
    "                'text': sentence,\n",
    "                'morpheme_count': len(meaningful_morphemes),\n",
    "                'key_morphemes': meaningful_morphemes[:5],  # ìƒìœ„ 5ê°œ\n",
    "                'sentence_type': classify_sentence_type_azure(sentence),\n",
    "                'importance_score': calculate_importance_score_azure(sentence, meaningful_morphemes)\n",
    "            }\n",
    "            \n",
    "            groups.append(group)\n",
    "        \n",
    "        return groups\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return [{'text': ' '.join(sentences), 'morpheme_count': 0, 'key_morphemes': []}]\n",
    "\n",
    "def classify_sentence_type_azure(sentence):\n",
    "    \"\"\"ë¬¸ì¥ ìœ í˜• ë¶„ë¥˜\"\"\"\n",
    "    try:\n",
    "        if re.search(r'[0-9]+[.]', sentence):\n",
    "            return 'numbered_list'\n",
    "        elif re.search(r'^[â€¢â–ªâ–«-]', sentence.strip()):\n",
    "            return 'bullet_point'\n",
    "        elif '?' in sentence:\n",
    "            return 'question'\n",
    "        elif re.search(r'í‘œ[\\\\s]*[0-9]+|ê·¸ë¦¼[\\\\s]*[0-9]+|ë„[\\\\s]*[0-9]+', sentence):\n",
    "            return 'figure_reference'\n",
    "        elif len(sentence) > 100:\n",
    "            return 'paragraph'\n",
    "        else:\n",
    "            return 'sentence'\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def calculate_importance_score_azure(sentence, morphemes):\n",
    "    \"\"\"ë¬¸ì¥ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    try:\n",
    "        score = 0\n",
    "        \n",
    "        # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜\n",
    "        score += min(len(sentence) / 50, 2.0)\n",
    "        \n",
    "        # í˜•íƒœì†Œ ë‹¤ì–‘ì„± ì ìˆ˜\n",
    "        score += len(set(morphemes)) * 0.1\n",
    "        \n",
    "        # íŠ¹ìˆ˜ í‚¤ì›Œë“œ ì ìˆ˜\n",
    "        important_patterns = [\n",
    "            r'ê²°ë¡ |ìš”ì•½|ì •ë¦¬',\n",
    "            r'ì¤‘ìš”|í•µì‹¬|ì£¼ìš”',\n",
    "            r'ì²«ì§¸|ë‘˜ì§¸|ì…‹ì§¸',\n",
    "            r'ë”°ë¼ì„œ|ê·¸ëŸ¬ë¯€ë¡œ|ê²°êµ­'\n",
    "        ]\n",
    "        \n",
    "        for pattern in important_patterns:\n",
    "            if re.search(pattern, sentence):\n",
    "                score += 1.0\n",
    "        \n",
    "        return min(score, 5.0)  # ìµœëŒ€ 5ì \n",
    "        \n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "def finalize_chunk_azure(chunk_data, page_number, page_result, document_metadata, chunk_index):\n",
    "    \"\"\"ì²­í¬ ë°ì´í„° ìµœì¢… ì²˜ë¦¬\"\"\"\n",
    "    try:\n",
    "        # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        keywords = extract_keywords_from_chunk_azure(chunk_data['text'])\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ìƒì„±\n",
    "        metadata = {\n",
    "            'page_number': page_number,\n",
    "            'chunk_index': chunk_index,\n",
    "            'layout_type': page_result.get('layout_type', 'unknown'),\n",
    "            'confidence': page_result.get('avg_confidence', 1.0),\n",
    "            'extraction_method': 'azure_document_intelligence',\n",
    "            'document_metadata': document_metadata,\n",
    "            'processing_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'text': chunk_data['text'].strip(),\n",
    "            'token_count': chunk_data['token_count'],\n",
    "            'morpheme_count': chunk_data['morpheme_count'],\n",
    "            'sentence_count': len(chunk_data['sentences']),\n",
    "            'keywords': keywords,\n",
    "            'metadata': metadata,\n",
    "            'chunk_type': 'text_content'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ì²­í¬ ì™„ì„± ì‹¤íŒ¨: {e}\")\n",
    "        return {\n",
    "            'text': chunk_data['text'],\n",
    "            'token_count': chunk_data['token_count'],\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def extract_keywords_from_chunk_azure(text):\n",
    "    \"\"\"ì²­í¬ì—ì„œ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        # í˜•íƒœì†Œ ë¶„ì„\n",
    "        morphemes = kiwi.analyze(text)\n",
    "        \n",
    "        # ëª…ì‚¬ ì¶”ì¶œ\n",
    "        nouns = []\n",
    "        for token, pos, _, _ in morphemes[0]:\n",
    "            if pos in ['NNG', 'NNP'] and len(token) > 1:\n",
    "                nouns.append(token)\n",
    "        \n",
    "        # ë¹ˆë„ ê³„ì‚°\n",
    "        noun_freq = defaultdict(int)\n",
    "        for noun in nouns:\n",
    "            noun_freq[noun] += 1\n",
    "        \n",
    "        # ìƒìœ„ í‚¤ì›Œë“œ ì„ ë³„ (ë¹ˆë„ + ê¸¸ì´ ê³ ë ¤)\n",
    "        keywords = sorted(\n",
    "            noun_freq.items(),\n",
    "            key=lambda x: x[1] * len(x[0]),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return [word for word, freq in keywords[:10]]  # ìƒìœ„ 10ê°œ\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"ğŸ“¦ í•œêµ­ì–´ ìµœì í™” í…ìŠ¤íŠ¸ ì²­í‚¹ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… prepare_text_for_rag_azure: RAGìš© í…ìŠ¤íŠ¸ ì¤€ë¹„\")\n",
    "print(\"   âœ… create_korean_chunks_azure: í•œêµ­ì–´ ì²­í‚¹\")\n",
    "print(\"   âœ… analyze_morpheme_groups_azure: í˜•íƒœì†Œ ë¶„ì„ ê·¸ë£¹í™”\")\n",
    "print(\"   âœ… extract_keywords_from_chunk_azure: í‚¤ì›Œë“œ ì¶”ì¶œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e489d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ í…Œì´ë¸” ë° í‚¤-ê°’ ìŒ RAG ì²˜ë¦¬ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… process_tables_for_rag_azure: í…Œì´ë¸” RAG ì²˜ë¦¬\n",
      "   âœ… process_key_values_for_rag_azure: í‚¤-ê°’ ìŒ RAG ì²˜ë¦¬\n",
      "   âœ… validate_and_improve_chunks_azure: ì²­í¬ í’ˆì§ˆ ê²€ì¦\n"
     ]
    }
   ],
   "source": [
    "# í…Œì´ë¸” ë° í‚¤-ê°’ ìŒ RAG ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "\n",
    "def process_tables_for_rag_azure(tables, chunk_size):\n",
    "    \"\"\"í…Œì´ë¸”ì„ RAGìš© ì²­í¬ë¡œ ë³€í™˜\"\"\"\n",
    "    try:\n",
    "        chunks = []\n",
    "        \n",
    "        for table_idx, table in enumerate(tables):\n",
    "            print(f\"   ğŸ“Š í…Œì´ë¸” {table_idx + 1} ì²˜ë¦¬ ì¤‘...\")\n",
    "            \n",
    "            # í…Œì´ë¸” ìš”ì•½ ìƒì„±\n",
    "            table_summary = create_table_summary_azure(table)\n",
    "            \n",
    "            # í…Œì´ë¸” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "            table_text = convert_table_to_text_azure(table)\n",
    "            \n",
    "            # í° í…Œì´ë¸”ì€ í–‰ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "            if count_tokens_korean(table_text) > chunk_size:\n",
    "                row_chunks = split_large_table_azure(table, chunk_size)\n",
    "                chunks.extend(row_chunks)\n",
    "            else:\n",
    "                # ì „ì²´ í…Œì´ë¸”ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ\n",
    "                chunk = {\n",
    "                    'text': table_text,\n",
    "                    'token_count': count_tokens_korean(table_text),\n",
    "                    'chunk_type': 'table',\n",
    "                    'metadata': {\n",
    "                        'table_index': table_idx,\n",
    "                        'row_count': table.get('row_count', 0),\n",
    "                        'column_count': table.get('column_count', 0),\n",
    "                        'table_summary': table_summary,\n",
    "                        'extraction_method': 'azure_table'\n",
    "                    },\n",
    "                    'keywords': extract_keywords_from_chunk_azure(table_text)\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        print(f\"   âœ… í…Œì´ë¸” ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬\")\n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ í…Œì´ë¸” ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_table_summary_azure(table):\n",
    "    \"\"\"í…Œì´ë¸” ìš”ì•½ ìƒì„±\"\"\"\n",
    "    try:\n",
    "        summary = f\"í‘œ ì •ë³´: {table.get('row_count', 0)}í–‰ {table.get('column_count', 0)}ì—´\"\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ê°€ì •í•˜ê³  ìš”ì•½\n",
    "        cells = table.get('cells', {})\n",
    "        if 0 in cells:  # ì²« ë²ˆì§¸ í–‰ ì¡´ì¬\n",
    "            headers = []\n",
    "            for col_idx in range(table.get('column_count', 0)):\n",
    "                if col_idx in cells[0]:\n",
    "                    headers.append(cells[0][col_idx]['content'])\n",
    "            \n",
    "            if headers:\n",
    "                summary += f\", ì»¬ëŸ¼: {', '.join(headers[:5])}\"  # ìƒìœ„ 5ê°œ ì»¬ëŸ¼ë§Œ\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"í‘œ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}\"\n",
    "\n",
    "def convert_table_to_text_azure(table):\n",
    "    \"\"\"í…Œì´ë¸”ì„ í•œêµ­ì–´ ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "    try:\n",
    "        cells = table.get('cells', {})\n",
    "        if not cells:\n",
    "            return \"ë¹ˆ í…Œì´ë¸”\"\n",
    "        \n",
    "        text_parts = []\n",
    "        text_parts.append(f\"ë‹¤ìŒì€ {table.get('row_count', 0)}í–‰ {table.get('column_count', 0)}ì—´ì˜ í‘œì…ë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì²˜ë¦¬\n",
    "        if 0 in cells:\n",
    "            headers = []\n",
    "            for col_idx in range(table.get('column_count', 0)):\n",
    "                if col_idx in cells[0]:\n",
    "                    headers.append(cells[0][col_idx]['content'])\n",
    "            \n",
    "            if headers:\n",
    "                text_parts.append(f\"ì»¬ëŸ¼ ì œëª©: {', '.join(headers)}\")\n",
    "        \n",
    "        # ë°ì´í„° í–‰ë“¤ì„ ìì—°ì–´ë¡œ ë³€í™˜\n",
    "        for row_idx in range(1, table.get('row_count', 0)):  # í—¤ë” ì œì™¸\n",
    "            if row_idx in cells:\n",
    "                row_data = []\n",
    "                for col_idx in range(table.get('column_count', 0)):\n",
    "                    if col_idx in cells[row_idx]:\n",
    "                        content = cells[row_idx][col_idx]['content']\n",
    "                        if content.strip():\n",
    "                            row_data.append(content)\n",
    "                \n",
    "                if row_data:\n",
    "                    text_parts.append(f\"{row_idx}í–‰: {', '.join(row_data)}\")\n",
    "        \n",
    "        return '\\\\n'.join(text_parts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"í…Œì´ë¸” í…ìŠ¤íŠ¸ ë³€í™˜ ì‹¤íŒ¨: {str(e)}\"\n",
    "\n",
    "def split_large_table_azure(table, chunk_size):\n",
    "    \"\"\"í° í…Œì´ë¸”ì„ í–‰ ë‹¨ìœ„ë¡œ ë¶„í• \"\"\"\n",
    "    try:\n",
    "        chunks = []\n",
    "        cells = table.get('cells', {})\n",
    "        \n",
    "        # í—¤ë” ì •ë³´\n",
    "        headers = []\n",
    "        if 0 in cells:\n",
    "            for col_idx in range(table.get('column_count', 0)):\n",
    "                if col_idx in cells[0]:\n",
    "                    headers.append(cells[0][col_idx]['content'])\n",
    "        \n",
    "        header_text = f\"ì»¬ëŸ¼ ì œëª©: {', '.join(headers)}\" if headers else \"\"\n",
    "        header_tokens = count_tokens_korean(header_text)\n",
    "        \n",
    "        current_chunk_rows = []\n",
    "        current_tokens = header_tokens\n",
    "        \n",
    "        # ë°ì´í„° í–‰ë“¤ì„ ì²­í¬ë¡œ ë¶„í• \n",
    "        for row_idx in range(1, table.get('row_count', 0)):\n",
    "            if row_idx in cells:\n",
    "                row_data = []\n",
    "                for col_idx in range(table.get('column_count', 0)):\n",
    "                    if col_idx in cells[row_idx]:\n",
    "                        content = cells[row_idx][col_idx]['content']\n",
    "                        if content.strip():\n",
    "                            row_data.append(content)\n",
    "                \n",
    "                if row_data:\n",
    "                    row_text = f\"{row_idx}í–‰: {', '.join(row_data)}\"\n",
    "                    row_tokens = count_tokens_korean(row_text)\n",
    "                    \n",
    "                    # ì²­í¬ í¬ê¸° ì²´í¬\n",
    "                    if current_tokens + row_tokens > chunk_size and current_chunk_rows:\n",
    "                        # í˜„ì¬ ì²­í¬ ì™„ì„±\n",
    "                        chunk_text = header_text + '\\\\n' + '\\\\n'.join(current_chunk_rows)\n",
    "                        chunk = {\n",
    "                            'text': chunk_text,\n",
    "                            'token_count': current_tokens,\n",
    "                            'chunk_type': 'table_partial',\n",
    "                            'metadata': {\n",
    "                                'table_rows': len(current_chunk_rows),\n",
    "                                'is_table_continuation': len(chunks) > 0\n",
    "                            }\n",
    "                        }\n",
    "                        chunks.append(chunk)\n",
    "                        \n",
    "                        # ìƒˆ ì²­í¬ ì‹œì‘\n",
    "                        current_chunk_rows = [row_text]\n",
    "                        current_tokens = header_tokens + row_tokens\n",
    "                    else:\n",
    "                        current_chunk_rows.append(row_text)\n",
    "                        current_tokens += row_tokens\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬\n",
    "        if current_chunk_rows:\n",
    "            chunk_text = header_text + '\\\\n' + '\\\\n'.join(current_chunk_rows)\n",
    "            chunk = {\n",
    "                'text': chunk_text,\n",
    "                'token_count': current_tokens,\n",
    "                'chunk_type': 'table_partial',\n",
    "                'metadata': {\n",
    "                    'table_rows': len(current_chunk_rows),\n",
    "                    'is_table_continuation': len(chunks) > 0\n",
    "                }\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"í° í…Œì´ë¸” ë¶„í•  ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_key_values_for_rag_azure(key_value_pairs):\n",
    "    \"\"\"í‚¤-ê°’ ìŒì„ RAGìš© ì²­í¬ë¡œ ë³€í™˜\"\"\"\n",
    "    try:\n",
    "        if not key_value_pairs:\n",
    "            return []\n",
    "        \n",
    "        print(f\"   ğŸ”‘ í‚¤-ê°’ ìŒ {len(key_value_pairs)}ê°œ ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # í‚¤-ê°’ ìŒë“¤ì„ ì˜ë¯¸ë³„ë¡œ ê·¸ë£¹í™”\n",
    "        grouped_kvs = group_key_values_by_context_azure(key_value_pairs)\n",
    "        \n",
    "        chunks = []\n",
    "        for group_name, kv_group in grouped_kvs.items():\n",
    "            # ê·¸ë£¹ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "            group_text = convert_key_values_to_text_azure(kv_group, group_name)\n",
    "            \n",
    "            chunk = {\n",
    "                'text': group_text,\n",
    "                'token_count': count_tokens_korean(group_text),\n",
    "                'chunk_type': 'key_value_pairs',\n",
    "                'metadata': {\n",
    "                    'kv_group': group_name,\n",
    "                    'kv_count': len(kv_group),\n",
    "                    'extraction_method': 'azure_form_recognizer'\n",
    "                },\n",
    "                'keywords': extract_keywords_from_chunk_azure(group_text)\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        print(f\"   âœ… í‚¤-ê°’ ìŒ ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ê·¸ë£¹\")\n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ í‚¤-ê°’ ìŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "def group_key_values_by_context_azure(key_value_pairs):\n",
    "    \"\"\"í‚¤-ê°’ ìŒì„ ì˜ë¯¸ì  ì»¨í…ìŠ¤íŠ¸ë¡œ ê·¸ë£¹í™”\"\"\"\n",
    "    try:\n",
    "        groups = defaultdict(list)\n",
    "        \n",
    "        for kv in key_value_pairs:\n",
    "            key = kv.get('key', '').lower()\n",
    "            \n",
    "            # í‚¤ íŒ¨í„´ ê¸°ë°˜ ê·¸ë£¹í™”\n",
    "            if any(pattern in key for pattern in ['ì´ë¦„', 'ì„±ëª…', 'ëª…ì¹­']):\n",
    "                groups['ê°œì¸ì •ë³´'].append(kv)\n",
    "            elif any(pattern in key for pattern in ['ë‚ ì§œ', 'ì¼ì‹œ', 'ì‹œê°„']):\n",
    "                groups['ë‚ ì§œì •ë³´'].append(kv)\n",
    "            elif any(pattern in key for pattern in ['ê¸ˆì•¡', 'ê°€ê²©', 'ë¹„ìš©', 'ì›']):\n",
    "                groups['ê¸ˆì•¡ì •ë³´'].append(kv)\n",
    "            elif any(pattern in key for pattern in ['ì£¼ì†Œ', 'ìœ„ì¹˜', 'ì¥ì†Œ']):\n",
    "                groups['ìœ„ì¹˜ì •ë³´'].append(kv)\n",
    "            elif any(pattern in key for pattern in ['ì „í™”', 'ë²ˆí˜¸', 'ì—°ë½ì²˜']):\n",
    "                groups['ì—°ë½ì²˜ì •ë³´'].append(kv)\n",
    "            else:\n",
    "                groups['ê¸°íƒ€ì •ë³´'].append(kv)\n",
    "        \n",
    "        return dict(groups)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"í‚¤-ê°’ ê·¸ë£¹í™” ì‹¤íŒ¨: {e}\")\n",
    "        return {'ì „ì²´': key_value_pairs}\n",
    "\n",
    "def convert_key_values_to_text_azure(kv_group, group_name):\n",
    "    \"\"\"í‚¤-ê°’ ìŒ ê·¸ë£¹ì„ ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "    try:\n",
    "        text_parts = [f\"ë‹¤ìŒì€ {group_name} ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\"]\n",
    "        \n",
    "        for kv in kv_group:\n",
    "            key = kv.get('key', '')\n",
    "            value = kv.get('value', '')\n",
    "            confidence = kv.get('confidence', 1.0)\n",
    "            \n",
    "            if key and value:\n",
    "                confidence_text = f\" (ì‹ ë¢°ë„: {confidence:.2f})\" if confidence < 0.9 else \"\"\n",
    "                text_parts.append(f\"- {key}: {value}{confidence_text}\")\n",
    "        \n",
    "        return '\\\\n'.join(text_parts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"{group_name} ì •ë³´ ë³€í™˜ ì‹¤íŒ¨: {str(e)}\"\n",
    "\n",
    "def validate_and_improve_chunks_azure(chunks):\n",
    "    \"\"\"ì²­í¬ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ \"\"\"\n",
    "    try:\n",
    "        validated_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # ìµœì†Œ ê¸¸ì´ ì²´í¬\n",
    "            if chunk.get('token_count', 0) < 10:\n",
    "                continue\n",
    "                \n",
    "            # í…ìŠ¤íŠ¸ ì •ì œ\n",
    "            cleaned_text = clean_chunk_text_azure(chunk['text'])\n",
    "            if not cleaned_text.strip():\n",
    "                continue\n",
    "                \n",
    "            # ì²­í¬ ê°œì„ \n",
    "            improved_chunk = improve_chunk_quality_azure(chunk, cleaned_text)\n",
    "            validated_chunks.append(improved_chunk)\n",
    "        \n",
    "        print(f\"   âœ… ì²­í¬ ê²€ì¦ ì™„ë£Œ: {len(validated_chunks)}/{len(chunks)}ê°œ ìœ íš¨\")\n",
    "        return validated_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ì²­í¬ ê²€ì¦ ì‹¤íŒ¨: {e}\")\n",
    "        return chunks\n",
    "\n",
    "def clean_chunk_text_azure(text):\n",
    "    \"\"\"ì²­í¬ í…ìŠ¤íŠ¸ ì •ì œ\"\"\"\n",
    "    try:\n",
    "        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "        text = re.sub(r'\\\\s+', ' ', text)\n",
    "        \n",
    "        # ë°˜ë³µë˜ëŠ” íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "        text = re.sub(r'[\\\\-_=]{3,}', '', text)\n",
    "        \n",
    "        # ì˜ë¯¸ì—†ëŠ” ì§§ì€ ë‹¨ì–´ ì œê±°\n",
    "        text = re.sub(r'\\\\b[a-zA-Z]{1,2}\\\\b', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return text\n",
    "\n",
    "def improve_chunk_quality_azure(chunk, cleaned_text):\n",
    "    \"\"\"ì²­í¬ í’ˆì§ˆ ê°œì„ \"\"\"\n",
    "    try:\n",
    "        improved_chunk = chunk.copy()\n",
    "        improved_chunk['text'] = cleaned_text\n",
    "        improved_chunk['token_count'] = count_tokens_korean(cleaned_text)\n",
    "        \n",
    "        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n",
    "        quality_score = calculate_chunk_quality_score_azure(improved_chunk)\n",
    "        improved_chunk['quality_score'] = quality_score\n",
    "        \n",
    "        return improved_chunk\n",
    "        \n",
    "    except Exception as e:\n",
    "        return chunk\n",
    "\n",
    "def calculate_chunk_quality_score_azure(chunk):\n",
    "    \"\"\"ì²­í¬ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    try:\n",
    "        score = 0.0\n",
    "        \n",
    "        # ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´)\n",
    "        token_count = chunk.get('token_count', 0)\n",
    "        if 50 <= token_count <= 800:\n",
    "            score += 2.0\n",
    "        elif token_count > 10:\n",
    "            score += 1.0\n",
    "        \n",
    "        # í‚¤ì›Œë“œ ë‹¤ì–‘ì„± ì ìˆ˜\n",
    "        keywords = chunk.get('keywords', [])\n",
    "        score += min(len(set(keywords)), 3) * 0.5\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ì ìˆ˜\n",
    "        metadata = chunk.get('metadata', {})\n",
    "        if metadata.get('page_number'):\n",
    "            score += 0.5\n",
    "        if metadata.get('confidence', 0) > 0.8:\n",
    "            score += 0.5\n",
    "        \n",
    "        return min(score, 5.0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 1.0\n",
    "\n",
    "print(\"ğŸ“¦ í…Œì´ë¸” ë° í‚¤-ê°’ ìŒ RAG ì²˜ë¦¬ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… process_tables_for_rag_azure: í…Œì´ë¸” RAG ì²˜ë¦¬\")\n",
    "print(\"   âœ… process_key_values_for_rag_azure: í‚¤-ê°’ ìŒ RAG ì²˜ë¦¬\")\n",
    "print(\"   âœ… validate_and_improve_chunks_azure: ì²­í¬ í’ˆì§ˆ ê²€ì¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71bd7b",
   "metadata": {},
   "source": [
    "## 7. ë°±ì—”ë“œ ì‹œìŠ¤í…œ ì—°ë™ ë° ì¸ì¦\n",
    "\n",
    "Azure Document Intelligenceë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œ ë°ì´í„°ë¥¼ ê¸°ì¡´ ë°±ì—”ë“œ ì‹œìŠ¤í…œê³¼ ì—°ë™í•˜ê³ , ì¸ì¦ ë° ì„¸ì…˜ ê´€ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ê¸°ëŠ¥:\n",
    "- ê¸°ì¡´ ë°±ì—”ë“œ API ì—”ë“œí¬ì¸íŠ¸ ì—°ë™\n",
    "- ì‚¬ìš©ì ì¸ì¦ ë° ì„¸ì…˜ ê´€ë¦¬\n",
    "- ë¬¸ì„œ ì—…ë¡œë“œ ë° ì²˜ë¦¬ ìƒíƒœ ì¶”ì \n",
    "- RAG ë°ì´í„° ì €ì¥ ë° ê²€ìƒ‰ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸\n",
    "- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02c40be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ë°±ì—”ë“œ ì‹œìŠ¤í…œ ì—°ë™ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… authenticate_user_azure: ì‚¬ìš©ì ì¸ì¦\n",
      "   âœ… upload_document_to_backend_azure: ë¬¸ì„œ ì—…ë¡œë“œ\n",
      "   âœ… send_chunks_to_backend_azure: ì²­í¬ ë°ì´í„° ì „ì†¡\n",
      "   âœ… complete_document_processing_azure: ì™„ì „ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°\n",
      "   âœ… test_search_integration_azure: ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\n"
     ]
    }
   ],
   "source": [
    "# ë°±ì—”ë“œ ì‹œìŠ¤í…œ ì—°ë™ ë° ì¸ì¦ í•¨ìˆ˜ë“¤\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# ë°±ì—”ë“œ ì„¤ì •\n",
    "BACKEND_BASE_URL = \"http://localhost:8000\"  # ë°±ì—”ë“œ ì„œë²„ URL\n",
    "DEFAULT_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# ì„¸ì…˜ ê´€ë¦¬\n",
    "session_manager = {\n",
    "    'token': None,\n",
    "    'user_id': None,\n",
    "    'session_id': None,\n",
    "    'authenticated': False\n",
    "}\n",
    "\n",
    "def authenticate_user_azure(username, password):\n",
    "    \"\"\"ì‚¬ìš©ì ì¸ì¦ ë° í† í° íšë“\"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸ” ì‚¬ìš©ì ì¸ì¦ ì‹œë„: {username}\")\n",
    "        \n",
    "        login_data = {\n",
    "            'username': username,\n",
    "            'password': password\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            urljoin(BACKEND_BASE_URL, '/api/auth/login'),\n",
    "            json=login_data,\n",
    "            headers=DEFAULT_HEADERS,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            auth_data = response.json()\n",
    "            \n",
    "            # ì„¸ì…˜ ì •ë³´ ì €ì¥\n",
    "            session_manager['token'] = auth_data.get('access_token')\n",
    "            session_manager['user_id'] = auth_data.get('user_id')\n",
    "            session_manager['session_id'] = auth_data.get('session_id')\n",
    "            session_manager['authenticated'] = True\n",
    "            \n",
    "            print(f\"âœ… ì¸ì¦ ì„±ê³µ: ì‚¬ìš©ì ID {session_manager['user_id']}\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'token': session_manager['token'],\n",
    "                'user_id': session_manager['user_id']\n",
    "            }\n",
    "        else:\n",
    "            print(f\"âŒ ì¸ì¦ ì‹¤íŒ¨: {response.status_code} - {response.text}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'ì¸ì¦ ì‹¤íŒ¨: {response.status_code}',\n",
    "                'details': response.text\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì¸ì¦ ì˜¤ë¥˜: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def get_authenticated_headers():\n",
    "    \"\"\"ì¸ì¦ í—¤ë” ìƒì„±\"\"\"\n",
    "    headers = DEFAULT_HEADERS.copy()\n",
    "    if session_manager['token']:\n",
    "        headers['Authorization'] = f\"Bearer {session_manager['token']}\"\n",
    "    return headers\n",
    "\n",
    "def upload_document_to_backend_azure(file_path, document_metadata=None):\n",
    "    \"\"\"ë¬¸ì„œë¥¼ ë°±ì—”ë“œì— ì—…ë¡œë“œ\"\"\"\n",
    "    if not session_manager['authenticated']:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ìì…ë‹ˆë‹¤'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ ì‹œì‘: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # íŒŒì¼ ì¤€ë¹„\n",
    "        with open(file_path, 'rb') as file:\n",
    "            files = {\n",
    "                'file': (os.path.basename(file_path), file, 'application/pdf')\n",
    "            }\n",
    "            \n",
    "            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„\n",
    "            data = {\n",
    "                'user_id': session_manager['user_id'],\n",
    "                'processing_method': 'azure_document_intelligence',\n",
    "                'metadata': json.dumps(document_metadata or {})\n",
    "            }\n",
    "            \n",
    "            # ì—…ë¡œë“œ ìš”ì²­\n",
    "            response = requests.post(\n",
    "                urljoin(BACKEND_BASE_URL, '/api/documents/upload'),\n",
    "                files=files,\n",
    "                data=data,\n",
    "                headers={'Authorization': f\"Bearer {session_manager['token']}\"},\n",
    "                timeout=120\n",
    "            )\n",
    "        \n",
    "        if response.status_code == 201:\n",
    "            upload_result = response.json()\n",
    "            print(f\"âœ… ì—…ë¡œë“œ ì„±ê³µ: ë¬¸ì„œ ID {upload_result.get('document_id')}\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'document_id': upload_result.get('document_id'),\n",
    "                'upload_id': upload_result.get('upload_id'),\n",
    "                'status': upload_result.get('status', 'uploaded')\n",
    "            }\n",
    "        else:\n",
    "            print(f\"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {response.status_code}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'ì—…ë¡œë“œ ì‹¤íŒ¨: {response.status_code}',\n",
    "                'details': response.text\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def send_chunks_to_backend_azure(chunks, document_id):\n",
    "    \"\"\"ì²­í¬ ë°ì´í„°ë¥¼ ë°±ì—”ë“œë¡œ ì „ì†¡\"\"\"\n",
    "    if not session_manager['authenticated']:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ìì…ë‹ˆë‹¤'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ“Š ì²­í¬ ë°ì´í„° ì „ì†¡ ì‹œì‘: {len(chunks)}ê°œ ì²­í¬\")\n",
    "        \n",
    "        chunk_data = {\n",
    "            'document_id': document_id,\n",
    "            'user_id': session_manager['user_id'],\n",
    "            'chunks': chunks,\n",
    "            'processing_metadata': {\n",
    "                'extraction_method': 'azure_document_intelligence',\n",
    "                'chunk_count': len(chunks),\n",
    "                'processing_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            urljoin(BACKEND_BASE_URL, '/api/documents/chunks'),\n",
    "            json=chunk_data,\n",
    "            headers=get_authenticated_headers(),\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 201:\n",
    "            result = response.json()\n",
    "            print(f\"âœ… ì²­í¬ ì „ì†¡ ì„±ê³µ: {result.get('stored_chunks')}ê°œ ì €ì¥ë¨\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'stored_chunks': result.get('stored_chunks'),\n",
    "                'chunk_ids': result.get('chunk_ids', []),\n",
    "                'indexing_status': result.get('indexing_status')\n",
    "            }\n",
    "        else:\n",
    "            print(f\"âŒ ì²­í¬ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'ì²­í¬ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}',\n",
    "                'details': response.text\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì²­í¬ ì „ì†¡ ì˜¤ë¥˜: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def update_document_status_azure(document_id, status, processing_result=None):\n",
    "    \"\"\"ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸\"\"\"\n",
    "    if not session_manager['authenticated']:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ìì…ë‹ˆë‹¤'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ“ ë¬¸ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸: {document_id} -> {status}\")\n",
    "        \n",
    "        update_data = {\n",
    "            'status': status,\n",
    "            'user_id': session_manager['user_id']\n",
    "        }\n",
    "        \n",
    "        if processing_result:\n",
    "            update_data['processing_result'] = processing_result\n",
    "        \n",
    "        response = requests.patch(\n",
    "            urljoin(BACKEND_BASE_URL, f'/api/documents/{document_id}/status'),\n",
    "            json=update_data,\n",
    "            headers=get_authenticated_headers(),\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ… ìƒíƒœ ì—…ë°ì´íŠ¸ ì„±ê³µ\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'updated_status': status\n",
    "            }\n",
    "        else:\n",
    "            print(f\"âŒ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {response.status_code}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {response.status_code}'\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ìƒíƒœ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def get_document_processing_status_azure(document_id):\n",
    "    \"\"\"ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ\"\"\"\n",
    "    if not session_manager['authenticated']:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ìì…ë‹ˆë‹¤'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            urljoin(BACKEND_BASE_URL, f'/api/documents/{document_id}/status'),\n",
    "            headers=get_authenticated_headers(),\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            status_data = response.json()\n",
    "            return {\n",
    "                'success': True,\n",
    "                'status': status_data.get('status'),\n",
    "                'progress': status_data.get('progress', 0),\n",
    "                'processing_time': status_data.get('processing_time'),\n",
    "                'error_message': status_data.get('error_message')\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}'\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def test_search_integration_azure(query, document_id=None):\n",
    "    \"\"\"ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    if not session_manager['authenticated']:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'ì¸ì¦ë˜ì§€ ì•Šì€ ì‚¬ìš©ìì…ë‹ˆë‹¤'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'\")\n",
    "        \n",
    "        search_data = {\n",
    "            'query': query,\n",
    "            'user_id': session_manager['user_id'],\n",
    "            'search_type': 'hybrid',\n",
    "            'max_results': 10\n",
    "        }\n",
    "        \n",
    "        if document_id:\n",
    "            search_data['document_filter'] = [document_id]\n",
    "        \n",
    "        response = requests.post(\n",
    "            urljoin(BACKEND_BASE_URL, '/api/search'),\n",
    "            json=search_data,\n",
    "            headers=get_authenticated_headers(),\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            search_results = response.json()\n",
    "            print(f\"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(search_results.get('results', []))}ê°œ ê²°ê³¼\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'results': search_results.get('results', []),\n",
    "                'total_results': search_results.get('total', 0),\n",
    "                'search_time': search_results.get('search_time')\n",
    "            }\n",
    "        else:\n",
    "            print(f\"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}',\n",
    "                'details': response.text\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def complete_document_processing_azure(file_path, username=\"testuser\", password=\"testpass\"):\n",
    "    \"\"\"ì™„ì „í•œ ë¬¸ì„œ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°\"\"\"\n",
    "    try:\n",
    "        print(\"ğŸš€ Azure Document Intelligence ì™„ì „ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ì‹œì‘\")\n",
    "        \n",
    "        # 1. ì‚¬ìš©ì ì¸ì¦\n",
    "        auth_result = authenticate_user_azure(username, password)\n",
    "        if not auth_result['success']:\n",
    "            return auth_result\n",
    "        \n",
    "        # 2. ë¬¸ì„œ ì—…ë¡œë“œ\n",
    "        upload_result = upload_document_to_backend_azure(file_path)\n",
    "        if not upload_result['success']:\n",
    "            return upload_result\n",
    "        \n",
    "        document_id = upload_result['document_id']\n",
    "        \n",
    "        # 3. Azure Document Intelligenceë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        extraction_result = extract_text_with_azure(file_path)\n",
    "        if not extraction_result['success']:\n",
    "            update_document_status_azure(document_id, 'extraction_failed', extraction_result)\n",
    "            return extraction_result\n",
    "        \n",
    "        # 4. RAGìš© ì²­í‚¹\n",
    "        chunking_result = prepare_text_for_rag_azure(extraction_result)\n",
    "        if not chunking_result['success']:\n",
    "            update_document_status_azure(document_id, 'chunking_failed', chunking_result)\n",
    "            return chunking_result\n",
    "        \n",
    "        # 5. ì²­í¬ ë°ì´í„° ë°±ì—”ë“œ ì „ì†¡\n",
    "        chunks_result = send_chunks_to_backend_azure(chunking_result['chunks'], document_id)\n",
    "        if not chunks_result['success']:\n",
    "            update_document_status_azure(document_id, 'storage_failed', chunks_result)\n",
    "            return chunks_result\n",
    "        \n",
    "        # 6. ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "        final_result = {\n",
    "            'extraction_metadata': extraction_result['metadata'],\n",
    "            'chunking_metadata': chunking_result['processing_metadata'],\n",
    "            'storage_metadata': chunks_result\n",
    "        }\n",
    "        \n",
    "        update_result = update_document_status_azure(document_id, 'completed', final_result)\n",
    "        \n",
    "        print(\"âœ… ì™„ì „ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ\")\n",
    "        return {\n",
    "            'success': True,\n",
    "            'document_id': document_id,\n",
    "            'extracted_pages': extraction_result['metadata']['page_count'],\n",
    "            'created_chunks': chunking_result['total_chunks'],\n",
    "            'stored_chunks': chunks_result['stored_chunks'],\n",
    "            'processing_result': final_result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì™„ì „ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"ğŸ“¦ ë°±ì—”ë“œ ì‹œìŠ¤í…œ ì—°ë™ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… authenticate_user_azure: ì‚¬ìš©ì ì¸ì¦\")\n",
    "print(\"   âœ… upload_document_to_backend_azure: ë¬¸ì„œ ì—…ë¡œë“œ\")\n",
    "print(\"   âœ… send_chunks_to_backend_azure: ì²­í¬ ë°ì´í„° ì „ì†¡\")\n",
    "print(\"   âœ… complete_document_processing_azure: ì™„ì „ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°\")\n",
    "print(\"   âœ… test_search_integration_azure: ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d91c9a6",
   "metadata": {},
   "source": [
    "## 8. ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ ë¶„ì„\n",
    "\n",
    "Azure Document Intelligenceì˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ê³  í’ˆì§ˆì„ ë¶„ì„í•©ë‹ˆë‹¤. ê¸°ì¡´ Textract ê²°ê³¼ì™€ì˜ ë¹„êµë„ í¬í•¨ë©ë‹ˆë‹¤.\n",
    "\n",
    "### ê¸°ëŠ¥:\n",
    "- í…ìŠ¤íŠ¸ ì¶”ì¶œ ì •í™•ë„ ê²€ì¦\n",
    "- í•œêµ­ì–´ ì²˜ë¦¬ í’ˆì§ˆ í‰ê°€\n",
    "- ë ˆì´ì•„ì›ƒ ì¸ì‹ ì •í™•ë„ ì¸¡ì •\n",
    "- Textract vs Azure ì„±ëŠ¥ ë¹„êµ\n",
    "- ì‹ ë¢°ë„ ì ìˆ˜ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2671c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… validate_azure_extraction_quality: ì „ì²´ í’ˆì§ˆ ê²€ì¦\n",
      "   âœ… validate_korean_processing_azure: í•œêµ­ì–´ ì²˜ë¦¬ í’ˆì§ˆ\n",
      "   âœ… analyze_confidence_scores_azure: ì‹ ë¢°ë„ ë¶„ì„\n",
      "   âœ… generate_quality_recommendations_azure: ê°œì„  ê¶Œì¥ì‚¬í•­\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜ë“¤\n",
    "import difflib\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def validate_azure_extraction_quality(extraction_result, expected_content=None):\n",
    "    \"\"\"Azure ì¶”ì¶œ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "    try:\n",
    "        print(\"ğŸ” Azure Document Intelligence ì¶”ì¶œ í’ˆì§ˆ ê²€ì¦ ì‹œì‘\")\n",
    "        \n",
    "        validation_result = {\n",
    "            'overall_score': 0.0,\n",
    "            'metrics': {},\n",
    "            'issues': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # 1. ê¸°ë³¸ ì¶”ì¶œ ì„±ê³µ ì—¬ë¶€ ê²€ì¦\n",
    "        if not extraction_result.get('success'):\n",
    "            validation_result['issues'].append(\"ì¶”ì¶œ ìì²´ê°€ ì‹¤íŒ¨í•¨\")\n",
    "            return validation_result\n",
    "        \n",
    "        # 2. í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦\n",
    "        text_quality = validate_text_quality_azure(extraction_result.get('text', ''))\n",
    "        validation_result['metrics']['text_quality'] = text_quality\n",
    "        \n",
    "        # 3. í•œêµ­ì–´ ì²˜ë¦¬ í’ˆì§ˆ ê²€ì¦\n",
    "        korean_quality = validate_korean_processing_azure(extraction_result.get('text', ''))\n",
    "        validation_result['metrics']['korean_quality'] = korean_quality\n",
    "        \n",
    "        # 4. í˜ì´ì§€ë³„ ì²˜ë¦¬ í’ˆì§ˆ ê²€ì¦\n",
    "        page_quality = validate_page_processing_azure(extraction_result.get('page_results', []))\n",
    "        validation_result['metrics']['page_quality'] = page_quality\n",
    "        \n",
    "        # 5. êµ¬ì¡°ì  ìš”ì†Œ í’ˆì§ˆ ê²€ì¦\n",
    "        structure_quality = validate_structure_processing_azure(extraction_result)\n",
    "        validation_result['metrics']['structure_quality'] = structure_quality\n",
    "        \n",
    "        # 6. ì‹ ë¢°ë„ ì ìˆ˜ ë¶„ì„\n",
    "        confidence_analysis = analyze_confidence_scores_azure(extraction_result)\n",
    "        validation_result['metrics']['confidence_analysis'] = confidence_analysis\n",
    "        \n",
    "        # 7. ì˜ˆìƒ ë‚´ìš©ê³¼ ë¹„êµ (ìˆëŠ” ê²½ìš°)\n",
    "        if expected_content:\n",
    "            content_similarity = compare_with_expected_content_azure(\n",
    "                extraction_result.get('text', ''), \n",
    "                expected_content\n",
    "            )\n",
    "            validation_result['metrics']['content_similarity'] = content_similarity\n",
    "        \n",
    "        # 8. ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n",
    "        overall_score = calculate_overall_quality_score_azure(validation_result['metrics'])\n",
    "        validation_result['overall_score'] = overall_score\n",
    "        \n",
    "        # 9. ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±\n",
    "        recommendations = generate_quality_recommendations_azure(validation_result['metrics'])\n",
    "        validation_result['recommendations'] = recommendations\n",
    "        \n",
    "        print(f\"âœ… í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: ì „ì²´ ì ìˆ˜ {overall_score:.2f}/5.0\")\n",
    "        return validation_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}\")\n",
    "        return {\n",
    "            'overall_score': 0.0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def validate_text_quality_azure(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ê¸°ë³¸ í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "    try:\n",
    "        metrics = {\n",
    "            'length': len(text),\n",
    "            'word_count': len(text.split()),\n",
    "            'line_count': len(text.split('\\\\n')),\n",
    "            'empty_ratio': 0.0,\n",
    "            'special_char_ratio': 0.0,\n",
    "            'korean_ratio': 0.0\n",
    "        }\n",
    "        \n",
    "        if metrics['length'] > 0:\n",
    "            # ë¹ˆ ì¤„ ë¹„ìœ¨\n",
    "            empty_lines = len([line for line in text.split('\\\\n') if not line.strip()])\n",
    "            metrics['empty_ratio'] = empty_lines / metrics['line_count'] if metrics['line_count'] > 0 else 0\n",
    "            \n",
    "            # íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨\n",
    "            special_chars = len(re.findall(r'[^\\\\w\\\\sê°€-í£]', text))\n",
    "            metrics['special_char_ratio'] = special_chars / metrics['length']\n",
    "            \n",
    "            # í•œêµ­ì–´ ë¹„ìœ¨\n",
    "            korean_chars = len(re.findall(r'[ê°€-í£]', text))\n",
    "            metrics['korean_ratio'] = korean_chars / metrics['length']\n",
    "        \n",
    "        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-5)\n",
    "        score = 0.0\n",
    "        if metrics['length'] > 100:\n",
    "            score += 1.0\n",
    "        if metrics['word_count'] > 20:\n",
    "            score += 1.0\n",
    "        if metrics['empty_ratio'] < 0.3:\n",
    "            score += 1.0\n",
    "        if metrics['special_char_ratio'] < 0.1:\n",
    "            score += 1.0\n",
    "        if metrics['korean_ratio'] > 0.3:\n",
    "            score += 1.0\n",
    "        \n",
    "        metrics['quality_score'] = score\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'quality_score': 0.0}\n",
    "\n",
    "def validate_korean_processing_azure(text):\n",
    "    \"\"\"í•œêµ­ì–´ ì²˜ë¦¬ í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "    try:\n",
    "        metrics = {\n",
    "            'morpheme_analysis_success': False,\n",
    "            'proper_spacing': 0.0,\n",
    "            'sentence_structure': 0.0,\n",
    "            'vocabulary_richness': 0.0\n",
    "        }\n",
    "        \n",
    "        # í˜•íƒœì†Œ ë¶„ì„ ì„±ê³µ ì—¬ë¶€\n",
    "        try:\n",
    "            morphemes = kiwi.analyze(text[:1000])  # ì²« 1000ìë§Œ í…ŒìŠ¤íŠ¸\n",
    "            metrics['morpheme_analysis_success'] = len(morphemes) > 0\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # ì ì ˆí•œ ë„ì–´ì“°ê¸° ê²€ì¦\n",
    "        words = text.split()\n",
    "        if words:\n",
    "            avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "            metrics['proper_spacing'] = min(avg_word_length / 3.0, 1.0)  # ì ì ˆí•œ í‰ê·  ë‹¨ì–´ ê¸¸ì´\n",
    "        \n",
    "        # ë¬¸ì¥ êµ¬ì¡° ê²€ì¦\n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        valid_sentences = [s for s in sentences if len(s.strip()) > 5]\n",
    "        if sentences:\n",
    "            metrics['sentence_structure'] = len(valid_sentences) / len(sentences)\n",
    "        \n",
    "        # ì–´íœ˜ ë‹¤ì–‘ì„±\n",
    "        if words:\n",
    "            unique_words = set(words)\n",
    "            metrics['vocabulary_richness'] = len(unique_words) / len(words)\n",
    "        \n",
    "        # ì „ì²´ í•œêµ­ì–´ ì²˜ë¦¬ ì ìˆ˜ (0-5)\n",
    "        score = 0.0\n",
    "        if metrics['morpheme_analysis_success']:\n",
    "            score += 1.5\n",
    "        score += metrics['proper_spacing'] * 1.0\n",
    "        score += metrics['sentence_structure'] * 1.5\n",
    "        score += metrics['vocabulary_richness'] * 1.0\n",
    "        \n",
    "        metrics['korean_quality_score'] = min(score, 5.0)\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'korean_quality_score': 0.0}\n",
    "\n",
    "def validate_page_processing_azure(page_results):\n",
    "    \"\"\"í˜ì´ì§€ë³„ ì²˜ë¦¬ í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "    try:\n",
    "        if not page_results:\n",
    "            return {'error': 'í˜ì´ì§€ ê²°ê³¼ ì—†ìŒ', 'page_quality_score': 0.0}\n",
    "        \n",
    "        metrics = {\n",
    "            'page_count': len(page_results),\n",
    "            'avg_confidence': 0.0,\n",
    "            'layout_consistency': 0.0,\n",
    "            'content_distribution': []\n",
    "        }\n",
    "        \n",
    "        confidences = []\n",
    "        content_lengths = []\n",
    "        \n",
    "        for page in page_results:\n",
    "            if 'avg_confidence' in page:\n",
    "                confidences.append(page['avg_confidence'])\n",
    "            content_lengths.append(len(page.get('text', '')))\n",
    "        \n",
    "        # í‰ê·  ì‹ ë¢°ë„\n",
    "        if confidences:\n",
    "            metrics['avg_confidence'] = sum(confidences) / len(confidences)\n",
    "        \n",
    "        # ì½˜í…ì¸  ë¶„í¬\n",
    "        metrics['content_distribution'] = content_lengths\n",
    "        \n",
    "        # ë ˆì´ì•„ì›ƒ ì¼ê´€ì„± (ë¹„ìŠ·í•œ ê¸¸ì´ì˜ í˜ì´ì§€ë“¤)\n",
    "        if content_lengths:\n",
    "            avg_length = sum(content_lengths) / len(content_lengths)\n",
    "            variance = sum((length - avg_length) ** 2 for length in content_lengths) / len(content_lengths)\n",
    "            normalized_variance = variance / (avg_length ** 2) if avg_length > 0 else 1.0\n",
    "            metrics['layout_consistency'] = max(0.0, 1.0 - normalized_variance)\n",
    "        \n",
    "        # í˜ì´ì§€ ì²˜ë¦¬ í’ˆì§ˆ ì ìˆ˜ (0-5)\n",
    "        score = 0.0\n",
    "        if metrics['page_count'] > 0:\n",
    "            score += 1.0\n",
    "        if metrics['avg_confidence'] > 0.8:\n",
    "            score += 2.0\n",
    "        elif metrics['avg_confidence'] > 0.6:\n",
    "            score += 1.0\n",
    "        score += metrics['layout_consistency'] * 2.0\n",
    "        \n",
    "        metrics['page_quality_score'] = min(score, 5.0)\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'page_quality_score': 0.0}\n",
    "\n",
    "def validate_structure_processing_azure(extraction_result):\n",
    "    \"\"\"êµ¬ì¡°ì  ìš”ì†Œ ì²˜ë¦¬ í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "    try:\n",
    "        metrics = {\n",
    "            'table_count': 0,\n",
    "            'table_quality': 0.0,\n",
    "            'kv_pair_count': 0,\n",
    "            'kv_quality': 0.0\n",
    "        }\n",
    "        \n",
    "        # í…Œì´ë¸” ì²˜ë¦¬ í’ˆì§ˆ\n",
    "        tables = extraction_result.get('tables', [])\n",
    "        metrics['table_count'] = len(tables)\n",
    "        \n",
    "        if tables:\n",
    "            table_scores = []\n",
    "            for table in tables:\n",
    "                table_score = 0.0\n",
    "                if table.get('row_count', 0) > 0:\n",
    "                    table_score += 1.0\n",
    "                if table.get('column_count', 0) > 0:\n",
    "                    table_score += 1.0\n",
    "                if table.get('csv') and len(table['csv']) > 0:\n",
    "                    table_score += 1.0\n",
    "                table_scores.append(table_score)\n",
    "            \n",
    "            metrics['table_quality'] = sum(table_scores) / len(table_scores) / 3.0\n",
    "        \n",
    "        # í‚¤-ê°’ ìŒ ì²˜ë¦¬ í’ˆì§ˆ\n",
    "        kv_pairs = extraction_result.get('key_value_pairs', [])\n",
    "        metrics['kv_pair_count'] = len(kv_pairs)\n",
    "        \n",
    "        if kv_pairs:\n",
    "            valid_kv_count = 0\n",
    "            for kv in kv_pairs:\n",
    "                if kv.get('key') and kv.get('value'):\n",
    "                    valid_kv_count += 1\n",
    "            \n",
    "            metrics['kv_quality'] = valid_kv_count / len(kv_pairs)\n",
    "        \n",
    "        # êµ¬ì¡° ì²˜ë¦¬ í’ˆì§ˆ ì ìˆ˜ (0-5)\n",
    "        score = 0.0\n",
    "        if metrics['table_count'] > 0:\n",
    "            score += 1.0 + metrics['table_quality'] * 2.0\n",
    "        if metrics['kv_pair_count'] > 0:\n",
    "            score += 1.0 + metrics['kv_quality'] * 1.0\n",
    "        \n",
    "        metrics['structure_quality_score'] = min(score, 5.0)\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'structure_quality_score': 0.0}\n",
    "\n",
    "def analyze_confidence_scores_azure(extraction_result):\n",
    "    \"\"\"ì‹ ë¢°ë„ ì ìˆ˜ ë¶„ì„\"\"\"\n",
    "    try:\n",
    "        all_confidences = []\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ ì‹ ë¢°ë„ ìˆ˜ì§‘\n",
    "        for page in extraction_result.get('page_results', []):\n",
    "            if 'avg_confidence' in page:\n",
    "                all_confidences.append(page['avg_confidence'])\n",
    "        \n",
    "        # í‚¤-ê°’ ìŒ ì‹ ë¢°ë„ ìˆ˜ì§‘\n",
    "        for kv in extraction_result.get('key_value_pairs', []):\n",
    "            if 'confidence' in kv:\n",
    "                all_confidences.append(kv['confidence'])\n",
    "        \n",
    "        if not all_confidences:\n",
    "            return {'error': 'ì‹ ë¢°ë„ ë°ì´í„° ì—†ìŒ', 'confidence_score': 0.0}\n",
    "        \n",
    "        metrics = {\n",
    "            'avg_confidence': sum(all_confidences) / len(all_confidences),\n",
    "            'min_confidence': min(all_confidences),\n",
    "            'max_confidence': max(all_confidences),\n",
    "            'confidence_variance': 0.0,\n",
    "            'low_confidence_ratio': 0.0\n",
    "        }\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ë¶„ì‚°\n",
    "        avg = metrics['avg_confidence']\n",
    "        variance = sum((conf - avg) ** 2 for conf in all_confidences) / len(all_confidences)\n",
    "        metrics['confidence_variance'] = variance\n",
    "        \n",
    "        # ë‚®ì€ ì‹ ë¢°ë„ ë¹„ìœ¨ (0.7 ë¯¸ë§Œ)\n",
    "        low_conf_count = len([conf for conf in all_confidences if conf < 0.7])\n",
    "        metrics['low_confidence_ratio'] = low_conf_count / len(all_confidences)\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ë¶„ì„ ì ìˆ˜ (0-5)\n",
    "        score = 0.0\n",
    "        if metrics['avg_confidence'] > 0.9:\n",
    "            score += 2.0\n",
    "        elif metrics['avg_confidence'] > 0.8:\n",
    "            score += 1.5\n",
    "        elif metrics['avg_confidence'] > 0.7:\n",
    "            score += 1.0\n",
    "        \n",
    "        if metrics['min_confidence'] > 0.5:\n",
    "            score += 1.0\n",
    "        \n",
    "        if metrics['low_confidence_ratio'] < 0.2:\n",
    "            score += 1.0\n",
    "        \n",
    "        if metrics['confidence_variance'] < 0.1:\n",
    "            score += 1.0\n",
    "        \n",
    "        metrics['confidence_score'] = min(score, 5.0)\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'confidence_score': 0.0}\n",
    "\n",
    "def compare_with_expected_content_azure(extracted_text, expected_content):\n",
    "    \"\"\"ì˜ˆìƒ ë‚´ìš©ê³¼ì˜ ìœ ì‚¬ë„ ë¹„êµ\"\"\"\n",
    "    try:\n",
    "        # í…ìŠ¤íŠ¸ ì •ê·œí™”\n",
    "        extracted_normalized = re.sub(r'\\\\s+', ' ', extracted_text.lower().strip())\n",
    "        expected_normalized = re.sub(r'\\\\s+', ' ', expected_content.lower().strip())\n",
    "        \n",
    "        # ìœ ì‚¬ë„ ê³„ì‚°\n",
    "        similarity_ratio = difflib.SequenceMatcher(None, extracted_normalized, expected_normalized).ratio()\n",
    "        \n",
    "        # ë‹¨ì–´ ìˆ˜ì¤€ ë¹„êµ\n",
    "        extracted_words = set(extracted_normalized.split())\n",
    "        expected_words = set(expected_normalized.split())\n",
    "        \n",
    "        word_overlap = len(extracted_words & expected_words)\n",
    "        word_total = len(extracted_words | expected_words)\n",
    "        word_similarity = word_overlap / word_total if word_total > 0 else 0.0\n",
    "        \n",
    "        # ê¸¸ì´ ë¹„êµ\n",
    "        length_ratio = min(len(extracted_text), len(expected_content)) / max(len(extracted_text), len(expected_content))\n",
    "        \n",
    "        metrics = {\n",
    "            'similarity_ratio': similarity_ratio,\n",
    "            'word_similarity': word_similarity,\n",
    "            'length_ratio': length_ratio,\n",
    "            'extracted_length': len(extracted_text),\n",
    "            'expected_length': len(expected_content)\n",
    "        }\n",
    "        \n",
    "        # ë‚´ìš© ìœ ì‚¬ë„ ì ìˆ˜ (0-5)\n",
    "        score = similarity_ratio * 2.0 + word_similarity * 2.0 + length_ratio * 1.0\n",
    "        metrics['content_similarity_score'] = min(score, 5.0)\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'content_similarity_score': 0.0}\n",
    "\n",
    "def calculate_overall_quality_score_azure(metrics):\n",
    "    \"\"\"ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    try:\n",
    "        scores = []\n",
    "        weights = []\n",
    "        \n",
    "        # ê° ë©”íŠ¸ë¦­ì˜ ì ìˆ˜ì™€ ê°€ì¤‘ì¹˜\n",
    "        metric_weights = {\n",
    "            'text_quality': (0.2, 'quality_score'),\n",
    "            'korean_quality': (0.3, 'korean_quality_score'),\n",
    "            'page_quality': (0.2, 'page_quality_score'),\n",
    "            'structure_quality': (0.1, 'structure_quality_score'),\n",
    "            'confidence_analysis': (0.2, 'confidence_score')\n",
    "        }\n",
    "        \n",
    "        for metric_name, (weight, score_key) in metric_weights.items():\n",
    "            if metric_name in metrics and score_key in metrics[metric_name]:\n",
    "                scores.append(metrics[metric_name][score_key])\n",
    "                weights.append(weight)\n",
    "        \n",
    "        if not scores:\n",
    "            return 0.0\n",
    "        \n",
    "        # ê°€ì¤‘ í‰ê·  ê³„ì‚°\n",
    "        weighted_sum = sum(score * weight for score, weight in zip(scores, weights))\n",
    "        total_weight = sum(weights)\n",
    "        \n",
    "        return weighted_sum / total_weight if total_weight > 0 else 0.0\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "def generate_quality_recommendations_azure(metrics):\n",
    "    \"\"\"í’ˆì§ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    try:\n",
    "        # í…ìŠ¤íŠ¸ í’ˆì§ˆ ê´€ë ¨\n",
    "        text_quality = metrics.get('text_quality', {})\n",
    "        if text_quality.get('quality_score', 0) < 3.0:\n",
    "            if text_quality.get('length', 0) < 100:\n",
    "                recommendations.append(\"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë¬¸ì„œ ìŠ¤ìº” í’ˆì§ˆì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "            if text_quality.get('korean_ratio', 0) < 0.3:\n",
    "                recommendations.append(\"í•œêµ­ì–´ ë¹„ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ë¬¸ì„œê°€ í•œêµ­ì–´ ë¬¸ì„œì¸ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        \n",
    "        # í•œêµ­ì–´ ì²˜ë¦¬ í’ˆì§ˆ ê´€ë ¨\n",
    "        korean_quality = metrics.get('korean_quality', {})\n",
    "        if korean_quality.get('korean_quality_score', 0) < 3.0:\n",
    "            if not korean_quality.get('morpheme_analysis_success', False):\n",
    "                recommendations.append(\"í˜•íƒœì†Œ ë¶„ì„ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "            if korean_quality.get('proper_spacing', 0) < 0.5:\n",
    "                recommendations.append(\"ë„ì–´ì“°ê¸°ê°€ ë¶€ì ì ˆí•©ë‹ˆë‹¤. í›„ì²˜ë¦¬ì—ì„œ ë„ì–´ì“°ê¸° êµì •ì„ ê³ ë ¤í•˜ì„¸ìš”.\")\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ê´€ë ¨\n",
    "        confidence_analysis = metrics.get('confidence_analysis', {})\n",
    "        if confidence_analysis.get('confidence_score', 0) < 3.0:\n",
    "            if confidence_analysis.get('avg_confidence', 0) < 0.8:\n",
    "                recommendations.append(\"ì „ì²´ì ì¸ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë¬¸ì„œ í•´ìƒë„ë¥¼ ë†’ì´ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.\")\n",
    "            if confidence_analysis.get('low_confidence_ratio', 0) > 0.3:\n",
    "                recommendations.append(\"ë‚®ì€ ì‹ ë¢°ë„ êµ¬ê°„ì´ ë§ìŠµë‹ˆë‹¤. í•´ë‹¹ êµ¬ê°„ì„ ìˆ˜ë™ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”.\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return recommendations\n",
    "        \n",
    "    except Exception as e:\n",
    "        return [f\"ê¶Œì¥ì‚¬í•­ ìƒì„± ì‹¤íŒ¨: {str(e)}\"]\n",
    "\n",
    "print(\"ğŸ“¦ ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… validate_azure_extraction_quality: ì „ì²´ í’ˆì§ˆ ê²€ì¦\")\n",
    "print(\"   âœ… validate_korean_processing_azure: í•œêµ­ì–´ ì²˜ë¦¬ í’ˆì§ˆ\")\n",
    "print(\"   âœ… analyze_confidence_scores_azure: ì‹ ë¢°ë„ ë¶„ì„\")\n",
    "print(\"   âœ… generate_quality_recommendations_azure: ê°œì„  ê¶Œì¥ì‚¬í•­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c36382",
   "metadata": {},
   "source": [
    "## 9. ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ì„±ëŠ¥ ë¶„ì„\n",
    "\n",
    "Azure Document Intelligenceì˜ ì‹¤ì œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ê¸°ì¡´ Textractì™€ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ê¸°ëŠ¥:\n",
    "- ì‹¤ì œ í•œêµ­ì–´ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "- ì²˜ë¦¬ ì‹œê°„ ë° ì •í™•ë„ ì¸¡ì •\n",
    "- Textract ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ\n",
    "- ë©”ëª¨ë¦¬ ë° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„\n",
    "- ìµœì¢… ê¶Œì¥ì‚¬í•­ ì œì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6fe696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Azure Document Intelligence í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… run_azure_document_test: ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
      "   ğŸ“‹ TEST_DOCUMENTS: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ëª©ë¡ ì„¤ì •ë¨\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   # ë‹¨ì¼ ë¬¸ì„œ í…ŒìŠ¤íŠ¸\n",
      "   test_result = run_azure_document_test('/path/to/document.pdf')\n",
      "   # ê²°ê³¼ í™•ì¸\n",
      "   print(test_result['success'])\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì²˜ë¦¬ ë° í˜ì´ì§€ë³„ ê²°ê³¼ ë¶„ì„\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def run_azure_document_test(file_path):\n",
    "    \"\"\"Azure Document Intelligence í…ŒìŠ¤íŠ¸ ì‹¤í–‰\"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸš€ Azure Document Intelligence í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "        print(f\"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {os.path.basename(file_path)}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # 1. ë¬¸ì„œ ë³µì¡ë„ ë¶„ì„\n",
    "        print(\"1ï¸âƒ£ ë¬¸ì„œ ë³µì¡ë„ ë¶„ì„\")\n",
    "        complexity_info = detect_document_complexity_azure(file_path)\n",
    "        print(f\"   ğŸ“Š ë³µì¡ë„: {complexity_info['complexity_level']}\")\n",
    "        print(f\"   ğŸ“„ í˜ì´ì§€: {complexity_info['estimated_pages']}ê°œ\")\n",
    "        print(f\"   ğŸ”§ ê¶Œì¥ ëª¨ë¸: {complexity_info['recommended_model']}\")\n",
    "        print()\n",
    "        \n",
    "        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤í–‰\n",
    "        print(\"2ï¸âƒ£ Azure Document Intelligence í…ìŠ¤íŠ¸ ì¶”ì¶œ\")\n",
    "        extraction_result = extract_text_with_azure(file_path)\n",
    "        \n",
    "        if not extraction_result['success']:\n",
    "            print(f\"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {extraction_result['error']}\")\n",
    "            return extraction_result\n",
    "        \n",
    "        print(f\"   âœ… ì¶”ì¶œ ì„±ê³µ!\")\n",
    "        print(f\"   ğŸ“„ í˜ì´ì§€: {extraction_result['metadata']['page_count']}ê°œ\")\n",
    "        print(f\"   ğŸ“ í…ìŠ¤íŠ¸: {len(extraction_result['text'])}ì\")\n",
    "        print(f\"   â±ï¸ ì²˜ë¦¬ì‹œê°„: {extraction_result.get('processing_time', 0):.1f}ì´ˆ\")\n",
    "        print()\n",
    "        \n",
    "        # 3. í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„\n",
    "        print(\"3ï¸âƒ£ í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„\")\n",
    "        page_analysis = analyze_pages_detailed_azure(extraction_result)\n",
    "        print()\n",
    "        \n",
    "        # 4. RAG ì²­í‚¹ ì²˜ë¦¬\n",
    "        print(\"4ï¸âƒ£ RAGìš© í…ìŠ¤íŠ¸ ì²­í‚¹\")\n",
    "        chunking_result = prepare_text_for_rag_azure(extraction_result)\n",
    "        \n",
    "        if chunking_result['success']:\n",
    "            print(f\"   âœ… ì²­í‚¹ ì™„ë£Œ!\")\n",
    "            print(f\"   ğŸ“¦ ì²­í¬: {chunking_result['total_chunks']}ê°œ\")\n",
    "            print(f\"   ğŸ”¢ í† í°: {chunking_result['total_tokens']}ê°œ\")\n",
    "            print(f\"   ğŸ“ í‰ê·  í¬ê¸°: {chunking_result['avg_chunk_size']:.1f} í† í°\")\n",
    "        else:\n",
    "            print(f\"   âŒ ì²­í‚¹ ì‹¤íŒ¨: {chunking_result['error']}\")\n",
    "        print()\n",
    "        \n",
    "        # 5. í’ˆì§ˆ ê²€ì¦\n",
    "        print(\"5ï¸âƒ£ ì¶”ì¶œ í’ˆì§ˆ ê²€ì¦\")\n",
    "        quality_result = validate_azure_extraction_quality(extraction_result)\n",
    "        print(f\"   ğŸ“Š ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {quality_result['overall_score']:.2f}/5.0\")\n",
    "        print()\n",
    "        \n",
    "        # ì „ì²´ ê²°ê³¼ ë°˜í™˜\n",
    "        return {\n",
    "            'success': True,\n",
    "            'file_path': file_path,\n",
    "            'complexity_analysis': complexity_info,\n",
    "            'extraction_result': extraction_result,\n",
    "            'page_analysis': page_analysis,\n",
    "            'chunking_result': chunking_result,\n",
    "            'quality_assessment': quality_result,\n",
    "            'test_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ì‚¬ìš©ì‹œ ìˆ˜ì • í•„ìš”)\n",
    "TEST_DOCUMENTS = [\n",
    "    '/home/admin/wkms-aws/test_template.pdf',\n",
    "    '/home/admin/wkms-aws/test_template2.pdf',\n",
    "    '/home/admin/wkms-aws/test_template3.pdf'\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ Azure Document Intelligence í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… run_azure_document_test: ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\")\n",
    "print(\"   ğŸ“‹ TEST_DOCUMENTS: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ëª©ë¡ ì„¤ì •ë¨\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "print(\"   # ë‹¨ì¼ ë¬¸ì„œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"   test_result = run_azure_document_test('/path/to/document.pdf')\")\n",
    "print(\"   # ê²°ê³¼ í™•ì¸\")\n",
    "print(\"   print(test_result['success'])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "362aeea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… analyze_pages_detailed_azure: ì „ì²´ í˜ì´ì§€ ë¶„ì„\n",
      "   âœ… analyze_single_page_azure: ë‹¨ì¼ í˜ì´ì§€ ë¶„ì„\n",
      "   âœ… calculate_page_statistics_azure: í˜ì´ì§€ í†µê³„ ê³„ì‚°\n",
      "   âœ… print_page_statistics_azure: í†µê³„ ê²°ê³¼ ì¶œë ¥\n"
     ]
    }
   ],
   "source": [
    "# í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ í•¨ìˆ˜ë“¤\n",
    "\n",
    "def analyze_pages_detailed_azure(extraction_result):\n",
    "    \"\"\"í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”\"\"\"\n",
    "    try:\n",
    "        page_results = extraction_result.get('page_results', [])\n",
    "        if not page_results:\n",
    "            print(\"   âŒ í˜ì´ì§€ ë°ì´í„° ì—†ìŒ\")\n",
    "            return {}\n",
    "        \n",
    "        analysis_summary = {\n",
    "            'total_pages': len(page_results),\n",
    "            'page_details': [],\n",
    "            'statistics': {}\n",
    "        }\n",
    "        \n",
    "        print(f\"   ğŸ“„ ì´ {len(page_results)}ê°œ í˜ì´ì§€ ë¶„ì„\")\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ ìƒì„¸ ì •ë³´\n",
    "        for i, page in enumerate(page_results, 1):\n",
    "            page_detail = analyze_single_page_azure(page, i)\n",
    "            analysis_summary['page_details'].append(page_detail)\n",
    "            \n",
    "            # í˜ì´ì§€ ìš”ì•½ ì¶œë ¥\n",
    "            print(f\"      í˜ì´ì§€ {i:2d}: {page_detail['text_length']:5,}ì | \"\n",
    "                  f\"ë ˆì´ì•„ì›ƒ: {page_detail['layout_type']:12s} | \"\n",
    "                  f\"ì‹ ë¢°ë„: {page_detail['confidence']:.2f} | \"\n",
    "                  f\"ë¬¸ë‹¨: {page_detail['paragraph_count']:2d}ê°œ\")\n",
    "        \n",
    "        # ì „ì²´ í†µê³„ ê³„ì‚°\n",
    "        analysis_summary['statistics'] = calculate_page_statistics_azure(analysis_summary['page_details'])\n",
    "        print_page_statistics_azure(analysis_summary['statistics'])\n",
    "        \n",
    "        return analysis_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def analyze_single_page_azure(page_result, page_number):\n",
    "    \"\"\"ë‹¨ì¼ í˜ì´ì§€ ìƒì„¸ ë¶„ì„\"\"\"\n",
    "    try:\n",
    "        page_text = page_result.get('text', '')\n",
    "        \n",
    "        # ê¸°ë³¸ ì •ë³´\n",
    "        page_detail = {\n",
    "            'page_number': page_number,\n",
    "            'text_length': len(page_text),\n",
    "            'layout_type': page_result.get('layout_type', 'unknown'),\n",
    "            'confidence': page_result.get('avg_confidence', 0.0),\n",
    "            'paragraph_count': page_result.get('paragraph_count', 0),\n",
    "            'table_count': page_result.get('table_count', 0),\n",
    "            'element_count': page_result.get('element_count', 0)\n",
    "        }\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ë¶„ì„\n",
    "        if page_text:\n",
    "            # ë¬¸ì¥ ìˆ˜ ê³„ì‚°\n",
    "            sentences = re.split(r'[.!?ã€‚]\\s+', page_text)\n",
    "            page_detail['sentence_count'] = len([s for s in sentences if s.strip()])\n",
    "            \n",
    "            # ë‹¨ì–´ ìˆ˜ ê³„ì‚°\n",
    "            words = page_text.split()\n",
    "            page_detail['word_count'] = len(words)\n",
    "            \n",
    "            # í•œêµ­ì–´ ë¹„ìœ¨\n",
    "            korean_chars = len(re.findall(r'[ê°€-í£]', page_text))\n",
    "            page_detail['korean_ratio'] = korean_chars / len(page_text) if page_text else 0.0\n",
    "            \n",
    "            # ìˆ«ì/ì˜ë¬¸ ë¹„ìœ¨\n",
    "            number_chars = len(re.findall(r'[0-9]', page_text))\n",
    "            english_chars = len(re.findall(r'[a-zA-Z]', page_text))\n",
    "            page_detail['number_ratio'] = number_chars / len(page_text) if page_text else 0.0\n",
    "            page_detail['english_ratio'] = english_chars / len(page_text) if page_text else 0.0\n",
    "            \n",
    "            # ë¹ˆ ì¤„ ë¹„ìœ¨\n",
    "            lines = page_text.split('\\\\n')\n",
    "            empty_lines = len([line for line in lines if not line.strip()])\n",
    "            page_detail['empty_line_ratio'] = empty_lines / len(lines) if lines else 0.0\n",
    "            \n",
    "            # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "            page_detail['keywords'] = extract_keywords_from_chunk_azure(page_text)[:5]\n",
    "        else:\n",
    "            # ë¹ˆ í˜ì´ì§€ ì²˜ë¦¬\n",
    "            page_detail.update({\n",
    "                'sentence_count': 0,\n",
    "                'word_count': 0,\n",
    "                'korean_ratio': 0.0,\n",
    "                'number_ratio': 0.0,\n",
    "                'english_ratio': 0.0,\n",
    "                'empty_line_ratio': 1.0,\n",
    "                'keywords': []\n",
    "            })\n",
    "        \n",
    "        return page_detail\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'page_number': page_number,\n",
    "            'error': str(e),\n",
    "            'text_length': 0\n",
    "        }\n",
    "\n",
    "def calculate_page_statistics_azure(page_details):\n",
    "    \"\"\"í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ì˜ ì „ì²´ í†µê³„ ê³„ì‚°\"\"\"\n",
    "    try:\n",
    "        if not page_details:\n",
    "            return {}\n",
    "        \n",
    "        # ìœ íš¨í•œ í˜ì´ì§€ë§Œ í•„í„°ë§\n",
    "        valid_pages = [p for p in page_details if 'error' not in p]\n",
    "        \n",
    "        if not valid_pages:\n",
    "            return {'error': 'ìœ íš¨í•œ í˜ì´ì§€ ì—†ìŒ'}\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        text_lengths = [p['text_length'] for p in valid_pages]\n",
    "        confidences = [p['confidence'] for p in valid_pages if p['confidence'] > 0]\n",
    "        korean_ratios = [p['korean_ratio'] for p in valid_pages]\n",
    "        \n",
    "        statistics = {\n",
    "            'total_pages': len(page_details),\n",
    "            'valid_pages': len(valid_pages),\n",
    "            'text_statistics': {\n",
    "                'total_characters': sum(text_lengths),\n",
    "                'avg_characters_per_page': sum(text_lengths) / len(valid_pages),\n",
    "                'min_characters': min(text_lengths),\n",
    "                'max_characters': max(text_lengths),\n",
    "                'empty_pages': len([p for p in valid_pages if p['text_length'] == 0])\n",
    "            },\n",
    "            'confidence_statistics': {},\n",
    "            'content_statistics': {},\n",
    "            'layout_distribution': {}\n",
    "        }\n",
    "        \n",
    "        # ì‹ ë¢°ë„ í†µê³„\n",
    "        if confidences:\n",
    "            statistics['confidence_statistics'] = {\n",
    "                'avg_confidence': sum(confidences) / len(confidences),\n",
    "                'min_confidence': min(confidences),\n",
    "                'max_confidence': max(confidences),\n",
    "                'low_confidence_pages': len([c for c in confidences if c < 0.7])\n",
    "            }\n",
    "        \n",
    "        # ì½˜í…ì¸  í†µê³„\n",
    "        total_words = sum(p['word_count'] for p in valid_pages if 'word_count' in p)\n",
    "        total_sentences = sum(p['sentence_count'] for p in valid_pages if 'sentence_count' in p)\n",
    "        \n",
    "        statistics['content_statistics'] = {\n",
    "            'total_words': total_words,\n",
    "            'total_sentences': total_sentences,\n",
    "            'avg_korean_ratio': sum(korean_ratios) / len(korean_ratios),\n",
    "            'pages_with_tables': len([p for p in valid_pages if p.get('table_count', 0) > 0]),\n",
    "            'total_tables': sum(p.get('table_count', 0) for p in valid_pages)\n",
    "        }\n",
    "        \n",
    "        # ë ˆì´ì•„ì›ƒ ë¶„í¬\n",
    "        layout_types = [p['layout_type'] for p in valid_pages]\n",
    "        layout_counter = Counter(layout_types)\n",
    "        statistics['layout_distribution'] = dict(layout_counter)\n",
    "        \n",
    "        return statistics\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def print_page_statistics_azure(statistics):\n",
    "    \"\"\"í˜ì´ì§€ í†µê³„ ê²°ê³¼ ì¶œë ¥\"\"\"\n",
    "    try:\n",
    "        if 'error' in statistics:\n",
    "            print(f\"   âŒ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {statistics['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(\"   ğŸ“Š í˜ì´ì§€ ë¶„ì„ í†µê³„:\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ í†µê³„\n",
    "        text_stats = statistics.get('text_statistics', {})\n",
    "        print(f\"      ğŸ“ ì´ ë¬¸ì: {text_stats.get('total_characters', 0):,}ì\")\n",
    "        print(f\"      ğŸ“„ í˜ì´ì§€ë‹¹ í‰ê· : {text_stats.get('avg_characters_per_page', 0):,.0f}ì\")\n",
    "        print(f\"      ğŸ“ ìµœì†Œ/ìµœëŒ€: {text_stats.get('min_characters', 0):,}ì / {text_stats.get('max_characters', 0):,}ì\")\n",
    "        \n",
    "        if text_stats.get('empty_pages', 0) > 0:\n",
    "            print(f\"      ğŸš« ë¹ˆ í˜ì´ì§€: {text_stats['empty_pages']}ê°œ\")\n",
    "        \n",
    "        # ì‹ ë¢°ë„ í†µê³„\n",
    "        conf_stats = statistics.get('confidence_statistics', {})\n",
    "        if conf_stats:\n",
    "            print(f\"      ğŸ¯ í‰ê·  ì‹ ë¢°ë„: {conf_stats.get('avg_confidence', 0):.3f}\")\n",
    "            print(f\"      ğŸ“‰ ìµœì†Œ/ìµœëŒ€: {conf_stats.get('min_confidence', 0):.3f} / {conf_stats.get('max_confidence', 0):.3f}\")\n",
    "            \n",
    "            if conf_stats.get('low_confidence_pages', 0) > 0:\n",
    "                print(f\"      âš ï¸ ë‚®ì€ ì‹ ë¢°ë„ í˜ì´ì§€: {conf_stats['low_confidence_pages']}ê°œ\")\n",
    "        \n",
    "        # ì½˜í…ì¸  í†µê³„\n",
    "        content_stats = statistics.get('content_statistics', {})\n",
    "        if content_stats:\n",
    "            print(f\"      ğŸ”¤ ì´ ë‹¨ì–´: {content_stats.get('total_words', 0):,}ê°œ\")\n",
    "            print(f\"      ğŸ“– ì´ ë¬¸ì¥: {content_stats.get('total_sentences', 0):,}ê°œ\")\n",
    "            print(f\"      ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë¹„ìœ¨: {content_stats.get('avg_korean_ratio', 0):.1%}\")\n",
    "            \n",
    "            if content_stats.get('total_tables', 0) > 0:\n",
    "                print(f\"      ğŸ“Š í…Œì´ë¸”: {content_stats['total_tables']}ê°œ ({content_stats.get('pages_with_tables', 0)}í˜ì´ì§€)\")\n",
    "        \n",
    "        # ë ˆì´ì•„ì›ƒ ë¶„í¬\n",
    "        layout_dist = statistics.get('layout_distribution', {})\n",
    "        if layout_dist:\n",
    "            print(f\"      ğŸ—ï¸ ë ˆì´ì•„ì›ƒ ë¶„í¬:\")\n",
    "            for layout_type, count in layout_dist.items():\n",
    "                print(f\"         {layout_type}: {count}í˜ì´ì§€\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ í†µê³„ ì¶œë ¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(\"ğŸ“¦ í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… analyze_pages_detailed_azure: ì „ì²´ í˜ì´ì§€ ë¶„ì„\")\n",
    "print(\"   âœ… analyze_single_page_azure: ë‹¨ì¼ í˜ì´ì§€ ë¶„ì„\")\n",
    "print(\"   âœ… calculate_page_statistics_azure: í˜ì´ì§€ í†µê³„ ê³„ì‚°\")\n",
    "print(\"   âœ… print_page_statistics_azure: í†µê³„ ê²°ê³¼ ì¶œë ¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82838f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ í˜ì´ì§€ë³„ ë‚´ìš© í™•ì¸ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… display_page_content_azure: í˜ì´ì§€ ë‚´ìš© í‘œì‹œ\n",
      "   âœ… compare_pages_azure: í˜ì´ì§€ ê°„ ë¹„êµ\n",
      "   âœ… export_page_analysis_azure: ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   # ëª¨ë“  í˜ì´ì§€ ë‚´ìš© ë³´ê¸°\n",
      "   display_page_content_azure(extraction_result)\n",
      "   # íŠ¹ì • í˜ì´ì§€ë§Œ ë³´ê¸°\n",
      "   display_page_content_azure(extraction_result, page_number=1)\n",
      "   # ë‘ í˜ì´ì§€ ë¹„êµ\n",
      "   compare_pages_azure(extraction_result, 1, 2)\n",
      "   # ê²°ê³¼ ë‚´ë³´ë‚´ê¸°\n",
      "   export_page_analysis_azure(extraction_result)\n"
     ]
    }
   ],
   "source": [
    "# í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ë‚´ìš© í™•ì¸ í•¨ìˆ˜ë“¤\n",
    "\n",
    "def display_page_content_azure(extraction_result, page_number=None, max_chars=1000):\n",
    "    \"\"\"ì§€ì •ëœ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ë‚´ìš© í‘œì‹œ\"\"\"\n",
    "    try:\n",
    "        page_results = extraction_result.get('page_results', [])\n",
    "        \n",
    "        if not page_results:\n",
    "            print(\"âŒ í˜ì´ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        if page_number is None:\n",
    "            # ëª¨ë“  í˜ì´ì§€ í‘œì‹œ\n",
    "            print(f\"ğŸ“„ ì „ì²´ {len(page_results)}ê°œ í˜ì´ì§€ ë‚´ìš©:\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            for i, page in enumerate(page_results, 1):\n",
    "                display_single_page_content_azure(page, i, max_chars)\n",
    "                if i < len(page_results):\n",
    "                    print(\"\\\\n\" + \"-\" * 80 + \"\\\\n\")\n",
    "        else:\n",
    "            # íŠ¹ì • í˜ì´ì§€ í‘œì‹œ\n",
    "            if 1 <= page_number <= len(page_results):\n",
    "                page = page_results[page_number - 1]\n",
    "                display_single_page_content_azure(page, page_number, max_chars)\n",
    "            else:\n",
    "                print(f\"âŒ ì˜ëª»ëœ í˜ì´ì§€ ë²ˆí˜¸: {page_number} (1-{len(page_results)} ë²”ìœ„)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í˜ì´ì§€ ë‚´ìš© í‘œì‹œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "def display_single_page_content_azure(page_result, page_number, max_chars=1000):\n",
    "    \"\"\"ë‹¨ì¼ í˜ì´ì§€ ë‚´ìš© í‘œì‹œ\"\"\"\n",
    "    try:\n",
    "        page_text = page_result.get('text', '')\n",
    "        layout_type = page_result.get('layout_type', 'unknown')\n",
    "        confidence = page_result.get('avg_confidence', 0.0)\n",
    "        \n",
    "        # í˜ì´ì§€ í—¤ë”\n",
    "        print(f\"ğŸ“„ í˜ì´ì§€ {page_number}\")\n",
    "        print(f\"   ğŸ—ï¸ ë ˆì´ì•„ì›ƒ: {layout_type}\")\n",
    "        print(f\"   ğŸ¯ ì‹ ë¢°ë„: {confidence:.3f}\")\n",
    "        print(f\"   ğŸ“ ê¸¸ì´: {len(page_text):,}ì\")\n",
    "        \n",
    "        if page_result.get('table_count', 0) > 0:\n",
    "            print(f\"   ğŸ“Š í…Œì´ë¸”: {page_result['table_count']}ê°œ\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ë‚´ìš©\n",
    "        if page_text:\n",
    "            if len(page_text) > max_chars:\n",
    "                print(f\"ğŸ“ í…ìŠ¤íŠ¸ ë‚´ìš© (ì²« {max_chars}ì):\")\n",
    "                display_text = page_text[:max_chars] + \"\\\\n\\\\n... (ìƒëµ) ...\"\n",
    "            else:\n",
    "                print(f\"ğŸ“ í…ìŠ¤íŠ¸ ë‚´ìš© (ì „ì²´):\")\n",
    "                display_text = page_text\n",
    "            \n",
    "            print(\"â”Œ\" + \"â”€\" * 78 + \"â”\")\n",
    "            for line in display_text.split('\\\\n'):\n",
    "                # ê¸´ ì¤„ì€ ìë¥´ê¸°\n",
    "                if len(line) > 76:\n",
    "                    line = line[:73] + \"...\"\n",
    "                print(f\"â”‚ {line:<76} â”‚\")\n",
    "            print(\"â””\" + \"â”€\" * 78 + \"â”˜\")\n",
    "            \n",
    "            # í‚¤ì›Œë“œ í‘œì‹œ\n",
    "            keywords = page_result.get('keywords', [])\n",
    "            if keywords:\n",
    "                print(f\"ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords[:5])}\")\n",
    "        else:\n",
    "            print(\"ğŸ“ í…ìŠ¤íŠ¸ ë‚´ìš©: (ë¹ˆ í˜ì´ì§€)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í˜ì´ì§€ {page_number} í‘œì‹œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "def compare_pages_azure(extraction_result, page1, page2):\n",
    "    \"\"\"ë‘ í˜ì´ì§€ ê°„ ë¹„êµ ë¶„ì„\"\"\"\n",
    "    try:\n",
    "        page_results = extraction_result.get('page_results', [])\n",
    "        \n",
    "        if not page_results or len(page_results) < max(page1, page2):\n",
    "            print(\"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ í˜ì´ì§€ ë²ˆí˜¸ì…ë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        p1 = page_results[page1 - 1]\n",
    "        p2 = page_results[page2 - 1]\n",
    "        \n",
    "        print(f\"ğŸ”„ í˜ì´ì§€ {page1} vs í˜ì´ì§€ {page2} ë¹„êµ\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # ê¸°ë³¸ ì •ë³´ ë¹„êµ\n",
    "        comparison_data = {\n",
    "            'í•­ëª©': ['í…ìŠ¤íŠ¸ ê¸¸ì´', 'ë ˆì´ì•„ì›ƒ íƒ€ì…', 'ì‹ ë¢°ë„', 'ë‹¨ì–´ ìˆ˜', 'ë¬¸ë‹¨ ìˆ˜', 'í…Œì´ë¸” ìˆ˜'],\n",
    "            f'í˜ì´ì§€ {page1}': [\n",
    "                f\"{len(p1.get('text', '')):,}ì\",\n",
    "                p1.get('layout_type', 'unknown'),\n",
    "                f\"{p1.get('avg_confidence', 0.0):.3f}\",\n",
    "                f\"{p1.get('word_count', 0):,}ê°œ\",\n",
    "                f\"{p1.get('paragraph_count', 0)}ê°œ\",\n",
    "                f\"{p1.get('table_count', 0)}ê°œ\"\n",
    "            ],\n",
    "            f'í˜ì´ì§€ {page2}': [\n",
    "                f\"{len(p2.get('text', '')):,}ì\",\n",
    "                p2.get('layout_type', 'unknown'),\n",
    "                f\"{p2.get('avg_confidence', 0.0):.3f}\",\n",
    "                f\"{p2.get('word_count', 0):,}ê°œ\",\n",
    "                f\"{p2.get('paragraph_count', 0)}ê°œ\",\n",
    "                f\"{p2.get('table_count', 0)}ê°œ\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        print(df.to_string(index=False))\n",
    "        print()\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "        text1 = p1.get('text', '')\n",
    "        text2 = p2.get('text', '')\n",
    "        \n",
    "        if text1 and text2:\n",
    "            similarity = difflib.SequenceMatcher(None, text1, text2).ratio()\n",
    "            print(f\"ğŸ“Š í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {similarity:.2%}\")\n",
    "            \n",
    "            # ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸°\n",
    "            keywords1 = set(p1.get('keywords', []))\n",
    "            keywords2 = set(p2.get('keywords', []))\n",
    "            common_keywords = keywords1 & keywords2\n",
    "            \n",
    "            if common_keywords:\n",
    "                print(f\"ğŸ”‘ ê³µí†µ í‚¤ì›Œë“œ: {', '.join(common_keywords)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í˜ì´ì§€ ë¹„êµ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "def export_page_analysis_azure(extraction_result, output_dir=\"./azure_analysis_results\"):\n",
    "    \"\"\"í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°\"\"\"\n",
    "    try:\n",
    "        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        Path(output_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        page_results = extraction_result.get('page_results', [])\n",
    "        if not page_results:\n",
    "            print(\"âŒ ë‚´ë³´ë‚¼ í˜ì´ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ğŸ“¤ í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°: {output_dir}\")\n",
    "        \n",
    "        # 1. ì „ì²´ ìš”ì•½ JSON\n",
    "        summary_data = {\n",
    "            'document_metadata': extraction_result.get('metadata', {}),\n",
    "            'page_count': len(page_results),\n",
    "            'processing_timestamp': datetime.now().isoformat(),\n",
    "            'total_text_length': len(extraction_result.get('text', '')),\n",
    "            'pages_summary': []\n",
    "        }\n",
    "        \n",
    "        # 2. í˜ì´ì§€ë³„ ìƒì„¸ ë°ì´í„°\n",
    "        for i, page in enumerate(page_results, 1):\n",
    "            page_summary = {\n",
    "                'page_number': i,\n",
    "                'text_length': len(page.get('text', '')),\n",
    "                'layout_type': page.get('layout_type', 'unknown'),\n",
    "                'confidence': page.get('avg_confidence', 0.0),\n",
    "                'element_count': page.get('element_count', 0),\n",
    "                'table_count': page.get('table_count', 0),\n",
    "                'paragraph_count': page.get('paragraph_count', 0)\n",
    "            }\n",
    "            summary_data['pages_summary'].append(page_summary)\n",
    "            \n",
    "            # ê°œë³„ í˜ì´ì§€ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥\n",
    "            page_text_file = Path(output_dir) / f\"page_{i:02d}_text.txt\"\n",
    "            with open(page_text_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"í˜ì´ì§€ {i} - {page.get('layout_type', 'unknown')} ë ˆì´ì•„ì›ƒ\\\\n\")\n",
    "                f.write(f\"ì‹ ë¢°ë„: {page.get('avg_confidence', 0.0):.3f}\\\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\\\n\\\\n\")\n",
    "                f.write(page.get('text', ''))\n",
    "        \n",
    "        # ìš”ì•½ JSON ì €ì¥\n",
    "        summary_file = Path(output_dir) / \"document_analysis_summary.json\"\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # CSV ìš”ì•½ ì €ì¥\n",
    "        if summary_data['pages_summary']:\n",
    "            df = pd.DataFrame(summary_data['pages_summary'])\n",
    "            csv_file = Path(output_dir) / \"pages_summary.csv\"\n",
    "            df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "        full_text_file = Path(output_dir) / \"full_document_text.txt\"\n",
    "        with open(full_text_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(extraction_result.get('text', ''))\n",
    "        \n",
    "        print(f\"   âœ… ìš”ì•½ JSON: document_analysis_summary.json\")\n",
    "        print(f\"   âœ… í˜ì´ì§€ CSV: pages_summary.csv\")\n",
    "        print(f\"   âœ… ì „ì²´ í…ìŠ¤íŠ¸: full_document_text.txt\")\n",
    "        print(f\"   âœ… í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸: page_XX_text.txt ({len(page_results)}ê°œ)\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'output_directory': output_dir,\n",
    "            'files_created': len(page_results) + 3\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"ğŸ“¦ í˜ì´ì§€ë³„ ë‚´ìš© í™•ì¸ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… display_page_content_azure: í˜ì´ì§€ ë‚´ìš© í‘œì‹œ\")\n",
    "print(\"   âœ… compare_pages_azure: í˜ì´ì§€ ê°„ ë¹„êµ\")\n",
    "print(\"   âœ… export_page_analysis_azure: ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "print(\"   # ëª¨ë“  í˜ì´ì§€ ë‚´ìš© ë³´ê¸°\")\n",
    "print(\"   display_page_content_azure(extraction_result)\")\n",
    "print(\"   # íŠ¹ì • í˜ì´ì§€ë§Œ ë³´ê¸°\")\n",
    "print(\"   display_page_content_azure(extraction_result, page_number=1)\")\n",
    "print(\"   # ë‘ í˜ì´ì§€ ë¹„êµ\")\n",
    "print(\"   compare_pages_azure(extraction_result, 1, 2)\")\n",
    "print(\"   # ê²°ê³¼ ë‚´ë³´ë‚´ê¸°\")\n",
    "print(\"   export_page_analysis_azure(extraction_result)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2b21121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ í˜ì´ì§€ë³„ í’ˆì§ˆ ê²€ì¦ ë° ë¬¸ì œì  ë¶„ì„ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… analyze_page_quality_issues_azure: í˜ì´ì§€ë³„ í’ˆì§ˆ ë¬¸ì œ ë¶„ì„\n",
      "   âœ… check_single_page_quality_azure: ë‹¨ì¼ í˜ì´ì§€ í’ˆì§ˆ ê²€ì‚¬\n",
      "   âœ… suggest_page_improvements_azure: ê°œì„  ë°©ì•ˆ ì œì•ˆ\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš©ë²•:\n",
      "   # í˜ì´ì§€ë³„ í’ˆì§ˆ ë¬¸ì œ ë¶„ì„\n",
      "   quality_issues = analyze_page_quality_issues_azure(extraction_result)\n",
      "   # ê°œì„  ë°©ì•ˆ ì œì•ˆ\n",
      "   suggest_page_improvements_azure(quality_issues)\n"
     ]
    }
   ],
   "source": [
    "# í˜ì´ì§€ë³„ í’ˆì§ˆ ê²€ì¦ ë° ë¬¸ì œì  ë¶„ì„\n",
    "\n",
    "def analyze_page_quality_issues_azure(extraction_result):\n",
    "    \"\"\"í˜ì´ì§€ë³„ í’ˆì§ˆ ë¬¸ì œì  ë¶„ì„\"\"\"\n",
    "    try:\n",
    "        page_results = extraction_result.get('page_results', [])\n",
    "        if not page_results:\n",
    "            print(\"âŒ ë¶„ì„í•  í˜ì´ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"ğŸ” í˜ì´ì§€ë³„ í’ˆì§ˆ ë¬¸ì œì  ë¶„ì„ ({len(page_results)}ê°œ í˜ì´ì§€)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        quality_issues = {\n",
    "            'total_pages': len(page_results),\n",
    "            'problematic_pages': [],\n",
    "            'issue_summary': {\n",
    "                'low_confidence': 0,\n",
    "                'empty_pages': 0,\n",
    "                'layout_issues': 0,\n",
    "                'text_quality_issues': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for i, page in enumerate(page_results, 1):\n",
    "            page_issues = check_single_page_quality_azure(page, i)\n",
    "            \n",
    "            if page_issues['has_issues']:\n",
    "                quality_issues['problematic_pages'].append(page_issues)\n",
    "                \n",
    "                # ì´ìŠˆ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸\n",
    "                for issue_type in page_issues['issues']:\n",
    "                    if 'confidence' in issue_type:\n",
    "                        quality_issues['issue_summary']['low_confidence'] += 1\n",
    "                    elif 'empty' in issue_type:\n",
    "                        quality_issues['issue_summary']['empty_pages'] += 1\n",
    "                    elif 'layout' in issue_type:\n",
    "                        quality_issues['issue_summary']['layout_issues'] += 1\n",
    "                    elif 'text' in issue_type:\n",
    "                        quality_issues['issue_summary']['text_quality_issues'] += 1\n",
    "            \n",
    "            # í˜ì´ì§€ë³„ ê²°ê³¼ ì¶œë ¥\n",
    "            status = \"âŒ ë¬¸ì œ\" if page_issues['has_issues'] else \"âœ… ì •ìƒ\"\n",
    "            confidence = page.get('avg_confidence', 0.0)\n",
    "            text_len = len(page.get('text', ''))\n",
    "            \n",
    "            print(f\"í˜ì´ì§€ {i:2d}: {status} | ì‹ ë¢°ë„: {confidence:.3f} | í…ìŠ¤íŠ¸: {text_len:,}ì\")\n",
    "            \n",
    "            if page_issues['has_issues']:\n",
    "                for issue in page_issues['issues']:\n",
    "                    print(f\"         â€¢ {issue}\")\n",
    "        \n",
    "        # ìš”ì•½ ì¶œë ¥\n",
    "        print(\"\\\\nğŸ“Š í’ˆì§ˆ ë¬¸ì œ ìš”ì•½:\")\n",
    "        print(f\"   ğŸ”´ ë¬¸ì œ í˜ì´ì§€: {len(quality_issues['problematic_pages'])}/{quality_issues['total_pages']}ê°œ\")\n",
    "        print(f\"   ğŸ“‰ ë‚®ì€ ì‹ ë¢°ë„: {quality_issues['issue_summary']['low_confidence']}ê°œ\")\n",
    "        print(f\"   ğŸ“„ ë¹ˆ í˜ì´ì§€: {quality_issues['issue_summary']['empty_pages']}ê°œ\")\n",
    "        print(f\"   ğŸ—ï¸ ë ˆì´ì•„ì›ƒ ë¬¸ì œ: {quality_issues['issue_summary']['layout_issues']}ê°œ\")\n",
    "        print(f\"   ğŸ“ í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¬¸ì œ: {quality_issues['issue_summary']['text_quality_issues']}ê°œ\")\n",
    "        \n",
    "        return quality_issues\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def check_single_page_quality_azure(page_result, page_number):\n",
    "    \"\"\"ë‹¨ì¼ í˜ì´ì§€ í’ˆì§ˆ ë¬¸ì œ í™•ì¸\"\"\"\n",
    "    try:\n",
    "        issues = []\n",
    "        page_text = page_result.get('text', '')\n",
    "        confidence = page_result.get('avg_confidence', 0.0)\n",
    "        layout_type = page_result.get('layout_type', 'unknown')\n",
    "        \n",
    "        # 1. ì‹ ë¢°ë„ ë¬¸ì œ\n",
    "        if confidence < 0.7:\n",
    "            if confidence < 0.5:\n",
    "                issues.append(f\"ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„ ({confidence:.3f})\")\n",
    "            else:\n",
    "                issues.append(f\"ë‚®ì€ ì‹ ë¢°ë„ ({confidence:.3f})\")\n",
    "        \n",
    "        # 2. ë¹ˆ í˜ì´ì§€ ë¬¸ì œ\n",
    "        if not page_text or len(page_text.strip()) < 10:\n",
    "            issues.append(\"ë¹ˆ í˜ì´ì§€ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶€ì¡±\")\n",
    "        \n",
    "        # 3. ë ˆì´ì•„ì›ƒ ë¬¸ì œ\n",
    "        if layout_type == 'unknown':\n",
    "            issues.append(\"ë ˆì´ì•„ì›ƒ íƒ€ì… ì¸ì‹ ì‹¤íŒ¨\")\n",
    "        \n",
    "        # 4. í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¬¸ì œ\n",
    "        if page_text:\n",
    "            # í•œê¸€ ë¹„ìœ¨ í™•ì¸\n",
    "            korean_chars = len(re.findall(r'[ê°€-í£]', page_text))\n",
    "            korean_ratio = korean_chars / len(page_text)\n",
    "            \n",
    "            if korean_ratio < 0.1:\n",
    "                issues.append(f\"í•œêµ­ì–´ ë¹„ìœ¨ ë‚®ìŒ ({korean_ratio:.1%})\")\n",
    "            \n",
    "            # íŠ¹ìˆ˜ ë¬¸ì ê³¼ë‹¤\n",
    "            special_chars = len(re.findall(r'[^\\\\w\\\\sê°€-í£]', page_text))\n",
    "            special_ratio = special_chars / len(page_text)\n",
    "            \n",
    "            if special_ratio > 0.3:\n",
    "                issues.append(f\"íŠ¹ìˆ˜ ë¬¸ì ê³¼ë‹¤ ({special_ratio:.1%})\")\n",
    "            \n",
    "            # ë°˜ë³µ ë¬¸ì í™•ì¸\n",
    "            if re.search(r'(.)\\\\1{5,}', page_text):\n",
    "                issues.append(\"ë°˜ë³µ ë¬¸ì íŒ¨í„´ ë°œê²¬\")\n",
    "            \n",
    "            # ë„ˆë¬´ ê¸´ ë‹¨ì–´ (OCR ì˜¤ë¥˜ ê°€ëŠ¥ì„±)\n",
    "            words = page_text.split()\n",
    "            long_words = [w for w in words if len(w) > 20]\n",
    "            if len(long_words) > 5:\n",
    "                issues.append(f\"ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ë‹¨ì–´ {len(long_words)}ê°œ\")\n",
    "        \n",
    "        # 5. êµ¬ì¡°ì  ë¬¸ì œ\n",
    "        element_count = page_result.get('element_count', 0)\n",
    "        if element_count == 0 and page_text:\n",
    "            issues.append(\"êµ¬ì¡° ìš”ì†Œ ì¸ì‹ ì‹¤íŒ¨\")\n",
    "        \n",
    "        return {\n",
    "            'page_number': page_number,\n",
    "            'has_issues': len(issues) > 0,\n",
    "            'issues': issues,\n",
    "            'issue_count': len(issues),\n",
    "            'confidence': confidence,\n",
    "            'text_length': len(page_text)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'page_number': page_number,\n",
    "            'has_issues': True,\n",
    "            'issues': [f\"í’ˆì§ˆ ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}\"],\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def suggest_page_improvements_azure(quality_issues):\n",
    "    \"\"\"í˜ì´ì§€ë³„ ê°œì„  ë°©ì•ˆ ì œì•ˆ\"\"\"\n",
    "    try:\n",
    "        if not quality_issues or 'problematic_pages' not in quality_issues:\n",
    "            print(\"ğŸ“‹ ê°œì„  ë°©ì•ˆ: ëª¨ë“  í˜ì´ì§€ê°€ ì–‘í˜¸í•œ í’ˆì§ˆì…ë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        problematic_pages = quality_issues['problematic_pages']\n",
    "        if not problematic_pages:\n",
    "            print(\"ğŸ“‹ ê°œì„  ë°©ì•ˆ: ë¬¸ì œê°€ ìˆëŠ” í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ğŸ“‹ í˜ì´ì§€ë³„ ê°œì„  ë°©ì•ˆ ({len(problematic_pages)}ê°œ í˜ì´ì§€)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for page_issue in problematic_pages:\n",
    "            page_num = page_issue['page_number']\n",
    "            issues = page_issue['issues']\n",
    "            \n",
    "            print(f\"\\\\nğŸ“„ í˜ì´ì§€ {page_num}:\")\n",
    "            print(f\"   ğŸ”´ ë¬¸ì œì : {len(issues)}ê°œ\")\n",
    "            \n",
    "            for issue in issues:\n",
    "                print(f\"      â€¢ {issue}\")\n",
    "            \n",
    "            print(f\"   ğŸ’¡ ê°œì„  ë°©ì•ˆ:\")\n",
    "            \n",
    "            # ì´ìŠˆë³„ ë§ì¶¤ ê°œì„  ë°©ì•ˆ\n",
    "            for issue in issues:\n",
    "                if 'ì‹ ë¢°ë„' in issue:\n",
    "                    print(f\"      âœ“ ë¬¸ì„œ í•´ìƒë„ í–¥ìƒ ë˜ëŠ” ìŠ¤ìº” í’ˆì§ˆ ê°œì„ \")\n",
    "                    print(f\"      âœ“ ë‹¤ë¥¸ Azure ëª¨ë¸(prebuilt-layout) ì‹œë„\")\n",
    "                elif 'ë¹ˆ í˜ì´ì§€' in issue:\n",
    "                    print(f\"      âœ“ í˜ì´ì§€ ì´ë¯¸ì§€ í’ˆì§ˆ í™•ì¸\")\n",
    "                    print(f\"      âœ“ ë¬¸ì„œ ë°©í–¥ ë° íšŒì „ ìƒíƒœ ì ê²€\")\n",
    "                elif 'ë ˆì´ì•„ì›ƒ' in issue:\n",
    "                    print(f\"      âœ“ prebuilt-layout ëª¨ë¸ ì‚¬ìš© ê¶Œì¥\")\n",
    "                    print(f\"      âœ“ í˜ì´ì§€ ì „ì²˜ë¦¬ (íšŒì „, ê¸°ìš¸ê¸° ë³´ì •)\")\n",
    "                elif 'í•œêµ­ì–´ ë¹„ìœ¨' in issue:\n",
    "                    print(f\"      âœ“ í•œêµ­ì–´ ë¬¸ì„œì¸ì§€ ì¬í™•ì¸\")\n",
    "                    print(f\"      âœ“ OCR ì–¸ì–´ ì„¤ì • ì ê²€\")\n",
    "                elif 'íŠ¹ìˆ˜ ë¬¸ì' in issue:\n",
    "                    print(f\"      âœ“ í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ë¡œ ë…¸ì´ì¦ˆ ì œê±°\")\n",
    "                    print(f\"      âœ“ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¡œ ë…¸ì´ì¦ˆ ê°ì†Œ\")\n",
    "                elif 'ë°˜ë³µ ë¬¸ì' in issue:\n",
    "                    print(f\"      âœ“ OCR í›„ì²˜ë¦¬ë¡œ ë°˜ë³µ íŒ¨í„´ ì œê±°\")\n",
    "                elif 'ê¸´ ë‹¨ì–´' in issue:\n",
    "                    print(f\"      âœ“ ë‹¨ì–´ ë¶„í•  í›„ì²˜ë¦¬ ì ìš©\")\n",
    "                    print(f\"      âœ“ í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ê²€ì¦\")\n",
    "        \n",
    "        # ì „ì²´ì ì¸ ê¶Œì¥ì‚¬í•­\n",
    "        print(\"\\\\nğŸ¯ ì „ì²´ì ì¸ ê¶Œì¥ì‚¬í•­:\")\n",
    "        issue_summary = quality_issues['issue_summary']\n",
    "        \n",
    "        if issue_summary['low_confidence'] > 0:\n",
    "            print(f\"   ğŸ“ˆ {issue_summary['low_confidence']}ê°œ í˜ì´ì§€ì˜ ë‚®ì€ ì‹ ë¢°ë„ í•´ê²°ì´ ìš°ì„ \")\n",
    "        \n",
    "        if issue_summary['empty_pages'] > 0:\n",
    "            print(f\"   ğŸ“„ {issue_summary['empty_pages']}ê°œ ë¹ˆ í˜ì´ì§€ ì›ì¸ ë¶„ì„ í•„ìš”\")\n",
    "        \n",
    "        total_issues = sum(issue_summary.values())\n",
    "        total_pages = quality_issues['total_pages']\n",
    "        \n",
    "        if total_issues / total_pages > 0.5:\n",
    "            print(f\"   âš ï¸ ì „ì²´ì ì¸ ë¬¸ì„œ í’ˆì§ˆ ê°œì„  í•„ìš” (ë¬¸ì œìœ¨: {total_issues/total_pages:.1%})\")\n",
    "            print(f\"   ğŸ”§ ë‹¤ë¥¸ Azure ëª¨ë¸ ë˜ëŠ” ì „ì²˜ë¦¬ ë°©ë²• ê³ ë ¤\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê°œì„  ë°©ì•ˆ ì œì•ˆ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(\"ğŸ“¦ í˜ì´ì§€ë³„ í’ˆì§ˆ ê²€ì¦ ë° ë¬¸ì œì  ë¶„ì„ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… analyze_page_quality_issues_azure: í˜ì´ì§€ë³„ í’ˆì§ˆ ë¬¸ì œ ë¶„ì„\")\n",
    "print(\"   âœ… check_single_page_quality_azure: ë‹¨ì¼ í˜ì´ì§€ í’ˆì§ˆ ê²€ì‚¬\")\n",
    "print(\"   âœ… suggest_page_improvements_azure: ê°œì„  ë°©ì•ˆ ì œì•ˆ\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "print(\"   # í˜ì´ì§€ë³„ í’ˆì§ˆ ë¬¸ì œ ë¶„ì„\")\n",
    "print(\"   quality_issues = analyze_page_quality_issues_azure(extraction_result)\")\n",
    "print(\"   # ê°œì„  ë°©ì•ˆ ì œì•ˆ\")\n",
    "print(\"   suggest_page_improvements_azure(quality_issues)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1162db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì œ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… run_comprehensive_page_analysis_example: í¬ê´„ì  ë¶„ì„ ì˜ˆì œ\n",
      "   âœ… quick_page_test_example: ë¹ ë¥¸ í˜ì´ì§€ í…ŒìŠ¤íŠ¸\n",
      "\n",
      "ğŸ¯ ì‹¤í–‰ ë°©ë²•:\n",
      "   # í¬ê´„ì  ë¶„ì„ ì‹¤í–‰\n",
      "   result = run_comprehensive_page_analysis_example()\n",
      "\n",
      "   # íŠ¹ì • íŒŒì¼ì˜ íŠ¹ì • í˜ì´ì§€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n",
      "   quick_page_test_example('/path/to/document.pdf', page_number=1)\n",
      "\n",
      "   # ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ í™•ì¸\n",
      "   for doc in TEST_DOCUMENTS:\n",
      "       if os.path.exists(doc):\n",
      "           print(f'âœ… {doc}')\n",
      "       else:\n",
      "           print(f'âŒ {doc}')\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì œ\n",
    "\n",
    "def run_comprehensive_page_analysis_example():\n",
    "    \"\"\"í¬ê´„ì ì¸ í˜ì´ì§€ ë¶„ì„ ì˜ˆì œ ì‹¤í–‰\"\"\"\n",
    "    try:\n",
    "        print(\"ğŸš€ Azure Document Intelligence í¬ê´„ì  í˜ì´ì§€ ë¶„ì„ ì˜ˆì œ\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œ íŒŒì¼ ì„ íƒ\n",
    "        available_docs = [doc for doc in TEST_DOCUMENTS if os.path.exists(doc)]\n",
    "        \n",
    "        if not available_docs:\n",
    "            print(\"âŒ í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            print(\"   ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”:\")\n",
    "            for doc in TEST_DOCUMENTS:\n",
    "                print(f\"   â€¢ {doc}\")\n",
    "            return\n",
    "        \n",
    "        test_file = available_docs[0]\n",
    "        print(f\"ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {os.path.basename(test_file)}\")\n",
    "        print()\n",
    "        \n",
    "        # 1. ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "        print(\"1ï¸âƒ£ ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\")\n",
    "        test_result = run_azure_document_test(test_file)\n",
    "        \n",
    "        if not test_result['success']:\n",
    "            print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_result['error']}\")\n",
    "            return\n",
    "        \n",
    "        extraction_result = test_result['extraction_result']\n",
    "        print()\n",
    "        \n",
    "        # 2. ì²« ë²ˆì§¸ í˜ì´ì§€ ìƒì„¸ ë³´ê¸°\n",
    "        print(\"2ï¸âƒ£ ì²« ë²ˆì§¸ í˜ì´ì§€ ìƒì„¸ ë‚´ìš©\")\n",
    "        display_page_content_azure(extraction_result, page_number=1, max_chars=500)\n",
    "        print()\n",
    "        \n",
    "        # 3. í˜ì´ì§€ë³„ í’ˆì§ˆ ë¬¸ì œ ë¶„ì„\n",
    "        print(\"3ï¸âƒ£ í˜ì´ì§€ë³„ í’ˆì§ˆ ë¬¸ì œ ë¶„ì„\")\n",
    "        quality_issues = analyze_page_quality_issues_azure(extraction_result)\n",
    "        print()\n",
    "        \n",
    "        # 4. ê°œì„  ë°©ì•ˆ ì œì•ˆ\n",
    "        print(\"4ï¸âƒ£ ê°œì„  ë°©ì•ˆ ì œì•ˆ\")\n",
    "        suggest_page_improvements_azure(quality_issues)\n",
    "        print()\n",
    "        \n",
    "        # 5. í˜ì´ì§€ ë¹„êµ (2í˜ì´ì§€ ì´ìƒì¸ ê²½ìš°)\n",
    "        page_count = len(extraction_result.get('page_results', []))\n",
    "        if page_count >= 2:\n",
    "            print(\"5ï¸âƒ£ í˜ì´ì§€ ë¹„êµ ë¶„ì„\")\n",
    "            compare_pages_azure(extraction_result, 1, 2)\n",
    "            print()\n",
    "        \n",
    "        # 6. ê²°ê³¼ ë‚´ë³´ë‚´ê¸°\n",
    "        print(\"6ï¸âƒ£ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°\")\n",
    "        export_result = export_page_analysis_azure(extraction_result)\n",
    "        \n",
    "        if export_result['success']:\n",
    "            print(f\"   âœ… {export_result['files_created']}ê°œ íŒŒì¼ì´ {export_result['output_directory']}ì— ì €ì¥ë¨\")\n",
    "        \n",
    "        print()\n",
    "        print(\"ğŸ‰ í¬ê´„ì  í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ!\")\n",
    "        \n",
    "        return test_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜ˆì œ ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def quick_page_test_example(file_path, page_number=1):\n",
    "    \"\"\"íŠ¹ì • í˜ì´ì§€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì˜ˆì œ\"\"\"\n",
    "    try:\n",
    "        print(f\"âš¡ ë¹ ë¥¸ í˜ì´ì§€ í…ŒìŠ¤íŠ¸: {os.path.basename(file_path)}\")\n",
    "        print(f\"ğŸ¯ ëŒ€ìƒ í˜ì´ì§€: {page_number}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "            return\n",
    "        \n",
    "        # Azureë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        print(\"ğŸ“¤ Azure Document Intelligence ì¶”ì¶œ ì¤‘...\")\n",
    "        extraction_result = extract_text_with_azure(file_path)\n",
    "        \n",
    "        if not extraction_result['success']:\n",
    "            print(f\"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {extraction_result['error']}\")\n",
    "            return\n",
    "        \n",
    "        page_results = extraction_result.get('page_results', [])\n",
    "        \n",
    "        if page_number > len(page_results):\n",
    "            print(f\"âŒ í˜ì´ì§€ {page_number}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì´ {len(page_results)}í˜ì´ì§€)\")\n",
    "            return\n",
    "        \n",
    "        # íŠ¹ì • í˜ì´ì§€ ë¶„ì„\n",
    "        print(f\"\\\\nğŸ” í˜ì´ì§€ {page_number} ìƒì„¸ ë¶„ì„:\")\n",
    "        page_detail = analyze_single_page_azure(page_results[page_number - 1], page_number)\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {page_detail['text_length']:,}ì\")\n",
    "        print(f\"   ğŸ—ï¸ ë ˆì´ì•„ì›ƒ: {page_detail['layout_type']}\")\n",
    "        print(f\"   ğŸ¯ ì‹ ë¢°ë„: {page_detail['confidence']:.3f}\")\n",
    "        print(f\"   ğŸ”¤ ë‹¨ì–´ ìˆ˜: {page_detail.get('word_count', 0):,}ê°œ\")\n",
    "        print(f\"   ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë¹„ìœ¨: {page_detail.get('korean_ratio', 0):.1%}\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ë‚´ìš© í‘œì‹œ\n",
    "        print(f\"\\\\nğŸ“„ í˜ì´ì§€ {page_number} ë‚´ìš©:\")\n",
    "        display_page_content_azure(extraction_result, page_number, max_chars=300)\n",
    "        \n",
    "        # í’ˆì§ˆ ë¬¸ì œ í™•ì¸\n",
    "        page_quality = check_single_page_quality_azure(page_results[page_number - 1], page_number)\n",
    "        \n",
    "        if page_quality['has_issues']:\n",
    "            print(f\"\\\\nâš ï¸ í’ˆì§ˆ ë¬¸ì œ ë°œê²¬:\")\n",
    "            for issue in page_quality['issues']:\n",
    "                print(f\"   â€¢ {issue}\")\n",
    "        else:\n",
    "            print(f\"\\\\nâœ… í˜ì´ì§€ í’ˆì§ˆ ì–‘í˜¸\")\n",
    "        \n",
    "        return extraction_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ ì‹¤í–‰ ì¤€ë¹„\n",
    "print(\"ğŸ“¦ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì œ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   âœ… run_comprehensive_page_analysis_example: í¬ê´„ì  ë¶„ì„ ì˜ˆì œ\")\n",
    "print(\"   âœ… quick_page_test_example: ë¹ ë¥¸ í˜ì´ì§€ í…ŒìŠ¤íŠ¸\")\n",
    "print()\n",
    "print(\"ğŸ¯ ì‹¤í–‰ ë°©ë²•:\")\n",
    "print(\"   # í¬ê´„ì  ë¶„ì„ ì‹¤í–‰\")\n",
    "print(\"   result = run_comprehensive_page_analysis_example()\")\n",
    "print()\n",
    "print(\"   # íŠ¹ì • íŒŒì¼ì˜ íŠ¹ì • í˜ì´ì§€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"   quick_page_test_example('/path/to/document.pdf', page_number=1)\")\n",
    "print()\n",
    "print(\"   # ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ í™•ì¸\")\n",
    "print(\"   for doc in TEST_DOCUMENTS:\")\n",
    "print(\"       if os.path.exists(doc):\")\n",
    "print(\"           print(f'âœ… {doc}')\")\n",
    "print(\"       else:\")\n",
    "print(\"           print(f'âŒ {doc}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de22648",
   "metadata": {},
   "source": [
    "## ğŸ”¥ í•µì‹¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸: ì‹¤ì œ ë¬¸ì„œ ì²˜ë¦¬ í™•ì¸\n",
    "\n",
    "ì´ì œ ì‹¤ì œ PDF ë¬¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ Azure Document Intelligenceì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ**: ë¬¸ì„œì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ”ì§€ í™•ì¸\n",
    "2. **êµ¬ì¡°ì  ìš”ì†Œ ì¸ì‹**: í˜ì´ì§€ë³„ ê·¸ë¦¼, í…Œì´ë¸”, ë ˆì´ì•„ì›ƒ ìš”ì†Œ ì¸ì‹ í™•ì¸  \n",
    "3. **í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„**: ê° í˜ì´ì§€ì˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê°œë³„ì ìœ¼ë¡œ ê²€í† \n",
    "\n",
    "**í…ŒìŠ¤íŠ¸ ì§„í–‰ ìˆœì„œ:**\n",
    "- ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„ íƒ ë° ë¡œë“œ\n",
    "- ğŸ” ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í™•ì¸\n",
    "- ğŸ“Š êµ¬ì¡°ì  ìš”ì†Œ(í…Œì´ë¸”, ì´ë¯¸ì§€) ì¸ì‹ í…ŒìŠ¤íŠ¸\n",
    "- ğŸ“ƒ í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ë° ê²°ê³¼ ê²€í† "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a02bf184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "   ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/input_docs\n",
      "âœ… 3ê°œì˜ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n",
      "   1. 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf (0.32MB)\n",
      "   2. test1.pdf (1.26MB)\n",
      "   3. test.pdf (11.85MB)\n",
      "\n",
      "ğŸ¯ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„ íƒ: test.pdf\n",
      "   ğŸ“ ê²½ë¡œ: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "   ğŸ“ í¬ê¸°: 11.85MB\n",
      "ğŸ“„ ë¬¸ì„œ ê¸°ë³¸ ì •ë³´:\n",
      "   ğŸ“ƒ ì´ í˜ì´ì§€: 27í˜ì´ì§€\n",
      "   ğŸ“ ì œëª©: 1\n",
      "   ğŸ‘¤ ì‘ì„±ì: 3PC\n",
      "   ğŸ› ï¸ ìƒì„±ë„êµ¬: Adobe Acrobat Pro 9.0.0\n",
      "\n",
      "ğŸš€ Azure Document Intelligence í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ!\n",
      "   ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: test.pdf\n",
      "   ğŸ“Š ì˜ˆìƒ í˜ì´ì§€: 27\n",
      "   ğŸ’¾ íŒŒì¼ í¬ê¸°: 11.85MB\n",
      "   ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "   ğŸ“ ì¶œë ¥ íŒŒì¼ëª… ê¸°ë³¸: test_20250923_125927\n",
      "ğŸ“„ ë¬¸ì„œ ê¸°ë³¸ ì •ë³´:\n",
      "   ğŸ“ƒ ì´ í˜ì´ì§€: 27í˜ì´ì§€\n",
      "   ğŸ“ ì œëª©: 1\n",
      "   ğŸ‘¤ ì‘ì„±ì: 3PC\n",
      "   ğŸ› ï¸ ìƒì„±ë„êµ¬: Adobe Acrobat Pro 9.0.0\n",
      "\n",
      "ğŸš€ Azure Document Intelligence í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ!\n",
      "   ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: test.pdf\n",
      "   ğŸ“Š ì˜ˆìƒ í˜ì´ì§€: 27\n",
      "   ğŸ’¾ íŒŒì¼ í¬ê¸°: 11.85MB\n",
      "   ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "   ğŸ“ ì¶œë ¥ íŒŒì¼ëª… ê¸°ë³¸: test_20250923_125927\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“„ 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„ íƒ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ì…ë ¥/ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "INPUT_DOCS_DIR = \"/home/admin/wkms-aws/jupyter_notebook/data/input_docs\"\n",
    "OUTPUT_TEXTS_DIR = \"/home/admin/wkms-aws/jupyter_notebook/data/output_texts\"\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)\n",
    "os.makedirs(INPUT_DOCS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_TEXTS_DIR, exist_ok=True)\n",
    "\n",
    "def select_test_document():\n",
    "    \"\"\"ì§€ì •ëœ ì…ë ¥ ë””ë ‰í† ë¦¬ì—ì„œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„ íƒ\"\"\"\n",
    "    print(\"ğŸ“ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\")\n",
    "    print(f\"   ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: {INPUT_DOCS_DIR}\")\n",
    "    \n",
    "    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ê²½ë¡œ\n",
    "    default_test_file = os.path.join(INPUT_DOCS_DIR, \"test.pdf\")\n",
    "    \n",
    "    available_docs = []\n",
    "    \n",
    "    # ì…ë ¥ ë””ë ‰í† ë¦¬ì—ì„œ PDF íŒŒì¼ ì°¾ê¸°\n",
    "    if os.path.exists(INPUT_DOCS_DIR):\n",
    "        try:\n",
    "            for file in os.listdir(INPUT_DOCS_DIR):\n",
    "                if file.endswith('.pdf'):\n",
    "                    full_path = os.path.join(INPUT_DOCS_DIR, file)\n",
    "                    size = os.path.getsize(full_path)\n",
    "                    available_docs.append({\n",
    "                        'path': full_path,\n",
    "                        'name': file,\n",
    "                        'size_mb': round(size / (1024*1024), 2)\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ë””ë ‰í† ë¦¬ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    if not available_docs:\n",
    "        print(\"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ì—ì„œ PDF ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        print(f\"ğŸ’¡ ë‹¤ìŒ ìœ„ì¹˜ì— PDF íŒŒì¼ì„ ë°°ì¹˜í•´ì£¼ì„¸ìš”: {INPUT_DOCS_DIR}\")\n",
    "        print(f\"ğŸ“‹ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ íŒŒì¼: test.pdf\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"âœ… {len(available_docs)}ê°œì˜ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\")\n",
    "    for i, doc in enumerate(available_docs, 1):\n",
    "        print(f\"   {i}. {doc['name']} ({doc['size_mb']}MB)\")\n",
    "    \n",
    "    # test.pdfê°€ ìˆìœ¼ë©´ ìš°ì„  ì„ íƒ, ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë¬¸ì„œ ì„ íƒ\n",
    "    selected_doc = None\n",
    "    for doc in available_docs:\n",
    "        if doc['name'] == 'test.pdf':\n",
    "            selected_doc = doc\n",
    "            print(f\"\\nğŸ¯ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„ íƒ: {doc['name']}\")\n",
    "            break\n",
    "    \n",
    "    if not selected_doc:\n",
    "        selected_doc = available_docs[0]\n",
    "        print(f\"\\nğŸ¯ ì²« ë²ˆì§¸ ë¬¸ì„œ ì„ íƒ: {selected_doc['name']}\")\n",
    "    \n",
    "    print(f\"   ğŸ“ ê²½ë¡œ: {selected_doc['path']}\")\n",
    "    print(f\"   ğŸ“ í¬ê¸°: {selected_doc['size_mb']}MB\")\n",
    "    \n",
    "    return selected_doc['path']\n",
    "\n",
    "def get_document_basic_info(file_path):\n",
    "    \"\"\"ë¬¸ì„œ ê¸°ë³¸ ì •ë³´ í™•ì¸\"\"\"\n",
    "    try:\n",
    "        import PyPDF2\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            page_count = len(pdf_reader.pages)\n",
    "            \n",
    "            # ë©”íƒ€ë°ì´í„° í™•ì¸\n",
    "            metadata = pdf_reader.metadata\n",
    "            \n",
    "            print(f\"ğŸ“„ ë¬¸ì„œ ê¸°ë³¸ ì •ë³´:\")\n",
    "            print(f\"   ğŸ“ƒ ì´ í˜ì´ì§€: {page_count}í˜ì´ì§€\")\n",
    "            \n",
    "            if metadata:\n",
    "                if metadata.get('/Title'):\n",
    "                    print(f\"   ğŸ“ ì œëª©: {metadata.get('/Title')}\")\n",
    "                if metadata.get('/Author'):\n",
    "                    print(f\"   ğŸ‘¤ ì‘ì„±ì: {metadata.get('/Author')}\")\n",
    "                if metadata.get('/Creator'):\n",
    "                    print(f\"   ğŸ› ï¸ ìƒì„±ë„êµ¬: {metadata.get('/Creator')}\")\n",
    "            \n",
    "            return {\n",
    "                'page_count': page_count,\n",
    "                'metadata': metadata,\n",
    "                'file_size': os.path.getsize(file_path)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ PDF ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "        print(\"   (Azure Document Intelligenceë¡œ ê³„ì† ì§„í–‰)\")\n",
    "        return {\n",
    "            'page_count': 'ì•Œ ìˆ˜ ì—†ìŒ',\n",
    "            'metadata': None,\n",
    "            'file_size': os.path.getsize(file_path)\n",
    "        }\n",
    "\n",
    "def create_output_filename(input_path, suffix=\"\"):\n",
    "    \"\"\"ì¶œë ¥ íŒŒì¼ëª… ìƒì„±\"\"\"\n",
    "    input_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if suffix:\n",
    "        return f\"{input_name}_{suffix}_{timestamp}\"\n",
    "    else:\n",
    "        return f\"{input_name}_{timestamp}\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„ íƒ\n",
    "test_document_path = select_test_document()\n",
    "\n",
    "if test_document_path:\n",
    "    doc_info = get_document_basic_info(test_document_path)\n",
    "    \n",
    "    # ì¶œë ¥ íŒŒì¼ ê¸°ë³¸ ì´ë¦„ ì„¤ì •\n",
    "    output_base_name = create_output_filename(test_document_path)\n",
    "    \n",
    "    print(f\"\\nğŸš€ Azure Document Intelligence í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "    print(f\"   ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {os.path.basename(test_document_path)}\")\n",
    "    print(f\"   ğŸ“Š ì˜ˆìƒ í˜ì´ì§€: {doc_info['page_count']}\")\n",
    "    print(f\"   ğŸ’¾ íŒŒì¼ í¬ê¸°: {round(doc_info['file_size']/(1024*1024), 2)}MB\")\n",
    "    print(f\"   ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_TEXTS_DIR}\")\n",
    "    print(f\"   ğŸ“ ì¶œë ¥ íŒŒì¼ëª… ê¸°ë³¸: {output_base_name}\")\n",
    "    \n",
    "    # ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "    globals()['test_document_path'] = test_document_path\n",
    "    globals()['output_base_name'] = output_base_name\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f41f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...\n",
      "ğŸ“„ íŒŒì¼: test.pdf\n",
      "\n",
      "ğŸ“„ íŒŒì¼: test.pdf\n",
      "â³ Azure Document Intelligence ë¶„ì„ ì¤‘...\n",
      "â³ Azure Document Intelligence ë¶„ì„ ì¤‘...\n",
      "â±ï¸ ë¶„ì„ ì™„ë£Œ (ê²½ê³¼ ì‹œê°„: 10.1s)\n",
      "\n",
      "âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!\n",
      "   ğŸ“ƒ ì´ í˜ì´ì§€: 27í˜ì´ì§€\n",
      "   ğŸ“ ì´ ê¸€ììˆ˜: 80,072ì\n",
      "   ğŸ“„ ì´ ì¤„ìˆ˜: 680ì¤„\n",
      "   ğŸ”¤ ì´ ë‹¨ì–´ìˆ˜: 12,160ê°œ\n",
      "\n",
      "ğŸ“– í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²« 500ì):\n",
      "================================================================================\n",
      "éŸ“åœ‹ITì„œë¹„ìŠ¤å­¸æœƒèªŒ ç¬¬22å· ç¬¬3è™Ÿ 2023å¹´ 6æœˆ, pp.1-27\n",
      "Journal of Information Technology Services https://doi.org/10.9716/KITS.2023.22.3.001\n",
      "ì–‘ì†ì¡ì´ ë¦¬ë”ì‹­ê³¼ í˜ì‹ ì ì¸ ì—…ë¬´ í–‰ë™: í•œêµ­ ë°˜ë„ì²´ ì‚°ì—…ì˜ ì¦ê±°\n",
      "ë”í˜ í—¨ë¦¬ ì•„ë©”ìš”* Â· ì˜¤í¬ë¦¬ í—¨ë¦¬ *** Â· ìœ¤ì†Œë¼ **** . ê°•ì£¼ì˜ *****\n",
      "Ambidextrous Leadership and Innovative Work Behavior: Evidence from South Korea Semiconductor Industry*\n",
      "Henry Ameyaw Domfeh ** Â· Henry Ofori *** Â· Sora Yoon *** **** Â· Juyoung Kang* ******\n",
      "Abstract\n",
      "The semiconductor industry is a competitive, complicated and a cyclical sector with a highly dy\n",
      "\n",
      "... (ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ìƒëµ) ...\n",
      "================================================================================\n",
      "ğŸ’¾ í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ:\n",
      "   ğŸ“„ ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_full_text.txt\n",
      "   ğŸ“Š í†µê³„ ì •ë³´ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_statistics.json\n",
      "\n",
      "ğŸ“‚ í…ìŠ¤íŠ¸/í†µê³„ ì €ì¥ ìœ„ì¹˜: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "\n",
      "ğŸ¯ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ ìš”ì•½:\n",
      "   âœ… ì„±ê³µì ìœ¼ë¡œ 27í˜ì´ì§€ì—ì„œ 80,072ì ì¶”ì¶œ\n",
      "â±ï¸ ë¶„ì„ ì™„ë£Œ (ê²½ê³¼ ì‹œê°„: 10.1s)\n",
      "\n",
      "âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!\n",
      "   ğŸ“ƒ ì´ í˜ì´ì§€: 27í˜ì´ì§€\n",
      "   ğŸ“ ì´ ê¸€ììˆ˜: 80,072ì\n",
      "   ğŸ“„ ì´ ì¤„ìˆ˜: 680ì¤„\n",
      "   ğŸ”¤ ì´ ë‹¨ì–´ìˆ˜: 12,160ê°œ\n",
      "\n",
      "ğŸ“– í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²« 500ì):\n",
      "================================================================================\n",
      "éŸ“åœ‹ITì„œë¹„ìŠ¤å­¸æœƒèªŒ ç¬¬22å· ç¬¬3è™Ÿ 2023å¹´ 6æœˆ, pp.1-27\n",
      "Journal of Information Technology Services https://doi.org/10.9716/KITS.2023.22.3.001\n",
      "ì–‘ì†ì¡ì´ ë¦¬ë”ì‹­ê³¼ í˜ì‹ ì ì¸ ì—…ë¬´ í–‰ë™: í•œêµ­ ë°˜ë„ì²´ ì‚°ì—…ì˜ ì¦ê±°\n",
      "ë”í˜ í—¨ë¦¬ ì•„ë©”ìš”* Â· ì˜¤í¬ë¦¬ í—¨ë¦¬ *** Â· ìœ¤ì†Œë¼ **** . ê°•ì£¼ì˜ *****\n",
      "Ambidextrous Leadership and Innovative Work Behavior: Evidence from South Korea Semiconductor Industry*\n",
      "Henry Ameyaw Domfeh ** Â· Henry Ofori *** Â· Sora Yoon *** **** Â· Juyoung Kang* ******\n",
      "Abstract\n",
      "The semiconductor industry is a competitive, complicated and a cyclical sector with a highly dy\n",
      "\n",
      "... (ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ìƒëµ) ...\n",
      "================================================================================\n",
      "ğŸ’¾ í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ:\n",
      "   ğŸ“„ ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_full_text.txt\n",
      "   ğŸ“Š í†µê³„ ì •ë³´ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_statistics.json\n",
      "\n",
      "ğŸ“‚ í…ìŠ¤íŠ¸/í†µê³„ ì €ì¥ ìœ„ì¹˜: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "\n",
      "ğŸ¯ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ ìš”ì•½:\n",
      "   âœ… ì„±ê³µì ìœ¼ë¡œ 27í˜ì´ì§€ì—ì„œ 80,072ì ì¶”ì¶œ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” 2ë‹¨ê³„: ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í™•ì¸\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_extracted_text(text_content, statistics, output_base_name):\n",
    "    \"\"\"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì „ì²´ ê²½ë¡œë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥\"\"\"\n",
    "    try:\n",
    "        # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "        text_filename = f\"{output_base_name}_full_text.txt\"\n",
    "        text_filepath = os.path.join(OUTPUT_TEXTS_DIR, text_filename)\n",
    "        \n",
    "        with open(text_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"Azure Document Intelligence í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(f\"ì¶”ì¶œ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"ì´ í˜ì´ì§€: {statistics['page_count']}í˜ì´ì§€\\n\")\n",
    "            f.write(f\"ì´ ê¸€ììˆ˜: {statistics['char_count']:,}ì\\n\")\n",
    "            f.write(f\"ì´ ì¤„ìˆ˜: {statistics['line_count']:,}ì¤„\\n\")\n",
    "            f.write(f\"ì´ ë‹¨ì–´ìˆ˜: {statistics['word_count']:,}ê°œ\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            f.write(text_content)\n",
    "        \n",
    "        # í†µê³„ ì •ë³´ JSONìœ¼ë¡œ ì €ì¥\n",
    "        stats_filename = f\"{output_base_name}_statistics.json\"\n",
    "        stats_filepath = os.path.join(OUTPUT_TEXTS_DIR, stats_filename)\n",
    "        \n",
    "        stats_data = {\n",
    "            \"extraction_time\": datetime.now().isoformat(),\n",
    "            \"statistics\": statistics,\n",
    "            \"output_files\": {\n",
    "                \"full_text\": text_filename,\n",
    "                \"statistics\": stats_filename\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(stats_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(\"ğŸ’¾ í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ:\")\n",
    "        print(f\"   ğŸ“„ ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: {text_filepath}\")\n",
    "        print(f\"   ğŸ“Š í†µê³„ ì •ë³´ íŒŒì¼: {stats_filepath}\")\n",
    "        \n",
    "        return {\n",
    "            'text_file': text_filepath,\n",
    "            'stats_file': stats_filepath\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_full_text_test(file_path, pages: str | None = None, fast_mode: bool = False):\n",
    "    \"\"\"ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
    "    pages: Azure 'pages' íŒŒë¼ë¯¸í„°. ì˜ˆ) '1-3', '1,3,5' (ì„¤ì • ì‹œ í•´ë‹¹ í˜ì´ì§€ë§Œ ë¶„ì„)\n",
    "    fast_mode: Trueì´ë©´ ë¯¸ë¦¬ë³´ê¸°ë§Œ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê³  ì €ì¥ì„ ìƒëµ\n",
    "    \"\"\"\n",
    "    if not azure_client:\n",
    "        print(\"âŒ Azure í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸš€ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...\")\n",
    "    print(f\"ğŸ“„ íŒŒì¼: {os.path.basename(file_path)}\")\n",
    "    if pages:\n",
    "        print(f\"ğŸ”§ í˜ì´ì§€ ì œí•œ ì ìš©: pages='{pages}' (í•´ë‹¹ í˜ì´ì§€ë§Œ ë¶„ì„)\")\n",
    "    if fast_mode:\n",
    "        print(\"âš¡ Fast Mode: í…ìŠ¤íŠ¸ ì €ì¥ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°/í†µê³„ë§Œ ìˆ˜í–‰\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # Azure Document Intelligenceë¡œ ë¶„ì„\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            if pages:\n",
    "                poller = azure_client.begin_analyze_document(\n",
    "                    model_id=\"prebuilt-read\",  # í…ìŠ¤íŠ¸ ì¶”ì¶œì— ìµœì í™”ëœ ëª¨ë¸\n",
    "                    document=f,\n",
    "                    pages=pages\n",
    "                )\n",
    "            else:\n",
    "                poller = azure_client.begin_analyze_document(\n",
    "                    model_id=\"prebuilt-read\",  # í…ìŠ¤íŠ¸ ì¶”ì¶œì— ìµœì í™”ëœ ëª¨ë¸\n",
    "                    document=f\n",
    "                )\n",
    "        \n",
    "        print(\"â³ Azure Document Intelligence ë¶„ì„ ì¤‘...\")\n",
    "        result = poller.result()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"â±ï¸ ë¶„ì„ ì™„ë£Œ (ê²½ê³¼ ì‹œê°„: {elapsed:.1f}s)\")\n",
    "        \n",
    "        # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        full_text = result.content or \"\"\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        char_count = len(full_text)\n",
    "        line_count = len(full_text.split('\\n'))\n",
    "        word_count = len(full_text.split())\n",
    "        page_count = len(result.pages) if getattr(result, 'pages', None) else 0\n",
    "        \n",
    "        print(f\"\\nâœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "        print(f\"   ğŸ“ƒ ì´ í˜ì´ì§€: {page_count}í˜ì´ì§€\")\n",
    "        print(f\"   ğŸ“ ì´ ê¸€ììˆ˜: {char_count:,}ì\")\n",
    "        print(f\"   ğŸ“„ ì´ ì¤„ìˆ˜: {line_count:,}ì¤„\")\n",
    "        print(f\"   ğŸ”¤ ì´ ë‹¨ì–´ìˆ˜: {word_count:,}ê°œ\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²« 500ì)\n",
    "        preview_length = min(500, len(full_text))\n",
    "        preview_text = full_text[:preview_length]\n",
    "        \n",
    "        print(f\"\\nğŸ“– í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²« {preview_length}ì):\")\n",
    "        print(\"=\" * 80)\n",
    "        print(preview_text)\n",
    "        if len(full_text) > preview_length:\n",
    "            print(\"\\n... (ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ìƒëµ) ...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        statistics = {\n",
    "            'page_count': page_count,\n",
    "            'char_count': char_count,\n",
    "            'line_count': line_count,\n",
    "            'word_count': word_count\n",
    "        }\n",
    "        \n",
    "        # Fast Modeë©´ ì €ì¥ ìŠ¤í‚µ\n",
    "        if fast_mode:\n",
    "            return {\n",
    "                'full_text': full_text,\n",
    "                'statistics': statistics,\n",
    "                'azure_result': result\n",
    "            }\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥\n",
    "        if 'output_base_name' in globals():\n",
    "            save_result = save_extracted_text(full_text, statistics, output_base_name)\n",
    "            if save_result:\n",
    "                print(f\"\\nğŸ“‚ í…ìŠ¤íŠ¸/í†µê³„ ì €ì¥ ìœ„ì¹˜: {OUTPUT_TEXTS_DIR}\")\n",
    "                globals()['text_save_result'] = save_result\n",
    "        \n",
    "        return {\n",
    "            'full_text': full_text,\n",
    "            'statistics': statistics,\n",
    "            'azure_result': result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ë¡œ ë¹ ë¥¸ ì‹¤í–‰ ì„¤ì • ì§€ì›\n",
    "READ_PAGES_RANGE = os.getenv('AZURE_READ_PAGES_RANGE', '').strip() or None  # ì˜ˆ: '1-3', '1,5'\n",
    "FAST_MODE = os.getenv('AZURE_FAST_MODE', 'false').strip().lower() in ('1', 'true', 'yes', 'y')\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤í–‰\n",
    "if 'test_document_path' in globals() and test_document_path:\n",
    "    text_result = extract_full_text_test(test_document_path, pages=READ_PAGES_RANGE, fast_mode=FAST_MODE)\n",
    "    \n",
    "    if text_result:\n",
    "        print(f\"\\nğŸ¯ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ ìš”ì•½:\")\n",
    "        stats = text_result['statistics']\n",
    "        print(f\"   âœ… ì„±ê³µì ìœ¼ë¡œ {stats['page_count']}í˜ì´ì§€ì—ì„œ {stats['char_count']:,}ì ì¶”ì¶œ\")\n",
    "        \n",
    "        # ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥\n",
    "        globals()['extracted_text'] = text_result['full_text']\n",
    "        globals()['azure_analysis_result'] = text_result['azure_result']\n",
    "        globals()['text_statistics'] = text_result['statistics']\n",
    "    else:\n",
    "        print(\"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3712a4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì‹œì‘...\n",
      "ğŸ“Š ëª¨ë¸: prebuilt-layout (ë ˆì´ì•„ì›ƒ + í…Œì´ë¸” ì¸ì‹)\n",
      "â³ êµ¬ì¡° ë¶„ì„ ì¤‘...\n",
      "â³ êµ¬ì¡° ë¶„ì„ ì¤‘...\n",
      "\n",
      "âœ… êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!\n",
      "ğŸ“Š ì „ì²´ êµ¬ì¡° ìš”ì•½:\n",
      "   ğŸ“„ ì´ í˜ì´ì§€: 27ê°œ\n",
      "   ğŸ“ ì´ ë¬¸ë‹¨: 727ê°œ\n",
      "   ğŸ“ ì´ í…ìŠ¤íŠ¸ ë¼ì¸: 2154ê°œ\n",
      "   ğŸ“‹ í…Œì´ë¸”: 11ê°œ\n",
      "   â˜‘ï¸ ì²´í¬ë°•ìŠ¤/ì„ íƒ ë§ˆí¬: 97ê°œ\n",
      "\n",
      "ğŸ“‹ í…Œì´ë¸” ìƒì„¸ ì •ë³´:\n",
      "   í…Œì´ë¸” 1: 14í–‰ Ã— 3ì—´ (36ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 11í˜ì´ì§€\n",
      "   í…Œì´ë¸” 2: 23í–‰ Ã— 3ì—´ (61ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 12í˜ì´ì§€\n",
      "   í…Œì´ë¸” 3: 8í–‰ Ã— 3ì—´ (24ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 12í˜ì´ì§€\n",
      "   í…Œì´ë¸” 4: 12í–‰ Ã— 12ì—´ (144ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 13í˜ì´ì§€\n",
      "   í…Œì´ë¸” 5: 6í–‰ Ã— 3ì—´ (18ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 13í˜ì´ì§€\n",
      "   í…Œì´ë¸” 6: 8í–‰ Ã— 8ì—´ (64ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 13í˜ì´ì§€\n",
      "   í…Œì´ë¸” 7: 4í–‰ Ã— 8ì—´ (22ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 14í˜ì´ì§€\n",
      "   í…Œì´ë¸” 8: 7í–‰ Ã— 8ì—´ (44ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 14í˜ì´ì§€\n",
      "   í…Œì´ë¸” 9: 6í–‰ Ã— 3ì—´ (18ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 15í˜ì´ì§€\n",
      "   í…Œì´ë¸” 10: 3í–‰ Ã— 5ì—´ (15ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 16í˜ì´ì§€\n",
      "   í…Œì´ë¸” 11: 7í–‰ Ã— 4ì—´ (25ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 17í˜ì´ì§€\n",
      "\n",
      "ğŸ“ƒ í˜ì´ì§€ë³„ ìƒì„¸ ì •ë³´:\n",
      "   í˜ì´ì§€  1: í…ìŠ¤íŠ¸ ë¼ì¸  44ê°œ, ë‹¨ì–´  486ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  2: í…ìŠ¤íŠ¸ ë¼ì¸  77ê°œ, ë‹¨ì–´  533ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  3: í…ìŠ¤íŠ¸ ë¼ì¸  75ê°œ, ë‹¨ì–´  539ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  4: í…ìŠ¤íŠ¸ ë¼ì¸  77ê°œ, ë‹¨ì–´  521ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  5: í…ìŠ¤íŠ¸ ë¼ì¸  70ê°œ, ë‹¨ì–´  346ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  6: í…ìŠ¤íŠ¸ ë¼ì¸  77ê°œ, ë‹¨ì–´  517ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  7: í…ìŠ¤íŠ¸ ë¼ì¸  76ê°œ, ë‹¨ì–´  520ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  8: í…ìŠ¤íŠ¸ ë¼ì¸  76ê°œ, ë‹¨ì–´  517ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  9: í…ìŠ¤íŠ¸ ë¼ì¸  76ê°œ, ë‹¨ì–´  556ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 10: í…ìŠ¤íŠ¸ ë¼ì¸  72ê°œ, ë‹¨ì–´  499ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 11: í…ìŠ¤íŠ¸ ë¼ì¸  97ê°œ, ë‹¨ì–´  466ê°œ, í…Œì´ë¸” 1ê°œ\n",
      "   í˜ì´ì§€ 12: í…ìŠ¤íŠ¸ ë¼ì¸ 126ê°œ, ë‹¨ì–´  370ê°œ, í…Œì´ë¸” 2ê°œ\n",
      "   í˜ì´ì§€ 13: í…ìŠ¤íŠ¸ ë¼ì¸ 166ê°œ, ë‹¨ì–´  393ê°œ, í…Œì´ë¸” 3ê°œ\n",
      "   í˜ì´ì§€ 14: í…ìŠ¤íŠ¸ ë¼ì¸ 104ê°œ, ë‹¨ì–´  472ê°œ, í…Œì´ë¸” 2ê°œ\n",
      "   í˜ì´ì§€ 15: í…ìŠ¤íŠ¸ ë¼ì¸  81ê°œ, ë‹¨ì–´  350ê°œ, í…Œì´ë¸” 1ê°œ\n",
      "   í˜ì´ì§€ 16: í…ìŠ¤íŠ¸ ë¼ì¸  82ê°œ, ë‹¨ì–´  509ê°œ, í…Œì´ë¸” 1ê°œ\n",
      "   í˜ì´ì§€ 17: í…ìŠ¤íŠ¸ ë¼ì¸  96ê°œ, ë‹¨ì–´  519ê°œ, í…Œì´ë¸” 1ê°œ\n",
      "   í˜ì´ì§€ 18: í…ìŠ¤íŠ¸ ë¼ì¸  76ê°œ, ë‹¨ì–´  564ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 19: í…ìŠ¤íŠ¸ ë¼ì¸  75ê°œ, ë‹¨ì–´  537ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 20: í…ìŠ¤íŠ¸ ë¼ì¸  74ê°œ, ë‹¨ì–´  466ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 21: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  416ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 22: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  444ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 23: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  432ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 24: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  407ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 25: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  438ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 26: í…ìŠ¤íŠ¸ ë¼ì¸  35ê°œ, ë‹¨ì–´  193ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 27: í…ìŠ¤íŠ¸ ë¼ì¸  32ê°œ, ë‹¨ì–´  259ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "ğŸ’¾ êµ¬ì¡° ë¶„ì„ ì €ì¥ ì™„ë£Œ:\n",
      "   ğŸ“„ êµ¬ì¡° ë³´ê³ ì„œ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_structure_analysis.txt\n",
      "   ğŸ“Š êµ¬ì¡° ë°ì´í„° íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_structure_data.json\n",
      "   ğŸ“ˆ í…Œì´ë¸” ìš”ì•½ CSV: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_tables_summary.csv\n",
      "   ğŸ“ í…Œì´ë¸” ë³„ CSV (11ê°œ):\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_1_p11.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_2_p12.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_3_p12.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_4_p13.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_5_p13.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_6_p13.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_7_p14.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_8_p14.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_9_p15.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_10_p16.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_11_p17.csv\n",
      "\n",
      "ğŸ“‚ êµ¬ì¡°/í…Œì´ë¸” ì €ì¥ ìœ„ì¹˜: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "\n",
      "ğŸ¯ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!\n",
      "   âœ… í…Œì´ë¸”, ë¬¸ë‹¨, ë ˆì´ì•„ì›ƒ ìš”ì†Œ ë¶„ì„ ì„±ê³µ\n",
      "\n",
      "âœ… êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!\n",
      "ğŸ“Š ì „ì²´ êµ¬ì¡° ìš”ì•½:\n",
      "   ğŸ“„ ì´ í˜ì´ì§€: 27ê°œ\n",
      "   ğŸ“ ì´ ë¬¸ë‹¨: 727ê°œ\n",
      "   ğŸ“ ì´ í…ìŠ¤íŠ¸ ë¼ì¸: 2154ê°œ\n",
      "   ğŸ“‹ í…Œì´ë¸”: 11ê°œ\n",
      "   â˜‘ï¸ ì²´í¬ë°•ìŠ¤/ì„ íƒ ë§ˆí¬: 97ê°œ\n",
      "\n",
      "ğŸ“‹ í…Œì´ë¸” ìƒì„¸ ì •ë³´:\n",
      "   í…Œì´ë¸” 1: 14í–‰ Ã— 3ì—´ (36ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 11í˜ì´ì§€\n",
      "   í…Œì´ë¸” 2: 23í–‰ Ã— 3ì—´ (61ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 12í˜ì´ì§€\n",
      "   í…Œì´ë¸” 3: 8í–‰ Ã— 3ì—´ (24ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 12í˜ì´ì§€\n",
      "   í…Œì´ë¸” 4: 12í–‰ Ã— 12ì—´ (144ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 13í˜ì´ì§€\n",
      "   í…Œì´ë¸” 5: 6í–‰ Ã— 3ì—´ (18ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 13í˜ì´ì§€\n",
      "   í…Œì´ë¸” 6: 8í–‰ Ã— 8ì—´ (64ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 13í˜ì´ì§€\n",
      "   í…Œì´ë¸” 7: 4í–‰ Ã— 8ì—´ (22ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 14í˜ì´ì§€\n",
      "   í…Œì´ë¸” 8: 7í–‰ Ã— 8ì—´ (44ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 14í˜ì´ì§€\n",
      "   í…Œì´ë¸” 9: 6í–‰ Ã— 3ì—´ (18ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 15í˜ì´ì§€\n",
      "   í…Œì´ë¸” 10: 3í–‰ Ã— 5ì—´ (15ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 16í˜ì´ì§€\n",
      "   í…Œì´ë¸” 11: 7í–‰ Ã— 4ì—´ (25ê°œ ì…€)\n",
      "      ğŸ“ ìœ„ì¹˜: 17í˜ì´ì§€\n",
      "\n",
      "ğŸ“ƒ í˜ì´ì§€ë³„ ìƒì„¸ ì •ë³´:\n",
      "   í˜ì´ì§€  1: í…ìŠ¤íŠ¸ ë¼ì¸  44ê°œ, ë‹¨ì–´  486ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  2: í…ìŠ¤íŠ¸ ë¼ì¸  77ê°œ, ë‹¨ì–´  533ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  3: í…ìŠ¤íŠ¸ ë¼ì¸  75ê°œ, ë‹¨ì–´  539ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  4: í…ìŠ¤íŠ¸ ë¼ì¸  77ê°œ, ë‹¨ì–´  521ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  5: í…ìŠ¤íŠ¸ ë¼ì¸  70ê°œ, ë‹¨ì–´  346ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  6: í…ìŠ¤íŠ¸ ë¼ì¸  77ê°œ, ë‹¨ì–´  517ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  7: í…ìŠ¤íŠ¸ ë¼ì¸  76ê°œ, ë‹¨ì–´  520ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  8: í…ìŠ¤íŠ¸ ë¼ì¸  76ê°œ, ë‹¨ì–´  517ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€  9: í…ìŠ¤íŠ¸ ë¼ì¸  76ê°œ, ë‹¨ì–´  556ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 10: í…ìŠ¤íŠ¸ ë¼ì¸  72ê°œ, ë‹¨ì–´  499ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 11: í…ìŠ¤íŠ¸ ë¼ì¸  97ê°œ, ë‹¨ì–´  466ê°œ, í…Œì´ë¸” 1ê°œ\n",
      "   í˜ì´ì§€ 12: í…ìŠ¤íŠ¸ ë¼ì¸ 126ê°œ, ë‹¨ì–´  370ê°œ, í…Œì´ë¸” 2ê°œ\n",
      "   í˜ì´ì§€ 13: í…ìŠ¤íŠ¸ ë¼ì¸ 166ê°œ, ë‹¨ì–´  393ê°œ, í…Œì´ë¸” 3ê°œ\n",
      "   í˜ì´ì§€ 14: í…ìŠ¤íŠ¸ ë¼ì¸ 104ê°œ, ë‹¨ì–´  472ê°œ, í…Œì´ë¸” 2ê°œ\n",
      "   í˜ì´ì§€ 15: í…ìŠ¤íŠ¸ ë¼ì¸  81ê°œ, ë‹¨ì–´  350ê°œ, í…Œì´ë¸” 1ê°œ\n",
      "   í˜ì´ì§€ 16: í…ìŠ¤íŠ¸ ë¼ì¸  82ê°œ, ë‹¨ì–´  509ê°œ, í…Œì´ë¸” 1ê°œ\n",
      "   í˜ì´ì§€ 17: í…ìŠ¤íŠ¸ ë¼ì¸  96ê°œ, ë‹¨ì–´  519ê°œ, í…Œì´ë¸” 1ê°œ\n",
      "   í˜ì´ì§€ 18: í…ìŠ¤íŠ¸ ë¼ì¸  76ê°œ, ë‹¨ì–´  564ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 19: í…ìŠ¤íŠ¸ ë¼ì¸  75ê°œ, ë‹¨ì–´  537ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 20: í…ìŠ¤íŠ¸ ë¼ì¸  74ê°œ, ë‹¨ì–´  466ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 21: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  416ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 22: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  444ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 23: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  432ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 24: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  407ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 25: í…ìŠ¤íŠ¸ ë¼ì¸  78ê°œ, ë‹¨ì–´  438ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 26: í…ìŠ¤íŠ¸ ë¼ì¸  35ê°œ, ë‹¨ì–´  193ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "   í˜ì´ì§€ 27: í…ìŠ¤íŠ¸ ë¼ì¸  32ê°œ, ë‹¨ì–´  259ê°œ, í…Œì´ë¸” 0ê°œ\n",
      "ğŸ’¾ êµ¬ì¡° ë¶„ì„ ì €ì¥ ì™„ë£Œ:\n",
      "   ğŸ“„ êµ¬ì¡° ë³´ê³ ì„œ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_structure_analysis.txt\n",
      "   ğŸ“Š êµ¬ì¡° ë°ì´í„° íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_structure_data.json\n",
      "   ğŸ“ˆ í…Œì´ë¸” ìš”ì•½ CSV: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_tables_summary.csv\n",
      "   ğŸ“ í…Œì´ë¸” ë³„ CSV (11ê°œ):\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_1_p11.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_2_p12.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_3_p12.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_4_p13.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_5_p13.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_6_p13.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_7_p14.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_8_p14.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_9_p15.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_10_p16.csv\n",
      "      - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_11_p17.csv\n",
      "\n",
      "ğŸ“‚ êµ¬ì¡°/í…Œì´ë¸” ì €ì¥ ìœ„ì¹˜: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "\n",
      "ğŸ¯ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!\n",
      "   âœ… í…Œì´ë¸”, ë¬¸ë‹¨, ë ˆì´ì•„ì›ƒ ìš”ì†Œ ë¶„ì„ ì„±ê³µ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š 3ë‹¨ê³„: êµ¬ì¡°ì  ìš”ì†Œ ì¸ì‹ í…ŒìŠ¤íŠ¸ (í…Œì´ë¸”, ì´ë¯¸ì§€, ë ˆì´ì•„ì›ƒ)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def save_structure_analysis(structure_info, page_details, tables_data, output_base_name, raw_tables=None):\n",
    "    \"\"\"êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì „ì²´ ê²½ë¡œë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥\n",
    "    raw_tables: Azure ë¶„ì„ ì›ë³¸ì˜ table ê°ì²´ ë¦¬ìŠ¤íŠ¸ (ìˆìœ¼ë©´ í…Œì´ë¸” ë³„ CSVë¡œ ë‚´ë³´ëƒ„)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ ì €ì¥\n",
    "        report_filename = f\"{output_base_name}_structure_analysis.txt\"\n",
    "        report_filepath = os.path.join(OUTPUT_TEXTS_DIR, report_filename)\n",
    "        \n",
    "        with open(report_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"Azure Document Intelligence êµ¬ì¡° ë¶„ì„ ê²°ê³¼\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(f\"ë¶„ì„ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            f.write(\"ğŸ“Š ì „ì²´ êµ¬ì¡° ìš”ì•½:\\n\")\n",
    "            f.write(f\"   ğŸ“„ ì´ í˜ì´ì§€: {structure_info['pages']}ê°œ\\n\")\n",
    "            f.write(f\"   ğŸ“ ì´ ë¬¸ë‹¨: {structure_info['paragraphs']}ê°œ\\n\")\n",
    "            f.write(f\"   ğŸ“ ì´ í…ìŠ¤íŠ¸ ë¼ì¸: {structure_info['lines']}ê°œ\\n\")\n",
    "            f.write(f\"   ğŸ“‹ í…Œì´ë¸”: {structure_info['tables']}ê°œ\\n\")\n",
    "            f.write(f\"   â˜‘ï¸ ì²´í¬ë°•ìŠ¤/ì„ íƒ ë§ˆí¬: {structure_info['selection_marks']}ê°œ\\n\\n\")\n",
    "            \n",
    "            # í˜ì´ì§€ë³„ ìƒì„¸ ì •ë³´\n",
    "            f.write(\"ğŸ“ƒ í˜ì´ì§€ë³„ ìƒì„¸ ì •ë³´:\\n\")\n",
    "            for page in page_details:\n",
    "                f.write(f\"   í˜ì´ì§€ {page['page_number']:2d}: \"\n",
    "                       f\"í…ìŠ¤íŠ¸ ë¼ì¸ {page['lines']:3d}ê°œ, \"\n",
    "                       f\"ë‹¨ì–´ {page['words']:4d}ê°œ, \"\n",
    "                       f\"í…Œì´ë¸” {page['tables_on_page']}ê°œ\\n\")\n",
    "            \n",
    "            # í…Œì´ë¸” ìƒì„¸ ì •ë³´\n",
    "            if tables_data:\n",
    "                f.write(f\"\\nğŸ“‹ í…Œì´ë¸” ìƒì„¸ ì •ë³´:\\n\")\n",
    "                for i, table_info in enumerate(tables_data, 1):\n",
    "                    f.write(f\"   í…Œì´ë¸” {i}: {table_info['rows']}í–‰ Ã— {table_info['cols']}ì—´ \"\n",
    "                           f\"({table_info['cells']}ê°œ ì…€)\\n\")\n",
    "                    f.write(f\"      ğŸ“ ìœ„ì¹˜: {table_info['page']}í˜ì´ì§€\\n\")\n",
    "        \n",
    "        # JSON í˜•íƒœë¡œë„ ì €ì¥\n",
    "        json_filename = f\"{output_base_name}_structure_data.json\"\n",
    "        json_filepath = os.path.join(OUTPUT_TEXTS_DIR, json_filename)\n",
    "        \n",
    "        structure_data = {\n",
    "            \"analysis_time\": datetime.now().isoformat(),\n",
    "            \"structure_summary\": structure_info,\n",
    "            \"page_details\": page_details,\n",
    "            \"tables\": tables_data,\n",
    "            \"output_files\": {\n",
    "                \"structure_report\": report_filename,\n",
    "                \"structure_data\": json_filename\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(json_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(structure_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # CSVë¡œ í…Œì´ë¸” ìš”ì•½ ì €ì¥\n",
    "        csv_summary_filepath = None\n",
    "        try:\n",
    "            csv_summary_filename = f\"{output_base_name}_tables_summary.csv\"\n",
    "            csv_summary_filepath = os.path.join(OUTPUT_TEXTS_DIR, csv_summary_filename)\n",
    "            with open(csv_summary_filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\"table_id\", \"page\", \"rows\", \"cols\", \"cells\"])\n",
    "                for t in tables_data:\n",
    "                    writer.writerow([t.get('table_id'), t.get('page'), t.get('rows'), t.get('cols'), t.get('cells')])\n",
    "        except Exception:\n",
    "            csv_summary_filepath = None\n",
    "        \n",
    "        # í…Œì´ë¸” ë³„ CSV ë‚´ë³´ë‚´ê¸° (ë‚´ìš© í¬í•¨)\n",
    "        per_table_csv_files = []\n",
    "        if raw_tables:\n",
    "            try:\n",
    "                for idx, table in enumerate(raw_tables, 1):\n",
    "                    rows = getattr(table, 'row_count', None)\n",
    "                    cols = getattr(table, 'column_count', None)\n",
    "                    if rows is None or cols is None or rows <= 0 or cols <= 0:\n",
    "                        continue\n",
    "                    # í˜ì´ì§€ ë²ˆí˜¸\n",
    "                    page_num = 'Unknown'\n",
    "                    if hasattr(table, 'bounding_regions') and table.bounding_regions:\n",
    "                        page_num = table.bounding_regions[0].page_number\n",
    "                    # íŒŒì¼ëª…\n",
    "                    if page_num == 'Unknown':\n",
    "                        tbl_filename = f\"{output_base_name}_table_{idx}.csv\"\n",
    "                    else:\n",
    "                        tbl_filename = f\"{output_base_name}_table_{idx}_p{page_num}.csv\"\n",
    "                    tbl_filepath = os.path.join(OUTPUT_TEXTS_DIR, tbl_filename)\n",
    "                    # ê·¸ë¦¬ë“œ ì´ˆê¸°í™”\n",
    "                    grid = [[\"\" for _ in range(cols)] for _ in range(rows)]\n",
    "                    for cell in (table.cells or []):\n",
    "                        r = getattr(cell, 'row_index', None)\n",
    "                        c = getattr(cell, 'column_index', None)\n",
    "                        content = getattr(cell, 'content', '') or ''\n",
    "                        content = content.replace('\\n', ' ').strip()\n",
    "                        if r is not None and c is not None and 0 <= r < rows and 0 <= c < cols:\n",
    "                            if grid[r][c]:\n",
    "                                grid[r][c] = (grid[r][c] + ' ' + content).strip()\n",
    "                            else:\n",
    "                                grid[r][c] = content\n",
    "                    # CSV ì €ì¥\n",
    "                    with open(tbl_filepath, 'w', newline='', encoding='utf-8') as tbl_csv:\n",
    "                        writer = csv.writer(tbl_csv)\n",
    "                        writer.writerows(grid)\n",
    "                    per_table_csv_files.append(tbl_filepath)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ í…Œì´ë¸” ë³„ CSV ì €ì¥ ì¤‘ ì¼ë¶€ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        print(\"ğŸ’¾ êµ¬ì¡° ë¶„ì„ ì €ì¥ ì™„ë£Œ:\")\n",
    "        print(f\"   ğŸ“„ êµ¬ì¡° ë³´ê³ ì„œ íŒŒì¼: {report_filepath}\")\n",
    "        print(f\"   ğŸ“Š êµ¬ì¡° ë°ì´í„° íŒŒì¼: {json_filepath}\")\n",
    "        if csv_summary_filepath:\n",
    "            print(f\"   ğŸ“ˆ í…Œì´ë¸” ìš”ì•½ CSV: {csv_summary_filepath}\")\n",
    "        if per_table_csv_files:\n",
    "            print(f\"   ğŸ“ í…Œì´ë¸” ë³„ CSV ({len(per_table_csv_files)}ê°œ):\")\n",
    "            for p in per_table_csv_files:\n",
    "                print(f\"      - {p}\")\n",
    "        \n",
    "        return {\n",
    "            'report_file': report_filepath,\n",
    "            'data_file': json_filepath,\n",
    "            'tables_summary_csv': csv_summary_filepath,\n",
    "            'table_csv_files': per_table_csv_files\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ êµ¬ì¡° ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_document_structure():\n",
    "    \"\"\"ë¬¸ì„œì˜ êµ¬ì¡°ì  ìš”ì†Œ ë¶„ì„\"\"\"\n",
    "    if not azure_client:\n",
    "        print(\"âŒ Azure í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    if 'test_document_path' not in globals():\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    file_path = test_document_path\n",
    "    print(\"ğŸ” ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì‹œì‘...\")\n",
    "    print(\"ğŸ“Š ëª¨ë¸: prebuilt-layout (ë ˆì´ì•„ì›ƒ + í…Œì´ë¸” ì¸ì‹)\")\n",
    "    \n",
    "    try:\n",
    "        # ë ˆì´ì•„ì›ƒ ë¶„ì„ ëª¨ë¸ ì‚¬ìš©\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            poller = azure_client.begin_analyze_document(\n",
    "                model_id=\"prebuilt-layout\",  # êµ¬ì¡° ë¶„ì„ì— ìµœì í™”ëœ ëª¨ë¸\n",
    "                document=f\n",
    "            )\n",
    "        \n",
    "        print(\"â³ êµ¬ì¡° ë¶„ì„ ì¤‘...\")\n",
    "        result = poller.result()\n",
    "        \n",
    "        # êµ¬ì¡°ì  ìš”ì†Œ ë¶„ì„\n",
    "        structure_info = {\n",
    "            'pages': len(result.pages),\n",
    "            'tables': len(result.tables) if result.tables else 0,\n",
    "            'paragraphs': len(result.paragraphs) if result.paragraphs else 0,\n",
    "            'lines': 0,\n",
    "            'selection_marks': 0,\n",
    "            'figures': 0\n",
    "        }\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„\n",
    "        page_details = []\n",
    "        \n",
    "        for i, page in enumerate(result.pages, 1):\n",
    "            page_info = {\n",
    "                'page_number': i,\n",
    "                'lines': len(page.lines) if page.lines else 0,\n",
    "                'words': len(page.words) if page.words else 0,\n",
    "                'selection_marks': len(page.selection_marks) if page.selection_marks else 0,\n",
    "                'tables_on_page': 0,\n",
    "                'has_figures': False\n",
    "            }\n",
    "            \n",
    "            # ì´ í˜ì´ì§€ì˜ í…Œì´ë¸” ê°œìˆ˜ ê³„ì‚°\n",
    "            if result.tables:\n",
    "                for table in result.tables:\n",
    "                    table_page = getattr(table, 'bounding_regions', [])\n",
    "                    if table_page and table_page[0].page_number == i:\n",
    "                        page_info['tables_on_page'] += 1\n",
    "            \n",
    "            structure_info['lines'] += page_info['lines']\n",
    "            structure_info['selection_marks'] += page_info['selection_marks']\n",
    "            \n",
    "            page_details.append(page_info)\n",
    "        \n",
    "        print(f\"\\nâœ… êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ“Š ì „ì²´ êµ¬ì¡° ìš”ì•½:\")\n",
    "        print(f\"   ğŸ“„ ì´ í˜ì´ì§€: {structure_info['pages']}ê°œ\")\n",
    "        print(f\"   ğŸ“ ì´ ë¬¸ë‹¨: {structure_info['paragraphs']}ê°œ\")\n",
    "        print(f\"   ğŸ“ ì´ í…ìŠ¤íŠ¸ ë¼ì¸: {structure_info['lines']}ê°œ\")\n",
    "        print(f\"   ğŸ“‹ í…Œì´ë¸”: {structure_info['tables']}ê°œ\")\n",
    "        print(f\"   â˜‘ï¸ ì²´í¬ë°•ìŠ¤/ì„ íƒ ë§ˆí¬: {structure_info['selection_marks']}ê°œ\")\n",
    "        \n",
    "        # í…Œì´ë¸” ìƒì„¸ ì •ë³´ ìˆ˜ì§‘\n",
    "        tables_data = []\n",
    "        if result.tables and len(result.tables) > 0:\n",
    "            print(f\"\\nğŸ“‹ í…Œì´ë¸” ìƒì„¸ ì •ë³´:\")\n",
    "            for i, table in enumerate(result.tables, 1):\n",
    "                row_count = table.row_count\n",
    "                col_count = table.column_count\n",
    "                cell_count = len(table.cells) if table.cells else 0\n",
    "                \n",
    "                table_info = {\n",
    "                    'table_id': i,\n",
    "                    'rows': row_count,\n",
    "                    'cols': col_count,\n",
    "                    'cells': cell_count,\n",
    "                    'page': 'Unknown'\n",
    "                }\n",
    "                \n",
    "                # í…Œì´ë¸” ìœ„ì¹˜ ì •ë³´\n",
    "                if hasattr(table, 'bounding_regions') and table.bounding_regions:\n",
    "                    page_num = table.bounding_regions[0].page_number\n",
    "                    table_info['page'] = page_num\n",
    "                    print(f\"   í…Œì´ë¸” {i}: {row_count}í–‰ Ã— {col_count}ì—´ ({cell_count}ê°œ ì…€)\")\n",
    "                    print(f\"      ğŸ“ ìœ„ì¹˜: {page_num}í˜ì´ì§€\")\n",
    "                \n",
    "                tables_data.append(table_info)\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ ìƒì„¸ ì •ë³´\n",
    "        print(f\"\\nğŸ“ƒ í˜ì´ì§€ë³„ ìƒì„¸ ì •ë³´:\")\n",
    "        for page in page_details:\n",
    "            print(f\"   í˜ì´ì§€ {page['page_number']:2d}: \"\n",
    "                  f\"í…ìŠ¤íŠ¸ ë¼ì¸ {page['lines']:3d}ê°œ, \"\n",
    "                  f\"ë‹¨ì–´ {page['words']:4d}ê°œ, \"\n",
    "                  f\"í…Œì´ë¸” {page['tables_on_page']}ê°œ\")\n",
    "        \n",
    "        # êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì €ì¥ (í…Œì´ë¸” ì›ë³¸ë„ ì „ë‹¬í•˜ì—¬ per-table CSV ë‚´ë³´ëƒ„)\n",
    "        if 'output_base_name' in globals():\n",
    "            _save = save_structure_analysis(structure_info, page_details, tables_data, output_base_name, raw_tables=result.tables)\n",
    "            if _save:\n",
    "                # ì „ì—­ìœ¼ë¡œ ë…¸ì¶œ\n",
    "                globals()['structure_save_result'] = _save\n",
    "                print(f\"\\nğŸ“‚ êµ¬ì¡°/í…Œì´ë¸” ì €ì¥ ìœ„ì¹˜: {OUTPUT_TEXTS_DIR}\")\n",
    "        \n",
    "        # ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥\n",
    "        globals()['structure_analysis_result'] = {\n",
    "            'structure_info': structure_info,\n",
    "            'page_details': page_details,\n",
    "            'azure_result': result\n",
    "        }\n",
    "        globals()['document_tables'] = result.tables\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "# êµ¬ì¡° ë¶„ì„ ì‹¤í–‰\n",
    "if 'test_document_path' in globals() and test_document_path:\n",
    "    structure_success = analyze_document_structure()\n",
    "    \n",
    "    if structure_success:\n",
    "        print(f\"\\nğŸ¯ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!\")\n",
    "        print(f\"   âœ… í…Œì´ë¸”, ë¬¸ë‹¨, ë ˆì´ì•„ì›ƒ ìš”ì†Œ ë¶„ì„ ì„±ê³µ\")\n",
    "    else:\n",
    "        print(\"âŒ êµ¬ì¡° ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05f14345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ì‹œì‘...\n",
      "ğŸ’¾ í˜ì´ì§€ ë¶„ì„ ì €ì¥ ì™„ë£Œ:\n",
      "   ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_page_analysis.txt\n",
      "   ğŸ“Š ë¶„ì„ ë°ì´í„°: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_page_analysis_data.json\n",
      "\n",
      "ğŸ“‚ í˜ì´ì§€ ë¶„ì„ ì €ì¥ ìœ„ì¹˜: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "\n",
      "ğŸ¯ í˜ì´ì§€ë³„ ë¶„ì„ ì™„ë£Œ!\n",
      "   âœ… 27ê°œ í˜ì´ì§€ ë¶„ì„ ì„±ê³µ\n",
      "   ğŸ“‹ 11ê°œ í…Œì´ë¸” ë‚´ìš© ì¶”ì¶œ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ƒ 4ë‹¨ê³„: í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ë° ê²°ê³¼ ê²€í† \n",
    "\n",
    "import re\n",
    "\n",
    "def save_page_analysis(page_analysis_data, table_contents, summary_data, output_base_name):\n",
    "    \"\"\"í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    try:\n",
    "        # í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥\n",
    "        report_filename = f\"{output_base_name}_page_analysis.txt\"\n",
    "        report_filepath = os.path.join(OUTPUT_TEXTS_DIR, report_filename)\n",
    "        \n",
    "        with open(report_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"Azure Document Intelligence í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ê²°ê³¼\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(f\"ë¶„ì„ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            # í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼\n",
    "            f.write(\"ğŸ“ƒ í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ê²°ê³¼\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            \n",
    "            for page_data in page_analysis_data:\n",
    "                f.write(f\"\\nğŸ“„ í˜ì´ì§€ {page_data['page_number']}\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                f.write(f\"   ğŸ“ í¬ê¸°: {page_data['width']} Ã— {page_data['height']}\\n\")\n",
    "                f.write(f\"   ğŸ“ í…ìŠ¤íŠ¸ ë¼ì¸: {page_data['line_count']}ê°œ\\n\")\n",
    "                f.write(f\"   ğŸ”¤ ë‹¨ì–´: {page_data['word_count']}ê°œ\\n\")\n",
    "                f.write(f\"   ğŸ¯ í‰ê·  ì‹ ë¢°ë„: {page_data['avg_confidence']:.3f}\\n\")\n",
    "                f.write(f\"   ğŸ“‰ ìµœì†Œ ì‹ ë¢°ë„: {page_data['min_confidence']:.3f}\\n\")\n",
    "                f.write(f\"   ğŸ–¼ï¸ ì¶”ì • ê·¸ë¦¼ ìº¡ì…˜ ìˆ˜: {page_data.get('figure_captions', 0)}\\n\")\n",
    "                f.write(f\"   ğŸ“‹ í‘œ ìº¡ì…˜ ìˆ˜: {page_data.get('table_captions', 0)}\\n\")\n",
    "                \n",
    "                f.write(f\"   ğŸ“– í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²« 5ì¤„):\\n\")\n",
    "                for i, line in enumerate(page_data['preview_lines'], 1):\n",
    "                    f.write(f\"      {i}. {line}\\n\")\n",
    "                \n",
    "                if page_data['remaining_lines'] > 0:\n",
    "                    f.write(f\"      ... (ë‚˜ë¨¸ì§€ {page_data['remaining_lines']}ì¤„ ìƒëµ)\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # í…Œì´ë¸” ë‚´ìš©\n",
    "            if table_contents:\n",
    "                f.write(\"\\nğŸ“‹ í…Œì´ë¸” ë‚´ìš© ìƒì„¸ ë¶„ì„\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\")\n",
    "                \n",
    "                for table_data in table_contents:\n",
    "                    f.write(f\"\\ní…Œì´ë¸” {table_data['table_id']}: {table_data['rows']}í–‰ Ã— {table_data['cols']}ì—´\\n\")\n",
    "                    f.write(\"-\" * 40 + \"\\n\")\n",
    "                    \n",
    "                    for row_idx, row_data in enumerate(table_data['content']):\n",
    "                        f.write(f\"   í–‰ {row_idx+1:2d}: \" + \" | \".join(f\"{cell:20s}\" for cell in row_data) + \"\\n\")\n",
    "                    \n",
    "                    if table_data['rows'] > len(table_data['content']):\n",
    "                        f.write(f\"   ... (ë‚˜ë¨¸ì§€ {table_data['rows'] - len(table_data['content'])}í–‰ ìƒëµ)\\n\")\n",
    "            \n",
    "            # ì¢…í•© ìš”ì•½\n",
    "            f.write(f\"\\nğŸ¯ Azure Document Intelligence ì²˜ë¦¬ ê²°ê³¼ ì¢…í•© ìš”ì•½\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(f\"ğŸ“Š í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼:\\n\")\n",
    "            f.write(f\"   âœ… ì´ {summary_data['page_count']}í˜ì´ì§€ì—ì„œ {summary_data['char_count']:,}ì ì¶”ì¶œ ì„±ê³µ\\n\")\n",
    "            f.write(f\"   ğŸ“ ë‹¨ì–´ ìˆ˜: {summary_data['word_count']:,}ê°œ\\n\\n\")\n",
    "            \n",
    "            f.write(f\"ğŸ“‹ êµ¬ì¡° ë¶„ì„ ê²°ê³¼:\\n\")\n",
    "            f.write(f\"   ğŸ“„ í˜ì´ì§€: {summary_data['pages']}ê°œ\\n\")\n",
    "            f.write(f\"   ğŸ“ ë¬¸ë‹¨: {summary_data['paragraphs']}ê°œ\\n\")\n",
    "            f.write(f\"   ğŸ“‹ í…Œì´ë¸”: {summary_data['tables']}ê°œ\\n\")\n",
    "            f.write(f\"   ğŸ“ í…ìŠ¤íŠ¸ ë¼ì¸: {summary_data['lines']}ê°œ\\n\\n\")\n",
    "            \n",
    "            f.write(f\"ğŸ’¡ í™œìš© ê°€ëŠ¥ì„±:\\n\")\n",
    "            f.write(f\"   ğŸ“„ í…ìŠ¤íŠ¸ ê²€ìƒ‰: âœ… ê°€ëŠ¥\\n\")\n",
    "            f.write(f\"   ğŸ¤– AI ì§ˆì˜ì‘ë‹µ: âœ… ê°€ëŠ¥\\n\")\n",
    "            f.write(f\"   ğŸ“Š ë°ì´í„° ë¶„ì„: {'âœ… ê°€ëŠ¥' if summary_data['tables'] > 0 else 'âŒ í…Œì´ë¸” ì—†ìŒ'}\\n\")\n",
    "            f.write(f\"   ğŸ” ì˜ë¯¸ ê²€ìƒ‰: âœ… ê°€ëŠ¥\\n\")\n",
    "        \n",
    "        # JSON í˜•íƒœë¡œë„ ì €ì¥\n",
    "        json_filename = f\"{output_base_name}_page_analysis_data.json\"\n",
    "        json_filepath = os.path.join(OUTPUT_TEXTS_DIR, json_filename)\n",
    "        json_data = {\n",
    "            'analysis_time': datetime.now().isoformat(),\n",
    "            'page_analysis': page_analysis_data,\n",
    "            'table_contents': table_contents,\n",
    "            'summary': summary_data,\n",
    "            'output_files': {\n",
    "                'page_report': report_filename,\n",
    "                'page_data': json_filename\n",
    "            }\n",
    "        }\n",
    "        with open(json_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(\"ğŸ’¾ í˜ì´ì§€ ë¶„ì„ ì €ì¥ ì™„ë£Œ:\")\n",
    "        print(f\"   ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {report_filepath}\")\n",
    "        print(f\"   ğŸ“Š ë¶„ì„ ë°ì´í„°: {json_filepath}\")\n",
    "        \n",
    "        return {\n",
    "            'page_report': report_filepath,\n",
    "            'page_data': json_filepath\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í˜ì´ì§€ ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_pages_individually():\n",
    "    \"\"\"í˜ì´ì§€ë³„ ê°œë³„ ë¶„ì„ (Azure ê²°ê³¼ ê¸°ë°˜)\"\"\"\n",
    "    if 'azure_analysis_result' not in globals():\n",
    "        print(\"âŒ Azure ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "        return []\n",
    "    \n",
    "    result = azure_analysis_result\n",
    "    page_analysis_data = []\n",
    "    \n",
    "    for i, page in enumerate(result.pages, 1):\n",
    "        # ê¸°ë³¸ í˜ì´ì§€ ì •ë³´\n",
    "        page_info = {\n",
    "            'page_number': i,\n",
    "            'width': getattr(page, 'width', 0),\n",
    "            'height': getattr(page, 'height', 0),\n",
    "            'line_count': len(page.lines) if page.lines else 0,\n",
    "            'word_count': len(page.words) if page.words else 0,\n",
    "            'avg_confidence': 0.0,\n",
    "            'min_confidence': 1.0,\n",
    "            'preview_lines': [],\n",
    "            'remaining_lines': 0,\n",
    "            'figure_captions': 0,\n",
    "            'table_captions': 0\n",
    "        }\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ê³„ì‚° (ë‹¨ì–´ ê¸°ì¤€)\n",
    "        if page.words:\n",
    "            confidences = [getattr(word, 'confidence', 1.0) for word in page.words if hasattr(word, 'confidence')]\n",
    "            if confidences:\n",
    "                page_info['avg_confidence'] = sum(confidences) / len(confidences)\n",
    "                page_info['min_confidence'] = min(confidences)\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²« 5ì¤„)\n",
    "        if page.lines:\n",
    "            preview_count = min(5, len(page.lines))\n",
    "            for j in range(preview_count):\n",
    "                line_content = getattr(page.lines[j], 'content', '').strip()\n",
    "                page_info['preview_lines'].append(line_content)\n",
    "            page_info['remaining_lines'] = max(0, len(page.lines) - preview_count)\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ìº¡ì…˜ íœ´ë¦¬ìŠ¤í‹± (Figure/Table íŒ¨í„´ ê²€ìƒ‰)\n",
    "        page_text = ' '.join([getattr(line, 'content', '') for line in (page.lines or [])])\n",
    "        figure_patterns = [r'Figure\\s+\\d+', r'Fig\\.\\s*\\d+', r'ê·¸ë¦¼\\s+\\d+']\n",
    "        table_patterns = [r'Table\\s+\\d+', r'í‘œ\\s+\\d+']\n",
    "        \n",
    "        for pattern in figure_patterns:\n",
    "            page_info['figure_captions'] += len(re.findall(pattern, page_text, re.IGNORECASE))\n",
    "        \n",
    "        for pattern in table_patterns:\n",
    "            page_info['table_captions'] += len(re.findall(pattern, page_text, re.IGNORECASE))\n",
    "        \n",
    "        page_analysis_data.append(page_info)\n",
    "    \n",
    "    return page_analysis_data\n",
    "\n",
    "def show_table_contents():\n",
    "    \"\"\"í…Œì´ë¸” ë‚´ìš© í‘œì‹œ (document_tables ê¸°ë°˜)\"\"\"\n",
    "    if 'document_tables' not in globals() or not document_tables:\n",
    "        print(\"âš ï¸ ê°ì§€ëœ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "    \n",
    "    table_contents = []\n",
    "    for i, table in enumerate(document_tables, 1):\n",
    "        table_data = {\n",
    "            'table_id': i,\n",
    "            'rows': getattr(table, 'row_count', 0),\n",
    "            'cols': getattr(table, 'column_count', 0),\n",
    "            'content': []\n",
    "        }\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í…Œì´ë¸” ë‚´ìš© ì¶”ì¶œ (ì²« 5í–‰ë§Œ)\n",
    "        if hasattr(table, 'cells') and table.cells:\n",
    "            # ê·¸ë¦¬ë“œ ìƒì„±\n",
    "            max_rows = min(5, table_data['rows'])\n",
    "            max_cols = table_data['cols']\n",
    "            grid = [[\"\" for _ in range(max_cols)] for _ in range(max_rows)]\n",
    "            \n",
    "            for cell in table.cells:\n",
    "                row_idx = getattr(cell, 'row_index', -1)\n",
    "                col_idx = getattr(cell, 'column_index', -1)\n",
    "                content = getattr(cell, 'content', '').strip()\n",
    "                \n",
    "                if 0 <= row_idx < max_rows and 0 <= col_idx < max_cols:\n",
    "                    grid[row_idx][col_idx] = content[:20]  # 20ìë¡œ ì œí•œ\n",
    "            \n",
    "            table_data['content'] = grid\n",
    "        \n",
    "        table_contents.append(table_data)\n",
    "    \n",
    "    return table_contents\n",
    "\n",
    "def show_processing_summary():\n",
    "    \"\"\"ì²˜ë¦¬ ê²°ê³¼ ì¢…í•© ìš”ì•½\"\"\"\n",
    "    summary_data = {\n",
    "        'page_count': 0,\n",
    "        'char_count': 0,\n",
    "        'word_count': 0,\n",
    "        'pages': 0,\n",
    "        'paragraphs': 0,\n",
    "        'tables': 0,\n",
    "        'lines': 0\n",
    "    }\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ í†µê³„\n",
    "    if 'text_statistics' in globals():\n",
    "        summary_data.update({\n",
    "            'page_count': text_statistics.get('page_count', 0),\n",
    "            'char_count': text_statistics.get('char_count', 0),\n",
    "            'word_count': text_statistics.get('word_count', 0)\n",
    "        })\n",
    "    \n",
    "    # êµ¬ì¡° ë¶„ì„ í†µê³„\n",
    "    if 'structure_analysis_result' in globals():\n",
    "        structure_info = structure_analysis_result.get('structure_info', {})\n",
    "        summary_data.update({\n",
    "            'pages': structure_info.get('pages', 0),\n",
    "            'paragraphs': structure_info.get('paragraphs', 0),\n",
    "            'tables': structure_info.get('tables', 0),\n",
    "            'lines': structure_info.get('lines', 0)\n",
    "        })\n",
    "    \n",
    "    return summary_data\n",
    "\n",
    "# í˜ì´ì§€ë³„ ë¶„ì„ ì‹¤í–‰\n",
    "if 'azure_analysis_result' in globals() and azure_analysis_result:\n",
    "    print(\"ğŸ” í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ì‹œì‘...\")\n",
    "    \n",
    "    page_analysis_data = analyze_pages_individually()\n",
    "    table_contents = show_table_contents()\n",
    "    summary_data = show_processing_summary()\n",
    "    \n",
    "    if 'output_base_name' in globals() and page_analysis_data:\n",
    "        save_result = save_page_analysis(page_analysis_data, table_contents, summary_data, output_base_name)\n",
    "        if save_result:\n",
    "            globals()['page_save_result'] = save_result\n",
    "            globals()['page_analysis_data'] = page_analysis_data\n",
    "            globals()['table_contents'] = table_contents\n",
    "            globals()['summary_data'] = summary_data\n",
    "            print(f\"\\nğŸ“‚ í˜ì´ì§€ ë¶„ì„ ì €ì¥ ìœ„ì¹˜: {OUTPUT_TEXTS_DIR}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ í˜ì´ì§€ë³„ ë¶„ì„ ì™„ë£Œ!\")\n",
    "    print(f\"   âœ… {len(page_analysis_data)}ê°œ í˜ì´ì§€ ë¶„ì„ ì„±ê³µ\")\n",
    "    print(f\"   ğŸ“‹ {len(table_contents)}ê°œ í…Œì´ë¸” ë‚´ìš© ì¶”ì¶œ\")\n",
    "else:\n",
    "    print(\"âŒ í˜ì´ì§€ë³„ ë¶„ì„ì„ ì‹¤í–‰í•˜ë ¤ë©´ ë¨¼ì € í…ìŠ¤íŠ¸ ì¶”ì¶œ(27ë²ˆ ì…€)ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6aa66d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” í˜„ì¬ í™˜ê²½ ìƒíƒœ:\n",
      "ğŸ”§ ë¹ ë¥¸ í™˜ê²½ í…ŒìŠ¤íŠ¸\n",
      "------------------------------\n",
      "âœ… Azure í´ë¼ì´ì–¸íŠ¸: ì •ìƒ\n",
      "âœ… ì…ë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/input_docs\n",
      "âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: test.pdf\n",
      "âœ… í™˜ê²½ í…ŒìŠ¤íŠ¸ í†µê³¼ - ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê°€ëŠ¥\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš© ì˜µì…˜:\n",
      "   ğŸ“„ run_complete_test() - ì „ì²´ ë¶„ì„ (ê¸°ë³¸)\n",
      "   âš¡ run_complete_test(fast_mode=True) - ë¹ ë¥¸ ë¶„ì„ (ì €ì¥ ìµœì†Œ)\n",
      "   ğŸš€ fast_document_test() - ì´ˆê³ ì† í…ŒìŠ¤íŠ¸ (3í˜ì´ì§€ë§Œ)\n",
      "   ğŸ”§ quick_test() - í™˜ê²½ í™•ì¸ë§Œ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ ì™„ì „í•œ ë¬¸ì„œ ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "import time\n",
    "\n",
    "def run_complete_test(fast_mode=True, skip_pages_analysis=False, max_pages=None):\n",
    "    \"\"\"ì „ì²´ Azure Document Intelligence í…ŒìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰\n",
    "    \n",
    "    Args:\n",
    "        fast_mode (bool): ë¹ ë¥¸ ëª¨ë“œ (íŒŒì¼ ì €ì¥ ìƒëµ)\n",
    "        skip_pages_analysis (bool): í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ìƒëµ\n",
    "        max_pages (int): ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (Noneì´ë©´ ì „ì²´)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"ğŸš€ Azure Document Intelligence ì™„ì „ í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if fast_mode:\n",
    "        print(\"âš¡ ë¹ ë¥¸ ëª¨ë“œ í™œì„±í™” (íŒŒì¼ ì €ì¥ ìµœì†Œí™”)\")\n",
    "    if skip_pages_analysis:\n",
    "        print(\"â­ï¸ í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ìƒëµ\")\n",
    "    if max_pages:\n",
    "        print(f\"ğŸ“„ ì²˜ë¦¬ ì œí•œ: ìµœëŒ€ {max_pages}í˜ì´ì§€\")\n",
    "    \n",
    "    # 1. í™˜ê²½ ë° í´ë¼ì´ì–¸íŠ¸ í™•ì¸\n",
    "    print(f\"\\nğŸ”§ 1ë‹¨ê³„: í™˜ê²½ ê²€ì¦\")\n",
    "    if not globals().get('azure_client'):\n",
    "        print(\"âŒ Azure í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   ë¨¼ì € 1-5ë²ˆ ì…€ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "        return False\n",
    "    \n",
    "    # 2. í…ŒìŠ¤íŠ¸ ë¬¸ì„œ í™•ì¸\n",
    "    if not globals().get('test_document_path'):\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   ë¨¼ì € 26ë²ˆ ì…€(ë¬¸ì„œ ì„ íƒ)ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸ“‚ ì…ë ¥ ë¬¸ì„œ: {os.path.basename(test_document_path)}\")\n",
    "    print(f\"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_TEXTS_DIR}\")\n",
    "    print(f\"ğŸ“ ì¶œë ¥ íŒŒì¼ ê¸°ë³¸ëª…: {output_base_name}\")\n",
    "    print(\"   âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ\")\n",
    "    \n",
    "    # 3. ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)\n",
    "    print(f\"\\nğŸ“„ 2ë‹¨ê³„: ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ\")\n",
    "    text_start = time.time()\n",
    "    try:\n",
    "        # í˜ì´ì§€ ì œí•œ ì„¤ì •\n",
    "        pages_param = f\"1-{max_pages}\" if max_pages else None\n",
    "        \n",
    "        # ì„ì‹œë¡œ ë¹ ë¥¸ ëª¨ë“œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
    "        original_fast_mode = os.environ.get('AZURE_FAST_MODE', 'false')\n",
    "        if fast_mode:\n",
    "            os.environ['AZURE_FAST_MODE'] = 'true'\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤í–‰\n",
    "        text_result = extract_full_text_test(test_document_path, pages=pages_param, fast_mode=fast_mode)\n",
    "        \n",
    "        # í™˜ê²½ë³€ìˆ˜ ë³µì›\n",
    "        os.environ['AZURE_FAST_MODE'] = original_fast_mode\n",
    "        \n",
    "        if text_result:\n",
    "            globals()['extracted_text'] = text_result['full_text']\n",
    "            globals()['azure_analysis_result'] = text_result['azure_result']\n",
    "            globals()['text_statistics'] = text_result['statistics']\n",
    "            text_elapsed = time.time() - text_start\n",
    "            print(f\"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({text_elapsed:.1f}ì´ˆ)\")\n",
    "        else:\n",
    "            print(\"   âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # 4. êµ¬ì¡° ë¶„ì„ (ì¡°ê±´ë¶€ ì‹¤í–‰)\n",
    "    if not fast_mode or not skip_pages_analysis:\n",
    "        print(f\"\\nğŸ“Š 3ë‹¨ê³„: êµ¬ì¡° ë¶„ì„ (í…Œì´ë¸”, ë ˆì´ì•„ì›ƒ)\")\n",
    "        struct_start = time.time()\n",
    "        try:\n",
    "            structure_success = analyze_document_structure()\n",
    "            if structure_success:\n",
    "                struct_elapsed = time.time() - struct_start\n",
    "                print(f\"   âœ… êµ¬ì¡° ë¶„ì„ ì™„ë£Œ ({struct_elapsed:.1f}ì´ˆ)\")\n",
    "            else:\n",
    "                print(\"   âŒ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ êµ¬ì¡° ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"\\nğŸ“Š 3ë‹¨ê³„: êµ¬ì¡° ë¶„ì„ - ë¹ ë¥¸ ëª¨ë“œë¡œ ìƒëµ\")\n",
    "    \n",
    "    # 5. í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ (ì¡°ê±´ë¶€ ì‹¤í–‰)\n",
    "    if not skip_pages_analysis:\n",
    "        print(f\"\\nğŸ“ƒ 4ë‹¨ê³„: í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„\")\n",
    "        pages_start = time.time()\n",
    "        try:\n",
    "            page_analysis_data = analyze_pages_individually()\n",
    "            table_contents = show_table_contents()\n",
    "            summary_data = show_processing_summary()\n",
    "            \n",
    "            if page_analysis_data:\n",
    "                if not fast_mode:  # íŒŒì¼ ì €ì¥ì€ fast_modeê°€ ì•„ë‹ ë•Œë§Œ\n",
    "                    save_result = save_page_analysis(page_analysis_data, table_contents, summary_data, output_base_name)\n",
    "                    if save_result:\n",
    "                        globals()['page_save_result'] = save_result\n",
    "                \n",
    "                globals()['page_analysis_data'] = page_analysis_data\n",
    "                globals()['table_contents'] = table_contents\n",
    "                globals()['summary_data'] = summary_data\n",
    "                pages_elapsed = time.time() - pages_start\n",
    "                print(f\"   âœ… í˜ì´ì§€ë³„ ë¶„ì„ ì™„ë£Œ ({pages_elapsed:.1f}ì´ˆ)\")\n",
    "            else:\n",
    "                print(\"   âŒ í˜ì´ì§€ë³„ ë¶„ì„ ì‹¤íŒ¨\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ í˜ì´ì§€ë³„ ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"\\nğŸ“ƒ 4ë‹¨ê³„: í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ - ìƒëµë¨\")\n",
    "    \n",
    "    # 6. ìµœì¢… ìš”ì•½\n",
    "    total_elapsed = time.time() - start_time\n",
    "    print(f\"\\nâœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_elapsed:.1f}ì´ˆ)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # í•µì‹¬ í™•ì¸ì‚¬í•­ ìš”ì•½ í˜¸ì¶œ\n",
    "    try:\n",
    "        if 'evaluate_core_checks' in globals():\n",
    "            evaluate_core_checks()\n",
    "        else:\n",
    "            print(\"ğŸ“Š ê°„ë‹¨ ìš”ì•½:\")\n",
    "            if 'text_statistics' in globals():\n",
    "                stats = text_statistics\n",
    "                print(f\"   ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {stats.get('total_chars', 0):,}ì\")\n",
    "                print(f\"   ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: {stats.get('pages', 0)}í˜ì´ì§€\")\n",
    "            if 'structure_analysis_result' in globals():\n",
    "                struct = structure_analysis_result['structure_info']\n",
    "                print(f\"   ğŸ“‹ ë°œê²¬ëœ í…Œì´ë¸”: {struct.get('tables', 0)}ê°œ\")\n",
    "                print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ë¼ì¸: {struct.get('lines', 0)}ê°œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"ë¹ ë¥¸ í™˜ê²½ ë° ì—°ê²° í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"ğŸ”§ ë¹ ë¥¸ í™˜ê²½ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Azure í´ë¼ì´ì–¸íŠ¸ í™•ì¸\n",
    "    if globals().get('azure_client'):\n",
    "        print(\"âœ… Azure í´ë¼ì´ì–¸íŠ¸: ì •ìƒ\")\n",
    "    else:\n",
    "        print(\"âŒ Azure í´ë¼ì´ì–¸íŠ¸: ì´ˆê¸°í™” í•„ìš”\")\n",
    "        return False\n",
    "    \n",
    "    # ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "    if os.path.exists(INPUT_DOCS_DIR):\n",
    "        print(f\"âœ… ì…ë ¥ ë””ë ‰í† ë¦¬: {INPUT_DOCS_DIR}\")\n",
    "    else:\n",
    "        print(f\"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ ì—†ìŒ: {INPUT_DOCS_DIR}\")\n",
    "        return False\n",
    "    \n",
    "    if os.path.exists(OUTPUT_TEXTS_DIR):\n",
    "        print(f\"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_TEXTS_DIR}\")\n",
    "    else:\n",
    "        print(f\"âŒ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì—†ìŒ: {OUTPUT_TEXTS_DIR}\")\n",
    "        return False\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ í™•ì¸\n",
    "    if globals().get('test_document_path') and os.path.exists(test_document_path):\n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {os.path.basename(test_document_path)}\")\n",
    "    else:\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: ì„ íƒë˜ì§€ ì•ŠìŒ ë˜ëŠ” íŒŒì¼ ì—†ìŒ\")\n",
    "        return False\n",
    "    \n",
    "    print(\"âœ… í™˜ê²½ í…ŒìŠ¤íŠ¸ í†µê³¼ - ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê°€ëŠ¥\")\n",
    "    return True\n",
    "\n",
    "def fast_document_test():\n",
    "    \"\"\"ë§¤ìš° ë¹ ë¥¸ ë¬¸ì„œ í…ŒìŠ¤íŠ¸ (ì²« 3í˜ì´ì§€ë§Œ, íŒŒì¼ ì €ì¥ ì—†ìŒ)\"\"\"\n",
    "    print(\"âš¡ ì´ˆê³ ì† ë¬¸ì„œ í…ŒìŠ¤íŠ¸ (3í˜ì´ì§€, ì €ì¥ ì—†ìŒ)\")\n",
    "    return run_complete_test(fast_mode=True, skip_pages_analysis=True, max_pages=3)\n",
    "\n",
    "# í™˜ê²½ ì„¤ì • ìƒíƒœ í™•ì¸\n",
    "print(\"ğŸ” í˜„ì¬ í™˜ê²½ ìƒíƒœ:\")\n",
    "quick_test_result = quick_test()\n",
    "\n",
    "if quick_test_result:\n",
    "    print(\"\\nğŸ’¡ ì‚¬ìš© ì˜µì…˜:\")\n",
    "    print(\"   ğŸ“„ run_complete_test() - ì „ì²´ ë¶„ì„ (ê¸°ë³¸)\")\n",
    "    print(\"   âš¡ run_complete_test(fast_mode=True) - ë¹ ë¥¸ ë¶„ì„ (ì €ì¥ ìµœì†Œ)\")\n",
    "    print(\"   ğŸš€ fast_document_test() - ì´ˆê³ ì† í…ŒìŠ¤íŠ¸ (3í˜ì´ì§€ë§Œ)\")\n",
    "    print(\"   ğŸ”§ quick_test() - í™˜ê²½ í™•ì¸ë§Œ\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ê¸°ë³¸ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. í™˜ê²½ ì„¤ì •ì„ ë¨¼ì € í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3680a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Azure Document Intelligence í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜µì…˜\n",
      "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”:\n",
      "======================================================================\n",
      "ğŸ“‹ ì‹¤í–‰ ì˜µì…˜:\n",
      "   1ï¸âƒ£ ì´ˆê³ ì† í…ŒìŠ¤íŠ¸ (3í˜ì´ì§€ë§Œ, ì €ì¥ ì—†ìŒ) - 30ì´ˆ ì´ë‚´\n",
      "   2ï¸âƒ£ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì „ì²´ í˜ì´ì§€, ì €ì¥ ìµœì†Œ) - 1-2ë¶„\n",
      "   3ï¸âƒ£ ì™„ì „ í…ŒìŠ¤íŠ¸ (ì „ì²´ ë¶„ì„, ëª¨ë“  ì €ì¥) - 3-5ë¶„\n",
      "   ğŸ”§ í™˜ê²½ í™•ì¸ë§Œ\n",
      "\n",
      "âš¡ ìë™ ì‹¤í–‰ ëª¨ë“œ: ì´ˆê³ ì† í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
      "   (ë” ê¸´ ë¶„ì„ì´ í•„ìš”í•˜ë©´ auto_run=Falseë¡œ ë³€ê²½ í›„ ìˆ˜ë™ ì„ íƒ)\n",
      "âš¡ ì´ˆê³ ì† ë¬¸ì„œ í…ŒìŠ¤íŠ¸ (3í˜ì´ì§€, ì €ì¥ ì—†ìŒ)\n",
      "ğŸš€ Azure Document Intelligence ì™„ì „ í…ŒìŠ¤íŠ¸ ì‹œì‘\n",
      "======================================================================\n",
      "âš¡ ë¹ ë¥¸ ëª¨ë“œ í™œì„±í™” (íŒŒì¼ ì €ì¥ ìµœì†Œí™”)\n",
      "â­ï¸ í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ìƒëµ\n",
      "ğŸ“„ ì²˜ë¦¬ ì œí•œ: ìµœëŒ€ 3í˜ì´ì§€\n",
      "\n",
      "ğŸ”§ 1ë‹¨ê³„: í™˜ê²½ ê²€ì¦\n",
      "ğŸ“‚ ì…ë ¥ ë¬¸ì„œ: test.pdf\n",
      "ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "ğŸ“ ì¶œë ¥ íŒŒì¼ ê¸°ë³¸ëª…: test_20250923_125927\n",
      "   âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ\n",
      "\n",
      "ğŸ“„ 2ë‹¨ê³„: ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
      "ğŸš€ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...\n",
      "ğŸ“„ íŒŒì¼: test.pdf\n",
      "ğŸ”§ í˜ì´ì§€ ì œí•œ ì ìš©: pages='1-3' (í•´ë‹¹ í˜ì´ì§€ë§Œ ë¶„ì„)\n",
      "âš¡ Fast Mode: í…ìŠ¤íŠ¸ ì €ì¥ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°/í†µê³„ë§Œ ìˆ˜í–‰\n",
      "â³ Azure Document Intelligence ë¶„ì„ ì¤‘...\n",
      "â³ Azure Document Intelligence ë¶„ì„ ì¤‘...\n",
      "â±ï¸ ë¶„ì„ ì™„ë£Œ (ê²½ê³¼ ì‹œê°„: 3.7s)\n",
      "\n",
      "âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!\n",
      "   ğŸ“ƒ ì´ í˜ì´ì§€: 3í˜ì´ì§€\n",
      "   ğŸ“ ì´ ê¸€ììˆ˜: 9,775ì\n",
      "   ğŸ“„ ì´ ì¤„ìˆ˜: 32ì¤„\n",
      "   ğŸ”¤ ì´ ë‹¨ì–´ìˆ˜: 1,449ê°œ\n",
      "\n",
      "ğŸ“– í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²« 500ì):\n",
      "================================================================================\n",
      "éŸ“åœ‹ITì„œë¹„ìŠ¤å­¸æœƒèªŒ ç¬¬22å· ç¬¬3è™Ÿ 2023å¹´ 6æœˆ, pp.1-27\n",
      "Journal of Information Technology Services https://doi.org/10.9716/KITS.2023.22.3.001\n",
      "ì–‘ì†ì¡ì´ ë¦¬ë”ì‹­ê³¼ í˜ì‹ ì ì¸ ì—…ë¬´ í–‰ë™: í•œêµ­ ë°˜ë„ì²´ ì‚°ì—…ì˜ ì¦ê±°\n",
      "ë”í˜ í—¨ë¦¬ ì•„ë©”ìš”* Â· ì˜¤í¬ë¦¬ í—¨ë¦¬ *** Â· ìœ¤ì†Œë¼ **** . ê°•ì£¼ì˜ *****\n",
      "Ambidextrous Leadership and Innovative Work Behavior: Evidence from South Korea Semiconductor Industry*\n",
      "Henry Ameyaw Domfeh ** Â· Henry Ofori *** Â· Sora Yoon *** **** Â· Juyoung Kang* ******\n",
      "Abstract\n",
      "The semiconductor industry is a competitive, complicated and a cyclical sector with a highly dy\n",
      "\n",
      "... (ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ìƒëµ) ...\n",
      "================================================================================\n",
      "   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (3.7ì´ˆ)\n",
      "\n",
      "ğŸ“Š 3ë‹¨ê³„: êµ¬ì¡° ë¶„ì„ - ë¹ ë¥¸ ëª¨ë“œë¡œ ìƒëµ\n",
      "\n",
      "ğŸ“ƒ 4ë‹¨ê³„: í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ - ìƒëµë¨\n",
      "\n",
      "âœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: 3.7ì´ˆ)\n",
      "======================================================================\n",
      "ğŸ“Š ê°„ë‹¨ ìš”ì•½:\n",
      "   ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: 0ì\n",
      "   ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: 0í˜ì´ì§€\n",
      "   ğŸ“‹ ë°œê²¬ëœ í…Œì´ë¸”: 11ê°œ\n",
      "   ğŸ“ í…ìŠ¤íŠ¸ ë¼ì¸: 2154ê°œ\n",
      "\n",
      "ğŸ‰ ì´ˆê³ ì† í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: 3.7ì´ˆ)\n",
      "ğŸ’¡ ë” ìƒì„¸í•œ ë¶„ì„ì´ í•„ìš”í•˜ë©´ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì„¸ìš”:\n",
      "   run_complete_test(fast_mode=True)  # ë¹ ë¥¸ ì „ì²´ ë¶„ì„\n",
      "   run_complete_test(fast_mode=False) # ì™„ì „ ë¶„ì„ + ì €ì¥\n",
      "\n",
      "ğŸ’¡ Tip: auto_run ë³€ìˆ˜ë¥¼ ë³€ê²½í•˜ì—¬ ì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "â±ï¸ ë¶„ì„ ì™„ë£Œ (ê²½ê³¼ ì‹œê°„: 3.7s)\n",
      "\n",
      "âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!\n",
      "   ğŸ“ƒ ì´ í˜ì´ì§€: 3í˜ì´ì§€\n",
      "   ğŸ“ ì´ ê¸€ììˆ˜: 9,775ì\n",
      "   ğŸ“„ ì´ ì¤„ìˆ˜: 32ì¤„\n",
      "   ğŸ”¤ ì´ ë‹¨ì–´ìˆ˜: 1,449ê°œ\n",
      "\n",
      "ğŸ“– í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²« 500ì):\n",
      "================================================================================\n",
      "éŸ“åœ‹ITì„œë¹„ìŠ¤å­¸æœƒèªŒ ç¬¬22å· ç¬¬3è™Ÿ 2023å¹´ 6æœˆ, pp.1-27\n",
      "Journal of Information Technology Services https://doi.org/10.9716/KITS.2023.22.3.001\n",
      "ì–‘ì†ì¡ì´ ë¦¬ë”ì‹­ê³¼ í˜ì‹ ì ì¸ ì—…ë¬´ í–‰ë™: í•œêµ­ ë°˜ë„ì²´ ì‚°ì—…ì˜ ì¦ê±°\n",
      "ë”í˜ í—¨ë¦¬ ì•„ë©”ìš”* Â· ì˜¤í¬ë¦¬ í—¨ë¦¬ *** Â· ìœ¤ì†Œë¼ **** . ê°•ì£¼ì˜ *****\n",
      "Ambidextrous Leadership and Innovative Work Behavior: Evidence from South Korea Semiconductor Industry*\n",
      "Henry Ameyaw Domfeh ** Â· Henry Ofori *** Â· Sora Yoon *** **** Â· Juyoung Kang* ******\n",
      "Abstract\n",
      "The semiconductor industry is a competitive, complicated and a cyclical sector with a highly dy\n",
      "\n",
      "... (ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ìƒëµ) ...\n",
      "================================================================================\n",
      "   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (3.7ì´ˆ)\n",
      "\n",
      "ğŸ“Š 3ë‹¨ê³„: êµ¬ì¡° ë¶„ì„ - ë¹ ë¥¸ ëª¨ë“œë¡œ ìƒëµ\n",
      "\n",
      "ğŸ“ƒ 4ë‹¨ê³„: í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ - ìƒëµë¨\n",
      "\n",
      "âœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: 3.7ì´ˆ)\n",
      "======================================================================\n",
      "ğŸ“Š ê°„ë‹¨ ìš”ì•½:\n",
      "   ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: 0ì\n",
      "   ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: 0í˜ì´ì§€\n",
      "   ğŸ“‹ ë°œê²¬ëœ í…Œì´ë¸”: 11ê°œ\n",
      "   ğŸ“ í…ìŠ¤íŠ¸ ë¼ì¸: 2154ê°œ\n",
      "\n",
      "ğŸ‰ ì´ˆê³ ì† í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: 3.7ì´ˆ)\n",
      "ğŸ’¡ ë” ìƒì„¸í•œ ë¶„ì„ì´ í•„ìš”í•˜ë©´ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì„¸ìš”:\n",
      "   run_complete_test(fast_mode=True)  # ë¹ ë¥¸ ì „ì²´ ë¶„ì„\n",
      "   run_complete_test(fast_mode=False) # ì™„ì „ ë¶„ì„ + ì €ì¥\n",
      "\n",
      "ğŸ’¡ Tip: auto_run ë³€ìˆ˜ë¥¼ ë³€ê²½í•˜ì—¬ ì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ì„±ëŠ¥ ìµœì í™” ì˜µì…˜ í¬í•¨\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"ğŸš€ Azure Document Intelligence í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜µì…˜\")\n",
    "print(\"ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ì„±ëŠ¥ ìµœì í™” ì˜µì…˜ë“¤\n",
    "print(\"ğŸ“‹ ì‹¤í–‰ ì˜µì…˜:\")\n",
    "print(\"   1ï¸âƒ£ ì´ˆê³ ì† í…ŒìŠ¤íŠ¸ (3í˜ì´ì§€ë§Œ, ì €ì¥ ì—†ìŒ) - 30ì´ˆ ì´ë‚´\")\n",
    "print(\"   2ï¸âƒ£ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì „ì²´ í˜ì´ì§€, ì €ì¥ ìµœì†Œ) - 1-2ë¶„\")\n",
    "print(\"   3ï¸âƒ£ ì™„ì „ í…ŒìŠ¤íŠ¸ (ì „ì²´ ë¶„ì„, ëª¨ë“  ì €ì¥) - 3-5ë¶„\")\n",
    "print(\"   ğŸ”§ í™˜ê²½ í™•ì¸ë§Œ\")\n",
    "\n",
    "# ìë™ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸\n",
    "auto_run = True  # Falseë¡œ ë³€ê²½í•˜ë©´ ìˆ˜ë™ ì„ íƒ ëª¨ë“œ\n",
    "\n",
    "if auto_run:\n",
    "    print(f\"\\nâš¡ ìë™ ì‹¤í–‰ ëª¨ë“œ: ì´ˆê³ ì† í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n",
    "    print(\"   (ë” ê¸´ ë¶„ì„ì´ í•„ìš”í•˜ë©´ auto_run=Falseë¡œ ë³€ê²½ í›„ ìˆ˜ë™ ì„ íƒ)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # í•¨ìˆ˜ ì¡´ì¬ í™•ì¸\n",
    "    if 'fast_document_test' in globals():\n",
    "        success = fast_document_test()\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nğŸ‰ ì´ˆê³ ì† í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)\")\n",
    "            print(\"ğŸ’¡ ë” ìƒì„¸í•œ ë¶„ì„ì´ í•„ìš”í•˜ë©´ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "            print(\"   run_complete_test(fast_mode=True)  # ë¹ ë¥¸ ì „ì²´ ë¶„ì„\")\n",
    "            print(\"   run_complete_test(fast_mode=False) # ì™„ì „ ë¶„ì„ + ì €ì¥\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)\")\n",
    "    else:\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   30ë²ˆ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\nğŸ”§ ìˆ˜ë™ ì„ íƒ ëª¨ë“œ:\")\n",
    "    print(\"ë‹¤ìŒ ëª…ë ¹ì–´ ì¤‘ í•˜ë‚˜ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "    print(\"   fast_document_test()                    # ì´ˆê³ ì†\")\n",
    "    print(\"   run_complete_test(fast_mode=True)       # ë¹ ë¥¸ ì „ì²´\")\n",
    "    print(\"   run_complete_test(fast_mode=False)      # ì™„ì „ ë¶„ì„\")\n",
    "    print(\"   quick_test()                            # í™˜ê²½ í™•ì¸ë§Œ\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Tip: auto_run ë³€ìˆ˜ë¥¼ ë³€ê²½í•˜ì—¬ ì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1d409",
   "metadata": {},
   "source": [
    "## ğŸ¯ Azure Document Intelligence ì‹¤ì „ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
    "\n",
    "### âœ… êµ¬í˜„ëœ í•µì‹¬ ê¸°ëŠ¥ë“¤\n",
    "\n",
    "1. **ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ í™•ì¸** âœ…\n",
    "   - PDF ë¬¸ì„œì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "   - í…ìŠ¤íŠ¸ í’ˆì§ˆ ë° ì‹ ë¢°ë„ ì¸¡ì •\n",
    "   - í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ í†µê³„ ì œê³µ\n",
    "   - **ê²°ê³¼ íŒŒì¼ ìë™ ì €ì¥**: `*_full_text.txt`, `*_statistics.json`\n",
    "\n",
    "2. **í˜ì´ì§€ë³„ ê·¸ë¦¼/í…Œì´ë¸” ì¸ì‹** âœ…\n",
    "   - í…Œì´ë¸” êµ¬ì¡° ìë™ ì¸ì‹ ë° ì…€ ë‚´ìš© ì¶”ì¶œ\n",
    "   - ë¬¸ë‹¨, ì œëª©, ëª©ë¡ ë“± ë ˆì´ì•„ì›ƒ ìš”ì†Œ ë¶„ì„\n",
    "   - í˜ì´ì§€ë³„ êµ¬ì¡°ì  ìš”ì†Œ ìƒì„¸ ë¶„ì„\n",
    "   - **ê²°ê³¼ íŒŒì¼ ìë™ ì €ì¥**: `*_structure_analysis.txt`, `*_structure_data.json`\n",
    "\n",
    "3. **í˜ì´ì§€ë³„ ê²°ê³¼ ì²´í¬** âœ…\n",
    "   - ê° í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ ê²°ê³¼ í‘œì‹œ\n",
    "   - í…ìŠ¤íŠ¸ ë¼ì¸ ìˆ˜, ë‹¨ì–´ ìˆ˜, ì‹ ë¢°ë„ ì¸¡ì •\n",
    "   - í…Œì´ë¸” ë‚´ìš© í–‰/ì—´ë³„ ìƒì„¸ ì¶œë ¥\n",
    "   - **ê²°ê³¼ íŒŒì¼ ìë™ ì €ì¥**: `*_page_analysis.txt`, `*_page_analysis_data.json`\n",
    "\n",
    "### \udcc2 ë””ë ‰í† ë¦¬ êµ¬ì¡°\n",
    "\n",
    "#### ì…ë ¥ ë””ë ‰í† ë¦¬\n",
    "- **ê²½ë¡œ**: `/home/admin/wkms-aws/jupyter_notebook/data/input_docs`\n",
    "- **ìš©ë„**: ë¶„ì„í•  PDF ë¬¸ì„œ ì €ì¥\n",
    "- **ê¸°ë³¸ íŒŒì¼**: `test.pdf` (í•™ìˆ ë…¼ë¬¸ ì´ë¯¸ì§€ PDF)\n",
    "\n",
    "#### ì¶œë ¥ ë””ë ‰í† ë¦¬  \n",
    "- **ê²½ë¡œ**: `/home/admin/wkms-aws/jupyter_notebook/data/output_texts`\n",
    "- **ìš©ë„**: ë¶„ì„ ê²°ê³¼ ë° ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "- **ìƒì„± íŒŒì¼**:\n",
    "  - `{ë¬¸ì„œëª…}_full_text.txt`: ì „ì²´ ì¶”ì¶œ í…ìŠ¤íŠ¸\n",
    "  - `{ë¬¸ì„œëª…}_statistics.json`: í…ìŠ¤íŠ¸ ì¶”ì¶œ í†µê³„\n",
    "  - `{ë¬¸ì„œëª…}_structure_analysis.txt`: êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ\n",
    "  - `{ë¬¸ì„œëª…}_structure_data.json`: êµ¬ì¡° ë¶„ì„ ë°ì´í„°\n",
    "  - `{ë¬¸ì„œëª…}_page_analysis.txt`: í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„\n",
    "  - `{ë¬¸ì„œëª…}_page_analysis_data.json`: í˜ì´ì§€ë³„ ë¶„ì„ ë°ì´í„°\n",
    "\n",
    "### \ud83dğŸš€ ì‚¬ìš© ë°©ë²•\n",
    "\n",
    "#### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n",
    "- **ì…€ 5**: ê¸°ë³¸ ì—°ê²° ë° ì„¤ì • í™•ì¸\n",
    "\n",
    "#### ê°œë³„ ë‹¨ê³„ í…ŒìŠ¤íŠ¸\n",
    "- **ì…€ 6**: ë¬¸ì„œ ì„ íƒ ë° ê¸°ë³¸ ì •ë³´ (test.pdf ìš°ì„  ì„ íƒ)\n",
    "- **ì…€ 7**: ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ + íŒŒì¼ ì €ì¥\n",
    "- **ì…€ 8**: êµ¬ì¡°ì  ìš”ì†Œ ë¶„ì„ + íŒŒì¼ ì €ì¥\n",
    "- **ì…€ 9**: í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„ + íŒŒì¼ ì €ì¥\n",
    "\n",
    "#### ì „ì²´ ìë™ í…ŒìŠ¤íŠ¸\n",
    "- **ì…€ 11**: í•œ ë²ˆ ì‹¤í–‰ìœ¼ë¡œ ëª¨ë“  ë‹¨ê³„ ìë™ ìˆ˜í–‰ + ê²°ê³¼ ì €ì¥\n",
    "\n",
    "### ğŸ’¡ í™œìš© ê°€ëŠ¥ì„±\n",
    "- âœ… **í…ìŠ¤íŠ¸ ê²€ìƒ‰**: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰\n",
    "- âœ… **AI ì§ˆì˜ì‘ë‹µ**: ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€\n",
    "- âœ… **ë°ì´í„° ë¶„ì„**: í…Œì´ë¸” ë°ì´í„° êµ¬ì¡°í™” ë° ë¶„ì„\n",
    "- âœ… **ì˜ë¯¸ ê²€ìƒ‰**: ë¬¸ë§¥ ê¸°ë°˜ ê³ ê¸‰ ê²€ìƒ‰\n",
    "- âœ… **ê²°ê³¼ ë³´ê´€**: ëª¨ë“  ë¶„ì„ ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥\n",
    "\n",
    "### ğŸ“Š í’ˆì§ˆ ë³´ì¥\n",
    "- **ì‹ ë¢°ë„ ì¸¡ì •**: ê° ë‹¨ì–´ë³„ ì¸ì‹ ì •í™•ë„ í™•ì¸\n",
    "- **êµ¬ì¡° ì¸ì‹**: í…Œì´ë¸”, ë¬¸ë‹¨ ë“± ë ˆì´ì•„ì›ƒ ì •í™•ë„ ê²€ì¦\n",
    "- **ë‹¤ì¤‘ í˜•ì‹ ì§€ì›**: PDF, ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì²˜ë¦¬\n",
    "- **í•™ìˆ ë…¼ë¬¸ ìµœì í™”**: ë³µì¡í•œ ë ˆì´ì•„ì›ƒê³¼ ìˆ˜ì‹ì´ í¬í•¨ëœ ë…¼ë¬¸ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890bbdb0",
   "metadata": {},
   "source": [
    "# ğŸ”„ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¹„êµ ë¶„ì„\n",
    "\n",
    "## ğŸ“‹ ê°œìš”\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” í˜„ì¬ ë°±ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ê³  ë¹„êµë¶„ì„í•©ë‹ˆë‹¤:\n",
    "\n",
    "### 1ï¸âƒ£ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ë¬¸ì„œì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "- **PyPDF2, python-docx, python-pptx** ë“± ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©\n",
    "- **BeautifulSoup**ì„ í†µí•œ HTML/XML íŒŒì‹±\n",
    "- **ì •ê·œí‘œí˜„ì‹**ì„ í™œìš©í•œ í…ìŠ¤íŠ¸ ì •ì œ\n",
    "- **ë¹„ìš©**: ë¬´ë£Œ (ì»´í“¨íŒ… ìì›ë§Œ ì†Œëª¨)\n",
    "- **ì¥ì **: ë¹ ë¥¸ ì²˜ë¦¬, ì»¤ìŠ¤í„°ë§ˆì´ì§• ìš©ì´, ë¹„ìš© íš¨ìœ¨ì \n",
    "- **ë‹¨ì **: ë³µì¡í•œ ë ˆì´ì•„ì›ƒ ì²˜ë¦¬ í•œê³„, OCR ê¸°ëŠ¥ ì—†ìŒ\n",
    "\n",
    "### 2ï¸âƒ£ Azure Document Intelligence ê¸°ë°˜ íŒŒì´í”„ë¼ì¸  \n",
    "- **Azure AI Services** í™œìš©í•œ ê³ ê¸‰ ë¬¸ì„œ ë¶„ì„\n",
    "- **OCR, í…Œì´ë¸” ì¸ì‹, êµ¬ì¡° ë¶„ì„** ë“± AI ê¸°ëŠ¥\n",
    "- **prebuilt-layout, prebuilt-read** ëª¨ë¸ í™œìš©\n",
    "- **ë¹„ìš©**: ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆ\n",
    "- **ì¥ì **: ë†’ì€ ì •í™•ë„, ë³µì¡í•œ êµ¬ì¡° ì²˜ë¦¬, OCR ì§€ì›\n",
    "- **ë‹¨ì **: ë¹„ìš© ë°œìƒ, ì™¸ë¶€ ì„œë¹„ìŠ¤ ì˜ì¡´ì„±\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ë¹„êµ ë¶„ì„ ëª©í‘œ\n",
    "1. **ì„±ëŠ¥ ë¹„êµ**: ì²˜ë¦¬ ì†ë„, ì •í™•ë„, ì•ˆì •ì„±\n",
    "2. **ë¹„ìš© ë¶„ì„**: ìš´ì˜ ë¹„ìš©, ROI ê³„ì‚°\n",
    "3. **ì ìš© ì‹œë‚˜ë¦¬ì˜¤**: ë¬¸ì„œ ìœ í˜•ë³„ ìµœì  ì„ íƒ\n",
    "4. **ë°±í„°ìŠ¤í† ì–´ í˜¸í™˜ì„±**: ì„ë² ë”© ìƒì„± ë° ì €ì¥ íš¨ìœ¨ì„±\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f99b2c",
   "metadata": {},
   "source": [
    "## ğŸ”§ 1ë‹¨ê³„: ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "def install_opensource_libraries():\n",
    "    \"\"\"ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ ì²˜ë¦¬ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    libraries = [\n",
    "        'PyPDF2',           # PDF ì²˜ë¦¬\n",
    "        'python-docx',      # Word ë¬¸ì„œ ì²˜ë¦¬  \n",
    "        'python-pptx',      # PowerPoint ì²˜ë¦¬\n",
    "        'beautifulsoup4',   # HTML/XML íŒŒì‹±\n",
    "        'lxml',            # XML íŒŒì„œ\n",
    "        'openpyxl',        # Excel ì²˜ë¦¬\n",
    "        'pandas',          # ë°ì´í„° ì²˜ë¦¬\n",
    "        'pillow',          # ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "        'tiktoken'         # í† í° ê³„ì‚°\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ“¦ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘...\")\n",
    "    for lib in libraries:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib, \"--quiet\"])\n",
    "            print(f\"   âœ… {lib} ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"   âŒ {lib} ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì‹¤í–‰\n",
    "install_opensource_libraries()\n",
    "\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "try:\n",
    "    import PyPDF2\n",
    "    from docx import Document\n",
    "    from pptx import Presentation\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import tiktoken\n",
    "    from PIL import Image\n",
    "    \n",
    "    print(\"\\nâœ… ëª¨ë“  ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì„±ê³µ!\")\n",
    "    \n",
    "    # ë²„ì „ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „:\")\n",
    "    print(f\"   - PyPDF2: {PyPDF2.__version__}\")\n",
    "    print(f\"   - python-docx: ì‚¬ìš© ê°€ëŠ¥\")\n",
    "    print(f\"   - python-pptx: ì‚¬ìš© ê°€ëŠ¥\") \n",
    "    print(f\"   - BeautifulSoup4: {BeautifulSoup.__version__ if hasattr(BeautifulSoup, '__version__') else 'ì‚¬ìš© ê°€ëŠ¥'}\")\n",
    "    print(f\"   - pandas: {pd.__version__}\")\n",
    "    \n",
    "    # ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥\n",
    "    globals()['opensource_libraries_ready'] = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    globals()['opensource_libraries_ready'] = False\n",
    "\n",
    "# ì˜¤í”ˆì†ŒìŠ¤ ì²˜ë¦¬ìš© ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "OPENSOURCE_OUTPUT_DIR = os.path.join(OUTPUT_TEXTS_DIR, \"opensource_results\")\n",
    "os.makedirs(OPENSOURCE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nğŸ“ ì˜¤í”ˆì†ŒìŠ¤ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {OPENSOURCE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3e61d",
   "metadata": {},
   "source": [
    "## ğŸ“„ 2ë‹¨ê³„: ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ê¸° êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“„ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ê¸° í´ë˜ìŠ¤ êµ¬í˜„\n",
    "\n",
    "class OpenSourceDocumentProcessor:\n",
    "    \"\"\"ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ë¬¸ì„œ ì²˜ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir=None):\n",
    "        self.output_dir = output_dir or OPENSOURCE_OUTPUT_DIR\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.supported_formats = ['.pdf', '.docx', '.pptx', '.txt', '.html', '.xml']\n",
    "        \n",
    "    def extract_text_from_pdf(self, file_path):\n",
    "        \"\"\"PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyPDF2 ì‚¬ìš©)\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                metadata = {\n",
    "                    'pages': len(pdf_reader.pages),\n",
    "                    'title': pdf_reader.metadata.get('/Title', 'Unknown') if pdf_reader.metadata else 'Unknown',\n",
    "                    'author': pdf_reader.metadata.get('/Author', 'Unknown') if pdf_reader.metadata else 'Unknown'\n",
    "                }\n",
    "                \n",
    "                for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                    page_text = page.extract_text()\n",
    "                    text += f\"\\n\\n--- í˜ì´ì§€ {page_num} ---\\n{page_text}\"\n",
    "                \n",
    "                return {\n",
    "                    'text': text.strip(),\n",
    "                    'metadata': metadata,\n",
    "                    'success': True,\n",
    "                    'error': None\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'text': None,\n",
    "                'metadata': None,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def extract_text_from_docx(self, file_path):\n",
    "        \"\"\"Word ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (python-docx ì‚¬ìš©)\"\"\"\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            \n",
    "            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "            core_props = doc.core_properties\n",
    "            metadata = {\n",
    "                'title': core_props.title or 'Unknown',\n",
    "                'author': core_props.author or 'Unknown',\n",
    "                'created': str(core_props.created) if core_props.created else 'Unknown',\n",
    "                'modified': str(core_props.modified) if core_props.modified else 'Unknown',\n",
    "                'paragraphs': len(doc.paragraphs)\n",
    "            }\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            text = \"\"\n",
    "            for para in doc.paragraphs:\n",
    "                if para.text.strip():\n",
    "                    text += para.text + \"\\n\"\n",
    "            \n",
    "            # í…Œì´ë¸” ì¶”ì¶œ\n",
    "            tables_text = \"\"\n",
    "            for table in doc.tables:\n",
    "                tables_text += \"\\n\\n--- í…Œì´ë¸” ---\\n\"\n",
    "                for row in table.rows:\n",
    "                    row_text = []\n",
    "                    for cell in row.cells:\n",
    "                        row_text.append(cell.text.strip())\n",
    "                    tables_text += \" | \".join(row_text) + \"\\n\"\n",
    "            \n",
    "            full_text = text + tables_text\n",
    "            \n",
    "            return {\n",
    "                'text': full_text.strip(),\n",
    "                'metadata': metadata,\n",
    "                'success': True,\n",
    "                'error': None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'text': None,\n",
    "                'metadata': None,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def extract_text_from_pptx(self, file_path):\n",
    "        \"\"\"PowerPoint íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (python-pptx ì‚¬ìš©)\"\"\"\n",
    "        try:\n",
    "            prs = Presentation(file_path)\n",
    "            \n",
    "            metadata = {\n",
    "                'slides': len(prs.slides),\n",
    "                'title': 'PowerPoint Presentation'\n",
    "            }\n",
    "            \n",
    "            text = \"\"\n",
    "            for slide_num, slide in enumerate(prs.slides, 1):\n",
    "                slide_text = f\"\\n\\n--- ìŠ¬ë¼ì´ë“œ {slide_num} ---\\n\"\n",
    "                \n",
    "                for shape in slide.shapes:\n",
    "                    if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                        slide_text += shape.text + \"\\n\"\n",
    "                \n",
    "                text += slide_text\n",
    "            \n",
    "            return {\n",
    "                'text': text.strip(),\n",
    "                'metadata': metadata,\n",
    "                'success': True,\n",
    "                'error': None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'text': None,\n",
    "                'metadata': None,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def extract_text_from_txt(self, file_path):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            metadata = {\n",
    "                'size': len(text),\n",
    "                'lines': len(text.split('\\n'))\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'text': text,\n",
    "                'metadata': metadata,\n",
    "                'success': True,\n",
    "                'error': None\n",
    "            }\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='cp949') as file:\n",
    "                    text = file.read()\n",
    "                metadata = {\n",
    "                    'size': len(text),\n",
    "                    'lines': len(text.split('\\n')),\n",
    "                    'encoding': 'cp949'\n",
    "                }\n",
    "                return {\n",
    "                    'text': text,\n",
    "                    'metadata': metadata,\n",
    "                    'success': True,\n",
    "                    'error': None\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    'text': None,\n",
    "                    'metadata': None,\n",
    "                    'success': False,\n",
    "                    'error': f\"Encoding error: {e}\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'text': None,\n",
    "                'metadata': None,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def process_html_content(self, content):\n",
    "        \"\"\"HTML ì½˜í…ì¸  ì²˜ë¦¬ (ë°±ì—”ë“œ tools.pyì˜ process_file í•¨ìˆ˜ ê¸°ë°˜)\"\"\"\n",
    "        try:\n",
    "            # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            # í…Œì´ë¸” ì²˜ë¦¬\n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                table_text = self.convert_table_to_text(table)\n",
    "                table.replace_with(table_text)\n",
    "            \n",
    "            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°\n",
    "            for tag in soup(['script', 'style', 'meta', 'link']):\n",
    "                tag.decompose()\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì •ì œ\n",
    "            text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)  # HTML ì£¼ì„ ì œê±°\n",
    "            text = re.sub(r'\\n{3,}', '\\n\\n', text)  # ì—°ì†ëœ ê°œí–‰ ì •ë¦¬\n",
    "            text = re.sub(r'[ \\t]+', ' ', text)  # ì—°ì†ëœ ê³µë°± ì •ë¦¬\n",
    "            \n",
    "            return text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ HTML ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            return content\n",
    "    \n",
    "    def convert_table_to_text(self, table):\n",
    "        \"\"\"HTML í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "        try:\n",
    "            rows = table.find_all('tr')\n",
    "            if not rows:\n",
    "                return \"\"\n",
    "            \n",
    "            table_text = \"\\n\\n--- í…Œì´ë¸” ---\\n\"\n",
    "            for row in rows:\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                row_text = []\n",
    "                for cell in cells:\n",
    "                    cell_text = cell.get_text().strip()\n",
    "                    row_text.append(cell_text)\n",
    "                if row_text:\n",
    "                    table_text += \" | \".join(row_text) + \"\\n\"\n",
    "            \n",
    "            return table_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return \"\\n[í…Œì´ë¸” ì²˜ë¦¬ ì˜¤ë¥˜]\\n\"\n",
    "    \n",
    "    def calculate_tokens(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def chunk_text(self, text, max_tokens=6000, min_tokens=1000, overlap_percentage=0.1):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹ (ë°±ì—”ë“œ tools.pyì˜ chunked_texts í•¨ìˆ˜ ê¸°ë°˜)\"\"\"\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        overlap_tokens = int(max_tokens * overlap_percentage)\n",
    "        chunks = []\n",
    "        start_index = 0\n",
    "        \n",
    "        while start_index < len(tokens):\n",
    "            if start_index + max_tokens >= len(tokens):\n",
    "                # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬\n",
    "                remaining_tokens = len(tokens) - start_index\n",
    "                if remaining_tokens < min_tokens and chunks:\n",
    "                    # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì™€ í•©ì¹˜ê¸°\n",
    "                    chunks[-1] += self.tokenizer.decode(tokens[start_index:])\n",
    "                    break\n",
    "                else:\n",
    "                    chunks.append(self.tokenizer.decode(tokens[start_index:]))\n",
    "                    break\n",
    "            else:\n",
    "                end_index = min(start_index + max_tokens, len(tokens))\n",
    "                chunk = self.tokenizer.decode(tokens[start_index:end_index])\n",
    "                chunks.append(chunk)\n",
    "                start_index += max_tokens - overlap_tokens\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "print(\"ğŸ“„ ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ ì²˜ë¦¬ê¸° í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")\n",
    "if 'opensource_libraries_ready' in globals() and opensource_libraries_ready:\n",
    "    globals()['opensource_processor'] = OpenSourceDocumentProcessor()\n",
    "    print(\"âœ… ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ee832",
   "metadata": {},
   "source": [
    "## ğŸš€ 3ë‹¨ê³„: ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬í˜„\n",
    "\n",
    "def run_opensource_pipeline(file_path, save_results=True, chunk_text=True):\n",
    "    \"\"\"\n",
    "    ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ì²˜ë¦¬í•  ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ\n",
    "        save_results (bool): ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€\n",
    "        chunk_text (bool): í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í• ì§€ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        dict: ì²˜ë¦¬ ê²°ê³¼ ì •ë³´\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"ğŸš€ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not globals().get('opensource_processor'):\n",
    "        print(\"âŒ ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ ì²˜ë¦¬ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    processor = opensource_processor\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    file_extension = file_path.suffix.lower()\n",
    "    file_name = file_path.stem\n",
    "    \n",
    "    print(f\"ğŸ“‚ ì…ë ¥ íŒŒì¼: {file_path.name}\")\n",
    "    print(f\"ğŸ“ íŒŒì¼ í˜•ì‹: {file_extension}\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: íŒŒì¼ í˜•ì‹ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    print(f\"\\nğŸ“„ 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ ({file_extension})\")\n",
    "    extract_start = time.time()\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        result = processor.extract_text_from_pdf(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        result = processor.extract_text_from_docx(file_path)\n",
    "    elif file_extension == '.pptx':\n",
    "        result = processor.extract_text_from_pptx(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        result = processor.extract_text_from_txt(file_path)\n",
    "    else:\n",
    "        print(f\"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}\")\n",
    "        return None\n",
    "    \n",
    "    if not result['success']:\n",
    "        print(f\"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {result['error']}\")\n",
    "        return None\n",
    "    \n",
    "    extracted_text = result['text']\n",
    "    metadata = result['metadata']\n",
    "    extract_elapsed = time.time() - extract_start\n",
    "    \n",
    "    print(f\"   âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({extract_elapsed:.2f}ì´ˆ)\")\n",
    "    print(f\"   ğŸ“Š ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(extracted_text):,}ì\")\n",
    "    \n",
    "    # 2ë‹¨ê³„: HTML/ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  ì •ì œ\n",
    "    print(f\"\\nğŸ”§ 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ\")\n",
    "    clean_start = time.time()\n",
    "    \n",
    "    if '<' in extracted_text and '>' in extracted_text:\n",
    "        cleaned_text = processor.process_html_content(extracted_text)\n",
    "        print(f\"   âœ… HTML/XML íƒœê·¸ ì •ì œ ì™„ë£Œ\")\n",
    "    else:\n",
    "        cleaned_text = extracted_text\n",
    "        print(f\"   âœ… í…ìŠ¤íŠ¸ ì •ì œ ìƒëµ (HTML íƒœê·¸ ì—†ìŒ)\")\n",
    "    \n",
    "    clean_elapsed = time.time() - clean_start\n",
    "    print(f\"   â±ï¸ ì •ì œ ì‹œê°„: {clean_elapsed:.2f}ì´ˆ\")\n",
    "    \n",
    "    # 3ë‹¨ê³„: í† í° ê³„ì‚°\n",
    "    print(f\"\\nğŸ”¢ 3ë‹¨ê³„: í† í° ë¶„ì„\")\n",
    "    token_count = processor.calculate_tokens(cleaned_text)\\n    print(f\"   ğŸ“Š ì´ í† í° ìˆ˜: {token_count:,}ê°œ\")\n",
    "    \\n    # 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í‚¹ (ì˜µì…˜)\n",
    "    chunks = []\n",
    "    if chunk_text and token_count > 6000:\n",
    "        print(f\"\\nâœ‚ï¸ 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í‚¹\")\n",
    "        chunk_start = time.time()\n",
    "        \n",
    "        chunks = processor.chunk_text(cleaned_text, max_tokens=6000, overlap_percentage=0.1)\n",
    "        chunk_elapsed = time.time() - chunk_start\n",
    "        \n",
    "        print(f\"   âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬\")\n",
    "        print(f\"   â±ï¸ ì²­í‚¹ ì‹œê°„: {chunk_elapsed:.2f}ì´ˆ\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_tokens = processor.calculate_tokens(chunk)\n",
    "            print(f\"      ì²­í¬ {i+1}: {chunk_tokens:,} í† í°\")\n",
    "    else:\n",
    "        print(f\"\\nâœ‚ï¸ 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í‚¹ ìƒëµ\")\n",
    "        chunks = [cleaned_text]\n",
    "    \n",
    "    # 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥ (ì˜µì…˜)\n",
    "    saved_files = []\n",
    "    if save_results:\n",
    "        print(f\"\\nğŸ’¾ 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥\")\n",
    "        save_start = time.time()\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "        metadata_file = os.path.join(OPENSOURCE_OUTPUT_DIR, f\"{file_name}_metadata.json\")\n",
    "        full_metadata = {\n",
    "            'file_name': file_path.name,\n",
    "            'file_size': file_path.stat().st_size,\n",
    "            'processing_time': time.time() - start_time,\n",
    "            'extraction_method': 'opensource',\n",
    "            'file_type': file_extension,\n",
    "            'text_length': len(cleaned_text),\n",
    "            'token_count': token_count,\n",
    "            'chunks_count': len(chunks),\n",
    "            'metadata': metadata,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(full_metadata, f, ensure_ascii=False, indent=2)\n",
    "        saved_files.append(metadata_file)\n",
    "        \n",
    "        # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "        text_file = os.path.join(OPENSOURCE_OUTPUT_DIR, f\"{file_name}_fulltext.txt\")\n",
    "        with open(text_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_text)\n",
    "        saved_files.append(text_file)\n",
    "        \n",
    "        # ì²­í¬ë³„ ì €ì¥\n",
    "        if len(chunks) > 1:\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_file = os.path.join(OPENSOURCE_OUTPUT_DIR, f\"{file_name}_chunk_{i+1}.txt\")\n",
    "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(chunk)\n",
    "                saved_files.append(chunk_file)\n",
    "        \n",
    "        save_elapsed = time.time() - save_start\n",
    "        print(f\"   âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ ({save_elapsed:.2f}ì´ˆ)\")\n",
    "        print(f\"   ğŸ“ ì €ì¥ëœ íŒŒì¼ ìˆ˜: {len(saved_files)}ê°œ\")\n",
    "    \n",
    "    # ìµœì¢… ê²°ê³¼\n",
    "    total_elapsed = time.time() - start_time\n",
    "    \n",
    "    result_summary = {\n",
    "        'success': True,\n",
    "        'file_path': str(file_path),\n",
    "        'file_name': file_path.name,\n",
    "        'processing_method': 'opensource',\n",
    "        'text_length': len(cleaned_text),\n",
    "        'token_count': token_count,\n",
    "        'chunks': chunks,\n",
    "        'chunks_count': len(chunks),\n",
    "        'metadata': full_metadata if save_results else metadata,\n",
    "        'saved_files': saved_files,\n",
    "        'processing_time': total_elapsed,\n",
    "        'extract_time': extract_elapsed,\n",
    "        'clean_time': clean_elapsed\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ… ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "    print(f\"   â±ï¸ ì´ ì²˜ë¦¬ì‹œê°„: {total_elapsed:.2f}ì´ˆ\")\n",
    "    print(f\"   ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {len(cleaned_text):,}ì, {token_count:,} í† í°\")\n",
    "    print(f\"   ğŸ“ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ\")\n",
    "    \n",
    "    # ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥\n",
    "    globals()['opensource_result'] = result_summary\n",
    "    globals()['opensource_text'] = cleaned_text\n",
    "    globals()['opensource_chunks'] = chunks\n",
    "    \n",
    "    return result_summary\n",
    "\n",
    "# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
    "def quick_opensource_test(file_path):\n",
    "    \"\"\"ë¹ ë¥¸ ì˜¤í”ˆì†ŒìŠ¤ í…ŒìŠ¤íŠ¸ (ì €ì¥ ì—†ìŒ, ì²­í‚¹ ì—†ìŒ)\"\"\"\n",
    "    return run_opensource_pipeline(file_path, save_results=False, chunk_text=False)\n",
    "\n",
    "print(\"ğŸš€ ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "print(\"   - run_opensource_pipeline(file_path) : ì „ì²´ íŒŒì´í”„ë¼ì¸\")\n",
    "print(\"   - quick_opensource_test(file_path)   : ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247341c2",
   "metadata": {},
   "source": [
    "## ğŸ§ª 4ë‹¨ê³„: ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3eff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "\n",
    "print(\"ğŸ§ª ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ íŒŒì¼ í™•ì¸\n",
    "if 'test_document_path' in globals() and test_document_path:\n",
    "    print(f\"ğŸ“‚ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {os.path.basename(test_document_path)}\")\n",
    "    \n",
    "    # ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    print(f\"\\nğŸš€ ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...\")\n",
    "    opensource_result = run_opensource_pipeline(\n",
    "        file_path=test_document_path,\n",
    "        save_results=True,\n",
    "        chunk_text=True\n",
    "    )\n",
    "    \n",
    "    if opensource_result and opensource_result['success']:\n",
    "        print(f\"\\nâœ… ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!\")\n",
    "        \n",
    "        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "        print(f\"\\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:\")\n",
    "        print(f\"   ğŸ“„ íŒŒì¼ëª…: {opensource_result['file_name']}\")\n",
    "        print(f\"   â±ï¸ ì²˜ë¦¬ì‹œê°„: {opensource_result['processing_time']:.2f}ì´ˆ\")\n",
    "        print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {opensource_result['text_length']:,}ì\")\n",
    "        print(f\"   ğŸ”¢ í† í° ìˆ˜: {opensource_result['token_count']:,}ê°œ\")\n",
    "        print(f\"   ğŸ“ ì²­í¬ ìˆ˜: {opensource_result['chunks_count']}ê°œ\")\n",
    "        print(f\"   ğŸ’¾ ì €ì¥ íŒŒì¼: {len(opensource_result['saved_files'])}ê°œ\")\n",
    "        \n",
    "        # ì²­í¬ ë¯¸ë¦¬ë³´ê¸°\n",
    "        if 'opensource_chunks' in globals() and opensource_chunks:\n",
    "            print(f\"\\nğŸ“– ì²« ë²ˆì§¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì):\")\n",
    "            preview_text = opensource_chunks[0][:500]\n",
    "            print(f\"   {preview_text}...\")\n",
    "        \n",
    "        # ì €ì¥ëœ íŒŒì¼ ëª©ë¡\n",
    "        print(f\"\\nğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:\")\n",
    "        for file_path in opensource_result['saved_files']:\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            print(f\"   - {os.path.basename(file_path)} ({file_size:,} bytes)\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"âŒ ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   ë¨¼ì € ì´ì „ ì…€ë“¤ì„ ì‹¤í–‰í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\")\n",
    "    \n",
    "    # ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ëª©ë¡ í‘œì‹œ\n",
    "    if 'TEST_DOCUMENTS' in globals():\n",
    "        print(f\"\\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ:\")\n",
    "        for i, doc in enumerate(TEST_DOCUMENTS, 1):\n",
    "            doc_path = os.path.join(INPUT_DOCS_DIR, doc)\n",
    "            if os.path.exists(doc_path):\n",
    "                size = os.path.getsize(doc_path)\n",
    "                print(f\"   {i}. {doc} ({size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Tip: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ì›í•˜ë©´ quick_opensource_test(file_path)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23da796",
   "metadata": {},
   "source": [
    "## ğŸ—ƒï¸ 5ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ ì—°ë™ ë° ì„ë² ë”© ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ƒï¸ ë²¡í„°ìŠ¤í† ì–´ ì—°ë™ ë° ì„ë² ë”© ìƒì„± êµ¬í˜„\n",
    "\n",
    "class VectorStoreProcessor:\n",
    "    \"\"\"ë²¡í„°ìŠ¤í† ì–´ ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„± í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_model = \"text-embedding-ada-002\"  # ë°±ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© ëª¨ë¸\n",
    "        self.vector_dimension = 1536  # Ada-002 ëª¨ë¸ì˜ ë²¡í„° ì°¨ì›\n",
    "        \n",
    "    def create_document_chunks_for_vectorstore(self, text_chunks, file_info, method='opensource'):\n",
    "        \"\"\"\n",
    "        ë°±í„°ìŠ¤í† ì–´ ì €ì¥ìš© ë¬¸ì„œ ì²­í¬ ìƒì„± (ë°±ì—”ë“œ tools.pyì˜ preprocessing_documents í•¨ìˆ˜ ê¸°ë°˜)\n",
    "        \n",
    "        Args:\n",
    "            text_chunks (list): í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸\n",
    "            file_info (dict): íŒŒì¼ ì •ë³´\n",
    "            method (str): ì²˜ë¦¬ ë°©ë²• ('opensource' ë˜ëŠ” 'azure_di')\n",
    "        \n",
    "        Returns:\n",
    "            list: ë²¡í„°ìŠ¤í† ì–´ìš© ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        preprocessing_time = now.strftime(\"%Y-%m-%dT00:00:00Z\")\n",
    "        \n",
    "        file_name = file_info.get('file_name', 'unknown')\n",
    "        file_logical_name = os.path.splitext(file_name)[0]  # í™•ì¥ì ì œê±°\n",
    "        title = text_chunks[0].strip().split('\\n')[0] if text_chunks else 'Unknown Title'\n",
    "        \n",
    "        vectorstore_documents = []\n",
    "        \n",
    "        for chunk_idx, chunk_text in enumerate(text_chunks):\n",
    "            chunk_text = chunk_text.strip()\n",
    "            \n",
    "            # ë°±ì—”ë“œì™€ ë™ì¼í•œ main_text í˜•ì‹\n",
    "            main_text = f\"\"\"(file_name) {file_logical_name}\n",
    "\n",
    "(title) {title}\n",
    "\n",
    "(content)\n",
    "{chunk_text}\"\"\".strip()\n",
    "            \n",
    "            # ë°±ì—”ë“œì™€ ë™ì¼í•œ document êµ¬ì¡°\n",
    "            document = {\n",
    "                \"file_index_id\": f\"{method}_{file_logical_name}_{chunk_idx}\",\n",
    "                \"file_lgc_nm\": file_logical_name,\n",
    "                \"file_psl_nm\": file_name,\n",
    "                \"title\": title,\n",
    "                \"page_num\": chunk_idx + 1,  # ì²­í¬ë¥¼ í˜ì´ì§€ë¡œ ê°„ì£¼\n",
    "                \"chunk_num\": chunk_idx,\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"main_text\": main_text,\n",
    "                \"preprocessing_time\": preprocessing_time,\n",
    "                \"processing_method\": method,\n",
    "                \"del_yn\": False,\n",
    "                \"metadata\": {\n",
    "                    \"extraction_method\": method,\n",
    "                    \"chunk_index\": chunk_idx,\n",
    "                    \"total_chunks\": len(text_chunks),\n",
    "                    \"file_size\": file_info.get('file_size', 0),\n",
    "                    \"processing_time\": file_info.get('processing_time', 0)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # ë°±ì—”ë“œì™€ ë™ì¼í•œ ID ìƒì„± (base64 ì¸ì½”ë”©)\n",
    "            import base64\n",
    "            original_key = f\"{document['file_psl_nm']}_{document['page_num']}_{document['chunk_num']}\"\n",
    "            encoded_key = base64.urlsafe_b64encode(original_key.encode('utf-8')).decode('utf-8')\n",
    "            document['id'] = encoded_key\n",
    "            \n",
    "            vectorstore_documents.append(document)\n",
    "        \n",
    "        return vectorstore_documents\n",
    "    \n",
    "    def simulate_embedding_generation(self, documents):\n",
    "        \"\"\"\n",
    "        ì„ë² ë”© ìƒì„± ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Azure OpenAI ì‚¬ìš©)\n",
    "        \n",
    "        Args:\n",
    "            documents (list): ë²¡í„°ìŠ¤í† ì–´ìš© ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "        Returns:\n",
    "            list: ì„ë² ë”©ì´ ì¶”ê°€ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        import random\n",
    "        import numpy as np\n",
    "        \n",
    "        print(f\"ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘... ({len(documents)}ê°œ ë¬¸ì„œ)\")\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Azure OpenAI API í˜¸ì¶œ\n",
    "            # chunk_text_vector = aoai_instance.generate_embeddings(doc['chunk_text'])\n",
    "            # main_text_vector = aoai_instance.generate_embeddings(doc['main_text'])\n",
    "            \n",
    "            # ì‹œë®¬ë ˆì´ì…˜ìš© ëœë¤ ë²¡í„° ìƒì„±\n",
    "            np.random.seed(hash(doc['chunk_text']) % 2**32)  # ì¬í˜„ ê°€ëŠ¥í•œ ëœë¤\n",
    "            chunk_text_vector = np.random.randn(self.vector_dimension).tolist()\n",
    "            main_text_vector = np.random.randn(self.vector_dimension).tolist()\n",
    "            \n",
    "            doc['chunk_text_vector'] = chunk_text_vector\n",
    "            doc['main_text_vector'] = main_text_vector\n",
    "            \n",
    "            if i % 5 == 0:  # ì§„í–‰ë¥  í‘œì‹œ\n",
    "                print(f\"   ğŸ“Š ì§„í–‰ë¥ : {i+1}/{len(documents)} ({((i+1)/len(documents)*100):.1f}%)\")\n",
    "        \n",
    "        print(f\"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!\")\n",
    "        return documents\n",
    "    \n",
    "    def simulate_vectorstore_upload(self, documents, save_to_file=True):\n",
    "        \"\"\"\n",
    "        ë²¡í„°ìŠ¤í† ì–´ ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜\n",
    "        \n",
    "        Args:\n",
    "            documents (list): ì„ë² ë”©ì´ í¬í•¨ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "            save_to_file (bool): ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€\n",
    "        \n",
    "        Returns:\n",
    "            dict: ì—…ë¡œë“œ ê²°ê³¼ ì •ë³´\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ—ƒï¸ ë²¡í„°ìŠ¤í† ì–´ ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜...\")\n",
    "        \n",
    "        upload_results = {\n",
    "            'total_documents': len(documents),\n",
    "            'successful_uploads': 0,\n",
    "            'failed_uploads': 0,\n",
    "            'uploaded_documents': [],\n",
    "            'upload_errors': []\n",
    "        }\n",
    "        \n",
    "        for doc in documents:\n",
    "            try:\n",
    "                # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Cosmos DB ë˜ëŠ” AI Search ì—…ë¡œë“œ\n",
    "                # result = cosmos_db.upload_item(doc)\n",
    "                # result = ai_search.upload_to_index(doc)\n",
    "                \n",
    "                # ì‹œë®¬ë ˆì´ì…˜: ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆë‹¤ê³  ê°€ì •\n",
    "                upload_results['successful_uploads'] += 1\n",
    "                upload_results['uploaded_documents'].append({\n",
    "                    'id': doc['id'],\n",
    "                    'file_name': doc['file_psl_nm'],\n",
    "                    'chunk_num': doc['chunk_num'],\n",
    "                    'vector_dimension': len(doc['chunk_text_vector'])\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                upload_results['failed_uploads'] += 1\n",
    "                upload_results['upload_errors'].append({\n",
    "                    'id': doc['id'],\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        if save_to_file:\n",
    "            result_file = os.path.join(OPENSOURCE_OUTPUT_DIR, \"vectorstore_upload_result.json\")\n",
    "            with open(result_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(upload_results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"   ğŸ’¾ ì—…ë¡œë“œ ê²°ê³¼ ì €ì¥: {result_file}\")\n",
    "        \n",
    "        print(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ ì—…ë¡œë“œ ì™„ë£Œ!\")\n",
    "        print(f\"   ğŸ“Š ì„±ê³µ: {upload_results['successful_uploads']}ê°œ\")\n",
    "        print(f\"   âŒ ì‹¤íŒ¨: {upload_results['failed_uploads']}ê°œ\")\n",
    "        \n",
    "        return upload_results\n",
    "\n",
    "def run_vectorstore_pipeline(processing_result, method='opensource'):\n",
    "    \"\"\"\n",
    "    ë²¡í„°ìŠ¤í† ì–´ ì—°ë™ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    \n",
    "    Args:\n",
    "        processing_result (dict): ë¬¸ì„œ ì²˜ë¦¬ ê²°ê³¼\n",
    "        method (str): ì²˜ë¦¬ ë°©ë²•\n",
    "    \n",
    "    Returns:\n",
    "        dict: ë²¡í„°ìŠ¤í† ì–´ ì²˜ë¦¬ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ—ƒï¸ ë²¡í„°ìŠ¤í† ì–´ ì—°ë™ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ({method})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not processing_result or not processing_result.get('success'):\n",
    "        print(\"âŒ ìœ íš¨í•œ ë¬¸ì„œ ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    vector_processor = VectorStoreProcessor()\n",
    "    \n",
    "    # 1ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ìš© ë¬¸ì„œ ì²­í¬ ìƒì„±\n",
    "    print(f\"ğŸ“„ 1ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ìš© ë¬¸ì„œ ì²­í¬ ìƒì„±\")\n",
    "    file_info = {\n",
    "        'file_name': processing_result['file_name'],\n",
    "        'file_size': os.path.getsize(processing_result['file_path']) if os.path.exists(processing_result['file_path']) else 0,\n",
    "        'processing_time': processing_result.get('processing_time', 0)\n",
    "    }\n",
    "    \n",
    "    chunks = processing_result.get('chunks', [])\n",
    "    vectorstore_documents = vector_processor.create_document_chunks_for_vectorstore(\n",
    "        chunks, file_info, method\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… ë²¡í„°ìŠ¤í† ì–´ìš© ë¬¸ì„œ ìƒì„±: {len(vectorstore_documents)}ê°œ\")\n",
    "    \n",
    "    # 2ë‹¨ê³„: ì„ë² ë”© ìƒì„±\n",
    "    print(f\"\\nğŸ”„ 2ë‹¨ê³„: ì„ë² ë”© ìƒì„±\")\n",
    "    embedded_documents = vector_processor.simulate_embedding_generation(vectorstore_documents)\n",
    "    \n",
    "    # 3ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ ì—…ë¡œë“œ\n",
    "    print(f\"\\nğŸ—ƒï¸ 3ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ ì—…ë¡œë“œ\")\n",
    "    upload_result = vector_processor.simulate_vectorstore_upload(embedded_documents)\n",
    "    \n",
    "    # ìµœì¢… ê²°ê³¼\n",
    "    final_result = {\n",
    "        'method': method,\n",
    "        'vectorstore_documents_count': len(vectorstore_documents),\n",
    "        'embedding_dimension': vector_processor.vector_dimension,\n",
    "        'upload_result': upload_result,\n",
    "        'vectorstore_documents': vectorstore_documents  # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì œì™¸ ê°€ëŠ¥\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ… ë²¡í„°ìŠ¤í† ì–´ ì—°ë™ ì™„ë£Œ!\")\n",
    "    print(f\"   ğŸ“„ ì²˜ë¦¬ëœ ë¬¸ì„œ: {len(vectorstore_documents)}ê°œ\")\n",
    "    print(f\"   ğŸ”¢ ë²¡í„° ì°¨ì›: {vector_processor.vector_dimension}\")\n",
    "    print(f\"   ğŸ“Š ì—…ë¡œë“œ ì„±ê³µë¥ : {upload_result['successful_uploads']}/{upload_result['total_documents']}\")\n",
    "    \n",
    "    # ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥\n",
    "    globals()[f'{method}_vectorstore_result'] = final_result\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "print(\"ğŸ—ƒï¸ ë²¡í„°ìŠ¤í† ì–´ ì—°ë™ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•: run_vectorstore_pipeline(processing_result, method='opensource')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b73b9",
   "metadata": {},
   "source": [
    "## ğŸ“Š 6ë‹¨ê³„: ë‘ íŒŒì´í”„ë¼ì¸ ë¹„êµ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588eeefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ë‘ íŒŒì´í”„ë¼ì¸ ë¹„êµ ë¶„ì„ ì‹¤í–‰\n",
    "\n",
    "def compare_pipelines():\n",
    "    \"\"\"ë‘ íŒŒì´í”„ë¼ì¸ ë¹„êµ ë¶„ì„ í•¨ìˆ˜\"\"\"\n",
    "    print(\"ğŸ“Š ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¹„êµ ë¶„ì„\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ê²°ê³¼ ë°ì´í„° í™•ì¸\n",
    "    opensource_available = 'opensource_result' in globals() and opensource_result\n",
    "    azure_di_available = 'text_result' in globals() and text_result\n",
    "    \n",
    "    if not opensource_available and not azure_di_available:\n",
    "        print(\"âŒ ë¹„êµí•  íŒŒì´í”„ë¼ì¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   ë‘ íŒŒì´í”„ë¼ì¸ì„ ëª¨ë‘ ì‹¤í–‰í•œ í›„ ë¹„êµí•´ì£¼ì„¸ìš”.\")\n",
    "        return None\n",
    "    \n",
    "    comparison_result = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'opensource': {},\n",
    "        'azure_di': {},\n",
    "        'comparison': {}\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“‹ íŒŒì´í”„ë¼ì¸ë³„ ê²°ê³¼ ìš”ì•½:\")\n",
    "    \n",
    "    # 1. ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ ê²°ê³¼\n",
    "    if opensource_available:\n",
    "        os_result = opensource_result\n",
    "        comparison_result['opensource'] = {\n",
    "            'available': True,\n",
    "            'processing_time': os_result.get('processing_time', 0),\n",
    "            'text_length': os_result.get('text_length', 0),\n",
    "            'token_count': os_result.get('token_count', 0),\n",
    "            'chunks_count': os_result.get('chunks_count', 0),\n",
    "            'file_name': os_result.get('file_name', 'Unknown'),\n",
    "            'method': 'opensource_libraries'\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ”§ 1ï¸âƒ£ ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸:\")\n",
    "        print(f\"   â±ï¸ ì²˜ë¦¬ì‹œê°„: {os_result.get('processing_time', 0):.2f}ì´ˆ\")\n",
    "        print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {os_result.get('text_length', 0):,}ì\")\n",
    "        print(f\"   ğŸ”¢ í† í° ìˆ˜: {os_result.get('token_count', 0):,}ê°œ\")\n",
    "        print(f\"   ğŸ“ ì²­í¬ ìˆ˜: {os_result.get('chunks_count', 0)}ê°œ\")\n",
    "    else:\n",
    "        comparison_result['opensource'] = {'available': False}\n",
    "        print(f\"\\nğŸ”§ 1ï¸âƒ£ ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸: ì‹¤í–‰ë˜ì§€ ì•ŠìŒ\")\n",
    "    \n",
    "    # 2. Azure Document Intelligence íŒŒì´í”„ë¼ì¸ ê²°ê³¼\n",
    "    if azure_di_available:\n",
    "        azure_result = text_result\n",
    "        # Azure DI ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ\n",
    "        azure_text = azure_result.get('full_text', '')\n",
    "        azure_stats = azure_result.get('statistics', {})\n",
    "        \n",
    "        comparison_result['azure_di'] = {\n",
    "            'available': True,\n",
    "            'processing_time': azure_stats.get('processing_time', 0),\n",
    "            'text_length': len(azure_text),\n",
    "            'token_count': azure_stats.get('total_chars', 0),  # ì„ì‹œë¡œ ë¬¸ì ìˆ˜ ì‚¬ìš©\n",
    "            'pages_count': azure_stats.get('pages', 0),\n",
    "            'file_name': globals().get('test_document_path', 'Unknown'),\n",
    "            'method': 'azure_document_intelligence'\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nâ˜ï¸ 2ï¸âƒ£ Azure Document Intelligence íŒŒì´í”„ë¼ì¸:\")\n",
    "        print(f\"   â±ï¸ ì²˜ë¦¬ì‹œê°„: {azure_stats.get('processing_time', 0):.2f}ì´ˆ\")\n",
    "        print(f\"   ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(azure_text):,}ì\")\n",
    "        print(f\"   ğŸ“„ í˜ì´ì§€ ìˆ˜: {azure_stats.get('pages', 0)}ê°œ\")\n",
    "        print(f\"   ğŸ”¢ ë¬¸ì ìˆ˜: {azure_stats.get('total_chars', 0):,}ê°œ\")\n",
    "    else:\n",
    "        comparison_result['azure_di'] = {'available': False}\n",
    "        print(f\"\\nâ˜ï¸ 2ï¸âƒ£ Azure Document Intelligence íŒŒì´í”„ë¼ì¸: ì‹¤í–‰ë˜ì§€ ì•ŠìŒ\")\n",
    "    \n",
    "    # 3. ì„±ëŠ¥ ë¹„êµ (ë‘˜ ë‹¤ ìˆëŠ” ê²½ìš°ë§Œ)\n",
    "    if opensource_available and azure_di_available:\n",
    "        print(f\"\\nâš–ï¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "        \n",
    "        os_time = comparison_result['opensource']['processing_time']\n",
    "        azure_time = comparison_result['azure_di']['processing_time']\n",
    "        \n",
    "        os_text_len = comparison_result['opensource']['text_length']\n",
    "        azure_text_len = comparison_result['azure_di']['text_length']\n",
    "        \n",
    "        # ì²˜ë¦¬ ì‹œê°„ ë¹„êµ\n",
    "        if os_time > 0 and azure_time > 0:\n",
    "            time_ratio = azure_time / os_time\n",
    "            faster_method = \"ì˜¤í”ˆì†ŒìŠ¤\" if os_time < azure_time else \"Azure DI\"\n",
    "            print(f\"   â±ï¸ ì²˜ë¦¬ ì†ë„: {faster_method}ê°€ ë” ë¹ ë¦„\")\n",
    "            print(f\"      - ì˜¤í”ˆì†ŒìŠ¤: {os_time:.2f}ì´ˆ\")\n",
    "            print(f\"      - Azure DI: {azure_time:.2f}ì´ˆ\")\n",
    "            print(f\"      - ë¹„ìœ¨: {time_ratio:.2f}ë°°\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ\n",
    "        if os_text_len > 0 and azure_text_len > 0:\n",
    "            text_ratio = azure_text_len / os_text_len\n",
    "            more_text = \"Azure DI\" if azure_text_len > os_text_len else \"ì˜¤í”ˆì†ŒìŠ¤\"\n",
    "            print(f\"   ğŸ“ ì¶”ì¶œ í…ìŠ¤íŠ¸ëŸ‰: {more_text}ê°€ ë” ë§ìŒ\")\n",
    "            print(f\"      - ì˜¤í”ˆì†ŒìŠ¤: {os_text_len:,}ì\")\n",
    "            print(f\"      - Azure DI: {azure_text_len:,}ì\")\n",
    "            print(f\"      - ë¹„ìœ¨: {text_ratio:.2f}ë°°\")\n",
    "        \n",
    "        comparison_result['comparison'] = {\n",
    "            'both_available': True,\n",
    "            'time_ratio_azure_vs_os': azure_time / os_time if os_time > 0 else None,\n",
    "            'text_ratio_azure_vs_os': azure_text_len / os_text_len if os_text_len > 0 else None,\n",
    "            'faster_method': faster_method if 'faster_method' in locals() else None,\n",
    "            'more_text_method': more_text if 'more_text' in locals() else None\n",
    "        }\n",
    "    \n",
    "    # 4. ê¶Œì¥ì‚¬í•­\n",
    "    print(f\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\n",
    "    if opensource_available and azure_di_available:\n",
    "        if comparison_result['comparison'].get('time_ratio_azure_vs_os', 1) > 2:\n",
    "            print(f\"   âš¡ ë¹ ë¥¸ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°: ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ ê¶Œì¥\")\n",
    "        if comparison_result['comparison'].get('text_ratio_azure_vs_os', 1) > 1.2:\n",
    "            print(f\"   ğŸ“Š ì •í™•í•œ ì¶”ì¶œì´ í•„ìš”í•œ ê²½ìš°: Azure DI íŒŒì´í”„ë¼ì¸ ê¶Œì¥\")\n",
    "        print(f\"   ğŸ’° ë¹„ìš© ê³ ë ¤ì‚¬í•­: ì˜¤í”ˆì†ŒìŠ¤ëŠ” ë¬´ë£Œ, Azure DIëŠ” ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆ\")\n",
    "        print(f\"   ğŸ”§ ë³µì¡í•œ ë¬¸ì„œ êµ¬ì¡°: Azure DIê°€ í…Œì´ë¸”, OCR ë“±ì—ì„œ ìš°ìˆ˜\")\n",
    "    else:\n",
    "        print(f\"   ğŸ“‹ ë‘ íŒŒì´í”„ë¼ì¸ì„ ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ ì •í™•í•œ ë¹„êµë¥¼ ì§„í–‰í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    # 5. ê²°ê³¼ ì €ì¥\n",
    "    comparison_file = os.path.join(OUTPUT_TEXTS_DIR, \"pipeline_comparison_result.json\")\n",
    "    with open(comparison_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(comparison_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_file}\")\n",
    "    \n",
    "    # ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥\n",
    "    globals()['pipeline_comparison'] = comparison_result\n",
    "    \n",
    "    return comparison_result\n",
    "\n",
    "# í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_complete_pipeline_comparison(file_path):\n",
    "    \"\"\"ë‘ íŒŒì´í”„ë¼ì¸ì„ ëª¨ë‘ ì‹¤í–‰í•˜ê³  ë¹„êµ\"\"\"\n",
    "    print(\"ğŸš€ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì‹¤í–‰\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "    print(\"1ï¸âƒ£ ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰...\")\n",
    "    os_result = run_opensource_pipeline(file_path)\n",
    "    \n",
    "    if os_result:\n",
    "        # ë²¡í„°ìŠ¤í† ì–´ ì—°ë™\n",
    "        print(\"ğŸ—ƒï¸ ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„°ìŠ¤í† ì–´ ì—°ë™...\")\n",
    "        run_vectorstore_pipeline(os_result, 'opensource')\n",
    "    \n",
    "    # 2. Azure DI íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì´ë¯¸ ì‹¤í–‰ëœ ê²½ìš° ìŠ¤í‚µ)\n",
    "    if 'azure_analysis_result' not in globals():\n",
    "        print(\"2ï¸âƒ£ Azure DI íŒŒì´í”„ë¼ì¸ ì‹¤í–‰...\")\n",
    "        # Azure DI íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)\n",
    "        if 'run_complete_test' in globals():\n",
    "            run_complete_test()\n",
    "    \n",
    "    # 3. ë¹„êµ ë¶„ì„\n",
    "    print(\"3ï¸âƒ£ íŒŒì´í”„ë¼ì¸ ë¹„êµ ë¶„ì„...\")\n",
    "    comparison = compare_pipelines()\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# ë¹„êµ ë¶„ì„ ì‹¤í–‰\n",
    "print(\"ğŸ“Š íŒŒì´í”„ë¼ì¸ ë¹„êµ ë¶„ì„ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"ğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "print(\"   - compare_pipelines() : ê¸°ì¡´ ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"   - run_complete_pipeline_comparison(file_path) : ì „ì²´ ì‹¤í–‰ í›„ ë¹„êµ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f094c",
   "metadata": {},
   "source": [
    "## ğŸ¯ ìµœì¢… ì‹¤í–‰: íŒŒì´í”„ë¼ì¸ ë¹„êµ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ íŒŒì´í”„ë¼ì¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ìµœì¢… ì‹¤í–‰\n",
    "\n",
    "print(\"ğŸ¯ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ìµœì¢… ì‹¤í–‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ì‹¤í–‰ ì˜µì…˜ ì„¤ì •\n",
    "run_full_comparison = True  # True: ì „ì²´ ë¹„êµ ì‹¤í–‰, False: ê¸°ì¡´ ê²°ê³¼ë§Œ ë¹„êµ\n",
    "\n",
    "if run_full_comparison:\n",
    "    print(\"ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì‹¤í–‰ ëª¨ë“œ\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ í™•ì¸\n",
    "    if 'test_document_path' in globals() and test_document_path and os.path.exists(test_document_path):\n",
    "        print(f\"ğŸ“‚ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {os.path.basename(test_document_path)}\")\n",
    "        \n",
    "        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì‹¤í–‰\n",
    "        comparison_result = run_complete_pipeline_comparison(test_document_path)\n",
    "        \n",
    "        if comparison_result:\n",
    "            print(f\"\\nğŸ‰ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì™„ë£Œ!\")\n",
    "            \n",
    "            # ìƒì„¸ ê²°ê³¼ ì¶œë ¥\n",
    "            if comparison_result.get('comparison', {}).get('both_available'):\n",
    "                comp = comparison_result['comparison']\n",
    "                print(f\"\\nğŸ“ˆ ìƒì„¸ ë¹„êµ ê²°ê³¼:\")\n",
    "                \n",
    "                if comp.get('time_ratio_azure_vs_os'):\n",
    "                    ratio = comp['time_ratio_azure_vs_os']\n",
    "                    if ratio < 1:\n",
    "                        print(f\"   âš¡ Azure DIê°€ {1/ratio:.1f}ë°° ë” ë¹ ë¦„\")\n",
    "                    else:\n",
    "                        print(f\"   âš¡ ì˜¤í”ˆì†ŒìŠ¤ê°€ {ratio:.1f}ë°° ë” ë¹ ë¦„\")\n",
    "                \n",
    "                if comp.get('text_ratio_azure_vs_os'):\n",
    "                    ratio = comp['text_ratio_azure_vs_os']\n",
    "                    if ratio > 1:\n",
    "                        print(f\"   ğŸ“ Azure DIê°€ {ratio:.1f}ë°° ë” ë§ì€ í…ìŠ¤íŠ¸ ì¶”ì¶œ\")\n",
    "                    else:\n",
    "                        print(f\"   ğŸ“ ì˜¤í”ˆì†ŒìŠ¤ê°€ {1/ratio:.1f}ë°° ë” ë§ì€ í…ìŠ¤íŠ¸ ì¶”ì¶œ\")\n",
    "                \n",
    "                # ë²¡í„°ìŠ¤í† ì–´ ì—°ë™ ê²°ê³¼\n",
    "                if 'opensource_vectorstore_result' in globals():\n",
    "                    vs_result = opensource_vectorstore_result\n",
    "                    print(f\"   ğŸ—ƒï¸ ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„°ìŠ¤í† ì–´: {vs_result['vectorstore_documents_count']}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ\")\n",
    "                \n",
    "            # ê¶Œì¥ì‚¬í•­ ì¶œë ¥\n",
    "            print(f\"\\nğŸ’¡ ìµœì¢… ê¶Œì¥ì‚¬í•­:\")\n",
    "            print(f\"   ğŸ“‹ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¬¸ì„œ â†’ ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ (ë¹ ë¥´ê³  ê²½ì œì )\")\n",
    "            print(f\"   ğŸ“Š ë³µì¡í•œ êµ¬ì¡° ë¬¸ì„œ â†’ Azure DI íŒŒì´í”„ë¼ì¸ (ì •í™•í•˜ê³  í¬ê´„ì )\")\n",
    "            print(f\"   ğŸ’° ë¹„ìš© ë¯¼ê°ì„± â†’ ì˜¤í”ˆì†ŒìŠ¤ ìš°ì„ , ì •í™•ë„ í•„ìš”ì‹œ Azure DI\")\n",
    "            print(f\"   ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ â†’ ë¬¸ì„œ ìœ í˜•ë³„ íŒŒì´í”„ë¼ì¸ ì„ íƒì  ì ìš©\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì‹¤í–‰ ì‹¤íŒ¨\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   ë¨¼ì € í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì„ íƒí•˜ê³  ê¸°ë³¸ ì„¤ì •ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "else:\n",
    "    print(\"ğŸ“Š ê¸°ì¡´ ê²°ê³¼ ë¹„êµ ëª¨ë“œ\")\n",
    "    \n",
    "    # ê¸°ì¡´ ê²°ê³¼ë§Œ ë¹„êµ\n",
    "    comparison_result = compare_pipelines()\n",
    "    \n",
    "    if comparison_result:\n",
    "        print(f\"\\nâœ… ê¸°ì¡´ ê²°ê³¼ ë¹„êµ ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜ ì•ˆë‚´\n",
    "print(f\"\\nğŸ“ ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜:\")\n",
    "print(f\"   ğŸ”§ ì˜¤í”ˆì†ŒìŠ¤ ê²°ê³¼: {OPENSOURCE_OUTPUT_DIR}\")\n",
    "print(f\"   â˜ï¸ Azure DI ê²°ê³¼: {OUTPUT_TEXTS_DIR}\")\n",
    "print(f\"   ğŸ“Š ë¹„êµ ê²°ê³¼: {OUTPUT_TEXTS_DIR}/pipeline_comparison_result.json\")\n",
    "\n",
    "print(f\"\\nğŸ¯ íŒŒì´í”„ë¼ì¸ ë¹„êµ ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(f\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97b0e8",
   "metadata": {},
   "source": [
    "# ğŸ‰ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¹„êµ ë¶„ì„ ì™„ë£Œ!\n",
    "\n",
    "## ğŸ“‹ êµ¬í˜„ëœ ì£¼ìš” ê¸°ëŠ¥\n",
    "\n",
    "### ğŸ”§ 1. ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "- **PyPDF2**: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "- **python-docx**: Word ë¬¸ì„œ ì²˜ë¦¬\n",
    "- **python-pptx**: PowerPoint ì²˜ë¦¬\n",
    "- **BeautifulSoup**: HTML/XML íŒŒì‹± ë° ì •ì œ\n",
    "- **tiktoken**: í† í° ê³„ì‚° ë° ì²­í‚¹\n",
    "- **ë¬´ë£Œ ì‚¬ìš©**, ë¹ ë¥¸ ì²˜ë¦¬, ì»¤ìŠ¤í„°ë§ˆì´ì§• ìš©ì´\n",
    "\n",
    "### â˜ï¸ 2. Azure Document Intelligence ê¸°ë°˜ íŒŒì´í”„ë¼ì¸\n",
    "- **prebuilt-read**: ê³ ê¸‰ OCR ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "- **prebuilt-layout**: í…Œì´ë¸”, êµ¬ì¡° ì¸ì‹\n",
    "- **ê³ ì •ë°€ ë¶„ì„**, ë³µì¡í•œ ë¬¸ì„œ ì§€ì›\n",
    "- **ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆ**\n",
    "\n",
    "### ğŸ—ƒï¸ 3. ë²¡í„°ìŠ¤í† ì–´ ì—°ë™ ì‹œìŠ¤í…œ\n",
    "- **ë¬¸ì„œ ì²­í‚¹**: ë°±ì—”ë“œì™€ ë™ì¼í•œ ë°©ì‹\n",
    "- **ì„ë² ë”© ìƒì„±**: text-embedding-ada-002 í˜¸í™˜\n",
    "- **ë©”íƒ€ë°ì´í„° ê´€ë¦¬**: íŒŒì¼ ì •ë³´, ì²˜ë¦¬ ì´ë ¥\n",
    "- **Cosmos DB/AI Search í˜¸í™˜**\n",
    "\n",
    "### ğŸ“Š 4. ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n",
    "- **ì²˜ë¦¬ ì‹œê°„ ë¹„êµ**: ì†ë„ ì„±ëŠ¥ ì¸¡ì •\n",
    "- **í…ìŠ¤íŠ¸ ì¶”ì¶œëŸ‰ ë¹„êµ**: ì •í™•ë„ í‰ê°€\n",
    "- **ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„**: ROI ê³„ì‚°\n",
    "- **ê¶Œì¥ì‚¬í•­ ì œì‹œ**: ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœì  ì„ íƒ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ì£¼ìš” ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­\n",
    "\n",
    "### âš¡ ì„±ëŠ¥ íŠ¹ì„±\n",
    "| êµ¬ë¶„ | ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ | Azure DI íŒŒì´í”„ë¼ì¸ |\n",
    "|------|-------------------|-------------------|\n",
    "| **ì²˜ë¦¬ ì†ë„** | ë¹ ë¦„ (ìˆ˜ì´ˆ) | ë³´í†µ (ìˆ˜ì‹­ì´ˆ) |\n",
    "| **ì •í™•ë„** | ë³´í†µ | ë†’ìŒ |\n",
    "| **ë¹„ìš©** | ë¬´ë£Œ | ì‚¬ìš©ëŸ‰ ê¸°ë°˜ |\n",
    "| **OCR ì§€ì›** | ì—†ìŒ | ìš°ìˆ˜ |\n",
    "| **í…Œì´ë¸” ì²˜ë¦¬** | ê¸°ë³¸ì  | ê³ ê¸‰ |\n",
    "| **ì»¤ìŠ¤í„°ë§ˆì´ì§•** | ìš©ì´ | ì œí•œì  |\n",
    "\n",
    "### ğŸ’¡ ì ìš© ì‹œë‚˜ë¦¬ì˜¤\n",
    "1. **ğŸ“„ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¬¸ì„œ** â†’ ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸\n",
    "2. **ğŸ“Š ë³µì¡í•œ êµ¬ì¡°/í…Œì´ë¸”** â†’ Azure DI íŒŒì´í”„ë¼ì¸  \n",
    "3. **ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼** â†’ ë¬¸ì„œ ìœ í˜•ë³„ ì„ íƒì  ì ìš©\n",
    "4. **ğŸ’° ë¹„ìš© ë¯¼ê°ì„±** â†’ ì˜¤í”ˆì†ŒìŠ¤ ìš°ì„ , í•„ìš”ì‹œ Azure DI\n",
    "\n",
    "### ğŸš€ ë°±ì—”ë“œ í†µí•© ë°©ì•ˆ\n",
    "- **í™˜ê²½ ì„¤ì •**: íŒŒì´í”„ë¼ì¸ ì„ íƒ ì˜µì…˜ ì¶”ê°€\n",
    "- **ìë™ ì „í™˜**: ë¬¸ì„œ ë³µì¡ë„ì— ë”°ë¥¸ ìë™ ì„ íƒ\n",
    "- **ë¹„ìš© ê´€ë¦¬**: Azure DI ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§\n",
    "- **í’ˆì§ˆ ê´€ë¦¬**: ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦ ë° fallback\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ë‹¤ìŒ ë‹¨ê³„\n",
    "1. **ì‹¤ì œ ë°±ì—”ë“œ í†µí•©**: í™˜ê²½ë³„ íŒŒì´í”„ë¼ì¸ ì„¤ì •\n",
    "2. **ì„±ëŠ¥ ìµœì í™”**: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ê°œì„ \n",
    "3. **í’ˆì§ˆ ê²€ì¦**: ë‹¤ì–‘í•œ ë¬¸ì„œ ìœ í˜• í…ŒìŠ¤íŠ¸\n",
    "4. **ëª¨ë‹ˆí„°ë§**: ì²˜ë¦¬ ì„±ëŠ¥ ë° ë¹„ìš© ì¶”ì  ì‹œìŠ¤í…œ\n",
    "\n",
    "ì´ì œ ë°±ì—”ë“œì—ì„œ ë¬¸ì„œ ìœ í˜•ê³¼ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ìµœì ì˜ íŒŒì´í”„ë¼ì¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d1af4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== í•µì‹¬ í™•ì¸ì‚¬í•­ ìš”ì•½ =====\n",
      "[PASS] ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ: 9,775ì ì¶”ì¶œ\n",
      "[PASS] í…Œì´ë¸” ì¸ì‹: 11ê°œ í…Œì´ë¸” ê°ì§€\n",
      "[PASS] í˜ì´ì§€ë³„ ê²°ê³¼: 27ê°œ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ (ì´ í˜ì´ì§€=27)\n",
      "\n",
      "- í…ìŠ¤íŠ¸ ì €ì¥ íŒŒì¼:\n",
      "  â€¢ TXT: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_full_text.txt\n",
      "  â€¢ JSON(í†µê³„): /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_statistics.json\n",
      "- êµ¬ì¡°/í…Œì´ë¸” ì €ì¥ íŒŒì¼:\n",
      "  â€¢ êµ¬ì¡° ë³´ê³ ì„œ: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_structure_analysis.txt\n",
      "  â€¢ êµ¬ì¡° ë°ì´í„°(JSON): /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_structure_data.json\n",
      "  â€¢ í…Œì´ë¸” ìš”ì•½ CSV: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_tables_summary.csv\n",
      "  â€¢ í…Œì´ë¸” ë³„ CSV (11ê°œ):\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_1_p11.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_2_p12.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_3_p12.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_4_p13.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_5_p13.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_6_p13.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_7_p14.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_8_p14.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_9_p15.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_10_p16.csv\n",
      "    - /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_table_11_p17.csv\n",
      "- í˜ì´ì§€ ë¶„ì„ ì €ì¥ íŒŒì¼:\n",
      "  â€¢ ìƒì„¸ ë³´ê³ ì„œ: /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_page_analysis.txt\n",
      "  â€¢ ë¶„ì„ ë°ì´í„°(JSON): /home/admin/wkms-aws/jupyter_notebook/data/output_texts/test_20250923_125927_page_analysis_data.json\n",
      "================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… í•µì‹¬ í™•ì¸ì‚¬í•­ PASS/FAIL ìš”ì•½ ì²´í¬\n",
    "\n",
    "def evaluate_core_checks():\n",
    "    \"\"\"ìš”êµ¬ëœ 3ê°€ì§€ í•µì‹¬ í™•ì¸ì‚¬í•­ì— ëŒ€í•´ PASS/FAILì„ ìš”ì•½ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    1) ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ í™•ì¸\n",
    "    2) í˜ì´ì§€ë³„ ê·¸ë¦¼/í…Œì´ë¸” ë“± í…ìŠ¤íŠ¸ ì™¸ ìš”ì†Œ ì¸ì‹ (í…Œì´ë¸” ê¸°ì¤€)\n",
    "    3) í˜ì´ì§€ë³„ ê²°ê³¼ ì²´í¬\n",
    "    \"\"\"\n",
    "    # 1) ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ í™•ì¸\n",
    "    text_ok = False\n",
    "    total_chars = 0\n",
    "    page_count = None\n",
    "    try:\n",
    "        if 'extracted_text' in globals() and isinstance(extracted_text, str):\n",
    "            total_chars = len(extracted_text.strip())\n",
    "            text_ok = total_chars > 20\n",
    "        if not text_ok and 'text_statistics' in globals():\n",
    "            total_chars = int(text_statistics.get('char_count', 0))\n",
    "            text_ok = total_chars > 0\n",
    "        # ì €ì¥ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê²½ë¡œë„ í•¨ê»˜ ê¸°ë¡\n",
    "        text_files = None\n",
    "        if 'text_save_result' in globals() and isinstance(text_save_result, dict):\n",
    "            text_files = text_save_result\n",
    "    except Exception:\n",
    "        text_files = None\n",
    "\n",
    "    # í˜ì´ì§€ ìˆ˜ (ê°€ëŠ¥í•˜ë©´ êµ¬ì¡° ë¶„ì„ì—ì„œ)\n",
    "    try:\n",
    "        if 'structure_analysis_result' in globals() and structure_analysis_result:\n",
    "            page_count = structure_analysis_result['structure_info'].get('pages')\n",
    "    except Exception:\n",
    "        page_count = None\n",
    "\n",
    "    # 2) í…Œì´ë¸” ì¸ì‹ í™•ì¸ (í˜ì´ì§€ë³„ ê·¸ë¦¼/í…Œì´ë¸” ì¤‘ í…Œì´ë¸”ì„ ê¸°ì¤€)\n",
    "    tables_ok = False\n",
    "    tables_count = 0\n",
    "    try:\n",
    "        if 'structure_analysis_result' in globals() and structure_analysis_result:\n",
    "            tables_count = int(structure_analysis_result['structure_info'].get('tables', 0))\n",
    "        if not tables_count and 'document_tables' in globals() and document_tables:\n",
    "            tables_count = len(document_tables)\n",
    "        tables_ok = tables_count > 0\n",
    "        structure_files = None\n",
    "        if 'structure_save_result' in globals() and isinstance(structure_save_result, dict):\n",
    "            structure_files = structure_save_result\n",
    "    except Exception:\n",
    "        structure_files = None\n",
    "\n",
    "    # 3) í˜ì´ì§€ë³„ ê²°ê³¼ ì²´í¬ (page_analysis_data ì¡´ì¬ ë° í˜ì´ì§€ ë ˆë²¨ í†µê³„ ìˆ˜ì§‘ ì—¬ë¶€)\n",
    "    page_ok = False\n",
    "    analyzed_pages = 0\n",
    "    try:\n",
    "        if 'page_analysis_data' in globals() and isinstance(page_analysis_data, list):\n",
    "            analyzed_pages = len(page_analysis_data)\n",
    "            page_ok = analyzed_pages > 0\n",
    "        page_files = None\n",
    "        if 'page_save_result' in globals() and isinstance(page_save_result, dict):\n",
    "            page_files = page_save_result\n",
    "    except Exception:\n",
    "        page_files = None\n",
    "\n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\n===== í•µì‹¬ í™•ì¸ì‚¬í•­ ìš”ì•½ =====\")\n",
    "    if text_ok:\n",
    "        print(f\"[PASS] ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ: {total_chars:,}ì ì¶”ì¶œ\")\n",
    "    else:\n",
    "        print(f\"[FAIL] ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ìŒ (chars={total_chars})\")\n",
    "\n",
    "    if tables_ok:\n",
    "        print(f\"[PASS] í…Œì´ë¸” ì¸ì‹: {tables_count}ê°œ í…Œì´ë¸” ê°ì§€\")\n",
    "    else:\n",
    "        print(\"[WARN] í…Œì´ë¸” ì¸ì‹: ê°ì§€ëœ í…Œì´ë¸” ì—†ìŒ (ë¬¸ì„œê°€ í…Œì´ë¸”ì„ í¬í•¨í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)\")\n",
    "\n",
    "    if page_ok:\n",
    "        if page_count is not None:\n",
    "            print(f\"[PASS] í˜ì´ì§€ë³„ ê²°ê³¼: {analyzed_pages}ê°œ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ (ì´ í˜ì´ì§€={page_count})\")\n",
    "        else:\n",
    "            print(f\"[PASS] í˜ì´ì§€ë³„ ê²°ê³¼: {analyzed_pages}ê°œ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(\"[FAIL] í˜ì´ì§€ë³„ ê²°ê³¼: í˜ì´ì§€ ë‹¨ìœ„ ë¶„ì„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ\")\n",
    "\n",
    "    # ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë‚˜ íŒíŠ¸ ì¶œë ¥ (ê°€ëŠ¥í•œ ë²”ìœ„)\n",
    "    if text_files:\n",
    "        print(\"\\n- í…ìŠ¤íŠ¸ ì €ì¥ íŒŒì¼:\")\n",
    "        if text_files.get('text_file'):\n",
    "            print(f\"  â€¢ TXT: {text_files['text_file']}\")\n",
    "        if text_files.get('stats_file'):\n",
    "            print(f\"  â€¢ JSON(í†µê³„): {text_files['stats_file']}\")\n",
    "\n",
    "    if structure_files:\n",
    "        print(\"- êµ¬ì¡°/í…Œì´ë¸” ì €ì¥ íŒŒì¼:\")\n",
    "        if structure_files.get('report_file'):\n",
    "            print(f\"  â€¢ êµ¬ì¡° ë³´ê³ ì„œ: {structure_files['report_file']}\")\n",
    "        if structure_files.get('data_file'):\n",
    "            print(f\"  â€¢ êµ¬ì¡° ë°ì´í„°(JSON): {structure_files['data_file']}\")\n",
    "        if structure_files.get('tables_summary_csv'):\n",
    "            print(f\"  â€¢ í…Œì´ë¸” ìš”ì•½ CSV: {structure_files['tables_summary_csv']}\")\n",
    "        if structure_files.get('table_csv_files'):\n",
    "            print(f\"  â€¢ í…Œì´ë¸” ë³„ CSV ({len(structure_files['table_csv_files'])}ê°œ):\")\n",
    "            for p in structure_files['table_csv_files']:\n",
    "                print(f\"    - {p}\")\n",
    "\n",
    "    if page_files:\n",
    "        print(\"- í˜ì´ì§€ ë¶„ì„ ì €ì¥ íŒŒì¼:\")\n",
    "        if page_files.get('page_report'):\n",
    "            print(f\"  â€¢ ìƒì„¸ ë³´ê³ ì„œ: {page_files['page_report']}\")\n",
    "        if page_files.get('page_data'):\n",
    "            print(f\"  â€¢ ë¶„ì„ ë°ì´í„°(JSON): {page_files['page_data']}\")\n",
    "\n",
    "    print(\"================================\\n\")\n",
    "\n",
    "# ì§€ê¸ˆ ì‹¤í–‰ ì¤‘ì¸ ì„¸ì…˜ì—ì„œ ë°”ë¡œ í‰ê°€ ì‹¤í–‰ (ì´ë¯¸ ì• ë‹¨ê³„ê°€ ì‹¤í–‰ëœ ìƒíƒœë¼ ê°€ì •)\n",
    "evaluate_core_checks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

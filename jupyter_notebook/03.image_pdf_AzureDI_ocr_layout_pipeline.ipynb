{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6923c037",
   "metadata": {},
   "source": [
    "# ğŸ“„ ì´ë¯¸ì§€ PDF Azure Document Intelligence OCR + ë ˆì´ì•„ì›ƒ(í‘œ) ì¶”ì¶œ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ëª©ì : ì´ë¯¸ì§€ ê¸°ë°˜ PDFë¥¼ Azure Document Intelligence(êµ¬ Form Recognizer)ë¡œ ë¶„ì„í•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ì™€ í‘œë¥¼ ì¶”ì¶œí•˜ê³ , ì½ê¸° ìˆœì„œë¥¼ ì»¬ëŸ¼(ì¢Œâ†’ìš°) ê¸°ì¤€ìœ¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•µì‹¬ ê¸°ëŠ¥:\n",
    "- /home/admin/wkms-aws/backend/.env ì—ì„œ ì—”ë“œí¬ì¸íŠ¸/í‚¤ ë˜ëŠ” AAD ìê²©ì •ë³´ ë¡œë“œ\n",
    "- PDFë¥¼ ì§ì ‘ Azureì— ë¶„ì„ ìš”ì²­(ì´ë¯¸ì§€ ë³€í™˜ ë¶ˆí•„ìš”), í•„ìš” ì‹œ í˜ì´ì§€ ì´ë¯¸ì§€ ì €ì¥ì€ pdf2image ì‚¬ìš©\n",
    "- N-ì»¬ëŸ¼(2ì—´ í¬í•¨) ìë™ ê°ì§€/ì •ë ¬ë¡œ ì¢Œâ†’ìš° ì»¬ëŸ¼ ìˆœì„œ ë³‘í•©\n",
    "- ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼, í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€, í‘œ í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°(JSON) ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3165fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ í™•ì¸:\n",
      "âœ” azure.ai.documentintelligence ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” azure.ai.formrecognizer ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” azure.core.credentials ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ– pdf2image ë¶ˆê°€: No module named 'pdf2image'\n",
      "âœ” PIL ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” dotenv ì‚¬ìš© ê°€ëŠ¥\n",
      "âš  ì„¤ì¹˜ í•„ìš”: pdf2image\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì˜ì¡´ì„± ì ê²€\n",
    "import importlib\n",
    "\n",
    "def check_lib(name):\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "        print(f'âœ” {name} ì‚¬ìš© ê°€ëŠ¥', flush=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'âœ– {name} ë¶ˆê°€: {e}', flush=True)\n",
    "        return False\n",
    "\n",
    "needed = [\n",
    "    'azure.ai.documentintelligence',  # ìµœì‹  SDK (ìˆìœ¼ë©´ ìš°ì„ )\n",
    "    'azure.ai.formrecognizer',        # êµ¬ SDK (fallback)\n",
    "    'azure.core.credentials',\n",
    "    'pdf2image',                      # ì„ íƒ: í˜ì´ì§€ ì´ë¯¸ì§€ ì €ì¥\n",
    "    'PIL',\n",
    "    'dotenv'\n",
    "]\n",
    "print('ğŸ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ í™•ì¸:', flush=True)\n",
    "status = {n: check_lib(n) for n in needed}\n",
    "missing = [n for n, ok in status.items() if not ok]\n",
    "if missing:\n",
    "    print(f'âš  ì„¤ì¹˜ í•„ìš”: {\", \".join(missing)}', flush=True)\n",
    "else:\n",
    "    print('âœ… ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ë¨', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7b0529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… .env ë¡œë“œ: /home/wjadmin/Dev/InsightBridge/backend/.env\n",
      "ğŸ“‚ ì¶œë ¥ ê²½ë¡œ ì„¤ì • ì™„ë£Œ:\n",
      " - í”„ë¡œì íŠ¸ ë£¨íŠ¸: /home/wjadmin/Dev/InsightBridge\n",
      " - ë…¸íŠ¸ë¶ ê²½ë¡œ: /home/wjadmin/Dev/InsightBridge/jupyter_notebook\n",
      " - ì…ë ¥ PDF: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/input_docs/test.pdf (exists=True)\n",
      " - í˜ì´ì§€ í…ìŠ¤íŠ¸: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/azure_di_output/extracted_texts\n",
      " - í˜ì´ì§€ ì´ë¯¸ì§€: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/azure_di_output/extracted_texts/page_images\n",
      " - í‘œ ì¶œë ¥: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/azure_di_output/extracted_texts/page_tables\n",
      "ğŸ” Azure ì„¤ì •:\n",
      " - SDK ì„ íƒ: formrecognizer\n",
      " - ENDPOINT ì œê³µ ì—¬ë¶€: True\n",
      " - KEY: ì„¤ì •ë¨ (3v1uuPY5********Edm3)\n",
      "ğŸ§­ ì»¬ëŸ¼/ëª¨ë¸ ì„¤ì •:\n",
      " - model_read=prebuilt-read, model_layout=prebuilt-layout\n",
      "ğŸ“‚ ì¶œë ¥ ê²½ë¡œ ì„¤ì • ì™„ë£Œ:\n",
      " - í”„ë¡œì íŠ¸ ë£¨íŠ¸: /home/wjadmin/Dev/InsightBridge\n",
      " - ë…¸íŠ¸ë¶ ê²½ë¡œ: /home/wjadmin/Dev/InsightBridge/jupyter_notebook\n",
      " - ì…ë ¥ PDF: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/input_docs/test.pdf (exists=True)\n",
      " - í˜ì´ì§€ í…ìŠ¤íŠ¸: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/azure_di_output/extracted_texts\n",
      " - í˜ì´ì§€ ì´ë¯¸ì§€: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/azure_di_output/extracted_texts/page_images\n",
      " - í‘œ ì¶œë ¥: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/azure_di_output/extracted_texts/page_tables\n",
      "ğŸ” Azure ì„¤ì •:\n",
      " - SDK ì„ íƒ: formrecognizer\n",
      " - ENDPOINT ì œê³µ ì—¬ë¶€: True\n",
      " - KEY: ì„¤ì •ë¨ (3v1uuPY5********Edm3)\n",
      "ğŸ§­ ì»¬ëŸ¼/ëª¨ë¸ ì„¤ì •:\n",
      " - model_read=prebuilt-read, model_layout=prebuilt-layout\n",
      " - mode=auto, max_cols=3, min_lines/col=3\n",
      " - confidence=0.8, max_pages=150\n",
      "â„¹ ì°¸ê³ : ì¼ë¶€ ì§€ì—­/ì‹ ê·œ ë¦¬ì†ŒìŠ¤ì—ì„œëŠ” prebuilt-read ëŒ€ì‹  prebuilt-document ê¶Œì¥\n",
      " - mode=auto, max_cols=3, min_lines/col=3\n",
      " - confidence=0.8, max_pages=150\n",
      "â„¹ ì°¸ê³ : ì¼ë¶€ ì§€ì—­/ì‹ ê·œ ë¦¬ì†ŒìŠ¤ì—ì„œëŠ” prebuilt-read ëŒ€ì‹  prebuilt-document ê¶Œì¥\n"
     ]
    }
   ],
   "source": [
    "# âš™ï¸ ê²½ë¡œ/ì„¤ì • ë° Azure DI ì´ˆê¸°í™” (.env ë¡œë“œ)\n",
    "import os, io, json, time, sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# í”„ë¡œì íŠ¸/ë…¸íŠ¸ë¶ ê²½ë¡œ ë™ì  íƒìƒ‰ (í•˜ë“œì½”ë”© ì œê±°)\n",
    "# ------------------------------------------------------------------\n",
    "THIS_NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# ìµœìƒìœ„ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ì • (backend ë””ë ‰í„°ë¦¬ê°€ ìˆëŠ” ìƒìœ„)\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    cur = start\n",
    "    for _ in range(10):  # ìµœëŒ€ 10 ë‹¨ê³„ ìƒìœ„ íƒìƒ‰\n",
    "        if (cur / 'backend').is_dir():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start  # ì‹¤íŒ¨ ì‹œ í˜„ì¬ ë””ë ‰í„°ë¦¬\n",
    "\n",
    "PROJECT_ROOT = find_project_root(THIS_NOTEBOOK_DIR)\n",
    "JUPYTER_DIR = PROJECT_ROOT / 'jupyter_notebook'\n",
    "DATA_DIR = JUPYTER_DIR / 'data'\n",
    "INPUT_PDF = DATA_DIR / 'input_docs' / 'test.pdf'   # ìš”ì²­ëœ í…ŒìŠ¤íŠ¸ íŒŒì¼\n",
    "OUTPUT_DIR = JUPYTER_DIR / 'data' / 'azure_di_output' / 'extracted_texts'\n",
    "ENV_PATH_CANDIDATES = [\n",
    "    PROJECT_ROOT / 'backend' / '.env',                  # ë°±ì—”ë“œ .env (ìš°ì„ )\n",
    "    PROJECT_ROOT / '.env',                              # ë£¨íŠ¸ .env\n",
    "]\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡°\n",
    "@dataclass\n",
    "class Paths:\n",
    "    output_root: Path = OUTPUT_DIR\n",
    "    page_texts: Path = field(default_factory=lambda: OUTPUT_DIR)\n",
    "    page_images: Path = field(default_factory=lambda: OUTPUT_DIR / 'page_images')\n",
    "    page_tables: Path = field(default_factory=lambda: OUTPUT_DIR / 'page_tables')\n",
    "\n",
    "paths = Paths()\n",
    "for d in [paths.output_root, paths.page_images, paths.page_tables]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# .env ë¡œë“œ (ì²« ë²ˆì§¸ ì¡´ì¬í•˜ëŠ” ê²½ë¡œ)\n",
    "# ------------------------------------------------------------------\n",
    "loaded_env = None\n",
    "for p in ENV_PATH_CANDIDATES:\n",
    "    if p.exists():\n",
    "        try:\n",
    "            load_dotenv(p, override=False)\n",
    "            loaded_env = p\n",
    "            print(f'âœ… .env ë¡œë“œ: {p}', flush=True)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'âš  .env ë¡œë“œ ì‹¤íŒ¨({p}): {e}', flush=True)\n",
    "if not loaded_env:\n",
    "    print('âš  ë¡œë“œ ê°€ëŠ¥í•œ .env íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©).', flush=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# í™˜ê²½ ë³€ìˆ˜ ì •ê·œí™” & í˜¸í™˜ í‚¤ ë§¤í•‘\n",
    "# ------------------------------------------------------------------\n",
    "ENV = os.environ\n",
    "\n",
    "def first_nonempty(*keys: str) -> Optional[str]:\n",
    "    for k in keys:\n",
    "        v = ENV.get(k)\n",
    "        if v:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = first_nonempty(\n",
    "    'AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT',\n",
    "    'AZURE_DOCUMENTINTELLIGENCE_ENDPOINT',\n",
    "    'AZURE_FORMRECOGNIZER_ENDPOINT'\n",
    ")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_KEY = first_nonempty(\n",
    "    'AZURE_DOCUMENT_INTELLIGENCE_API_KEY',\n",
    "    'AZURE_DOCUMENTINTELLIGENCE_API_KEY',\n",
    "    'AZURE_FORMRECOGNIZER_API_KEY'\n",
    ")\n",
    "\n",
    "# ëª¨ë¸: prebuilt-document â†’ prebuilt-read â†’ prebuilt-layout ìˆœ í´ë°±\n",
    "AZURE_DOC_MODEL = first_nonempty('AZURE_DOCUMENT_INTELLIGENCE_DEFAULT_MODEL') or 'prebuilt-document'\n",
    "if AZURE_DOC_MODEL not in {'prebuilt-document', 'prebuilt-read', 'prebuilt-layout'}:\n",
    "    AZURE_DOC_MODEL = 'prebuilt-document'\n",
    "\n",
    "CONFIDENCE_THRESHOLD = float(ENV.get('AZURE_DOCUMENT_INTELLIGENCE_CONFIDENCE_THRESHOLD', '0.75'))\n",
    "MAX_PAGES = int(ENV.get('AZURE_DOCUMENT_INTELLIGENCE_MAX_ASYNC_PAGES', ENV.get('AZURE_DOCUMENT_INTELLIGENCE_MAX_PAGES', '50')))\n",
    "\n",
    "AZURE_TENANT_ID = ENV.get('AZURE_TENANT_ID')\n",
    "AZURE_CLIENT_ID = ENV.get('AZURE_CLIENT_ID')\n",
    "AZURE_CLIENT_SECRET = ENV.get('AZURE_CLIENT_SECRET')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SDK ì„í¬íŠ¸: formrecognizer ìš°ì„ , ì‹¤íŒ¨ ì‹œ documentintelligence\n",
    "# ------------------------------------------------------------------\n",
    "_Client = None\n",
    "AzureKeyCredential = None\n",
    "client = None\n",
    "preferred_sdk = 'none'\n",
    "\n",
    "try:\n",
    "    from azure.ai.formrecognizer import DocumentAnalysisClient as _Client  # type: ignore\n",
    "    from azure.core.credentials import AzureKeyCredential  # type: ignore\n",
    "    preferred_sdk = 'formrecognizer'\n",
    "except Exception:\n",
    "    try:\n",
    "        from azure.ai.documentintelligence import DocumentIntelligenceClient as _Client  # type: ignore\n",
    "        from azure.core.credentials import AzureKeyCredential  # type: ignore\n",
    "        preferred_sdk = 'documentintelligence'\n",
    "    except Exception:\n",
    "        _Client = None\n",
    "        preferred_sdk = 'none'\n",
    "\n",
    "if _Client is None:\n",
    "    raise RuntimeError('Azure Document Intelligence/Form Recognizer SDKê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install azure-ai-formrecognizer')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# í´ë¼ì´ì–¸íŠ¸ ìƒì„± (Key ìš°ì„ , ì—†ìœ¼ë©´ AAD) + ì—”ë“œí¬ì¸íŠ¸ ì •ë¦¬\n",
    "# ------------------------------------------------------------------\n",
    "if AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT:\n",
    "    AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT.rstrip('/')\n",
    "\n",
    "if AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT and AZURE_DOCUMENT_INTELLIGENCE_KEY:\n",
    "    client = _Client(\n",
    "        endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT,\n",
    "        credential=AzureKeyCredential(AZURE_DOCUMENT_INTELLIGENCE_KEY)\n",
    "    )\n",
    "elif (AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT and AZURE_TENANT_ID and AZURE_CLIENT_ID and AZURE_CLIENT_SECRET):\n",
    "    try:\n",
    "        from azure.identity import ClientSecretCredential  # type: ignore\n",
    "        cred = ClientSecretCredential(\n",
    "            tenant_id=AZURE_TENANT_ID,\n",
    "            client_id=AZURE_CLIENT_ID,\n",
    "            client_secret=AZURE_CLIENT_SECRET\n",
    "        )\n",
    "        client = _Client(endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT, credential=cred)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'AAD ìê²©ì¦ëª… ìƒì„± ì‹¤íŒ¨: {e}')\n",
    "else:\n",
    "    raise RuntimeError('ì—”ë“œí¬ì¸íŠ¸/ìê²©ì¦ëª… ì„¤ì •ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. AZURE_* í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# íŒŒì´í”„ë¼ì¸ ì„¤ì • êµ¬ì¡°ì²´\n",
    "# ------------------------------------------------------------------\n",
    "@dataclass\n",
    "class DIConfig:\n",
    "    model_read: str = AZURE_DOC_MODEL  # ê¸°ë³¸ ëª¨ë¸ (prebuilt-document ê¶Œì¥)\n",
    "    model_layout: str = 'prebuilt-layout'\n",
    "    save_page_images: bool = True\n",
    "    ocr_dpi: int = 220  # ì´ë¯¸ì§€ ì €ì¥ ì‹œì—ë§Œ ì‚¬ìš©\n",
    "    use_tables: bool = True\n",
    "    # ì»¬ëŸ¼ ì²˜ë¦¬\n",
    "    column_mode: str = 'auto'   # 'auto' | 'single' | 'two' | 'n'\n",
    "    max_columns: int = 3\n",
    "    min_lines_per_column: int = 3\n",
    "    # ì¶”ê°€ ì„¤ì •\n",
    "    confidence_threshold: float = CONFIDENCE_THRESHOLD\n",
    "    max_pages: int = MAX_PAGES\n",
    "\n",
    "di_cfg = DIConfig()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# ìš”ì•½ ì¶œë ¥ (ì§„ë‹¨ ê°•í™”)\n",
    "# ------------------------------------------------------------------\n",
    "print('ğŸ“‚ ì¶œë ¥ ê²½ë¡œ ì„¤ì • ì™„ë£Œ:', flush=True)\n",
    "print(f' - í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}', flush=True)\n",
    "print(f' - ë…¸íŠ¸ë¶ ê²½ë¡œ: {THIS_NOTEBOOK_DIR}', flush=True)\n",
    "print(f' - ì…ë ¥ PDF: {INPUT_PDF} (exists={INPUT_PDF.exists()})', flush=True)\n",
    "print(f' - í˜ì´ì§€ í…ìŠ¤íŠ¸: {paths.page_texts}', flush=True)\n",
    "print(f' - í˜ì´ì§€ ì´ë¯¸ì§€: {paths.page_images}', flush=True)\n",
    "print(f' - í‘œ ì¶œë ¥: {paths.page_tables}', flush=True)\n",
    "print('ğŸ” Azure ì„¤ì •:', flush=True)\n",
    "print(f' - SDK ì„ íƒ: {preferred_sdk}', flush=True)\n",
    "print(f' - ENDPOINT ì œê³µ ì—¬ë¶€: {bool(AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT)}', flush=True)\n",
    "if AZURE_DOCUMENT_INTELLIGENCE_KEY:\n",
    "    masked = AZURE_DOCUMENT_INTELLIGENCE_KEY[:8] + '*'*8 + AZURE_DOCUMENT_INTELLIGENCE_KEY[-4:]\n",
    "    print(f' - KEY: ì„¤ì •ë¨ ({masked})', flush=True)\n",
    "else:\n",
    "    print(' - KEY: ë¯¸ì„¤ì • (AAD ë˜ëŠ” ì‹¤íŒ¨)', flush=True)\n",
    "print('ğŸ§­ ì»¬ëŸ¼/ëª¨ë¸ ì„¤ì •:', flush=True)\n",
    "print(f' - model_read={di_cfg.model_read}, model_layout={di_cfg.model_layout}', flush=True)\n",
    "print(f' - mode={di_cfg.column_mode}, max_cols={di_cfg.max_columns}, min_lines/col={di_cfg.min_lines_per_column}', flush=True)\n",
    "print(f' - confidence={di_cfg.confidence_threshold}, max_pages={di_cfg.max_pages}', flush=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# ì‚¬ì „ ê²½ê³ : í”í•œ 404 ì ê²€\n",
    "# ------------------------------------------------------------------\n",
    "if preferred_sdk == 'formrecognizer' and di_cfg.model_read == 'prebuilt-read':\n",
    "    print('â„¹ ì°¸ê³ : ì¼ë¶€ ì§€ì—­/ì‹ ê·œ ë¦¬ì†ŒìŠ¤ì—ì„œëŠ” prebuilt-read ëŒ€ì‹  prebuilt-document ê¶Œì¥', flush=True)\n",
    "if preferred_sdk == 'formrecognizer' and di_cfg.model_read == 'prebuilt-document':\n",
    "    print('âœ… prebuilt-document ëª¨ë¸ ì‚¬ìš© (ê¶Œì¥)', flush=True)\n",
    "\n",
    "# NOTE: ì´í›„ ì…€ì—ì„œ client / di_cfg / paths / INPUT_PDF ì‚¬ìš©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae64c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ìœ í‹¸ë¦¬í‹°\n",
    "def write_text(path: Path, content: str):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def write_json(path: Path, data: Dict[str, Any]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def is_image_pdf(pdf_path: Path, sample_pages: int = 3) -> bool:\n",
    "    try:\n",
    "        import pdfplumber  # type: ignore\n",
    "        total_txt = ''\n",
    "        with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "            for i, page in enumerate(pdf.pages[:sample_pages], 1):\n",
    "                try:\n",
    "                    total_txt += (page.extract_text() or '')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return len((total_txt or '').strip()) < 100\n",
    "    except Exception:\n",
    "        return True  # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ PDF ê°„ì£¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ee8629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§® ì»¬ëŸ¼ ê°ì§€/ì •ë ¬ ìœ í‹¸(1D k-means + ì •ë ¬)\n",
    "from typing import Tuple\n",
    "\n",
    "def _kmeans_1d(values: List[float], k: int, iters: int = 15) -> List[int]:\n",
    "    if k <= 1 or len(values) <= k:\n",
    "        return [0 for _ in values]\n",
    "    sorted_vals = sorted(set(values))\n",
    "    if len(sorted_vals) < k:\n",
    "        k = len(sorted_vals)\n",
    "    centers = [sorted_vals[max(0, min(len(sorted_vals)-1, round((i+0.5)*len(sorted_vals)/k)-1))] for i in range(k)]\n",
    "    assign = [0]*len(values)\n",
    "    for _ in range(iters):\n",
    "        for i, v in enumerate(values):\n",
    "            ci = min(range(k), key=lambda j: abs(v - centers[j]))\n",
    "            assign[i] = ci\n",
    "        new_centers = centers[:]\n",
    "        for j in range(k):\n",
    "            group = [values[i] for i, a in enumerate(assign) if a == j]\n",
    "            if group:\n",
    "                new_centers[j] = sum(group)/len(group)\n",
    "        if all(abs(new_centers[j]-centers[j]) < 1e-4 for j in range(k)):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    return assign\n",
    "\n",
    "def _line_left_top_norm(line, page_w: float, page_h: float) -> Tuple[float, float]:\n",
    "    poly = getattr(line, 'polygon', None) or getattr(line, 'bounding_polygon', None)\n",
    "    if poly:\n",
    "        xs = [getattr(p, 'x', p[0]) for p in poly]\n",
    "        ys = [getattr(p, 'y', p[1]) for p in poly]\n",
    "        left = (min(xs) / (page_w or 1.0))\n",
    "        top = (min(ys) / (page_h or 1.0))\n",
    "    else:\n",
    "        left, top = 0.0, 0.0\n",
    "    return left, top\n",
    "\n",
    "def _split_into_n_columns(lines, page_w: float, page_h: float, max_cols: int, min_lines_per_col: int):\n",
    "    if not lines:\n",
    "        return [lines]\n",
    "    xs = [_line_left_top_norm(l, page_w, page_h)[0] for l in lines]\n",
    "    best_groups = [lines]\n",
    "    for k in range(2, max_cols+1):\n",
    "        assign = _kmeans_1d(xs, k)\n",
    "        groups = [[] for _ in range(k)]\n",
    "        for l, a in zip(lines, assign):\n",
    "            groups[a].append(l)\n",
    "        groups = [g for g in groups if len(g) >= min_lines_per_col]\n",
    "        if len(groups) <= 1:\n",
    "            continue\n",
    "        def left_mean(g):\n",
    "            vals = [_line_left_top_norm(x, page_w, page_h)[0] for x in g]\n",
    "            return sum(vals)/len(vals)\n",
    "        groups.sort(key=left_mean)\n",
    "        for g in groups:\n",
    "            g.sort(key=lambda b: (_line_left_top_norm(b, page_w, page_h)[1], _line_left_top_norm(b, page_w, page_h)[0]))\n",
    "        best_groups = groups\n",
    "    return best_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba687bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¤ Azure DI OCR + í˜ì´ì§€ë³„ ì €ì¥ (ì»¬ëŸ¼ ì •ë ¬ í¬í•¨)\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "def analyze_pdf_with_azure_di(pdf_path: Path, paths: Paths, cfg: DIConfig, client) -> Dict[str, Any]:\n",
    "    # í˜ì´ì§€ ì´ë¯¸ì§€ ì €ì¥(ì„ íƒ)ìš©\n",
    "    page_image_files: List[str] = []\n",
    "    if cfg.save_page_images:\n",
    "        try:\n",
    "            from pdf2image import convert_from_path  # type: ignore\n",
    "            images = convert_from_path(str(pdf_path), dpi=cfg.ocr_dpi)\n",
    "            for idx, img in enumerate(images, 1):\n",
    "                page_img_path = paths.page_images / f'{pdf_path.stem}_page_{idx:03d}.png'\n",
    "                try:\n",
    "                    img.save(page_img_path)\n",
    "                    page_image_files.append(str(page_img_path))\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    start = time.time()\n",
    "    print('ğŸ”„ Azure Read ëª¨ë¸ ë¶„ì„ ìš”ì²­...', flush=True)\n",
    "    poller = client.begin_analyze_document(cfg.model_read, document=data)\n",
    "    result = poller.result()\n",
    "    print('   âœ… Read ë¶„ì„ ì™„ë£Œ', flush=True)\n",
    "\n",
    "    full_text_parts: List[str] = []\n",
    "    page_text_files: List[str] = []\n",
    "    page_meta: List[Dict[str, Any]] = []\n",
    "\n",
    "    # í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¼ì¸ ìˆ˜ì§‘ ë° ì»¬ëŸ¼ ì •ë ¬ í›„ ì €ì¥\n",
    "    pages = getattr(result, 'pages', None) or []\n",
    "    for page in pages:\n",
    "        idx = int(getattr(page, 'page_number', None) or getattr(page, 'page', None) or 0)\n",
    "        page_w = float(getattr(page, 'width', None) or 1.0)\n",
    "        page_h = float(getattr(page, 'height', None) or 1.0)\n",
    "        lines = list(getattr(page, 'lines', None) or [])\n",
    "        # ì»¬ëŸ¼ ê·¸ë£¹ ê²°ì •\n",
    "        if cfg.column_mode == 'single':\n",
    "            groups = [lines]\n",
    "        elif cfg.column_mode == 'two':\n",
    "            groups = _split_into_n_columns(lines, page_w, page_h, max_cols=2, min_lines_per_col=cfg.min_lines_per_column)\n",
    "        elif cfg.column_mode == 'n':\n",
    "            groups = _split_into_n_columns(lines, page_w, page_h, max_cols=cfg.max_columns, min_lines_per_col=cfg.min_lines_per_column)\n",
    "        else:\n",
    "            groups = _split_into_n_columns(lines, page_w, page_h, max_cols=2, min_lines_per_col=cfg.min_lines_per_column)\n",
    "            if len(groups) <= 1 and cfg.max_columns > 2:\n",
    "                groups = _split_into_n_columns(lines, page_w, page_h, max_cols=cfg.max_columns, min_lines_per_col=cfg.min_lines_per_column)\n",
    "            if len(groups) <= 1:\n",
    "                groups = [lines]\n",
    "        # ê·¸ë£¹ ë‚´ ì •ë ¬ ë³´ê°•\n",
    "        for g in groups:\n",
    "            g.sort(key=lambda b: (_line_left_top_norm(b, page_w, page_h)[1], _line_left_top_norm(b, page_w, page_h)[0]))\n",
    "        # í…ìŠ¤íŠ¸ ë³‘í•©(ì¢Œ ì»¬ëŸ¼ ì „ì²´ â†’ ìš° ì»¬ëŸ¼ ì „ì²´)\n",
    "        ordered_text = ['\\n'.join([getattr(b, 'content', getattr(b, 'text', '')).strip() for b in g if getattr(b, 'content', getattr(b, 'text', ''))]).strip() for g in groups if g]\n",
    "        txt = '\\n\\n'.join([t for t in ordered_text if t]).strip()\n",
    "        page_txt_path = paths.page_texts / f'{pdf_path.stem}_page_{idx:03d}.txt'\n",
    "        write_text(page_txt_path, txt)\n",
    "        page_text_files.append(str(page_txt_path))\n",
    "        full_text_parts.append(f'\\n\\n=== í˜ì´ì§€ {idx} ===\\n' + txt)\n",
    "        page_meta.append({\n",
    "            'page': idx,\n",
    "            'text_length': len(txt),\n",
    "            'text_file': str(page_txt_path),\n",
    "            'image_file': page_image_files[idx-1] if 0 < idx <= len(page_image_files) else None,\n",
    "            'columns_detected': len(groups)\n",
    "        })\n",
    "\n",
    "    full_text = '\\n'.join(full_text_parts).strip()\n",
    "    full_txt_path = paths.output_root / f'{pdf_path.stem}_azuredi_full.txt'\n",
    "    write_text(full_txt_path, full_text)\n",
    "\n",
    "    meta: Dict[str, Any] = {\n",
    "        'method': 'azure-di-read',\n",
    "        'pages': len(pages),\n",
    "        'total_time': round(time.time() - start, 3),\n",
    "        'page_details': page_meta,\n",
    "        'output_file': str(full_txt_path),\n",
    "        'page_text_files': page_text_files,\n",
    "        'page_image_files': page_image_files\n",
    "    }\n",
    "    return {'text': full_text, 'metadata': meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d0c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Azure DI í‘œ ì¶”ì¶œ(prebuilt-layout) â†’ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "def extract_tables_with_azure_layout(pdf_path: Path, paths: Paths, client, model_layout: str = 'prebuilt-layout') -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        print('ğŸ“Š Azure Layout ëª¨ë¸ ë¶„ì„ ìš”ì²­(í…Œì´ë¸”)...', flush=True)\n",
    "        poller = client.begin_analyze_document(model_layout, document=data)\n",
    "        res = poller.result()\n",
    "    except Exception as e:\n",
    "        print(f'âš  Azure Layout ë¶„ì„ ì‹¤íŒ¨: {e}', flush=True)\n",
    "        return {'tables': [], 'files': []}\n",
    "\n",
    "    saved: List[str] = []\n",
    "    tables_summary: List[Dict[str, Any]] = []\n",
    "    tables = getattr(res, 'tables', None) or []\n",
    "    for t_i, tbl in enumerate(tables, 1):\n",
    "        # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì •\n",
    "        page_num = None\n",
    "        try:\n",
    "            # ì¼ë¶€ SDKì—ì„  cellsê°€ page_numberë¥¼ ê°€ì§\n",
    "            if getattr(tbl, 'cells', None):\n",
    "                for c in tbl.cells:\n",
    "                    page_num = getattr(c, 'page_number', None) or getattr(c, 'page', None)\n",
    "                    if page_num:\n",
    "                        break\n",
    "        except Exception:\n",
    "            pass\n",
    "        cells = getattr(tbl, 'cells', None) or []\n",
    "        max_row = max([getattr(c, 'row_index', 0) for c in cells] + [0])\n",
    "        max_col = max([getattr(c, 'column_index', 0) for c in cells] + [0])\n",
    "        grid = {r: {} for r in range(1, max_row + 2)}\n",
    "        for c in cells:\n",
    "            r = getattr(c, 'row_index', 0) + 1\n",
    "            col = getattr(c, 'column_index', 0) + 1\n",
    "            content = getattr(c, 'content', getattr(c, 'text', '')) or ''\n",
    "            grid.setdefault(r, {})[col] = content.strip()\n",
    "        lines = []\n",
    "        for r in range(1, max_row + 2):\n",
    "            row_vals = [grid.get(r, {}).get(c, '') for c in range(1, max_col + 2)]\n",
    "            lines.append(' | '.join([v.strip() for v in row_vals]))\n",
    "        header = f\"=== í˜ì´ì§€ {page_num or '?'} í…Œì´ë¸” {t_i} ===\\n\"\n",
    "        content = (header + '\\n'.join(lines)).strip()\n",
    "        out = paths.page_tables / f'{pdf_path.stem}_p{(int(page_num) if page_num else 0):03d}_t{t_i:02d}.txt'\n",
    "        write_text(out, content)\n",
    "        saved.append(str(out))\n",
    "        tables_summary.append({'page': int(page_num or 0), 'table_index': t_i, 'rows': len(lines), 'cols': (max_col + 1)})\n",
    "    return {'tables': tables_summary, 'files': saved}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8890606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°\n",
    "def process_pdf_with_azure_di(pdf_path: Path, paths: Paths, cfg: DIConfig, client) -> Dict[str, Any]:\n",
    "    print(f'ğŸ“¥ ì…ë ¥ íŒŒì¼: {pdf_path}', flush=True)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f'íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}')\n",
    "    img_pdf = is_image_pdf(pdf_path)\n",
    "    print(f'ğŸ§ª ì´ë¯¸ì§€ PDF íŒë³„: {\"ì˜ˆ\" if img_pdf else \"ì•„ë‹ˆì˜¤\"}', flush=True)\n",
    "    print(f'ğŸ§­ ì»¬ëŸ¼ ëª¨ë“œ: {cfg.column_mode}, max_cols={cfg.max_columns}, min_lines/col={cfg.min_lines_per_column}', flush=True)\n",
    "\n",
    "    # 1) Read ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ(+ì»¬ëŸ¼ ì •ë ¬)\n",
    "    res = analyze_pdf_with_azure_di(pdf_path, paths, cfg, client)\n",
    "\n",
    "    # 2) í‘œ ì¶”ì¶œ(ì„ íƒ: layout)\n",
    "    table_info = {'tables': [], 'files': []}\n",
    "    if cfg.use_tables:\n",
    "        table_info = extract_tables_with_azure_layout(pdf_path, paths, client, cfg.model_layout)\n",
    "\n",
    "    # 3) ë©”íƒ€ë°ì´í„° í•©ì„±\n",
    "    meta = res.get('metadata', {})\n",
    "    meta.update({'tables': table_info.get('tables', []), 'table_files': table_info.get('files', [])})\n",
    "    doc_meta_path = paths.output_root / f'{pdf_path.stem}_azuredi_metadata.json'\n",
    "    write_json(doc_meta_path, meta)\n",
    "\n",
    "    print('âœ… ì²˜ë¦¬ ì™„ë£Œ', flush=True)\n",
    "    print(f' - ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: {meta.get(\"output_file\")}', flush=True)\n",
    "    print(f' - í‘œ íŒŒì¼ ìˆ˜: {len(meta.get(\"table_files\", []))}', flush=True)\n",
    "    print(f' - í˜ì´ì§€ í…ìŠ¤íŠ¸ íŒŒì¼ ìˆ˜: {len(meta.get(\"page_text_files\", []))}', flush=True)\n",
    "    print(f' - í˜ì´ì§€ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(meta.get(\"page_image_files\", []))}', flush=True)\n",
    "    return {'text': res.get('text', ''), 'metadata': meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20169423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Azure DI OCR + í‘œ ì¶”ì¶œ ë°ëª¨ ì‹œì‘\n",
      "ì…ë ¥: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ“¥ ì…ë ¥ íŒŒì¼: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/input_docs/test.pdf\n",
      "ì…ë ¥: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ“¥ ì…ë ¥ íŒŒì¼: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ§ª ì´ë¯¸ì§€ PDF íŒë³„: ì˜ˆ\n",
      "ğŸ§­ ì»¬ëŸ¼ ëª¨ë“œ: auto, max_cols=3, min_lines/col=3\n",
      "ğŸ§ª ì´ë¯¸ì§€ PDF íŒë³„: ì˜ˆ\n",
      "ğŸ§­ ì»¬ëŸ¼ ëª¨ë“œ: auto, max_cols=3, min_lines/col=3\n",
      "ğŸ”„ Azure Read ëª¨ë¸ ë¶„ì„ ìš”ì²­...\n",
      "ğŸ”„ Azure Read ëª¨ë¸ ë¶„ì„ ìš”ì²­...\n",
      "   âœ… Read ë¶„ì„ ì™„ë£Œ\n",
      "   âœ… Read ë¶„ì„ ì™„ë£Œ\n",
      "ğŸ“Š Azure Layout ëª¨ë¸ ë¶„ì„ ìš”ì²­(í…Œì´ë¸”)...\n",
      "ğŸ“Š Azure Layout ëª¨ë¸ ë¶„ì„ ìš”ì²­(í…Œì´ë¸”)...\n",
      "âœ… ì²˜ë¦¬ ì™„ë£Œ\n",
      " - ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/azure_di_output/extracted_texts/test_azuredi_full.txt\n",
      " - í‘œ íŒŒì¼ ìˆ˜: 11\n",
      " - í˜ì´ì§€ í…ìŠ¤íŠ¸ íŒŒì¼ ìˆ˜: 27\n",
      " - í˜ì´ì§€ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: 0\n",
      "\n",
      "ğŸ” ì „ì²´ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 500ì):\n",
      "=== í˜ì´ì§€ 1 ===\n",
      "éŸ“åœ‹ITì„œë¹„ìŠ¤å­¸æœƒèªŒ\n",
      "ç¬¬22å· ç¬¬3è™Ÿ\n",
      "6æœˆ, pp.1-27\n",
      "2023å¹´\n",
      "ì–‘ì†ì¡ì´ ë¦¬ë”ì‹­ê³¼ í˜ì‹ ì ì¸ ì—…ë¬´ í–‰ë™:\n",
      "í•œêµ­ ë°˜ë„ì²´ ì‚°ì—…ì˜ ì¦ê±°\n",
      "ë”í˜ í—¨ë¦¬ ì•„ë©”ìš”* Â· ì˜¤í¬ë¦¬ í—¨ë¦¬ *** Â· ìœ¤ì†Œë¼ **** . ê°•ì£¼ì˜ *****\n",
      "Ambidextrous Leadership and Innovative Work Behavior:\n",
      "Evidence from South Korea Semiconductor Industry*\n",
      "Henry Ameyaw Domfeh ** Â· Henry Ofori *** Â· Sora Yoon ***\n",
      "The semiconductor industry is a competitive, complicated and a cyclical sector with a highly dynamic business\n",
      "climate which requires an effective leadership style to operate and succeed. This study explor\n",
      "âœ… ì²˜ë¦¬ ì™„ë£Œ\n",
      " - ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: /home/wjadmin/Dev/InsightBridge/jupyter_notebook/data/azure_di_output/extracted_texts/test_azuredi_full.txt\n",
      " - í‘œ íŒŒì¼ ìˆ˜: 11\n",
      " - í˜ì´ì§€ í…ìŠ¤íŠ¸ íŒŒì¼ ìˆ˜: 27\n",
      " - í˜ì´ì§€ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: 0\n",
      "\n",
      "ğŸ” ì „ì²´ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 500ì):\n",
      "=== í˜ì´ì§€ 1 ===\n",
      "éŸ“åœ‹ITì„œë¹„ìŠ¤å­¸æœƒèªŒ\n",
      "ç¬¬22å· ç¬¬3è™Ÿ\n",
      "6æœˆ, pp.1-27\n",
      "2023å¹´\n",
      "ì–‘ì†ì¡ì´ ë¦¬ë”ì‹­ê³¼ í˜ì‹ ì ì¸ ì—…ë¬´ í–‰ë™:\n",
      "í•œêµ­ ë°˜ë„ì²´ ì‚°ì—…ì˜ ì¦ê±°\n",
      "ë”í˜ í—¨ë¦¬ ì•„ë©”ìš”* Â· ì˜¤í¬ë¦¬ í—¨ë¦¬ *** Â· ìœ¤ì†Œë¼ **** . ê°•ì£¼ì˜ *****\n",
      "Ambidextrous Leadership and Innovative Work Behavior:\n",
      "Evidence from South Korea Semiconductor Industry*\n",
      "Henry Ameyaw Domfeh ** Â· Henry Ofori *** Â· Sora Yoon ***\n",
      "The semiconductor industry is a competitive, complicated and a cyclical sector with a highly dynamic business\n",
      "climate which requires an effective leadership style to operate and succeed. This study explor\n"
     ]
    }
   ],
   "source": [
    "# â–¶ï¸ ì‹¤í–‰ ëŸ¬ë„ˆ: test.pdf\n",
    "pdf_path = INPUT_PDF\n",
    "print('ğŸš€ Azure DI OCR + í‘œ ì¶”ì¶œ ë°ëª¨ ì‹œì‘', flush=True)\n",
    "print(f'ì…ë ¥: {pdf_path}', flush=True)\n",
    "result = process_pdf_with_azure_di(pdf_path, paths, di_cfg, client)\n",
    "\n",
    "# ìƒìœ„ 500ì ë¯¸ë¦¬ë³´ê¸°\n",
    "preview = (result.get('text') or '')[:500]\n",
    "print('\\nğŸ” ì „ì²´ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 500ì):', flush=True)\n",
    "print(preview if preview else '(ë¯¸ë¦¬ë³´ê¸° ì—†ìŒ)', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302eef9b",
   "metadata": {},
   "source": [
    "## ì°¸ê³ \n",
    "- .env í‚¤ ì°¸ì¡°(ìš°ì„ ìˆœìœ„):\n",
    "  - ì—”ë“œí¬ì¸íŠ¸: AZURE_DOCUMENTINTELLIGENCE_ENDPOINT ë˜ëŠ” AZURE_FORMRECOGNIZER_ENDPOINT\n",
    "  - í‚¤: AZURE_DOCUMENTINTELLIGENCE_API_KEY ë˜ëŠ” AZURE_FORMRECOGNIZER_API_KEY\n",
    "  - (ì˜µì…˜) AAD: AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET\n",
    "- AzureëŠ” PDFë¥¼ ë°”ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ OCRì„ ìœ„í•´ ì´ë¯¸ì§€ ë³€í™˜ì´ í•„ìˆ˜ëŠ” ì•„ë‹™ë‹ˆë‹¤. ë³¸ ë…¸íŠ¸ë¶ì€ í˜ì´ì§€ ì´ë¯¸ì§€ë¥¼ ì‹œê° ì ê²€ìš©ìœ¼ë¡œ ì„ íƒ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- ì»¬ëŸ¼ ìë™ ê°ì§€ ë¡œì§ì€ íœ´ë¦¬ìŠ¤í‹±ì…ë‹ˆë‹¤. ë¬¸ì„œ ìœ í˜•ì— ë”°ë¼ DIConfigì˜ column_mode/max_columns/min_lines_per_columnì„ ì¡°ì •í•˜ì„¸ìš”.\n",
    "- ì¶œë ¥ë¬¼:\n",
    "  - ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: <íŒŒì¼ëª…>_azuredi_full.txt\n",
    "  - í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ íŒŒì¼: <íŒŒì¼ëª…>_page_###.txt\n",
    "  - í˜ì´ì§€ ì´ë¯¸ì§€(ì˜µì…˜): page_images/ í´ë” ë‚´ PNG\n",
    "  - í‘œ í…ìŠ¤íŠ¸: page_tables/ í´ë” ë‚´ í…ìŠ¤íŠ¸ íŒŒì¼\n",
    "  - ë¬¸ì„œ ë©”íƒ€ë°ì´í„° JSON: <íŒŒì¼ëª…>_azuredi_metadata.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

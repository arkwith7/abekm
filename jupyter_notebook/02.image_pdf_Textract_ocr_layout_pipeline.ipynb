{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56729aa",
   "metadata": {},
   "source": [
    "# ğŸ“„ ì´ë¯¸ì§€ PDF Textract OCR + ë ˆì´ì•„ì›ƒ(í‘œ) ì¶”ì¶œ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ëª©ì : ì´ë¯¸ì§€ ê¸°ë°˜ PDFë¥¼ í˜ì´ì§€ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•œ ë’¤ AWS Textractë¡œ OCRì„ ìˆ˜í–‰í•˜ê³ , í•„ìš” ì‹œ í‘œ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¬¼ì„ ì €ì¥í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "í•µì‹¬ ê¸°ëŠ¥:\n",
    "- .envì—ì„œ AWS ìê²© ì¦ëª…/ë¦¬ì „ì„ ë¡œë“œí•˜ì—¬ boto3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "- PDFâ†’í˜ì´ì§€ ì´ë¯¸ì§€ ë³€í™˜(pdf2image), Textract DetectDocumentTextë¡œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "- ì„ íƒì ìœ¼ë¡œ Textract AnalyzeDocument(TABLES)ë¡œ í‘œ ì¶”ì¶œ ë° í…ìŠ¤íŠ¸ ì €ì¥\n",
    "- ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼, í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€, í‘œ í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°(JSON) ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751e6505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ í™•ì¸:\n",
      "âœ” boto3 ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” boto3 ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” pdf2image ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” PIL ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” dotenv ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ… ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ë¨\n",
      "âœ” pdf2image ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” PIL ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” dotenv ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ… ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ë¨\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì˜ì¡´ì„± ì ê²€\n",
    "import importlib\n",
    "\n",
    "def check_lib(name):\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "        print(f'âœ” {name} ì‚¬ìš© ê°€ëŠ¥', flush=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'âœ– {name} ë¶ˆê°€: {e}', flush=True)\n",
    "        return False\n",
    "\n",
    "needed = ['boto3', 'pdf2image', 'PIL', 'dotenv']\n",
    "print('ğŸ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ í™•ì¸:', flush=True)\n",
    "status = {n: check_lib(n) for n in needed}\n",
    "missing = [n for n, ok in status.items() if not ok]\n",
    "if missing:\n",
    "    print(f'âš  ì„¤ì¹˜ í•„ìš”: {\", \".join(missing)}', flush=True)\n",
    "else:\n",
    "    print('âœ… ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ë¨', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79417380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ì¶œë ¥ ê²½ë¡œ ì„¤ì • ì™„ë£Œ:\n",
      " - í˜ì´ì§€ í…ìŠ¤íŠ¸: /home/admin/wkms-aws/jupyter_notebook/data/textract_output/extracted_texts\n",
      " - í˜ì´ì§€ ì´ë¯¸ì§€: /home/admin/wkms-aws/jupyter_notebook/data/textract_output/extracted_texts/page_images\n",
      " - í‘œ ì¶œë ¥: /home/admin/wkms-aws/jupyter_notebook/data/textract_output/extracted_texts/page_tables\n",
      "ğŸ” AWS ì„¤ì •:\n",
      " - REGION: ap-northeast-2\n",
      " - ACCESS_KEY ì œê³µ ì—¬ë¶€: True\n",
      "ğŸ§­ ì»¬ëŸ¼ ì„¤ì •:\n",
      " - column_mode=auto, min_gap=0.08, max_columns=3, min_lines/col=3\n",
      " - í˜ì´ì§€ í…ìŠ¤íŠ¸: /home/admin/wkms-aws/jupyter_notebook/data/textract_output/extracted_texts\n",
      " - í˜ì´ì§€ ì´ë¯¸ì§€: /home/admin/wkms-aws/jupyter_notebook/data/textract_output/extracted_texts/page_images\n",
      " - í‘œ ì¶œë ¥: /home/admin/wkms-aws/jupyter_notebook/data/textract_output/extracted_texts/page_tables\n",
      "ğŸ” AWS ì„¤ì •:\n",
      " - REGION: ap-northeast-2\n",
      " - ACCESS_KEY ì œê³µ ì—¬ë¶€: True\n",
      "ğŸ§­ ì»¬ëŸ¼ ì„¤ì •:\n",
      " - column_mode=auto, min_gap=0.08, max_columns=3, min_lines/col=3\n"
     ]
    }
   ],
   "source": [
    "# âš™ï¸ ê²½ë¡œ/ì„¤ì • ë° AWS ì´ˆê¸°í™” (.env ë¡œë“œ)\n",
    "import os, io, json, time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, List\n",
    "from dotenv import dotenv_values\n",
    "import boto3\n",
    "\n",
    "BASE_DIR = Path('/home/admin/wkms-aws/jupyter_notebook')\n",
    "INPUT_PDF = BASE_DIR / 'data/input_docs/test.pdf'  # í•„ìš”ì‹œ ë³€ê²½\n",
    "OUTPUT_DIR = BASE_DIR / 'data/textract_output/extracted_texts'\n",
    "ENV_PATH = Path('/home/admin/wkms-aws/backend/.env')\n",
    "\n",
    "@dataclass\n",
    "class TextractConfig:\n",
    "    aws_region: str = 'ap-northeast-2'\n",
    "    ocr_dpi: int = 220\n",
    "    use_tables: bool = True  # TABLES ë¶„ì„ ì‹œë„\n",
    "    # ë ˆì´ì•„ì›ƒ/ì»¬ëŸ¼ ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •\n",
    "    column_mode: str = 'auto'        # 'auto' | 'single' | 'two' | 'n'\n",
    "    column_split_min_gap: float = 0.08  # ì¢Œìš° ì»¬ëŸ¼ ë¶„í•  ìµœì†Œ ê°­(0~1)\n",
    "    max_columns: int = 3             # auto/n ëª¨ë“œì—ì„œ ìµœëŒ€ ì»¬ëŸ¼ ìˆ˜ íƒìƒ‰\n",
    "    min_lines_per_column: int = 3    # ë„ˆë¬´ ì‘ì€ ì»¬ëŸ¼ì€ ë¬´ì‹œí•˜ê±°ë‚˜ ë³‘í•©\n",
    "\n",
    "@dataclass\n",
    "class Paths:\n",
    "    output_root: Path = OUTPUT_DIR\n",
    "    page_texts: Path = field(default_factory=lambda: OUTPUT_DIR)\n",
    "    page_images: Path = field(default_factory=lambda: OUTPUT_DIR / 'page_images')\n",
    "    page_tables: Path = field(default_factory=lambda: OUTPUT_DIR / 'page_tables')\n",
    "\n",
    "# ì¶œë ¥ ê²½ë¡œ ì¤€ë¹„\n",
    "paths = Paths()\n",
    "for d in [paths.output_root, paths.page_images, paths.page_tables]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# .env ë¡œë“œ ë° AWS ì„¸ì…˜/í´ë¼ì´ì–¸íŠ¸ êµ¬ì„±\n",
    "env = dotenv_values(str(ENV_PATH)) if ENV_PATH.exists() else {}\n",
    "aws_access_key_id = env.get('AWS_ACCESS_KEY_ID') or os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = env.get('AWS_SECRET_ACCESS_KEY') or os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "aws_session_token = env.get('AWS_SESSION_TOKEN') or os.environ.get('AWS_SESSION_TOKEN')\n",
    "aws_region = env.get('AWS_REGION') or os.environ.get('AWS_REGION') or 'ap-northeast-2'\n",
    "\n",
    "tex_cfg = TextractConfig(aws_region=aws_region)\n",
    "\n",
    "if aws_access_key_id and aws_secret_access_key:\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        aws_session_token=aws_session_token,\n",
    "        region_name=aws_region\n",
    "    )\n",
    "else:\n",
    "    # í™˜ê²½ë³€ìˆ˜/í”„ë¡œíŒŒì¼/IMDSë¥¼ í†µí•´ ìê²©ì¦ëª… ìë™ íƒìƒ‰\n",
    "    session = boto3.Session(region_name=aws_region)\n",
    "\n",
    "textract = session.client('textract', region_name=aws_region)\n",
    "\n",
    "print('ğŸ“‚ ì¶œë ¥ ê²½ë¡œ ì„¤ì • ì™„ë£Œ:', flush=True)\n",
    "print(f' - í˜ì´ì§€ í…ìŠ¤íŠ¸: {paths.page_texts}', flush=True)\n",
    "print(f' - í˜ì´ì§€ ì´ë¯¸ì§€: {paths.page_images}', flush=True)\n",
    "print(f' - í‘œ ì¶œë ¥: {paths.page_tables}', flush=True)\n",
    "print('ğŸ” AWS ì„¤ì •:', flush=True)\n",
    "print(f' - REGION: {aws_region}', flush=True)\n",
    "print(f' - ACCESS_KEY ì œê³µ ì—¬ë¶€: {bool(aws_access_key_id)}', flush=True)\n",
    "print('ğŸ§­ ì»¬ëŸ¼ ì„¤ì •:', flush=True)\n",
    "print(f' - column_mode={tex_cfg.column_mode}, min_gap={tex_cfg.column_split_min_gap}, max_columns={tex_cfg.max_columns}, min_lines/col={tex_cfg.min_lines_per_column}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3dabf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ìœ í‹¸ë¦¬í‹°\n",
    "from typing import Tuple\n",
    "\n",
    "def write_text(path: Path, content: str):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def write_json(path: Path, data: Dict[str, Any]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def is_image_pdf(pdf_path: Path, sample_pages: int = 3) -> bool:\n",
    "    \"\"\"ê°„ë‹¨ ê¸°ì¤€: pdfplumberë¡œ ì•ë¶€ë¶„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œ ë§¤ìš° ì ìœ¼ë©´ ì´ë¯¸ì§€ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨\"\"\"\n",
    "    try:\n",
    "        import pdfplumber  # type: ignore\n",
    "        total_txt = ''\n",
    "        with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "            for i, page in enumerate(pdf.pages[:sample_pages], 1):\n",
    "                try:\n",
    "                    total_txt += (page.extract_text() or '')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return len((total_txt or '').strip()) < 100\n",
    "    except Exception:\n",
    "        # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ OCR ëŒ€ìƒìœ¼ë¡œ ê°„ì£¼\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0e236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¤ Textract OCR: PDFâ†’ì´ë¯¸ì§€ ë³€í™˜ í›„ DetectDocumentTextë¡œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Nì»¬ëŸ¼ ìë™ ì •ë ¬ í¬í•¨)\n",
    "\n",
    "def _kmeans_1d(values: List[float], k: int, iters: int = 15) -> List[int]:\n",
    "    \"\"\"ì•„ì£¼ ê°„ë‹¨í•œ 1D k-means êµ¬í˜„: values -> cluster indices. ì•ˆì •ì„±ì„ ìœ„í•´ ì¤‘ì•™ê°’ ê¸°ë°˜ ì´ˆê¸°í™”.\"\"\"\n",
    "    if k <= 1 or len(values) <= k:\n",
    "        return list(range(len(values)))\n",
    "    import math, statistics\n",
    "    sorted_vals = sorted(set(values))\n",
    "    if len(sorted_vals) < k:\n",
    "        k = len(sorted_vals)\n",
    "    # ì´ˆê¸° ì¤‘ì‹¬: quantile ë¶„í• \n",
    "    centers = [sorted_vals[max(0, min(len(sorted_vals)-1, round((i+0.5)*len(sorted_vals)/k)-1))] for i in range(k)]\n",
    "    assign = [0]*len(values)\n",
    "    for _ in range(iters):\n",
    "        # í• ë‹¹\n",
    "        for i, v in enumerate(values):\n",
    "            ci = min(range(k), key=lambda j: abs(v - centers[j]))\n",
    "            assign[i] = ci\n",
    "        # ì¤‘ì‹¬ ì—…ë°ì´íŠ¸\n",
    "        new_centers = centers[:]\n",
    "        for j in range(k):\n",
    "            group = [values[i] for i, a in enumerate(assign) if a == j]\n",
    "            if group:\n",
    "                new_centers[j] = sum(group) / len(group)\n",
    "        if all(abs(new_centers[j]-centers[j]) < 1e-4 for j in range(k)):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    return assign\n",
    "\n",
    "\n",
    "def _split_into_n_columns(lines: List[Dict[str, Any]], max_cols: int, min_lines_per_col: int) -> List[List[Dict[str, Any]]]:\n",
    "    if not lines:\n",
    "        return [lines]\n",
    "    xs = [l.get('Geometry', {}).get('BoundingBox', {}).get('Left', 0.0) for l in lines]\n",
    "    cols = 1\n",
    "    best_groups: List[List[Dict[str, Any]]] = [lines]\n",
    "    for k in range(2, max_cols+1):\n",
    "        assign = _kmeans_1d(xs, k)\n",
    "        groups: List[List[Dict[str, Any]]] = [[] for _ in range(k)]\n",
    "        for l, a in zip(lines, assign):\n",
    "            groups[a].append(l)\n",
    "        groups = [g for g in groups if len(g) >= min_lines_per_col]\n",
    "        if len(groups) <= 1:\n",
    "            continue\n",
    "        # ì»¬ëŸ¼ ì¤‘ì‹¬(Left í‰ê· )ìœ¼ë¡œ ì¢Œ->ìš° ì •ë ¬, ê° ê·¸ë£¹ì€ ìœ„->ì•„ë˜->ì¢Œ->ìš° ì •ë ¬\n",
    "        def left_mean(g):\n",
    "            return sum([x.get('Geometry', {}).get('BoundingBox', {}).get('Left', 0.0) for x in g]) / len(g)\n",
    "        groups.sort(key=left_mean)\n",
    "        for g in groups:\n",
    "            g.sort(key=lambda b: (b.get('Geometry', {}).get('BoundingBox', {}).get('Top', 0.0),\n",
    "                                  b.get('Geometry', {}).get('BoundingBox', {}).get('Left', 0.0)))\n",
    "        best_groups = groups\n",
    "        cols = len(groups)\n",
    "    return best_groups\n",
    "\n",
    "\n",
    "def textract_detect_text_on_images(pdf_path: Path, paths: Paths, cfg: TextractConfig, textract_client) -> Dict[str, Any]:\n",
    "    from pdf2image import convert_from_path  # type: ignore\n",
    "    from PIL import Image  # type: ignore\n",
    "    start = time.time()\n",
    "    print(f'ğŸ”„ PDFâ†’ì´ë¯¸ì§€ ë³€í™˜ ì¤‘ (DPI={cfg.ocr_dpi})...', flush=True)\n",
    "    images = convert_from_path(str(pdf_path), dpi=cfg.ocr_dpi)\n",
    "    print(f'   âœ… ë³€í™˜ ì™„ë£Œ: {len(images)}í˜ì´ì§€', flush=True)\n",
    "\n",
    "    full_text_parts: List[str] = []\n",
    "    page_text_files: List[str] = []\n",
    "    page_image_files: List[str] = []\n",
    "    page_meta: List[Dict[str, Any]] = []\n",
    "\n",
    "    for idx, img in enumerate(images, 1):\n",
    "        p_start = time.time()\n",
    "        print(f'   ğŸ” í˜ì´ì§€ {idx}/{len(images)} Textract DetectDocumentText...', flush=True)\n",
    "        # ì´ë¯¸ì§€ ì €ì¥ ë° ë°”ì´íŠ¸ ì¤€ë¹„\n",
    "        page_img_path = paths.page_images / f'{pdf_path.stem}_page_{idx:03d}.png'\n",
    "        try:\n",
    "            img.save(page_img_path)\n",
    "            page_image_files.append(str(page_img_path))\n",
    "        except Exception:\n",
    "            pass\n",
    "        buf = io.BytesIO()\n",
    "        img.save(buf, format='PNG')\n",
    "        img_bytes = buf.getvalue()\n",
    "\n",
    "        resp = textract_client.detect_document_text(Document={'Bytes': img_bytes})\n",
    "        blocks = resp.get('Blocks', [])\n",
    "        lines = [b for b in blocks if b.get('BlockType') == 'LINE' and 'Text' in b]\n",
    "\n",
    "        # ë ˆì´ì•„ì›ƒ ì •ë ¬\n",
    "        if cfg.column_mode == 'single':\n",
    "            groups = [lines]\n",
    "        elif cfg.column_mode == 'two':\n",
    "            groups = _split_into_n_columns(lines, max_cols=2, min_lines_per_col=cfg.min_lines_per_column)\n",
    "        elif cfg.column_mode == 'n':\n",
    "            groups = _split_into_n_columns(lines, max_cols=cfg.max_columns, min_lines_per_col=cfg.min_lines_per_column)\n",
    "        else:  # 'auto'\n",
    "            # 2ì—´ ìš°ì„  ì‹œë„ â†’ ì¶©ë¶„ì¹˜ ì•Šìœ¼ë©´ nì—´ ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ ë‹¨ì¼\n",
    "            groups = _split_into_n_columns(lines, max_cols=2, min_lines_per_col=cfg.min_lines_per_column)\n",
    "            if len(groups) <= 1 and cfg.max_columns > 2:\n",
    "                groups = _split_into_n_columns(lines, max_cols=cfg.max_columns, min_lines_per_col=cfg.min_lines_per_column)\n",
    "            if len(groups) <= 1:\n",
    "                groups = [lines]\n",
    "        # ê·¸ë£¹ ë‚´ ì •ë ¬ ë³´ê°•\n",
    "        for g in groups:\n",
    "            g.sort(key=lambda b: (b.get('Geometry', {}).get('BoundingBox', {}).get('Top', 0.0),\n",
    "                                  b.get('Geometry', {}).get('BoundingBox', {}).get('Left', 0.0)))\n",
    "        # ì¢Œâ†’ìš° ì»¬ëŸ¼ ìˆœìœ¼ë¡œ ë³‘í•©\n",
    "        ordered_text = ['\\n'.join([b['Text'].strip() for b in g if b.get('Text')]).strip() for g in groups if g]\n",
    "        txt = '\\n\\n'.join([t for t in ordered_text if t]).strip()\n",
    "\n",
    "        page_txt_path = paths.page_texts / f'{pdf_path.stem}_page_{idx:03d}.txt'\n",
    "        write_text(page_txt_path, txt)\n",
    "        page_text_files.append(str(page_txt_path))\n",
    "        full_text_parts.append(f'\\n\\n=== í˜ì´ì§€ {idx} ===\\n' + txt)\n",
    "        page_meta.append({\n",
    "            'page': idx,\n",
    "            'text_length': len(txt),\n",
    "            'processing_time': round(time.time() - p_start, 3),\n",
    "            'text_file': str(page_txt_path),\n",
    "            'image_file': str(page_img_path) if page_image_files else None,\n",
    "            'columns_detected': len(groups)\n",
    "        })\n",
    "\n",
    "    full_text = '\\n'.join(full_text_parts).strip()\n",
    "    full_txt_path = paths.output_root / f'{pdf_path.stem}_textract_full.txt'\n",
    "    write_text(full_txt_path, full_text)\n",
    "\n",
    "    meta = {\n",
    "        'method': 'textract-detect',\n",
    "        'pages': len(images),\n",
    "        'aws_region': cfg.aws_region,\n",
    "        'total_time': round(time.time() - start, 3),\n",
    "        'page_details': page_meta,\n",
    "        'output_file': str(full_txt_path),\n",
    "        'page_text_files': page_text_files,\n",
    "        'page_image_files': page_image_files\n",
    "    }\n",
    "    return {'text': full_text, 'metadata': meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccedac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Textract TABLES: ì €ì¥ëœ í˜ì´ì§€ ì´ë¯¸ì§€ì— ëŒ€í•´ AnalyzeDocument(TABLES)ë¡œ í‘œ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "\n",
    "def textract_analyze_tables_on_images(pdf_path: Path, paths: Paths, textract_client, max_pages: Optional[int] = None) -> Dict[str, Any]:\n",
    "    import base64\n",
    "    saved: List[str] = []\n",
    "    tables_summary: List[Dict[str, Any]] = []\n",
    "\n",
    "    # ì €ì¥ëœ í˜ì´ì§€ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ì„ ìˆœíšŒ\n",
    "    page_imgs = sorted(paths.page_images.glob(f'{pdf_path.stem}_page_*.png'))\n",
    "    if max_pages is not None:\n",
    "        page_imgs = page_imgs[:max_pages]\n",
    "\n",
    "    # í—¬í¼: Block id ë§µ ë° í…ìŠ¤íŠ¸ ë³µì›\n",
    "    def build_block_map(blocks: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "        return {b['Id']: b for b in blocks if 'Id' in b}\n",
    "\n",
    "    def get_text_for_block(block_id: str, block_map: Dict[str, Dict[str, Any]]) -> str:\n",
    "        block = block_map.get(block_id, {})\n",
    "        if not block:\n",
    "            return ''\n",
    "        if block.get('BlockType') in ('WORD', 'SELECTION_ELEMENT') and 'Text' in block:\n",
    "            return block.get('Text', '')\n",
    "        text_parts: List[str] = []\n",
    "        for rel in block.get('Relationships', []) or []:\n",
    "            if rel.get('Type') == 'CHILD':\n",
    "                for cid in rel.get('Ids', []) or []:\n",
    "                    t = get_text_for_block(cid, block_map)\n",
    "                    if t:\n",
    "                        text_parts.append(t)\n",
    "        return ' '.join(text_parts).strip()\n",
    "\n",
    "    for page_idx, img_path in enumerate(page_imgs, 1):\n",
    "        try:\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_bytes = f.read()\n",
    "            print(f'   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ {page_idx}/{len(page_imgs)}...', flush=True)\n",
    "            resp = textract_client.analyze_document(Document={'Bytes': img_bytes}, FeatureTypes=['TABLES'])\n",
    "            blocks = resp.get('Blocks', [])\n",
    "            block_map = build_block_map(blocks)\n",
    "            # í˜ì´ì§€ë³„ TABLE ë¸”ë¡ë§Œ ì„ ë³„\n",
    "            page_tables = [b for b in blocks if b.get('BlockType') == 'TABLE' and b.get('Page') == page_idx]\n",
    "            if not page_tables:\n",
    "                continue\n",
    "            for t_i, tbl in enumerate(page_tables, 1):\n",
    "                # CELL ë¸”ë¡ ìˆ˜ì§‘ í›„ í–‰/ì—´ ì¸ë±ìŠ¤ë¡œ ì •ë ¬\n",
    "                cells = [b for b in blocks if b.get('BlockType') == 'CELL' and b.get('Page') == page_idx]\n",
    "                if not cells:\n",
    "                    continue\n",
    "                max_row = max([c.get('RowIndex', 0) for c in cells]) if cells else 0\n",
    "                max_col = max([c.get('ColumnIndex', 0) for c in cells]) if cells else 0\n",
    "                grid: Dict[int, Dict[int, str]] = {r: {} for r in range(1, max_row + 1)}\n",
    "                for cell in cells:\n",
    "                    r = cell.get('RowIndex', 0)\n",
    "                    c = cell.get('ColumnIndex', 0)\n",
    "                    txt = get_text_for_block(cell.get('Id', ''), block_map)\n",
    "                    if r and c:\n",
    "                        grid.setdefault(r, {})[c] = (txt or '').strip()\n",
    "                lines: List[str] = []\n",
    "                for r in range(1, max_row + 1):\n",
    "                    row_vals = [grid.get(r, {}).get(c, '') for c in range(1, max_col + 1)]\n",
    "                    lines.append(' | '.join([v.strip() for v in row_vals]))\n",
    "                content = (f'=== í˜ì´ì§€ {page_idx} í…Œì´ë¸” {t_i} ===\\n' + '\\n'.join(lines)).strip()\n",
    "                out = paths.page_tables / f'{pdf_path.stem}_p{page_idx:03d}_t{t_i:02d}.txt'\n",
    "                write_text(out, content)\n",
    "                saved.append(str(out))\n",
    "                tables_summary.append({'page': page_idx, 'table_index': t_i, 'rows': len(lines), 'cols': max_col})\n",
    "        except Exception as e:\n",
    "            print(f'âš  í‘œ ë¶„ì„ ì‹¤íŒ¨(page {page_idx}): {e}', flush=True)\n",
    "            continue\n",
    "    return {'tables': tables_summary, 'files': saved}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "046ccf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (ì»¬ëŸ¼ ëª¨ë“œ ì „ë‹¬ í¬í•¨)\n",
    "\n",
    "def process_pdf_with_textract(pdf_path: Path, paths: Paths, cfg: TextractConfig, textract_client) -> Dict[str, Any]:\n",
    "    print(f'ğŸ“¥ ì…ë ¥ íŒŒì¼: {pdf_path}', flush=True)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f'íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pdf_path}')\n",
    "    img_pdf = is_image_pdf(pdf_path)\n",
    "    print(f'ğŸ§ª ì´ë¯¸ì§€ PDF íŒë³„: {\"ì˜ˆ\" if img_pdf else \"ì•„ë‹ˆì˜¤\"}', flush=True)\n",
    "    print(f'ğŸ§­ ì»¬ëŸ¼ ëª¨ë“œ: {cfg.column_mode}, min_gap={cfg.column_split_min_gap}', flush=True)\n",
    "\n",
    "    # 1) Textract OCR\n",
    "    res = textract_detect_text_on_images(pdf_path, paths, cfg, textract_client)\n",
    "\n",
    "    # 2) í‘œ ì¶”ì¶œ(ì„ íƒ)\n",
    "    table_info = {'tables': [], 'files': []}\n",
    "    if cfg.use_tables:\n",
    "        print('ğŸ“Š í‘œ ì¶”ì¶œ ì‹œë„(Textract TABLES)', flush=True)\n",
    "        table_info = textract_analyze_tables_on_images(pdf_path, paths, textract_client)\n",
    "\n",
    "    # 3) ë©”íƒ€ë°ì´í„° í•©ì„±\n",
    "    meta = res.get('metadata', {})\n",
    "    meta.update({\n",
    "        'tables': table_info.get('tables', []),\n",
    "        'table_files': table_info.get('files', [])\n",
    "    })\n",
    "    doc_meta_path = paths.output_root / f'{pdf_path.stem}_textract_metadata.json'\n",
    "    write_json(doc_meta_path, meta)\n",
    "\n",
    "    print('âœ… ì²˜ë¦¬ ì™„ë£Œ', flush=True)\n",
    "    print(f' - ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: {meta.get(\"output_file\")}', flush=True)\n",
    "    print(f' - í‘œ íŒŒì¼ ìˆ˜: {len(meta.get(\"table_files\", []))}', flush=True)\n",
    "    print(f' - í˜ì´ì§€ í…ìŠ¤íŠ¸ íŒŒì¼ ìˆ˜: {len(meta.get(\"page_text_files\", []))}', flush=True)\n",
    "    print(f' - í˜ì´ì§€ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(meta.get(\"page_image_files\", []))}', flush=True)\n",
    "    return {'text': res.get('text', ''), 'metadata': meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa2effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Textract OCR + í‘œ ì¶”ì¶œ ë°ëª¨ ì‹œì‘\n",
      "ì…ë ¥: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ“¥ ì…ë ¥ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ§ª ì´ë¯¸ì§€ PDF íŒë³„: ì˜ˆ\n",
      "ğŸ§­ ì»¬ëŸ¼ ëª¨ë“œ: auto, min_gap=0.08\n",
      "ğŸ”„ PDFâ†’ì´ë¯¸ì§€ ë³€í™˜ ì¤‘ (DPI=220)...\n",
      "ì…ë ¥: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ“¥ ì…ë ¥ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "ğŸ§ª ì´ë¯¸ì§€ PDF íŒë³„: ì˜ˆ\n",
      "ğŸ§­ ì»¬ëŸ¼ ëª¨ë“œ: auto, min_gap=0.08\n",
      "ğŸ”„ PDFâ†’ì´ë¯¸ì§€ ë³€í™˜ ì¤‘ (DPI=220)...\n",
      "   âœ… ë³€í™˜ ì™„ë£Œ: 27í˜ì´ì§€\n",
      "   ğŸ” í˜ì´ì§€ 1/27 Textract DetectDocumentText...\n",
      "   âœ… ë³€í™˜ ì™„ë£Œ: 27í˜ì´ì§€\n",
      "   ğŸ” í˜ì´ì§€ 1/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 2/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 2/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 3/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 3/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 4/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 4/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 5/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 5/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 6/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 6/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 7/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 7/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 8/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 8/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 9/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 9/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 10/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 10/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 11/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 11/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 12/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 12/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 13/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 13/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 14/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 14/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 15/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 15/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 16/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 16/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 17/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 17/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 18/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 18/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 19/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 19/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 20/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 20/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 21/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 21/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 22/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 22/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 23/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 23/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 24/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 24/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 25/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 25/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 26/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 26/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 27/27 Textract DetectDocumentText...\n",
      "   ğŸ” í˜ì´ì§€ 27/27 Textract DetectDocumentText...\n",
      "ğŸ“Š í‘œ ì¶”ì¶œ ì‹œë„(Textract TABLES)\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 1/27...\n",
      "ğŸ“Š í‘œ ì¶”ì¶œ ì‹œë„(Textract TABLES)\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 1/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 2/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 2/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 3/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 3/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 4/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 4/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 5/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 5/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 6/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 6/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 7/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 7/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 8/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 8/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 9/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 9/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 10/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 10/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 11/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 11/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 12/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 12/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 13/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 13/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 14/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 14/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 15/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 15/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 16/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 16/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 17/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 17/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 18/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 18/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 19/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 19/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 20/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 20/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 21/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 21/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 22/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 22/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 23/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 23/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 24/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 24/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 25/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 25/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 26/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 26/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 27/27...\n",
      "   ğŸ“‘ í‘œ ë¶„ì„(AnalyzeDocument-TABLES) í˜ì´ì§€ 27/27...\n",
      "âœ… ì²˜ë¦¬ ì™„ë£Œ\n",
      " - ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/textract_output/extracted_texts/test_textract_full.txt\n",
      " - í‘œ íŒŒì¼ ìˆ˜: 0\n",
      " - í˜ì´ì§€ í…ìŠ¤íŠ¸ íŒŒì¼ ìˆ˜: 27\n",
      " - í˜ì´ì§€ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: 27\n",
      "\n",
      "ğŸ” ì „ì²´ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 500ì):\n",
      "=== í˜ì´ì§€ 1 ===\n",
      "22 3\n",
      "2023# 6 F, pp.1-27\n",
      "of 2/2/21 HHH OHE: 00\n",
      "2 12 I14\n",
      "Ambidextrous Leadership and Innovative Work Behavior:\n",
      "Evidence from South Korea Semiconductor Industry*\n",
      "Henry Ameyaw Domfeh** Henry Ofori*** Sora Yoon**** Juyoung Kang\n",
      "The semiconductor industry is a competitive, complicated and a cyclical sector with a highly dynamic business\n",
      "climate which requires an effective leadership style to operate and succeed. This study explores the important issue\n",
      "of how leadership facilitates employ\n",
      "âœ… ì²˜ë¦¬ ì™„ë£Œ\n",
      " - ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: /home/admin/wkms-aws/jupyter_notebook/data/textract_output/extracted_texts/test_textract_full.txt\n",
      " - í‘œ íŒŒì¼ ìˆ˜: 0\n",
      " - í˜ì´ì§€ í…ìŠ¤íŠ¸ íŒŒì¼ ìˆ˜: 27\n",
      " - í˜ì´ì§€ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: 27\n",
      "\n",
      "ğŸ” ì „ì²´ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 500ì):\n",
      "=== í˜ì´ì§€ 1 ===\n",
      "22 3\n",
      "2023# 6 F, pp.1-27\n",
      "of 2/2/21 HHH OHE: 00\n",
      "2 12 I14\n",
      "Ambidextrous Leadership and Innovative Work Behavior:\n",
      "Evidence from South Korea Semiconductor Industry*\n",
      "Henry Ameyaw Domfeh** Henry Ofori*** Sora Yoon**** Juyoung Kang\n",
      "The semiconductor industry is a competitive, complicated and a cyclical sector with a highly dynamic business\n",
      "climate which requires an effective leadership style to operate and succeed. This study explores the important issue\n",
      "of how leadership facilitates employ\n"
     ]
    }
   ],
   "source": [
    "# â–¶ï¸ ì‹¤í–‰ ëŸ¬ë„ˆ: test.pdf\n",
    "pdf_path = INPUT_PDF\n",
    "print('ğŸš€ Textract OCR + í‘œ ì¶”ì¶œ ë°ëª¨ ì‹œì‘', flush=True)\n",
    "print(f'ì…ë ¥: {pdf_path}', flush=True)\n",
    "result = process_pdf_with_textract(pdf_path, paths, tex_cfg, textract)\n",
    "\n",
    "# ìƒìœ„ 500ì ë¯¸ë¦¬ë³´ê¸°\n",
    "preview = (result.get('text') or '')[:500]\n",
    "print('\\nğŸ” ì „ì²´ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 500ì):', flush=True)\n",
    "print(preview if preview else '(ë¯¸ë¦¬ë³´ê¸° ì—†ìŒ)', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50ced3",
   "metadata": {},
   "source": [
    "## ì°¸ê³ \n",
    "- /home/admin/wkms-aws/backend/.env íŒŒì¼ì—ì„œ AWS_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY(í•„ìš”ì‹œ AWS_SESSION_TOKEN) ë“±ì„ ì½ì–´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- Textract DetectDocumentTextëŠ” ì´ë¯¸ì§€(ì˜ˆ: PNG/JPG) ë°”ì´íŠ¸ì— ë™ê¸°ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¸íŠ¸ë¶ì€ PDFë¥¼ í˜ì´ì§€ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- í‘œ ì¶”ì¶œì€ AnalyzeDocument(TABLES)ë¥¼ ì´ë¯¸ì§€ì— ëŒ€í•´ ë™ê¸°ì‹ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ê¸°ë³¸ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤(ê°„ë‹¨í•œ ë³µì›).\n",
    "- ë¹„ìš©/í˜¸ì¶œ í•œë„ì— ìœ ì˜í•˜ì„¸ìš”. ëŒ€ìš©ëŸ‰ ë¬¸ì„œëŠ” ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ì¶œë ¥ë¬¼:\n",
    "  - ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì¼: <íŒŒì¼ëª…>_textract_full.txt\n",
    "  - í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ íŒŒì¼: <íŒŒì¼ëª…>_page_###.txt\n",
    "  - í˜ì´ì§€ ì´ë¯¸ì§€: page_images/ í´ë” ë‚´ PNG\n",
    "  - í‘œ í…ìŠ¤íŠ¸: page_tables/ í´ë” ë‚´ í…ìŠ¤íŠ¸ íŒŒì¼\n",
    "  - ë¬¸ì„œ ë©”íƒ€ë°ì´í„° JSON: <íŒŒì¼ëª…>_textract_metadata.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38b3b23",
   "metadata": {},
   "source": [
    "# WIKL ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "AWS Textract ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ RAG ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ í•µì‹¬ ê¸°ëŠ¥ì„ ë‹¨ê³„ë³„ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "## íŒŒì´í”„ë¼ì¸ ê°œìš”\n",
    "1. **í™˜ê²½ ì„¤ì •**: ë°±ì—”ë“œ ì—°ê²° ë° AWS ì„œë¹„ìŠ¤ ì´ˆê¸°í™”\n",
    "2. **ë¬¸ì„œ ì—…ë¡œë“œ**: ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œ ì²˜ë¦¬\n",
    "3. **í…ìŠ¤íŠ¸ ì¶”ì¶œ**: AWS Textract ê¸°ë°˜ OCR ë° êµ¬ì¡° ë¶„ì„\n",
    "4. **ì²­í‚¹ ë° ì„ë² ë”©**: í•œêµ­ì–´ ìµœì í™” í…ìŠ¤íŠ¸ ë¶„í•  ë° ë²¡í„°í™”\n",
    "5. **ê²€ìƒ‰ í…ŒìŠ¤íŠ¸**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥ ê²€ì¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d008aa2",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë°±ì—”ë“œ ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f533aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ë°±ì—”ë“œ URL: http://localhost:8000\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ ì„¤ì •\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "BACKEND_URL = os.getenv(\"BACKEND_URL\", \"http://localhost:8000\")\n",
    "\n",
    "print(f\"ğŸ”§ ë°±ì—”ë“œ URL: {BACKEND_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa15d5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°±ì—”ë“œ ì—°ê²° ì„±ê³µ: (ì£¼)ì›…ì§„ AWS WIKL API v1.0.0\n"
     ]
    }
   ],
   "source": [
    "# ë°±ì—”ë“œ ì—°ê²° í™•ì¸\n",
    "def check_backend_connectivity():\n",
    "    \"\"\"ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸\"\"\"\n",
    "    try:\n",
    "        openapi_url = f\"{BACKEND_URL.rstrip('/')}/openapi.json\"\n",
    "        resp = requests.get(openapi_url, timeout=5)\n",
    "        resp.raise_for_status()\n",
    "        \n",
    "        data = resp.json()\n",
    "        title = data.get(\"info\", {}).get(\"title\", \"Unknown\")\n",
    "        version = data.get(\"info\", {}).get(\"version\", \"Unknown\")\n",
    "        \n",
    "        print(f\"âœ… ë°±ì—”ë“œ ì—°ê²° ì„±ê³µ: {title} v{version}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°±ì—”ë“œ ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "BACKEND_AVAILABLE = check_backend_connectivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e7b47",
   "metadata": {},
   "source": [
    "## 2. AWS Textract ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b462cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°±ì—”ë“œ í™˜ê²½ì„¤ì • ë¡œë“œ: /home/admin/wkms-aws/backend/.env\n",
      "ğŸŒ AWS ë¦¬ì „: ap-northeast-2\n",
      "ğŸ” AWS ì•¡ì„¸ìŠ¤ í‚¤: AKIA3PHZBP...\n",
      "ğŸª£ S3 ë²„í‚·: wikl-file-bucket-20250910\n",
      "ğŸ“„ Textract ì‚¬ìš©: True\n",
      "ğŸ”§ Textract ëª¨ë“œ: layout\n",
      "ğŸ“Š ì§€ì› ê¸°ëŠ¥: ['TABLES', 'FORMS', 'LAYOUT']\n",
      "ğŸ“„ ìµœëŒ€ í˜ì´ì§€: 50\n",
      "ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: 80.0%\n",
      "âš¡ ë¹„ë™ê¸° ì²˜ë¦¬ ì„ê³„ê°’: 5.0MB\n"
     ]
    }
   ],
   "source": [
    "# AWS Textract í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# ë°±ì—”ë“œ .env íŒŒì¼ì—ì„œ AWS ìê²©ì¦ëª… ë¡œë“œ\n",
    "backend_env_path = os.path.join(os.path.dirname(os.getcwd()), 'backend', '.env')\n",
    "if os.path.exists(backend_env_path):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(backend_env_path)\n",
    "    print(f\"âœ… ë°±ì—”ë“œ í™˜ê²½ì„¤ì • ë¡œë“œ: {backend_env_path}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ ë°±ì—”ë“œ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {backend_env_path}\")\n",
    "\n",
    "# AWS ì„¤ì • (ë°±ì—”ë“œ .envì—ì„œ ë¡œë“œ)\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"ap-northeast-2\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "# S3 ì„¤ì •\n",
    "AWS_S3_BUCKET = os.getenv(\"AWS_S3_BUCKET\", \"wikl-file-bucket-20250910\")\n",
    "TEXTRACT_S3_BUCKET = os.getenv(\"TEXTRACT_S3_BUCKET\", AWS_S3_BUCKET)\n",
    "TEXTRACT_RESULT_PREFIX = os.getenv(\"TEXTRACT_RESULT_PREFIX\", \"textract/results\")\n",
    "\n",
    "# Textract ì„¤ì •\n",
    "USE_AWS_TEXTRACT = os.getenv(\"USE_AWS_TEXTRACT\", \"true\").lower() == \"true\"\n",
    "TEXTRACT_MODE = os.getenv(\"TEXTRACT_MODE\", \"layout\")\n",
    "TEXTRACT_FEATURE_TYPES = os.getenv(\"TEXTRACT_FEATURE_TYPES\", \"TABLES,FORMS,LAYOUT\").split(\",\")\n",
    "TEXTRACT_MAX_PAGES = int(os.getenv(\"TEXTRACT_MAX_PAGES\", \"50\"))\n",
    "CONFIDENCE_THRESHOLD = float(os.getenv(\"TEXTRACT_CONFIDENCE_THRESHOLD\", \"80.0\"))\n",
    "TEXTRACT_USE_ASYNC_THRESHOLD_SIZE = int(os.getenv(\"TEXTRACT_USE_ASYNC_THRESHOLD_SIZE\", \"5242880\"))  # 5MB\n",
    "TEXTRACT_JOB_TIMEOUT = int(os.getenv(\"TEXTRACT_JOB_TIMEOUT\", \"600\"))  # 10ë¶„\n",
    "\n",
    "print(f\"ğŸŒ AWS ë¦¬ì „: {AWS_REGION}\")\n",
    "print(f\"ğŸ” AWS ì•¡ì„¸ìŠ¤ í‚¤: {AWS_ACCESS_KEY_ID[:10]}...\" if AWS_ACCESS_KEY_ID else \"âŒ AWS ì•¡ì„¸ìŠ¤ í‚¤ ì—†ìŒ\")\n",
    "print(f\"ğŸª£ S3 ë²„í‚·: {TEXTRACT_S3_BUCKET}\")\n",
    "print(f\"ğŸ“„ Textract ì‚¬ìš©: {USE_AWS_TEXTRACT}\")\n",
    "print(f\"ğŸ”§ Textract ëª¨ë“œ: {TEXTRACT_MODE}\")\n",
    "print(f\"ğŸ“Š ì§€ì› ê¸°ëŠ¥: {TEXTRACT_FEATURE_TYPES}\")\n",
    "print(f\"ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {TEXTRACT_MAX_PAGES}\")\n",
    "print(f\"ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’: {CONFIDENCE_THRESHOLD}%\")\n",
    "print(f\"âš¡ ë¹„ë™ê¸° ì²˜ë¦¬ ì„ê³„ê°’: {TEXTRACT_USE_ASYNC_THRESHOLD_SIZE/1024/1024:.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9081c539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” í˜„ì¬ AWS ìê²©ì¦ëª…:\n",
      "   Account: 788648786912\n",
      "   UserId: AIDA3PHZBPPQITWNRIEOY\n",
      "   Arn: arn:aws:iam::788648786912:user/wj77107791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AWS ìê²©ì¦ëª… í™•ì¸ ë° ì„¤ì •\n",
    "def check_aws_credentials():\n",
    "    \"\"\"AWS ìê²©ì¦ëª… ìƒíƒœ í™•ì¸\"\"\"\n",
    "    try:\n",
    "        # í˜„ì¬ ìê²©ì¦ëª… í™•ì¸\n",
    "        import boto3\n",
    "        sts = boto3.client('sts', region_name=AWS_REGION)\n",
    "        identity = sts.get_caller_identity()\n",
    "        \n",
    "        print(f\"ğŸ” í˜„ì¬ AWS ìê²©ì¦ëª…:\")\n",
    "        print(f\"   Account: {identity.get('Account')}\")\n",
    "        print(f\"   UserId: {identity.get('UserId')}\")\n",
    "        print(f\"   Arn: {identity.get('Arn')}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ AWS ìê²©ì¦ëª… í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
    "        print(\"\\nğŸ”§ í•´ê²° ë°©ë²•:\")\n",
    "        print(\"1. EC2 ì—­í• ì— Textract ê¶Œí•œ ì¶”ê°€\")\n",
    "        print(\"2. ë˜ëŠ” AWS CLIë¡œ ìê²©ì¦ëª… ì„¤ì •: aws configure\")\n",
    "        print(\"3. ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •:\")\n",
    "        print(\"   export AWS_ACCESS_KEY_ID=your_key\")\n",
    "        print(\"   export AWS_SECRET_ACCESS_KEY=your_secret\")\n",
    "        return False\n",
    "\n",
    "check_aws_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fdb7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª…ì‹œì  ìê²©ì¦ëª…ìœ¼ë¡œ Textract í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
      "âœ… AWS Textract í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ (ê¶Œí•œ í™•ì¸ë¨)\n",
      "âœ… AWS Textract í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ (ê¶Œí•œ í™•ì¸ë¨)\n"
     ]
    }
   ],
   "source": [
    "# Textract í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "def init_textract_client():\n",
    "    \"\"\"AWS Textract í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\"\"\"\n",
    "    try:\n",
    "        # ëª…ì‹œì ìœ¼ë¡œ ìê²©ì¦ëª… ì‚¬ìš©\n",
    "        if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:\n",
    "            client = boto3.client(\n",
    "                'textract',\n",
    "                region_name=AWS_REGION,\n",
    "                aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "                config=Config(\n",
    "                    retries={'max_attempts': 3, 'mode': 'standard'},\n",
    "                    read_timeout=60\n",
    "                )\n",
    "            )\n",
    "            print(f\"âœ… ëª…ì‹œì  ìê²©ì¦ëª…ìœ¼ë¡œ Textract í´ë¼ì´ì–¸íŠ¸ ìƒì„±\")\n",
    "        else:\n",
    "            # ê¸°ë³¸ ìê²©ì¦ëª… ì²´ì¸ ì‚¬ìš©\n",
    "            client = boto3.client(\n",
    "                'textract',\n",
    "                region_name=AWS_REGION,\n",
    "                config=Config(\n",
    "                    retries={'max_attempts': 3, 'mode': 'standard'},\n",
    "                    read_timeout=60\n",
    "                )\n",
    "            )\n",
    "            print(f\"âš ï¸ ê¸°ë³¸ ìê²©ì¦ëª… ì²´ì¸ìœ¼ë¡œ Textract í´ë¼ì´ì–¸íŠ¸ ìƒì„±\")\n",
    "        \n",
    "        # ë” ì•ˆì „í•œ ì—°ê²° í…ŒìŠ¤íŠ¸ (ê¶Œí•œì´ ì ì€ ì‘ì—…ìœ¼ë¡œ í…ŒìŠ¤íŠ¸)\n",
    "        try:\n",
    "            # ê°„ë‹¨í•œ ê¶Œí•œ í™•ì¸ - ì‹¤ì œ ë¬¸ì„œ ì—†ì´ í…ŒìŠ¤íŠ¸\n",
    "            client.detect_document_text(\n",
    "                Document={'Bytes': b'invalid'}  # ì˜ë„ì ìœ¼ë¡œ ì˜ëª»ëœ ë°ì´í„°\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            if error_code in ['InvalidParameterException', 'UnsupportedDocumentException']:\n",
    "                print(\"âœ… AWS Textract í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ (ê¶Œí•œ í™•ì¸ë¨)\")\n",
    "                return client\n",
    "            elif error_code == 'AccessDeniedException':\n",
    "                print(f\"âŒ Textract ê¶Œí•œ ë¶€ì¡±: {e}\")\n",
    "                print(\"ğŸ”§ í•´ê²° í•„ìš”: ì‚¬ìš©ìì—ê²Œ Textract ê¶Œí•œ ì¶”ê°€ í•„ìš”\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {error_code}\")\n",
    "                return client  # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” í´ë¼ì´ì–¸íŠ¸ê°€ ì‘ë™í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Textract í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "textract_client = init_textract_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c8cfb",
   "metadata": {},
   "source": [
    "## 3. ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8b1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf: complex ëª¨ë“œ\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ë³µì¡ë„ íŒë³„\n",
    "def detect_document_complexity(file_path):\n",
    "    \"\"\"ë¬¸ì„œ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ Textract API ì„ íƒ\"\"\"\n",
    "    try:\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        # íŒŒì¼ í¬ê¸° ê¸°ì¤€ (5MB ì´ìƒì€ ë³µì¡í•œ ë¬¸ì„œë¡œ ê°„ì£¼)\n",
    "        if file_size > 5 * 1024 * 1024:\n",
    "            return 'complex'\n",
    "        \n",
    "        # PDFì˜ ê²½ìš° í˜ì´ì§€ ìˆ˜ í™•ì¸\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            try:\n",
    "                import fitz\n",
    "                doc = fitz.open(file_path)\n",
    "                page_count = len(doc)\n",
    "                doc.close()\n",
    "                \n",
    "                return 'complex' if page_count > 10 else 'simple'\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return 'simple'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ë³µì¡ë„ ê°ì§€ ì‹¤íŒ¨: {e}\")\n",
    "        return 'simple'\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_file = \"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\"\n",
    "if os.path.exists(test_file):\n",
    "    complexity = detect_document_complexity(test_file)\n",
    "    print(f\"ğŸ“„ {test_file}: {complexity} ëª¨ë“œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ec96d",
   "metadata": {},
   "source": [
    "## ê²€ì¦/ì €ì¥ ë‹¨ê³„ ì•ˆë‚´\n",
    "\n",
    "- ì´ ë‹¨ê³„(ë‹¤ìŒ ì½”ë“œ ì…€)ëŠ” `result` ê°ì²´ë¥¼ ê²€ì‚¬í•´ ì„±ê³µ/ì‹¤íŒ¨ ëª¨ë‘ì— ëŒ€í•´ ì‚°ì¶œë¬¼ì„ ë³´ì¡´í•©ë‹ˆë‹¤.\n",
    "- ì„±ê³µ ì‹œ: ì¶”ì¶œ í…ìŠ¤íŠ¸(.txt), ë©”íƒ€ë°ì´í„°(.json), ìš”ì•½(.md)ì„ `jupyter_notebook/data/output_texts/`ì— íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- ì‹¤íŒ¨ ì‹œ: ì˜¤ë¥˜ ë‚´ìš©ì„ ë‹´ì€ `error-*.json`ì„ ê°™ì€ ë””ë ‰í„°ë¦¬ì— ì €ì¥í•˜ì—¬ ì¶”í›„ ë””ë²„ê¹…ì— í™œìš©í•©ë‹ˆë‹¤.\n",
    "- í”„ë¦¬ë·° ê²°ê³¼ì¸ ê²½ìš°ì—ë„ ë™ì¼í•˜ê²Œ ì €ì¥ë˜ë©°, íŒŒì¼ëª…/ìš”ì•½ì— í”„ë¦¬ë·° í‘œì‹œê°€ í¬í•¨ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4cabd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ì „ì˜ ê¸´ ë¡œì§ì€ í•˜ìœ„ ì…€ë“¤ë¡œ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ì…€ë“¤ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# ë³¸ ì…€ì€ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ìƒˆ ì…€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.\n",
    "# 1) í™˜ê²½/í´ë¼ì´ì–¸íŠ¸/í† ê¸€ ì„¤ì •\n",
    "# 2) S3 ìœ í‹¸ë¦¬í‹°\n",
    "# 3) í”„ë¦¬ë·°(ê³ ì†) ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# 4) ì•ˆì • ì²˜ë¦¬ ì „ëµ ê²°ì • í•¨ìˆ˜\n",
    "# 5) ë™ê¸°/ë¹„ë™ê¸° ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# 6) ì½˜í…ì¸  ì¶”ì¶œ/ë©”íƒ€ë°ì´í„° êµ¬ì„± í•¨ìˆ˜\n",
    "# 7) ë©”ì¸ ì‹¤í–‰ (result ìƒì„±)\n",
    "print(\"ì´ì „ì˜ ê¸´ ë¡œì§ì€ í•˜ìœ„ ì…€ë“¤ë¡œ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ì…€ë“¤ì„ ì‚¬ìš©í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa75f27",
   "metadata": {},
   "source": [
    "## 1) í™˜ê²½/í´ë¼ì´ì–¸íŠ¸/í† ê¸€ ì„¤ì •\n",
    "\n",
    "- backend `.env`ë¥¼ í†µí•´ AWS ìê²©ê³¼ S3 ë²„í‚·/ë¦¬ì „ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "- Textract í´ë¼ì´ì–¸íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "- í”„ë¦¬ë·°/ëŒ€ê¸° ë™ì‘ì„ ì œì–´í•˜ëŠ” í™˜ê²½ë³€ìˆ˜ í† ê¸€ë„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "- ì´ ì…€ì€ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f33bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textract í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ (region=ap-northeast-2)\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½/í´ë¼ì´ì–¸íŠ¸/í† ê¸€ ì„¤ì •\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# ì´ë¯¸ ì•ì„  ì…€ì—ì„œ .env ë¡œë“œ ë° ê¸°ë³¸ ì„¤ì •ì´ ë˜ì—ˆìŒì„ ê°€ì •í•©ë‹ˆë‹¤.\n",
    "# ëˆ„ë½ ì‹œì—ë„ ì•ˆì „í•˜ê²Œ ê¸°ë³¸ê°’ì„ ì±„ìš°ë„ë¡ ë°©ì–´ì  ì½”ë“œë¥¼ ë‘¡ë‹ˆë‹¤.\n",
    "\n",
    "AWS_REGION = globals().get('AWS_REGION') or os.getenv('AWS_REGION') or 'us-east-1'\n",
    "AWS_ACCESS_KEY_ID = globals().get('AWS_ACCESS_KEY_ID') or os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = globals().get('AWS_SECRET_ACCESS_KEY') or os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "TEXTRACT_S3_BUCKET = globals().get('TEXTRACT_S3_BUCKET') or os.getenv('TEXTRACT_S3_BUCKET') or os.getenv('AWS_S3_BUCKET')\n",
    "TEXTRACT_JOB_TIMEOUT = int(globals().get('TEXTRACT_JOB_TIMEOUT') or os.getenv('TEXTRACT_JOB_TIMEOUT') or 120)\n",
    "\n",
    "# í† ê¸€: í”„ë¦¬ë·°/í˜ì´ì§€ìˆ˜/ë¹„ë™ê¸° ëŒ€ê¸° ì—¬ë¶€\n",
    "TEXTRACT_FAST_PREVIEW = os.getenv(\"TEXTRACT_FAST_PREVIEW\", \"true\").lower() == \"true\"\n",
    "TEXTRACT_PREVIEW_PAGES = int(os.getenv(\"TEXTRACT_PREVIEW_PAGES\", \"2\"))\n",
    "TEXTRACT_ASYNC_WAIT = os.getenv(\"TEXTRACT_ASYNC_WAIT\", \"false\").lower() == \"true\"\n",
    "\n",
    "# Textract í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "textract_client = globals().get('textract_client')\n",
    "if textract_client is None:\n",
    "    textract_client = boto3.client(\n",
    "        'textract',\n",
    "        region_name=AWS_REGION,\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "    )\n",
    "print(f\"Textract í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ (region={AWS_REGION})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e815914",
   "metadata": {},
   "source": [
    "## 2) S3 ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "\n",
    "- ëŒ€ìš©ëŸ‰ ë¬¸ì„œëŠ” S3ì— ì—…ë¡œë“œ í›„ Textract ë¹„ë™ê¸° APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ì—…ë¡œë“œ ì„±ê³µ ì‹œ ì˜¤ë¸Œì íŠ¸ í‚¤ë¥¼ ë°˜í™˜í•˜ê³ , ì‹¤íŒ¨ ì‹œ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af1f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 ì—…ë¡œë“œ ìœ í‹¸ë¦¬í‹°\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "def upload_to_s3(file_path, bucket_name, object_key=None):\n",
    "    try:\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            region_name=AWS_REGION,\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "        )\n",
    "        if not object_key:\n",
    "            filename = os.path.basename(file_path)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            object_key = f\"textract/input/{timestamp}_{filename}\"\n",
    "        print(f\"ğŸ“¤ S3 ì—…ë¡œë“œ: s3://{bucket_name}/{object_key}\")\n",
    "        s3_client.upload_file(file_path, bucket_name, object_key)\n",
    "        print(\"âœ… ì—…ë¡œë“œ ì™„ë£Œ\")\n",
    "        return object_key\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cf0b7",
   "metadata": {},
   "source": [
    "## 3) í”„ë¦¬ë·°(ê³ ì†) ì²˜ë¦¬ í•¨ìˆ˜\n",
    "\n",
    "- ëŒ€ìš©ëŸ‰ ë¬¸ì„œì—ì„œ ë¹ ë¥´ê²Œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¥¼ ì–»ê¸° ìœ„í•´ ì²« N í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë Œë”ë§í•©ë‹ˆë‹¤.\n",
    "- ê° ì´ë¯¸ì§€ì— ëŒ€í•´ `detect_document_text`ë¥¼ í˜¸ì¶œí•˜ê³ , 2ì—´ ë¬¸ì„œ ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ê°„ë‹¨í•œ ì¢Œ/ìš° ì—´ ì¬êµ¬ì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "- ê²°ê³¼ëŠ” `processing_mode='preview'`ë¡œ í‘œê¸°ë˜ì–´ ì´í›„ ë‹¨ê³„(ì €ì¥/ê²€ì¦)ì—ì„œ ì‹ë³„ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6a51e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ í–¥ìƒëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ë“¤ ë¡œë“œ ì™„ë£Œ (í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì§€ì›)\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ë“¤ ì •ì˜ (í–¥ìƒëœ ë²„ì „ - ë‹¤ì¤‘ ì»¬ëŸ¼, í‘œ/í¼ ë¶„ì„, íšŒì „ ë³´ì •, ë™ì  ì¤Œ, í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ)\n",
    "import io\n",
    "import boto3\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def auto_orient_and_deskew_image(img_bytes):\n",
    "    \"\"\"ì´ë¯¸ì§€ íšŒì „/ê¸°ìš¸ê¸° ìë™ ë³´ì •\"\"\"\n",
    "    try:\n",
    "        # OpenCVë¡œ ì´ë¯¸ì§€ ì½ê¸°\n",
    "        nparr = np.frombuffer(img_bytes, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            return img_bytes\n",
    "        \n",
    "        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ íšŒì „ ê°ì§€ ë° ë³´ì • (ê°„ë‹¨í•œ ë²„ì „)\n",
    "        edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)\n",
    "        \n",
    "        if lines is not None and len(lines) > 0:\n",
    "            angles = []\n",
    "            for rho, theta in lines[:10]:\n",
    "                angle = np.degrees(theta) - 90\n",
    "                if -45 <= angle <= 45:\n",
    "                    angles.append(angle)\n",
    "            \n",
    "            if angles:\n",
    "                median_angle = np.median(angles)\n",
    "                if abs(median_angle) > 0.5:  # 0.5ë„ ì´ìƒ ê¸°ìš¸ì–´ì§„ ê²½ìš°ë§Œ ë³´ì •\n",
    "                    h, w = gray.shape[:2]\n",
    "                    center = (w // 2, h // 2)\n",
    "                    rotation_matrix = cv2.getRotationMatrix2D(center, median_angle, 1.0)\n",
    "                    img = cv2.warpAffine(img, rotation_matrix, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "        \n",
    "        # ë³´ì •ëœ ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜\n",
    "        _, buffer = cv2.imencode('.png', img)\n",
    "        return buffer.tobytes()\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ ì´ë¯¸ì§€ ë³´ì • ì‹¤íŒ¨: {e}\")\n",
    "        return img_bytes\n",
    "\n",
    "def compute_dynamic_zoom(page_width_pt, page_height_pt):\n",
    "    \"\"\"ë™ì  ì¤Œ ê³„ì‚° - í•œê¸€ OCRì— ìµœì í™”\"\"\"\n",
    "    try:\n",
    "        # í˜ì´ì§€ì˜ ë” ê¸´ ìª½ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°\n",
    "        long_side_pt = max(page_width_pt, page_height_pt)\n",
    "        \n",
    "        # ëª©í‘œ: ~360 DPI í•´ìƒë„ (1800í”½ì…€ì„ ê¸°ì¤€ìœ¼ë¡œ)\n",
    "        if long_side_pt > 0:\n",
    "            optimal_zoom = 1800 / long_side_pt\n",
    "            # ìµœì†Œ 2.0, ìµœëŒ€ 4.0ìœ¼ë¡œ ì œí•œ\n",
    "            zoom = max(2.0, min(optimal_zoom, 4.0))\n",
    "        else:\n",
    "            zoom = 3.0  # ê¸°ë³¸ê°’\n",
    "        \n",
    "        return zoom\n",
    "    except Exception:\n",
    "        return 3.0\n",
    "\n",
    "def reconstruct_layout_by_page(lines, page_num, layout_settings=None):\n",
    "    \"\"\"í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì¬êµ¬ì„±\"\"\"\n",
    "    if not lines:\n",
    "        return lines\n",
    "    \n",
    "    # ì„¤ì • í™•ì¸ (íŒŒë¼ë¯¸í„° ìš°ì„ , ì—†ìœ¼ë©´ ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©)\n",
    "    if layout_settings is None:\n",
    "        layout_settings = {\n",
    "            'enable_page_specific': globals().get('ENABLE_PAGE_SPECIFIC_LAYOUT', True),\n",
    "            'first_page_layout': globals().get('FIRST_PAGE_LAYOUT', 'full_page')\n",
    "        }\n",
    "    \n",
    "    enable_page_specific = layout_settings.get('enable_page_specific', True)\n",
    "    first_page_layout = layout_settings.get('first_page_layout', 'full_page')\n",
    "    \n",
    "    if not enable_page_specific:\n",
    "        # í˜ì´ì§€ë³„ ì²˜ë¦¬ ë¹„í™œì„±í™”: ê¸°ë³¸ ì •ë ¬\n",
    "        return sorted(lines, key=lambda l: (round(l['y'], 2), l['x']))\n",
    "    \n",
    "    if page_num == 1 and first_page_layout == 'full_page':\n",
    "        # ì²« í˜ì´ì§€: ì „ì²´ í˜ì´ì§€ ìƒí•˜ ìˆœì„œ\n",
    "        return sorted(lines, key=lambda l: (round(l['y'], 2), l['x']))\n",
    "    else:\n",
    "        # ë‘ ë²ˆì§¸ í˜ì´ì§€ë¶€í„°: 2ì—´ ë ˆì´ì•„ì›ƒ\n",
    "        if len(lines) < 2:\n",
    "            return lines\n",
    "        \n",
    "        # ì¤‘ì‹¬ì (cx) ê¸°ì¤€ìœ¼ë¡œ 2ì—´ ë¶„í• \n",
    "        cxs = [l['cx'] for l in lines]\n",
    "        cxs_sorted = sorted(set(cxs))\n",
    "        \n",
    "        if len(cxs_sorted) < 2:\n",
    "            # ì—´ì´ í•˜ë‚˜ë¿ì¸ ê²½ìš°: ì¼ë°˜ ì •ë ¬\n",
    "            return sorted(lines, key=lambda l: (l['y'], l['x']))\n",
    "        \n",
    "        # ê°€ì¥ í° ê°„ê²©ì„ ì°¾ì•„ ë¶„í• ì  ê²°ì •\n",
    "        gaps = []\n",
    "        for i in range(len(cxs_sorted) - 1):\n",
    "            gap_size = cxs_sorted[i+1] - cxs_sorted[i]\n",
    "            gap_center = (cxs_sorted[i] + cxs_sorted[i+1]) / 2\n",
    "            gaps.append((gap_size, gap_center))\n",
    "        \n",
    "        gaps.sort(reverse=True)  # ê°€ì¥ í° ê°„ê²©ë¶€í„°\n",
    "        threshold = gaps[0][1] if gaps else 0.5\n",
    "        \n",
    "        # ì¢Œìš° ë¶„í• \n",
    "        left_lines = [l for l in lines if l['cx'] <= threshold]\n",
    "        right_lines = [l for l in lines if l['cx'] > threshold]\n",
    "        \n",
    "        # ê° ì—´ ë‚´ì—ì„œ ìƒí•˜ ì •ë ¬\n",
    "        left_sorted = sorted(left_lines, key=lambda l: (l['y'], l['x']))\n",
    "        right_sorted = sorted(right_lines, key=lambda l: (l['y'], l['x']))\n",
    "        \n",
    "        # ì¢Œì—´ â†’ ìš°ì—´ ìˆœì„œë¡œ ê²°í•©\n",
    "        return left_sorted + right_sorted\n",
    "\n",
    "def extract_text_fast_preview(file_path, page_limit=None, layout_settings=None):\n",
    "    \"\"\"ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ - í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì§€ì›\"\"\"\n",
    "    print(\"ğŸš€ ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\")\n",
    "    \n",
    "    # ë ˆì´ì•„ì›ƒ ì„¤ì • ì¤€ë¹„\n",
    "    if layout_settings is None:\n",
    "        layout_settings = {\n",
    "            'enable_page_specific': globals().get('ENABLE_PAGE_SPECIFIC_LAYOUT', True),\n",
    "            'first_page_layout': globals().get('FIRST_PAGE_LAYOUT', 'full_page')\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        total_pages = len(doc)\n",
    "        \n",
    "        if page_limit:\n",
    "            total_pages = min(total_pages, page_limit)\n",
    "            print(f\"   ğŸ“„ í˜ì´ì§€ ì œí•œ: {page_limit}í˜ì´ì§€\")\n",
    "        \n",
    "        print(f\"   ğŸ“– ì´ {total_pages}í˜ì´ì§€ ì²˜ë¦¬\")\n",
    "        print(f\"   ğŸ¯ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: {layout_settings['enable_page_specific']}\")\n",
    "        print(f\"   ğŸ“‹ ì²« í˜ì´ì§€ ëª¨ë“œ: {layout_settings['first_page_layout']}\")\n",
    "        \n",
    "        all_text = \"\"\n",
    "        page_results = []\n",
    "        \n",
    "        for page_num in range(total_pages):\n",
    "            page = doc[page_num]\n",
    "            \n",
    "            # ë™ì  ì¤Œ ê³„ì‚°\n",
    "            w_pt = float(page.rect.width)\n",
    "            h_pt = float(page.rect.height)\n",
    "            zoom = compute_dynamic_zoom(w_pt, h_pt)\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ë Œë”ë§\n",
    "            pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)\n",
    "            img_bytes = pix.tobytes(\"png\")\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ë³´ì •\n",
    "            img_bytes = auto_orient_and_deskew_image(img_bytes)\n",
    "            \n",
    "            # Textract OCR\n",
    "            response = textract_client.detect_document_text(Document={'Bytes': img_bytes})\n",
    "            \n",
    "            # ë¼ì¸ ë¸”ë¡ ì¶”ì¶œ\n",
    "            lines = []\n",
    "            for block in response.get('Blocks', []):\n",
    "                if block.get('BlockType') == 'LINE' and block.get('Geometry'):\n",
    "                    bbox = block['Geometry'].get('BoundingBox', {})\n",
    "                    text = (block.get('Text') or '').strip()\n",
    "                    if text:\n",
    "                        left = bbox.get('Left', 0.0)\n",
    "                        top = bbox.get('Top', 0.0)\n",
    "                        width = bbox.get('Width', 0.0)\n",
    "                        cx = left + width / 2.0\n",
    "                        lines.append({'x': left, 'y': top, 'cx': cx, 'text': text})\n",
    "            \n",
    "            # í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì¬êµ¬ì„±\n",
    "            lines_sorted = reconstruct_layout_by_page(lines, page_num + 1, layout_settings)\n",
    "            \n",
    "            # í˜ì´ì§€ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "            page_text = \"\\n\".join(line['text'] for line in lines_sorted)\n",
    "            all_text += f\"\\n--- í˜ì´ì§€ {page_num + 1} ---\\n{page_text}\\n\"\n",
    "            \n",
    "            # í˜ì´ì§€ ê²°ê³¼ ì €ì¥\n",
    "            layout_type = 'full_page' if (page_num == 0 and layout_settings.get('first_page_layout') == 'full_page') else 'two_column'\n",
    "            page_results.append({\n",
    "                'page': page_num + 1,\n",
    "                'text': page_text,\n",
    "                'lines_count': len(lines),\n",
    "                'layout_type': layout_type\n",
    "            })\n",
    "            \n",
    "            print(f\"   âœ… í˜ì´ì§€ {page_num + 1}: {len(lines)}ë¼ì¸, {len(page_text)}ì, ë ˆì´ì•„ì›ƒ={layout_type}\")\n",
    "        \n",
    "        doc.close()\n",
    "        print(f\"âœ… ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸° ì™„ë£Œ: ì´ {len(all_text)}ì ì¶”ì¶œ\")\n",
    "        \n",
    "        return {\n",
    "            'full_text': all_text,\n",
    "            'page_results': page_results,\n",
    "            'total_pages': total_pages,\n",
    "            'extraction_method': 'fast_preview_with_page_layout',\n",
    "            'layout_settings': layout_settings\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def extract_full_via_images(file_path, layout_settings=None):\n",
    "    \"\"\"ì´ë¯¸ì§€ ë³€í™˜ í›„ ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ - í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì§€ì›\"\"\"\n",
    "    print(\"ğŸ”„ PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...\")\n",
    "    \n",
    "    # ë ˆì´ì•„ì›ƒ ì„¤ì • ì¤€ë¹„\n",
    "    if layout_settings is None:\n",
    "        layout_settings = {\n",
    "            'enable_page_specific': globals().get('ENABLE_PAGE_SPECIFIC_LAYOUT', True),\n",
    "            'first_page_layout': globals().get('FIRST_PAGE_LAYOUT', 'full_page')\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        total_pages = len(doc)\n",
    "        print(f\"   ğŸ“– ì´ {total_pages}í˜ì´ì§€ ì²˜ë¦¬\")\n",
    "        print(f\"   ğŸ¯ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: {layout_settings['enable_page_specific']}\")\n",
    "        print(f\"   ğŸ“‹ ì²« í˜ì´ì§€ ëª¨ë“œ: {layout_settings['first_page_layout']}\")\n",
    "        \n",
    "        all_text = \"\"\n",
    "        page_results = []\n",
    "        \n",
    "        for page_num in range(total_pages):\n",
    "            print(f\"   ğŸ” í˜ì´ì§€ {page_num + 1}/{total_pages} ì²˜ë¦¬ ì¤‘...\")\n",
    "            page = doc[page_num]\n",
    "            \n",
    "            # ë™ì  ì¤Œ ê³„ì‚°\n",
    "            w_pt = float(page.rect.width)\n",
    "            h_pt = float(page.rect.height)\n",
    "            zoom = compute_dynamic_zoom(w_pt, h_pt)\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ë Œë”ë§\n",
    "            pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)\n",
    "            img_bytes = pix.tobytes(\"png\")\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ë³´ì •\n",
    "            img_bytes = auto_orient_and_deskew_image(img_bytes)\n",
    "            \n",
    "            # Textract OCR\n",
    "            response = textract_client.detect_document_text(Document={'Bytes': img_bytes})\n",
    "            \n",
    "            # ë¼ì¸ ë¸”ë¡ ì¶”ì¶œ\n",
    "            lines = []\n",
    "            for block in response.get('Blocks', []):\n",
    "                if block.get('BlockType') == 'LINE' and block.get('Geometry'):\n",
    "                    bbox = block['Geometry'].get('BoundingBox', {})\n",
    "                    text = (block.get('Text') or '').strip()\n",
    "                    if text:\n",
    "                        left = bbox.get('Left', 0.0)\n",
    "                        top = bbox.get('Top', 0.0)\n",
    "                        width = bbox.get('Width', 0.0)\n",
    "                        cx = left + width / 2.0\n",
    "                        lines.append({'x': left, 'y': top, 'cx': cx, 'text': text})\n",
    "            \n",
    "            # í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì¬êµ¬ì„±\n",
    "            lines_sorted = reconstruct_layout_by_page(lines, page_num + 1, layout_settings)\n",
    "            \n",
    "            # í˜ì´ì§€ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "            page_text = \"\\n\".join(line['text'] for line in lines_sorted)\n",
    "            all_text += f\"\\n--- í˜ì´ì§€ {page_num + 1} ---\\n{page_text}\\n\"\n",
    "            \n",
    "            # í˜ì´ì§€ ê²°ê³¼ ì €ì¥\n",
    "            layout_type = 'full_page' if (page_num == 0 and layout_settings.get('first_page_layout') == 'full_page') else 'two_column'\n",
    "            page_results.append({\n",
    "                'page': page_num + 1,\n",
    "                'text': page_text,\n",
    "                'lines_count': len(lines),\n",
    "                'layout_type': layout_type\n",
    "            })\n",
    "        \n",
    "        doc.close()\n",
    "        print(f\"âœ… ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(all_text)}ì ì¶”ì¶œ\")\n",
    "        \n",
    "        return {\n",
    "            'full_text': all_text,\n",
    "            'page_results': page_results,\n",
    "            'total_pages': total_pages,\n",
    "            'extraction_method': 'full_image_with_page_layout',\n",
    "            'layout_settings': layout_settings\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"ğŸ“¦ í–¥ìƒëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ë“¤ ë¡œë“œ ì™„ë£Œ (í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì§€ì›)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40385053",
   "metadata": {},
   "source": [
    "## 4) ì•ˆì • ì²˜ë¦¬ ì „ëµ ê²°ì • í•¨ìˆ˜\n",
    "\n",
    "- íŒŒì¼ í¬ê¸°ì™€ ë³µì¡ë„(`complexity`)ì— ë”°ë¼ ë™ê¸°/ë¹„ë™ê¸°, ë¶„ì„/ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "- 5MB ì´ˆê³¼ â†’ ë¹„ë™ê¸°(S3 ê¸°ë°˜)\n",
    "- 1~5MB â†’ ë™ê¸° detect_document_text\n",
    "- 1MB ì´í•˜ â†’ ë³µì¡ë„ê°€ ë†’ìœ¼ë©´ analyze_document(LAYOUT), ì•„ë‹ˆë©´ detect_document_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff2d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•ˆì • ì²˜ë¦¬ ì „ëµ ê²°ì •\n",
    "\n",
    "def _determine_stable_processing_strategy(file_size, complexity):\n",
    "    MAX_SYNC_SIZE = 5 * 1024 * 1024  # 5MB\n",
    "    if file_size > MAX_SYNC_SIZE:\n",
    "        return {\n",
    "            'name': 'ëŒ€ìš©ëŸ‰ ë¹„ë™ê¸° ì²˜ë¦¬',\n",
    "            'method': 'async',\n",
    "            'expected_time': '1-3ë¶„',\n",
    "            'features': ['LAYOUT'] if complexity == 'complex' else [],\n",
    "            'api_type': 'start_document_analysis' if complexity == 'complex' else 'start_document_text_detection'\n",
    "        }\n",
    "    elif file_size > 1024 * 1024:\n",
    "        return {\n",
    "            'name': 'ì¤‘ê°„ í¬ê¸° ë™ê¸° ì²˜ë¦¬',\n",
    "            'method': 'sync',\n",
    "            'expected_time': '10-30ì´ˆ',\n",
    "            'features': [],\n",
    "            'api_type': 'detect_document_text'\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'name': 'ì†Œí˜• íŒŒì¼ ë¶„ì„',\n",
    "            'method': 'sync',\n",
    "            'expected_time': '5-15ì´ˆ',\n",
    "            'features': ['LAYOUT'] if complexity == 'complex' else [],\n",
    "            'api_type': 'analyze_document' if complexity == 'complex' else 'detect_document_text'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc103f2f",
   "metadata": {},
   "source": [
    "## 5) ë™ê¸°/ë¹„ë™ê¸° ì²˜ë¦¬ í•¨ìˆ˜\n",
    "\n",
    "- ì „ëµì— ë”°ë¼ Textract ë™ê¸°/ë¹„ë™ê¸° APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "- ë¹„ë™ê¸°ëŠ” S3 ì—…ë¡œë“œ â†’ start_document_* â†’ get_document_* í´ë§(ë˜ëŠ” í”„ë¦¬ë·° ëŒ€ì²´)ì˜ ìˆœì„œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "- ì˜ˆì™¸ ë°œìƒ ì‹œ `result = { success: False, error, pipeline_stage }` í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a08b1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë™ê¸°/ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬í˜„ (PDF ì•ˆì „ í´ë°± + í˜ì´ì§€ë„¤ì´ì…˜) - í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì§€ì›\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def _paginate_all_results(get_func, job_id):\n",
    "    blocks = []\n",
    "    next_token = None\n",
    "    while True:\n",
    "        if next_token:\n",
    "            res = get_func(JobId=job_id, NextToken=next_token)\n",
    "        else:\n",
    "            res = get_func(JobId=job_id)\n",
    "        blocks.extend(res.get('Blocks', []))\n",
    "        next_token = res.get('NextToken')\n",
    "        if not next_token:\n",
    "            break\n",
    "    return { 'Blocks': blocks }\n",
    "\n",
    "\n",
    "def _process_sync_for_rag(file_path, layout_settings, strategy):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_bytes = f.read()\n",
    "        if strategy['api_type'] == 'analyze_document' and strategy['features']:\n",
    "            response = textract_client.analyze_document(Document={'Bytes': file_bytes}, FeatureTypes=strategy['features'])\n",
    "            return response\n",
    "        else:\n",
    "            try:\n",
    "                return textract_client.detect_document_text(Document={'Bytes': file_bytes})\n",
    "            except ClientError as ce:\n",
    "                code = ce.response.get('Error', {}).get('Code')\n",
    "                if code in ('UnsupportedDocumentException', 'InvalidParameterException'):\n",
    "                    print(f\"âš ï¸ detect_document_text ì‹¤íŒ¨({code}) â†’ analyze_document(LAYOUT) í´ë°± ì‹œë„\")\n",
    "                    try:\n",
    "                        return textract_client.analyze_document(Document={'Bytes': file_bytes}, FeatureTypes=['LAYOUT'])\n",
    "                    except Exception as e2:\n",
    "                        print(f\"âš ï¸ analyze_document í´ë°± ì‹¤íŒ¨: {e2} â†’ ì „ì²´ ì´ë¯¸ì§€ ê¸°ë°˜ í´ë°±\")\n",
    "                        return extract_full_via_images(file_path, layout_settings)\n",
    "                raise\n",
    "    except Exception as e:\n",
    "        return {\"_error\": str(e), \"_stage\": \"sync_processing\"}\n",
    "\n",
    "\n",
    "def _process_async_for_rag(file_path, layout_settings, strategy):\n",
    "    try:\n",
    "        if not TEXTRACT_S3_BUCKET:\n",
    "            print(\"âš ï¸ S3 ë²„í‚· ë¯¸ì„¤ì • â†’ ì „ì²´ ì´ë¯¸ì§€ ê¸°ë°˜ í´ë°± ì‚¬ìš©\")\n",
    "            return extract_full_via_images(file_path, layout_settings)\n",
    "        s3_key = upload_to_s3(file_path, TEXTRACT_S3_BUCKET)\n",
    "        if not s3_key:\n",
    "            return {\"_error\": \"S3 ì—…ë¡œë“œ ì‹¤íŒ¨\", \"_stage\": \"s3_upload\"}\n",
    "        doc_loc = {'S3Object': {'Bucket': TEXTRACT_S3_BUCKET, 'Name': s3_key}}\n",
    "        if strategy['api_type'] == 'start_document_analysis' and strategy['features']:\n",
    "            start = textract_client.start_document_analysis(DocumentLocation=doc_loc, FeatureTypes=strategy['features'])\n",
    "            get_func = textract_client.get_document_analysis\n",
    "        else:\n",
    "            start = textract_client.start_document_text_detection(DocumentLocation=doc_loc)\n",
    "            get_func = textract_client.get_document_text_detection\n",
    "        job_id = start['JobId']\n",
    "        if not TEXTRACT_ASYNC_WAIT:\n",
    "            preview = extract_text_fast_preview(file_path, TEXTRACT_PREVIEW_PAGES, layout_settings)\n",
    "            if preview and preview.get('full_text'):\n",
    "                return preview\n",
    "        waited = 0\n",
    "        step = 5\n",
    "        max_wait = min(TEXTRACT_JOB_TIMEOUT, 300)\n",
    "        while waited < max_wait:\n",
    "            time.sleep(step)\n",
    "            waited += step\n",
    "            res = get_func(JobId=job_id)\n",
    "            if res.get('JobStatus') == 'SUCCEEDED':\n",
    "                full = _paginate_all_results(get_func, job_id)\n",
    "                return full\n",
    "            if res.get('JobStatus') == 'FAILED':\n",
    "                return {\"_error\": res.get('StatusMessage','Unknown'), \"_stage\": \"async_processing\"}\n",
    "        return {\"_error\": f\"ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼ ({max_wait}s)\", \"_stage\": \"timeout\"}\n",
    "    except Exception as e:\n",
    "        return {\"_error\": str(e), \"_stage\": \"async_processing\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94ebf6",
   "metadata": {},
   "source": [
    "## 6) ì½˜í…ì¸  ì¶”ì¶œ/ë©”íƒ€ë°ì´í„° êµ¬ì„± í•¨ìˆ˜\n",
    "\n",
    "- Textract ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ë¼ì¸ì„ ëª¨ì•„ ê¸´ í…ìŠ¤íŠ¸ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.\n",
    "- í‰ê·  ì‹ ë¢°ë„ë¥¼ í¬í•¨í•œ ê°„ë‹¨í•œ ë ˆì´ì•„ì›ƒ/ì½˜í…ì¸  í†µê³„ë¥¼ êµ¬ì„±í•´ RAG ë©”íƒ€ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "- í”„ë¦¬ë·° ê²°ê³¼(ì´ë¯¸ ì™„ì„±ëœ result dict)ë„ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b922370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ í–¥ìƒëœ ì½˜í…ì¸  ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ (í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ + í‘œ/í¼ ì§€ì›)\n"
     ]
    }
   ],
   "source": [
    "def _build_result_from_response(textract_response, extraction_method=\"textract_api\", source_file_path=None, layout_settings=None):\n",
    "    \"\"\"Textract ì‘ë‹µì—ì„œ ê²°ê³¼ êµ¬ì¡°ì²´ ìƒì„± - í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ë° í‘œ/í¼ íŒŒì‹± ì§€ì›\"\"\"\n",
    "    result = {\n",
    "        'success': False,\n",
    "        'text': '',\n",
    "        'metadata': {\n",
    "            'extraction_method': extraction_method,\n",
    "            'source_file': source_file_path,\n",
    "            'blocks_total': 0,\n",
    "            'blocks_by_type': {},\n",
    "            'confidence_stats': {},\n",
    "            'page_count': 0,\n",
    "            'layout_settings': layout_settings or {}\n",
    "        },\n",
    "        'page_results': [],\n",
    "        'tables': [],\n",
    "        'forms': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # ë¸”ë¡ ì •ë³´ ìˆ˜ì§‘\n",
    "        blocks = textract_response.get('Blocks', [])\n",
    "        result['metadata']['blocks_total'] = len(blocks)\n",
    "        \n",
    "        # ë¸”ë¡ íƒ€ì…ë³„ ì¹´ìš´íŠ¸\n",
    "        block_counts = {}\n",
    "        for block in blocks:\n",
    "            block_type = block.get('BlockType', 'UNKNOWN')\n",
    "            block_counts[block_type] = block_counts.get(block_type, 0) + 1\n",
    "        result['metadata']['blocks_by_type'] = block_counts\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ ì²˜ë¦¬\n",
    "        pages_data = {}\n",
    "        line_blocks = []\n",
    "        \n",
    "        for block in blocks:\n",
    "            if block.get('BlockType') == 'PAGE':\n",
    "                page_id = block.get('Id')\n",
    "                page_num = len(pages_data) + 1\n",
    "                pages_data[page_id] = {\n",
    "                    'page_number': page_num,\n",
    "                    'geometry': block.get('Geometry', {}),\n",
    "                    'lines': []\n",
    "                }\n",
    "            elif block.get('BlockType') == 'LINE':\n",
    "                line_blocks.append(block)\n",
    "        \n",
    "        result['metadata']['page_count'] = len(pages_data)\n",
    "        \n",
    "        # ë¼ì¸ ë¸”ë¡ì„ í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”\n",
    "        for line_block in line_blocks:\n",
    "            relationships = line_block.get('Relationships', [])\n",
    "            page_id = None\n",
    "            \n",
    "            # í˜ì´ì§€ ê´€ê³„ ì°¾ê¸°\n",
    "            for rel in relationships:\n",
    "                if rel.get('Type') == 'CHILD':\n",
    "                    # ë¶€ëª¨ í˜ì´ì§€ ì°¾ê¸° (ì—­ì¶”ì )\n",
    "                    for block in blocks:\n",
    "                        if (block.get('BlockType') == 'PAGE' and \n",
    "                            line_block.get('Id') in [child_id for child_rel in block.get('Relationships', []) \n",
    "                                                     if child_rel.get('Type') == 'CHILD' \n",
    "                                                     for child_id in child_rel.get('Ids', [])]):\n",
    "                            page_id = block.get('Id')\n",
    "                            break\n",
    "                    break\n",
    "            \n",
    "            # í˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²« ë²ˆì§¸ í˜ì´ì§€ë¡œ ë°°ì •\n",
    "            if not page_id and pages_data:\n",
    "                page_id = list(pages_data.keys())[0]\n",
    "            \n",
    "            # ë¼ì¸ ì •ë³´ ì¶”ì¶œ\n",
    "            if page_id and page_id in pages_data:\n",
    "                geometry = line_block.get('Geometry', {})\n",
    "                bbox = geometry.get('BoundingBox', {})\n",
    "                text = (line_block.get('Text') or '').strip()\n",
    "                \n",
    "                if text and bbox:\n",
    "                    left = bbox.get('Left', 0.0)\n",
    "                    top = bbox.get('Top', 0.0)\n",
    "                    width = bbox.get('Width', 0.0)\n",
    "                    cx = left + width / 2.0\n",
    "                    \n",
    "                    line_info = {\n",
    "                        'x': left,\n",
    "                        'y': top,\n",
    "                        'cx': cx,\n",
    "                        'text': text,\n",
    "                        'confidence': line_block.get('Confidence', 0.0)\n",
    "                    }\n",
    "                    pages_data[page_id]['lines'].append(line_info)\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¬êµ¬ì„± (í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì ìš©)\n",
    "        all_text_parts = []\n",
    "        page_results = []\n",
    "        \n",
    "        for page_id, page_data in pages_data.items():\n",
    "            page_num = page_data['page_number']\n",
    "            lines = page_data['lines']\n",
    "            \n",
    "            if lines:\n",
    "                # í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì¬êµ¬ì„±\n",
    "                lines_sorted = reconstruct_layout_by_page(lines, page_num, layout_settings)\n",
    "                \n",
    "                # í˜ì´ì§€ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "                page_text = \"\\n\".join(line['text'] for line in lines_sorted)\n",
    "                all_text_parts.append(f\"--- í˜ì´ì§€ {page_num} ---\\n{page_text}\")\n",
    "                \n",
    "                # ì‹ ë¢°ë„ í†µê³„\n",
    "                confidences = [line['confidence'] for line in lines if line['confidence'] > 0]\n",
    "                avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0\n",
    "                \n",
    "                # ë ˆì´ì•„ì›ƒ íƒ€ì… ê²°ì •\n",
    "                layout_type = 'full_page' if (page_num == 1 and layout_settings and layout_settings.get('first_page_layout') == 'full_page') else 'two_column'\n",
    "                \n",
    "                page_results.append({\n",
    "                    'page': page_num,\n",
    "                    'text': page_text,\n",
    "                    'lines_count': len(lines),\n",
    "                    'layout_type': layout_type,\n",
    "                    'avg_confidence': avg_confidence,\n",
    "                    'char_count': len(page_text)\n",
    "                })\n",
    "        \n",
    "        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©\n",
    "        result['text'] = '\\n\\n'.join(all_text_parts)\n",
    "        result['page_results'] = page_results\n",
    "        \n",
    "        # ì‹ ë¢°ë„ í†µê³„ ê³„ì‚°\n",
    "        all_confidences = []\n",
    "        for page_data in pages_data.values():\n",
    "            for line in page_data['lines']:\n",
    "                if line['confidence'] > 0:\n",
    "                    all_confidences.append(line['confidence'])\n",
    "        \n",
    "        if all_confidences:\n",
    "            result['metadata']['confidence_stats'] = {\n",
    "                'average': sum(all_confidences) / len(all_confidences),\n",
    "                'min': min(all_confidences),\n",
    "                'max': max(all_confidences),\n",
    "                'count': len(all_confidences)\n",
    "            }\n",
    "        \n",
    "        # í‘œ íŒŒì‹± (EXPORT_TABLES_FORMSê°€ Trueì¸ ê²½ìš°)\n",
    "        if globals().get('EXPORT_TABLES_FORMS', False):\n",
    "            result['tables'] = parse_tables(textract_response)\n",
    "            result['forms'] = parse_forms(textract_response)\n",
    "        \n",
    "        result['success'] = True\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê²°ê³¼ êµ¬ì¡°ì²´ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return result\n",
    "\n",
    "def parse_tables(textract_response):\n",
    "    \"\"\"í‘œ ë¸”ë¡ íŒŒì‹± (TSV/CSV í˜•íƒœë¡œ ë³€í™˜)\"\"\"\n",
    "    tables = []\n",
    "    \n",
    "    try:\n",
    "        blocks = textract_response.get('Blocks', [])\n",
    "        \n",
    "        # TABLE ë¸”ë¡ ì°¾ê¸°\n",
    "        for block in blocks:\n",
    "            if block.get('BlockType') == 'TABLE':\n",
    "                table_data = {\n",
    "                    'id': block.get('Id'),\n",
    "                    'confidence': block.get('Confidence', 0.0),\n",
    "                    'rows': [],\n",
    "                    'tsv': '',\n",
    "                    'csv': ''\n",
    "                }\n",
    "                \n",
    "                # í…Œì´ë¸” ì…€ ì •ë³´ ìˆ˜ì§‘\n",
    "                relationships = block.get('Relationships', [])\n",
    "                cell_ids = []\n",
    "                for rel in relationships:\n",
    "                    if rel.get('Type') == 'CHILD':\n",
    "                        cell_ids.extend(rel.get('Ids', []))\n",
    "                \n",
    "                # ì…€ ë¸”ë¡ë“¤ ì°¾ê¸°\n",
    "                cells = []\n",
    "                for cell_id in cell_ids:\n",
    "                    for b in blocks:\n",
    "                        if b.get('Id') == cell_id and b.get('BlockType') == 'CELL':\n",
    "                            cells.append(b)\n",
    "                \n",
    "                # í–‰/ì—´ ì •ë³´ë¡œ ì •ë ¬\n",
    "                cells.sort(key=lambda c: (c.get('RowIndex', 0), c.get('ColumnIndex', 0)))\n",
    "                \n",
    "                # í–‰ë³„ë¡œ ê·¸ë£¹í™”\n",
    "                rows = {}\n",
    "                for cell in cells:\n",
    "                    row_idx = cell.get('RowIndex', 0)\n",
    "                    col_idx = cell.get('ColumnIndex', 0)\n",
    "                    \n",
    "                    if row_idx not in rows:\n",
    "                        rows[row_idx] = {}\n",
    "                    \n",
    "                    # ì…€ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                    cell_text = \"\"\n",
    "                    cell_relationships = cell.get('Relationships', [])\n",
    "                    for rel in cell_relationships:\n",
    "                        if rel.get('Type') == 'CHILD':\n",
    "                            for word_id in rel.get('Ids', []):\n",
    "                                for b in blocks:\n",
    "                                    if b.get('Id') == word_id and b.get('BlockType') == 'WORD':\n",
    "                                        cell_text += (b.get('Text', '') + \" \")\n",
    "                    \n",
    "                    rows[row_idx][col_idx] = cell_text.strip()\n",
    "                \n",
    "                # TSV/CSV ìƒì„±\n",
    "                tsv_rows = []\n",
    "                csv_rows = []\n",
    "                for row_idx in sorted(rows.keys()):\n",
    "                    row_data = rows[row_idx]\n",
    "                    max_col = max(row_data.keys()) if row_data else 0\n",
    "                    \n",
    "                    tsv_row = []\n",
    "                    csv_row = []\n",
    "                    for col_idx in range(max_col + 1):\n",
    "                        cell_text = row_data.get(col_idx, \"\")\n",
    "                        tsv_row.append(cell_text)\n",
    "                        csv_row.append(f'\"{cell_text}\"' if ',' in cell_text else cell_text)\n",
    "                    \n",
    "                    tsv_rows.append('\\t'.join(tsv_row))\n",
    "                    csv_rows.append(','.join(csv_row))\n",
    "                \n",
    "                table_data['tsv'] = '\\n'.join(tsv_rows)\n",
    "                table_data['csv'] = '\\n'.join(csv_rows)\n",
    "                table_data['rows'] = list(rows.values())\n",
    "                \n",
    "                tables.append(table_data)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ í‘œ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    return tables\n",
    "\n",
    "def parse_forms(textract_response):\n",
    "    \"\"\"í¼ ë¸”ë¡ íŒŒì‹± (í‚¤-ê°’ ìŒ ì¶”ì¶œ)\"\"\"\n",
    "    forms = []\n",
    "    \n",
    "    try:\n",
    "        blocks = textract_response.get('Blocks', [])\n",
    "        \n",
    "        # KEY_VALUE_SET ë¸”ë¡ ì°¾ê¸°\n",
    "        for block in blocks:\n",
    "            if block.get('BlockType') == 'KEY_VALUE_SET':\n",
    "                entity_types = block.get('EntityTypes', [])\n",
    "                \n",
    "                if 'KEY' in entity_types:\n",
    "                    # í‚¤ ë¸”ë¡ ì²˜ë¦¬\n",
    "                    key_text = \"\"\n",
    "                    value_text = \"\"\n",
    "                    \n",
    "                    # í‚¤ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                    relationships = block.get('Relationships', [])\n",
    "                    for rel in relationships:\n",
    "                        if rel.get('Type') == 'CHILD':\n",
    "                            for word_id in rel.get('Ids', []):\n",
    "                                for b in blocks:\n",
    "                                    if b.get('Id') == word_id and b.get('BlockType') == 'WORD':\n",
    "                                        key_text += (b.get('Text', '') + \" \")\n",
    "                        elif rel.get('Type') == 'VALUE':\n",
    "                            # ì—°ê²°ëœ ê°’ ë¸”ë¡ ì°¾ê¸°\n",
    "                            for value_id in rel.get('Ids', []):\n",
    "                                for b in blocks:\n",
    "                                    if b.get('Id') == value_id and b.get('BlockType') == 'KEY_VALUE_SET':\n",
    "                                        value_relationships = b.get('Relationships', [])\n",
    "                                        for value_rel in value_relationships:\n",
    "                                            if value_rel.get('Type') == 'CHILD':\n",
    "                                                for value_word_id in value_rel.get('Ids', []):\n",
    "                                                    for word_b in blocks:\n",
    "                                                        if (word_b.get('Id') == value_word_id and \n",
    "                                                            word_b.get('BlockType') == 'WORD'):\n",
    "                                                            value_text += (word_b.get('Text', '') + \" \")\n",
    "                    \n",
    "                    if key_text.strip():\n",
    "                        forms.append({\n",
    "                            'key': key_text.strip(),\n",
    "                            'value': value_text.strip(),\n",
    "                            'confidence': block.get('Confidence', 0.0)\n",
    "                        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ í¼ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    return forms\n",
    "\n",
    "print(\"ğŸ“¦ í–¥ìƒëœ ì½˜í…ì¸  ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ (í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ + í‘œ/í¼ ì§€ì›)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25937a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ì¶”ì¶œ ëª¨ë“œ ì„¤ì • ì™„ë£Œ â†’ FAST_PREVIEW=False, ASYNC_WAIT=True, ZOOM=3.0\n",
      "í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì„¤ì • ì™„ë£Œ â†’ PAGE_SPECIFIC_LAYOUT=True, FIRST_PAGE_LAYOUT=full_page, SAVE_PAGE_RESULTS=True\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ ë¬¸ì„œ ì¶”ì¶œ í† ê¸€ ì„¤ì • + í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ í™œì„±í™”\n",
    "import os\n",
    "TEXTRACT_FAST_PREVIEW = False\n",
    "TEXTRACT_ASYNC_WAIT = True\n",
    "ENABLE_PAGE_SPECIFIC_LAYOUT = True\n",
    "FIRST_PAGE_LAYOUT = 'full_page'\n",
    "SAVE_PAGE_RESULTS = True\n",
    "os.environ['TEXTRACT_PREVIEW_ZOOM'] = os.getenv('TEXTRACT_PREVIEW_ZOOM', '3.0')\n",
    "print(f\"ì „ì²´ ì¶”ì¶œ ëª¨ë“œ ì„¤ì • ì™„ë£Œ â†’ FAST_PREVIEW={TEXTRACT_FAST_PREVIEW}, ASYNC_WAIT={TEXTRACT_ASYNC_WAIT}, ZOOM={os.environ['TEXTRACT_PREVIEW_ZOOM']}\")\n",
    "print(f\"í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì„¤ì • ì™„ë£Œ â†’ PAGE_SPECIFIC_LAYOUT={ENABLE_PAGE_SPECIFIC_LAYOUT}, FIRST_PAGE_LAYOUT={FIRST_PAGE_LAYOUT}, SAVE_PAGE_RESULTS={SAVE_PAGE_RESULTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48068e94",
   "metadata": {},
   "source": [
    "### ì „ì²´ ë¬¸ì„œ ì¶”ì¶œ ì‹¤í–‰ í† ê¸€ (í”„ë¦¬ë·° ë¹„í™œì„±í™” + ë¹„ë™ê¸° ëŒ€ê¸°)\n",
    "\n",
    "- ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ë©´ í”„ë¦¬ë·°ë¥¼ ë„ê³ (`TEXTRACT_FAST_PREVIEW=false`), ë¹„ë™ê¸° ì‘ì—… ì™„ë£Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤(`TEXTRACT_ASYNC_WAIT=true`).\n",
    "- í•œêµ­ì–´ OCR í’ˆì§ˆì„ ìœ„í•´ ì´ë¯¸ì§€ ê²½ë¡œ ì‚¬ìš© ì‹œ í™•ëŒ€ ë°°ìœ¨(zoom)ì„ 3.0 ì´ìƒìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0aa2b",
   "metadata": {},
   "source": [
    "## 7) ë©”ì¸ ì‹¤í–‰ (result ìƒì„±)\n",
    "\n",
    "- íŒŒì¼ í¬ê¸°/ë³µì¡ë„/í† ê¸€ì— ë”°ë¼ í”„ë¦¬ë·° ë˜ëŠ” ì•ˆì • ëª¨ë“œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "- í•­ìƒ `result` ë”•ì…”ë„ˆë¦¬ë¥¼ ì„¤ì •í•˜ì—¬ ë‹¤ìŒ ê²€ì¦/ì €ì¥ ì…€ì´ ë™ì‘í•˜ë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d372d",
   "metadata": {},
   "source": [
    "## 6.5) ì‹¤í–‰ ëŒ€ìƒ íŒŒì¼/ë³µì¡ë„ ì„¤ì •\n",
    "\n",
    "- ì•„ë˜ ì…€ì—ì„œ `test_file`ê³¼ `complexity`ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "- ê¸°ë³¸ê°’: `jupyter_notebook/data/input_docs` í´ë”ì˜ ìµœì‹  PDFë¥¼ ìë™ ì„ íƒ, ë³µì¡ë„ëŠ” íŒŒì¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ê°„ë‹¨ ì¶”ì •í•©ë‹ˆë‹¤.\n",
    "- í•„ìš” ì‹œ ê²½ë¡œ/ë³µì¡ë„ë¥¼ ì§ì ‘ ì§€ì •í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9703ce83",
   "metadata": {},
   "source": [
    "## 6.6) ê°•ì œ ëŒ€ìƒ ì„¤ì • ë° í† ê¸€ (ìš”ì²­ ë°˜ì˜)\n",
    "\n",
    "- ìš”ì²­í•˜ì‹  íŒŒì¼ë¡œ ê³ ì •: `/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf`\n",
    "- ë³µì¡ë„ëŠ” ê°•ì œë¡œ `complex`ë¡œ ì§€ì •í•´ ë ˆì´ì•„ì›ƒ(2ì—´ ë“±) ëŒ€ì‘ ê²½ë¡œë¥¼ ìš°ì„ í•©ë‹ˆë‹¤.\n",
    "- ë¹ ë¥¸ ê²°ê³¼ í™•ì¸ì„ ìœ„í•´ í”„ë¦¬ë·° ìš°ì„ , ë¹„ë™ê¸° ëŒ€ê¸°ëŠ” ë¹„í™œì„±í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf37ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¤ì • ì™„ë£Œ â†’ test_file=/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "complexity=complex, TEXTRACT_FAST_PREVIEW=true, TEXTRACT_ASYNC_WAIT=false, TEXTRACT_PREVIEW_PAGES=2\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤í–‰ ëŒ€ìƒ ê°•ì œ ì˜¤ë²„ë¼ì´ë“œ (ìš”ì²­ ë¬¸ì„œ)\n",
    "# - ì§€ì • ë¬¸ì„œ: /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
    "# - ë³µì¡ë„: complex (ë ˆì´ì•„ì›ƒ ë¶„ì„ ê°•ì¡°)\n",
    "# - ë¹„ë™ê¸° ëŒ€ê¸° ë¹„í™œì„±í™” + í”„ë¦¬ë·° í™œì„±í™” (ë¹ ë¥¸ í”¼ë“œë°±)\n",
    "import os\n",
    "\n",
    "forced_path = \"/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\"\n",
    "if not os.path.exists(forced_path):\n",
    "    print(f\"ì˜¤ë¥˜: ì§€ì •ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ â†’ {forced_path}\")\n",
    "else:\n",
    "    test_file = forced_path\n",
    "    complexity = 'complex'\n",
    "    # í”„ë¦¬ë·° ìš°ì„ , ë¹„ë™ê¸° ëŒ€ê¸°ëŠ” í•˜ì§€ ì•ŠìŒ\n",
    "    os.environ['TEXTRACT_FAST_PREVIEW'] = 'true'\n",
    "    os.environ['TEXTRACT_ASYNC_WAIT'] = 'false'\n",
    "    # í”„ë¦¬ë·° í˜ì´ì§€ ìˆ˜ (í•„ìš”ì‹œ ì¡°ì •)\n",
    "    os.environ['TEXTRACT_PREVIEW_PAGES'] = os.environ.get('TEXTRACT_PREVIEW_PAGES', '2')\n",
    "    print(f\"ì„¤ì • ì™„ë£Œ â†’ test_file={test_file}\")\n",
    "    print(f\"complexity={complexity}, TEXTRACT_FAST_PREVIEW={os.getenv('TEXTRACT_FAST_PREVIEW')}, TEXTRACT_ASYNC_WAIT={os.getenv('TEXTRACT_ASYNC_WAIT')}, TEXTRACT_PREVIEW_PAGES={os.getenv('TEXTRACT_PREVIEW_PAGES')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e138388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°•ì œ ì„¤ì •: test_file = /home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf\n",
      "ê°•ì œ ì„¤ì •: complexity = 'complex'\n",
      "í† ê¸€: FAST_PREVIEW=False, PREVIEW_PAGES=2, ASYNC_WAIT=False (ì´ë¯¸ì§€ ê¸°ë°˜ ì „ì²´ ì²˜ë¦¬)\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤í–‰ ëŒ€ìƒ íŒŒì¼/ë³µì¡ë„ ì„¤ì • (ê°•ì œ test.pdf + í† ê¸€ ì„¤ì •)\n",
    "import os\n",
    "\n",
    "# ì‚¬ìš©ì ì§€ì • ëŒ€ìƒ\n",
    "forced_path = '/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf'\n",
    "if os.path.exists(forced_path):\n",
    "    test_file = forced_path\n",
    "    print(f\"ê°•ì œ ì„¤ì •: test_file = {test_file}\")\n",
    "else:\n",
    "    print(f\"ê²½ê³ : ì§€ì •ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ â†’ {forced_path}\")\n",
    "    # ìœ ì§€: ê¸°ì¡´ ìë™ íƒìƒ‰ ë¡œì§ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ test_file ë¯¸ì„¤ì • ìƒíƒœë¡œ ë‘ \n",
    "\n",
    "# ë³µì¡ë„ ê°•ì œ (ë ˆì´ì•„ì›ƒ ê³ ë ¤)\n",
    "complexity = 'complex'\n",
    "print(\"ê°•ì œ ì„¤ì •: complexity = 'complex'\")\n",
    "\n",
    "# ì´ë¯¸ì§€ ê¸°ë°˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í† ê¸€ ì„¤ì • (í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì¸ì‹ í™œì„±í™”)\n",
    "TEXTRACT_FAST_PREVIEW = False  # ì „ì²´ ì¶”ì¶œ ëª¨ë“œ\n",
    "TEXTRACT_PREVIEW_PAGES = int(os.getenv('TEXTRACT_PREVIEW_PAGES', '2'))\n",
    "TEXTRACT_ASYNC_WAIT = False  # S3 ë¹„ë™ê¸° ëŒ€ì‹  ì´ë¯¸ì§€ ê¸°ë°˜ ì²˜ë¦¬\n",
    "print(f\"í† ê¸€: FAST_PREVIEW={TEXTRACT_FAST_PREVIEW}, PREVIEW_PAGES={TEXTRACT_PREVIEW_PAGES}, ASYNC_WAIT={TEXTRACT_ASYNC_WAIT} (ì´ë¯¸ì§€ ê¸°ë°˜ ì „ì²´ ì²˜ë¦¬)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b83521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì¸ ì‹¤í–‰: í”„ë¦¬ë·°/ì•ˆì • ëª¨ë“œ ì„ íƒ ë° result ìƒì„± (ë‹¤ì¤‘í˜ì´ì§€ PDFëŠ” ì „ì²´ ìš°ì„ ) - í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì§€ì›\n",
    "import os, time\n",
    "\n",
    "print(\"ğŸš€ RAG íŒŒì´í”„ë¼ì¸ Textract (í”„ë¦¬ë·°/ì•ˆì •) í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'test_file' in locals() and 'complexity' in locals():\n",
    "    if os.path.exists(test_file):\n",
    "        file_size = os.path.getsize(test_file)\n",
    "        is_pdf = test_file.lower().endswith('.pdf')\n",
    "        pages = None\n",
    "        if is_pdf:\n",
    "            try:\n",
    "                import fitz\n",
    "                with fitz.open(test_file) as _d:\n",
    "                    pages = len(_d)\n",
    "            except Exception:\n",
    "                pages = None\n",
    "        \n",
    "        # ë ˆì´ì•„ì›ƒ ì„¤ì • ì¤€ë¹„\n",
    "        layout_settings = {\n",
    "            'enable_page_specific': globals().get('ENABLE_PAGE_SPECIFIC_LAYOUT', True),\n",
    "            'first_page_layout': globals().get('FIRST_PAGE_LAYOUT', 'full_page')\n",
    "        }\n",
    "        \n",
    "        print(f\"\udcc2 íŒŒì¼: {test_file}\")\n",
    "        print(f\"ğŸ“ í¬ê¸°: {file_size/1024/1024:.2f}MB\")\n",
    "        print(f\"ğŸ“„ í˜ì´ì§€: {pages if pages else 'ì•Œ ìˆ˜ ì—†ìŒ'}\")\n",
    "        print(f\"ğŸ”§ ë³µì¡ë„: {complexity}\")\n",
    "        print(f\"ğŸ¯ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: {layout_settings['enable_page_specific']}\")\n",
    "        print(f\"ğŸ“‹ ì²« í˜ì´ì§€ ëª¨ë“œ: {layout_settings['first_page_layout']}\")\n",
    "        print()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ì²˜ë¦¬ ì „ëµ ê²°ì • (í”„ë¦¬ë·° ê°•ì œ ë˜ëŠ” ì •ì±…ë³„)\n",
    "        if TEXTRACT_FAST_PREVIEW:\n",
    "            # í”„ë¦¬ë·° ëª¨ë“œ ê°•ì œ\n",
    "            print(\"ğŸ” í”„ë¦¬ë·° ëª¨ë“œ ê°•ì œ í™œì„±í™”\")\n",
    "            result = extract_text_fast_preview(test_file, TEXTRACT_PREVIEW_PAGES, layout_settings)\n",
    "            if result and result.get('full_text'):\n",
    "                result['success'] = True\n",
    "                result['metadata'] = result.get('metadata', {})\n",
    "                result['metadata']['processing_mode'] = 'preview'\n",
    "                result['metadata']['source_file'] = test_file\n",
    "                result['metadata']['layout_settings'] = layout_settings\n",
    "            else:\n",
    "                result = {'success': False, 'error': 'í”„ë¦¬ë·° ì¶”ì¶œ ì‹¤íŒ¨', 'metadata': {'processing_mode': 'preview_failed'}}\n",
    "        \n",
    "        elif pages and pages >= 5:\n",
    "            # ë‹¤ì¤‘ í˜ì´ì§€ PDFëŠ” ì´ë¯¸ì§€ ê¸°ë°˜ ì „ì²´ ì²˜ë¦¬ ìš°ì„ \n",
    "            print(f\"\udcda ë‹¤ì¤‘ í˜ì´ì§€ PDF ({pages}p) â†’ ì´ë¯¸ì§€ ê¸°ë°˜ ì „ì²´ ì²˜ë¦¬\")\n",
    "            result = extract_full_via_images(test_file, layout_settings)\n",
    "            if result and result.get('full_text'):\n",
    "                result['success'] = True\n",
    "                result['metadata'] = result.get('metadata', {})\n",
    "                result['metadata']['processing_mode'] = 'full_image'\n",
    "                result['metadata']['source_file'] = test_file\n",
    "                result['metadata']['layout_settings'] = layout_settings\n",
    "            else:\n",
    "                result = {'success': False, 'error': 'ì´ë¯¸ì§€ ê¸°ë°˜ ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨', 'metadata': {'processing_mode': 'full_image_failed'}}\n",
    "        \n",
    "        else:\n",
    "            # ì•ˆì • ì²˜ë¦¬ ëª¨ë“œ (ì „ëµë³„)\n",
    "            strat = _determine_stable_processing_strategy(file_size, complexity)\n",
    "            print(f\"ğŸ“‹ ì „ëµ: {strat['name']} (ì˜ˆìƒ ì‹œê°„: {strat['expected_time']})\")\n",
    "            \n",
    "            if strat['method'] == 'async':\n",
    "                textract_response = _process_async_for_rag(test_file, layout_settings, strat)\n",
    "            else:\n",
    "                textract_response = _process_sync_for_rag(test_file, layout_settings, strat)\n",
    "            \n",
    "            # ì‘ë‹µ ì²˜ë¦¬\n",
    "            if textract_response.get('_error'):\n",
    "                result = {\n",
    "                    'success': False,\n",
    "                    'error': textract_response['_error'],\n",
    "                    'pipeline_stage': textract_response.get('_stage', 'unknown'),\n",
    "                    'metadata': {'processing_mode': 'stable_failed', 'strategy': strat}\n",
    "                }\n",
    "            elif textract_response.get('extraction_method'):\n",
    "                # ì´ë¯¸ ì™„ì„±ëœ result (í”„ë¦¬ë·° ë“±)\n",
    "                result = textract_response\n",
    "                result['metadata']['processing_mode'] = 'stable_complete'\n",
    "                result['metadata']['strategy'] = strat\n",
    "                result['metadata']['layout_settings'] = layout_settings\n",
    "            else:\n",
    "                # Textract ì‘ë‹µì„ resultë¡œ ë³€í™˜\n",
    "                result = _build_result_from_response(\n",
    "                    textract_response, \n",
    "                    extraction_method=strat['name'],\n",
    "                    source_file_path=test_file,\n",
    "                    layout_settings=layout_settings\n",
    "                )\n",
    "                result['metadata']['processing_mode'] = 'stable_textract'\n",
    "                result['metadata']['strategy'] = strat\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # ê²°ê³¼ ìš”ì•½\n",
    "        if result.get('success'):\n",
    "            text_len = len(result.get('text', ''))\n",
    "            print(f\"âœ… ì²˜ë¦¬ ì„±ê³µ: {text_len:,}ì ì¶”ì¶œ ({elapsed:.1f}ì´ˆ)\")\n",
    "            print(f\"\udcca ì²˜ë¦¬ ëª¨ë“œ: {result.get('metadata', {}).get('processing_mode', 'ì•Œ ìˆ˜ ì—†ìŒ')}\")\n",
    "            print(f\"ğŸ¯ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: {result.get('metadata', {}).get('layout_settings', {}).get('enable_page_specific', 'N/A')}\")\n",
    "            if result.get('page_results'):\n",
    "                print(f\"ğŸ“„ í˜ì´ì§€ ê²°ê³¼: {len(result['page_results'])}ê°œ\")\n",
    "                for pr in result['page_results'][:3]:  # ì²˜ìŒ 3í˜ì´ì§€ë§Œ í‘œì‹œ\n",
    "                    print(f\"   í˜ì´ì§€ {pr['page']}: {pr['lines_count']}ë¼ì¸, {pr['char_count']}ì, íƒ€ì…={pr.get('layout_type', 'N/A')}\")\n",
    "        else:\n",
    "            error_msg = result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')\n",
    "            print(f\"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {error_msg}\")\n",
    "            print(f\"ğŸš¨ ë‹¨ê³„: {result.get('pipeline_stage', 'ì•Œ ìˆ˜ ì—†ìŒ')}\")\n",
    "        \n",
    "        print(f\"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ\")\n",
    "        print(\"âœ… `result` ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_file}\")\n",
    "        result = {'success': False, 'error': 'File not found', 'metadata': {}}\n",
    "else:\n",
    "    print(\"âŒ test_file ë˜ëŠ” complexityê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    result = {'success': False, 'error': 'Missing configuration', 'metadata': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb380c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦\n",
      "==================================================\n",
      "âœ… 'result' ë³€ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "âœ… ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ëª¨ë“  í•„ìˆ˜ í‚¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: 72555 ë¬¸ì (ë¶„ì„ ì‚¬ìš©: 50000 ë¬¸ì)\n",
      "   ğŸ“Š ì´ ë¼ì¸ ìˆ˜(ë¶„ì„): 1464\n",
      "   ğŸ“Š ìœ íš¨ ë¼ì¸ ìˆ˜(ë¶„ì„): 1446\n",
      "   ğŸ“Š í‰ê·  ë¼ì¸ ê¸¸ì´(ë¶„ì„): 34.6 ë¬¸ì\n",
      "   ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë¹„ìœ¨(ìƒ˜í”Œ): 0.0%\n",
      "   ğŸ‡ºğŸ‡¸ ì˜ì–´ ë¹„ìœ¨(ìƒ˜í”Œ): 90.7%\n",
      "âœ… RAG ë©”íƒ€ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "   ğŸ“„ íŒŒì¼ í¬ê¸°: 11.85MB\n",
      "   ğŸ”§ ë³µì¡ë„: complex\n",
      "   ğŸ“ ì²˜ë¦¬ ë°©ë²•: full-image: detect_document_text on 27 page images (zoom=dynamic, page-aware layout)\n",
      "   ğŸ“Š í…ìŠ¤íŠ¸ ë¼ì¸: 1983 | í…Œì´ë¸”: 0 | í¼: 0\n",
      "   ğŸ¯ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: False\n",
      "\n",
      "ğŸ“ˆ ì „ì²´ ì¤€ë¹„ë„(ê²½ëŸ‰): 100% (4/4)\n",
      "\n",
      "ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...\n",
      "âœ… ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥: test_20250922_162903_extracted_text.txt\n",
      "âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: test_20250922_162903_metadata.json\n",
      "âœ… ìš”ì•½ ì €ì¥: test_20250922_162903_summary.md\n",
      "\n",
      "ğŸ“‚ ì €ì¥ ìœ„ì¹˜: /home/admin/wkms-aws/jupyter_notebook/data/output_texts\n",
      "\n",
      "==================================================\n",
      "ğŸ ê²€ì¦ ë° ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦ ë° ì €ì¥ (ì‹¤íŒ¨ ì¼€ì´ìŠ¤ í¬í•¨) - ë¹ ë¥¸ ëª¨ë“œ ì§€ì› + í‘œ/í¼ ë‚´ë³´ë‚´ê¸° + í˜ì´ì§€ë³„ ì €ì¥\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ë¹ ë¥¸ ê²€ì¦ ëª¨ë“œ í† ê¸€ ë° ì œí•œ ì„¤ì •\n",
    "FAST_VALIDATION = os.getenv('FAST_VALIDATION', 'true').lower() == 'true'\n",
    "MAX_ANALYSIS_CHARS = int(os.getenv('MAX_ANALYSIS_CHARS', '50000'))  # ê¸´ í…ìŠ¤íŠ¸ ë¶„ì„ ì ˆë‹¨\n",
    "PRINT_SAMPLE_LINES = int(os.getenv('PRINT_SAMPLE_LINES', '0'))     # ì¶œë ¥ ì¤„ ìˆ˜ ì œí•œ(ê¸°ë³¸ 0: ë¯¸ì¶œë ¥)\n",
    "EXPORT_TABLES_FORMS = os.getenv('EXPORT_TABLES_FORMS', 'true').lower() == 'true'\n",
    "SAVE_PAGE_RESULTS = os.getenv('SAVE_PAGE_RESULTS', 'true').lower() == 'true'\n",
    "\n",
    "print(\"ğŸ” RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# result ë³€ìˆ˜ ì¡´ì¬ í™•ì¸\n",
    "if 'result' not in locals():\n",
    "    print(\"âŒ 'result' ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ ë©”ì¸ ì‹¤í–‰ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âœ… 'result' ë³€ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì„±ê³µ ì—¬ë¶€ í™•ì¸\n",
    "    success = result.get('success', False)\n",
    "    \n",
    "    if success:\n",
    "        print(\"âœ… ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦ (ì„±ê³µí•œ ê²½ìš°)\n",
    "        required_keys = ['success', 'text', 'blocks_count', 'api_used', 'rag_metadata', 'pipeline_stage']\n",
    "        missing_keys = [key for key in required_keys if key not in result]\n",
    "        \n",
    "        if missing_keys:\n",
    "            print(f\"âš ï¸ ëˆ„ë½ëœ í‚¤: {missing_keys}\")\n",
    "        else:\n",
    "            print(\"âœ… ëª¨ë“  í•„ìˆ˜ í‚¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦\n",
    "        text = result.get('text', '') or ''\n",
    "        text_for_analysis = text[:MAX_ANALYSIS_CHARS] if FAST_VALIDATION else text\n",
    "\n",
    "        if len(text_for_analysis) > 0:\n",
    "            total_len = len(text)\n",
    "            print(f\"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {total_len} ë¬¸ì (ë¶„ì„ ì‚¬ìš©: {len(text_for_analysis)} ë¬¸ì)\")\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬/ì†ë„ ì¹œí™”ì  ë¼ì¸ ë¶„ì„\n",
    "            lines = text_for_analysis.split('\\n')\n",
    "            non_empty_lines = [line for line in lines if line.strip()]\n",
    "            print(f\"   ğŸ“Š ì´ ë¼ì¸ ìˆ˜(ë¶„ì„): {len(lines)}\")\n",
    "            print(f\"   ğŸ“Š ìœ íš¨ ë¼ì¸ ìˆ˜(ë¶„ì„): {len(non_empty_lines)}\")\n",
    "            print(f\"   ğŸ“Š í‰ê·  ë¼ì¸ ê¸¸ì´(ë¶„ì„): {len(text_for_analysis)/max(len(non_empty_lines), 1):.1f} ë¬¸ì\")\n",
    "\n",
    "            # í•œêµ­ì–´/ì˜ì–´ ë¹„ìœ¨(ìƒ˜í”Œ ê¸°ë°˜)\n",
    "            sample = text_for_analysis\n",
    "            total_chars = len(sample.replace(' ', '').replace('\\n', ''))\n",
    "            if total_chars > 0:\n",
    "                korean_chars = sum(1 for c in sample if '\\uac00' <= c <= '\\ud7a3')\n",
    "                english_chars = sum(1 for c in sample if c.isalpha() and c.isascii())\n",
    "                korean_ratio = (korean_chars / total_chars) * 100\n",
    "                english_ratio = (english_chars / total_chars) * 100\n",
    "                print(f\"   ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë¹„ìœ¨(ìƒ˜í”Œ): {korean_ratio:.1f}%\")\n",
    "                print(f\"   ğŸ‡ºğŸ‡¸ ì˜ì–´ ë¹„ìœ¨(ìƒ˜í”Œ): {english_ratio:.1f}%\")\n",
    "\n",
    "            if PRINT_SAMPLE_LINES > 0:\n",
    "                print(\"\\n--- í…ìŠ¤íŠ¸ ìƒ˜í”Œ ---\")\n",
    "                for ln in lines[:PRINT_SAMPLE_LINES]:\n",
    "                    print(ln[:200])\n",
    "                print(\"--- ìƒ˜í”Œ ë ---\\n\")\n",
    "        else:\n",
    "            print(\"âš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # RAG ë©”íƒ€ë°ì´í„° ê²€ì¦\n",
    "        rag_metadata = result.get('rag_metadata', {})\n",
    "        if rag_metadata:\n",
    "            print(\"âœ… RAG ë©”íƒ€ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "            content_types = rag_metadata.get('content_types', {})\n",
    "            layout_info = rag_metadata.get('layout_info', {})\n",
    "            print(f\"   ğŸ“„ íŒŒì¼ í¬ê¸°: {rag_metadata.get('file_size', 0)/1024/1024:.2f}MB\")\n",
    "            print(f\"   ğŸ”§ ë³µì¡ë„: {rag_metadata.get('complexity', 'unknown')}\")\n",
    "            print(f\"   ğŸ“ ì²˜ë¦¬ ë°©ë²•: {rag_metadata.get('processing_method', 'unknown')}\")\n",
    "            print(f\"   ğŸ“Š í…ìŠ¤íŠ¸ ë¼ì¸: {content_types.get('text_lines', 0)} | í…Œì´ë¸”: {content_types.get('tables', 0)} | í¼: {content_types.get('forms', 0)}\")\n",
    "            print(f\"   ğŸ¯ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: {layout_info.get('page_specific_layout', False)}\")\n",
    "            \n",
    "            # í˜ì´ì§€ë³„ ê²°ê³¼ ì •ë³´\n",
    "            page_results = rag_metadata.get('page_results', [])\n",
    "            if page_results:\n",
    "                print(f\"   ğŸ“‘ í˜ì´ì§€ë³„ ê²°ê³¼: {len(page_results)}ê°œ í˜ì´ì§€\")\n",
    "                for i, page_info in enumerate(page_results[:3]):  # ì²˜ìŒ 3í˜ì´ì§€ë§Œ ë¯¸ë¦¬ë³´ê¸°\n",
    "                    print(f\"      í˜ì´ì§€ {page_info.get('page_number', i+1)}: {page_info.get('layout_type', 'unknown')} ë ˆì´ì•„ì›ƒ, {page_info.get('line_count', 0)} ë¼ì¸\")\n",
    "        else:\n",
    "            print(\"âš ï¸ RAG ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„ë„ (ê²½ëŸ‰ ê³„ì‚°)\n",
    "        readiness_score = 0\n",
    "        max_score = 4\n",
    "        if len(text_for_analysis) > 100:\n",
    "            readiness_score += 1\n",
    "        if rag_metadata and rag_metadata.get('ready_for_chunking'):\n",
    "            readiness_score += 1\n",
    "        if 'non_empty_lines' in locals() and len(non_empty_lines) >= 5:\n",
    "            readiness_score += 1\n",
    "        est_chunks = rag_metadata.get('estimated_chunks', 0) if rag_metadata else 0\n",
    "        if est_chunks >= 3:\n",
    "            readiness_score += 1\n",
    "        readiness_percentage = (readiness_score / max_score) * 100\n",
    "        print(f\"\\nğŸ“ˆ ì „ì²´ ì¤€ë¹„ë„(ê²½ëŸ‰): {readiness_percentage:.0f}% ({readiness_score}/{max_score})\")\n",
    "\n",
    "        # ì €ì¥\n",
    "        print(f\"\\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...\")\n",
    "        try:\n",
    "            output_dir = \"/home/admin/wkms-aws/jupyter_notebook/data/output_texts\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            source_filename = os.path.splitext(os.path.basename(test_file))[0] if 'test_file' in locals() else \"unknown\"\n",
    "            base_filename = f\"{source_filename}_{timestamp}\"\n",
    "\n",
    "            # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "            text_filename = f\"{base_filename}_extracted_text.txt\"\n",
    "            with open(os.path.join(output_dir, text_filename), 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            print(f\"âœ… ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥: {text_filename}\")\n",
    "\n",
    "            # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "            page_results = rag_metadata.get('page_results', []) if rag_metadata else []\n",
    "            if SAVE_PAGE_RESULTS and page_results:\n",
    "                for page_info in page_results:\n",
    "                    page_num = page_info.get('page_number', 1)\n",
    "                    page_text = page_info.get('text', '')\n",
    "                    page_filename = f\"{base_filename}_page{page_num:02d}.txt\"\n",
    "                    with open(os.path.join(output_dir, page_filename), 'w', encoding='utf-8') as f:\n",
    "                        f.write(f\"# í˜ì´ì§€ {page_num}\\n\")\n",
    "                        f.write(f\"# ë ˆì´ì•„ì›ƒ: {page_info.get('layout_type', 'unknown')}\\n\")\n",
    "                        f.write(f\"# ë¼ì¸ ìˆ˜: {page_info.get('line_count', 0)}\\n\")\n",
    "                        f.write(f\"# ì¶”ì • ì—´ ìˆ˜: {page_info.get('estimated_columns', 0)}\\n\")\n",
    "                        f.write(f\"# ì²˜ë¦¬ ë°©ë²•: {page_info.get('processing_method', 'unknown')}\\n\")\n",
    "                        f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "                        f.write(page_text)\n",
    "                \n",
    "                # í˜ì´ì§€ë³„ ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "                pages_meta_filename = f\"{base_filename}_pages_metadata.json\"\n",
    "                with open(os.path.join(output_dir, pages_meta_filename), 'w', encoding='utf-8') as f:\n",
    "                    json.dump(page_results, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"âœ… í˜ì´ì§€ë³„ ê²°ê³¼ ì €ì¥: {len(page_results)}ê°œ í˜ì´ì§€, ë©”íƒ€ë°ì´í„°: {pages_meta_filename}\")\n",
    "\n",
    "            # ë©”íƒ€ë°ì´í„° ì €ì¥ (ê²½ëŸ‰)\n",
    "            metadata_filename = f\"{base_filename}_metadata.json\"\n",
    "            metadata_to_save = {\n",
    "                \"success\": result.get('success', False),\n",
    "                \"api_used\": result.get('api_used', 'unknown'),\n",
    "                \"blocks_count\": result.get('blocks_count', 0),\n",
    "                \"text_length\": len(text),\n",
    "                \"pipeline_stage\": result.get('pipeline_stage', 'unknown'),\n",
    "                \"rag_metadata\": rag_metadata or {},\n",
    "                \"fast_validation\": FAST_VALIDATION,\n",
    "                \"analysis_chars\": len(text_for_analysis),\n",
    "                \"readiness_percentage\": readiness_percentage,\n",
    "                \"page_results_saved\": len(page_results) if page_results else 0\n",
    "            }\n",
    "            with open(os.path.join(output_dir, metadata_filename), 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata_to_save, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_filename}\")\n",
    "\n",
    "            # í‘œ/í¼ ì €ì¥\n",
    "            if EXPORT_TABLES_FORMS:\n",
    "                if result.get('tables'):\n",
    "                    # í…Œì´ë¸”ì„ TSVì™€ CSVë¡œ ê°ê° ì €ì¥\n",
    "                    from pathlib import Path\n",
    "                    for idx, table in enumerate(result['tables'], 1):\n",
    "                        # TSV\n",
    "                        tsv_path = os.path.join(output_dir, f\"{base_filename}_table{idx}.tsv\")\n",
    "                        with open(tsv_path, 'w', encoding='utf-8') as f:\n",
    "                            for row in table:\n",
    "                                cells = [str(c).replace('\\t', ' ').replace('\\n', ' ').strip() for c in row]\n",
    "                                f.write('\\t'.join(cells) + '\\n')\n",
    "                        # CSV\n",
    "                        csv_path = os.path.join(output_dir, f\"{base_filename}_table{idx}.csv\")\n",
    "                        with open(csv_path, 'w', encoding='utf-8') as f:\n",
    "                            for row in table:\n",
    "                                cells = ['\"' + str(c).replace('\"', '\"\"') + '\"' for c in row]\n",
    "                                f.write(','.join(cells) + '\\n')\n",
    "                    print(f\"âœ… í…Œì´ë¸” {len(result['tables'])}ê°œ ì €ì¥ (TSV/CSV)\")\n",
    "                if result.get('forms'):\n",
    "                    forms_path = os.path.join(output_dir, f\"{base_filename}_forms.json\")\n",
    "                    with open(forms_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(result['forms'], f, ensure_ascii=False, indent=2)\n",
    "                    print(f\"âœ… í¼ í‚¤-ê°’ ì €ì¥: {forms_path}\")\n",
    "\n",
    "            # ìš”ì•½ ì €ì¥(ê°„ë‹¨)\n",
    "            summary_filename = f\"{base_filename}_summary.md\"\n",
    "            with open(os.path.join(output_dir, summary_filename), 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"# ê²°ê³¼ ìš”ì•½\\n\\n\")\n",
    "                f.write(f\"- ì›ë³¸ íŒŒì¼: `{test_file if 'test_file' in locals() else 'Unknown'}`\\n\")\n",
    "                f.write(f\"- ì²˜ë¦¬ ë°©ë²•: {result.get('api_used', 'unknown')}\\n\")\n",
    "                f.write(f\"- ì²˜ë¦¬ ìƒíƒœ: {'âœ… ì„±ê³µ' if result.get('success') else 'âŒ ì‹¤íŒ¨'}\\n\")\n",
    "                f.write(f\"- í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text):,} ë¬¸ì\\n\")\n",
    "                if rag_metadata:\n",
    "                    f.write(f\"- í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: {'âœ… í™œì„±í™”' if rag_metadata.get('layout_info',{}).get('page_specific_layout') else 'âŒ ë¹„í™œì„±í™”'}\\n\")\n",
    "                    f.write(f\"- í˜ì´ì§€ ìˆ˜: {len(page_results)}ê°œ\\n\")\n",
    "                    f.write(f\"- í‘œ ìˆ˜: {rag_metadata.get('content_types',{}).get('tables',0)}ê°œ\\n\")\n",
    "                    f.write(f\"- í¼ ìˆ˜: {rag_metadata.get('content_types',{}).get('forms',0)}ê°œ\\n\")\n",
    "                f.write(f\"- ì „ì²´ ì¤€ë¹„ë„(ê²½ëŸ‰): {readiness_percentage:.0f}%\\n\")\n",
    "                \n",
    "                # í˜ì´ì§€ë³„ ì •ë³´ ì¶”ê°€\n",
    "                if page_results:\n",
    "                    f.write(f\"\\n## í˜ì´ì§€ë³„ ì •ë³´\\n\\n\")\n",
    "                    for page_info in page_results:\n",
    "                        page_num = page_info.get('page_number', 1)\n",
    "                        layout_type = page_info.get('layout_type', 'unknown')\n",
    "                        line_count = page_info.get('line_count', 0)\n",
    "                        cols = page_info.get('estimated_columns', 0)\n",
    "                        f.write(f\"- í˜ì´ì§€ {page_num}: {layout_type} ë ˆì´ì•„ì›ƒ, {line_count}ë¼ì¸, {cols}ì—´\\n\")\n",
    "                        \n",
    "            print(f\"âœ… ìš”ì•½ ì €ì¥: {summary_filename}\")\n",
    "\n",
    "            print(f\"\\nğŸ“‚ ì €ì¥ ìœ„ì¹˜: {output_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    else:\n",
    "        # ì‹¤íŒ¨ ì²˜ë¦¬: ì˜¤ë¥˜ JSON ì €ì¥\n",
    "        print(\"âŒ ì²˜ë¦¬ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        error = result.get('error', 'Unknown error')\n",
    "        pipeline_stage = result.get('pipeline_stage', 'unknown')\n",
    "        print(f\"   ì˜¤ë¥˜: {error}\")\n",
    "        print(f\"   ì‹¤íŒ¨ ë‹¨ê³„: {pipeline_stage}\")\n",
    "        print(f\"\\nğŸ’¾ ì‹¤íŒ¨ ì •ë³´ ì €ì¥ ì¤‘...\")\n",
    "        try:\n",
    "            output_dir = \"/home/admin/wkms-aws/jupyter_notebook/data/output_texts\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            source_filename = os.path.splitext(os.path.basename(test_file))[0] if 'test_file' in locals() else \"unknown\"\n",
    "            error_filename = f\"{source_filename}_{timestamp}_error.json\"\n",
    "            error_info = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"source_file\": test_file if 'test_file' in locals() else \"Unknown\",\n",
    "                \"error\": error,\n",
    "                \"pipeline_stage\": pipeline_stage\n",
    "            }\n",
    "            with open(os.path.join(output_dir, error_filename), 'w', encoding='utf-8') as f:\n",
    "                json.dump(error_info, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ… ì˜¤ë¥˜ ì •ë³´ ì €ì¥: {error_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ì˜¤ë¥˜ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ ê²€ì¦ ë° ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f889aafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‘ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "ğŸ“– ì´ í˜ì´ì§€ ìˆ˜: 27\n",
      "\n",
      "ğŸ” í˜ì´ì§€ 1 ì²˜ë¦¬ ì¤‘...\n",
      "   í˜ì´ì§€ í¬ê¸°: 538.0 x 742.0 pt, ì¤Œ: 2.43\n",
      "   ì¶”ì¶œëœ ë¼ì¸ ìˆ˜: 41\n",
      "   ë ˆì´ì•„ì›ƒ: ì²« í˜ì´ì§€ (ìƒí•˜ ìˆœì„œ)\n",
      "   í…ìŠ¤íŠ¸ ê¸¸ì´: 2622 ë¬¸ì\n",
      "   ì²˜ìŒ 200ì:\n",
      "   22 3 Journal of Information Technology Services 2023# 6F, pp.1-27 https://doi.org/10.9716/KITS.2023.2.3.01 of 2/2/21 L HH HT OHE !7 EL Z7/ 2|2| 172 2 II4 TO Ambidextrous Leadership and Innovative Work\n",
      "   ì €ì¥ë¨: test_debug_page01_20250922_163111.txt\n",
      "\n",
      "ğŸ” í˜ì´ì§€ 2 ì²˜ë¦¬ ì¤‘...\n",
      "   í˜ì´ì§€ í¬ê¸°: 538.0 x 742.0 pt, ì¤Œ: 2.43\n",
      "   ì¶”ì¶œëœ ë¼ì¸ ìˆ˜: 80\n",
      "   ë ˆì´ì•„ì›ƒ: ë‘ ë²ˆì§¸ í˜ì´ì§€ (ì¢Œìš° 2ì—´: ì¢Œ39 + ìš°41)\n",
      "   í…ìŠ¤íŠ¸ ê¸¸ì´: 3578 ë¬¸ì\n",
      "   ì²˜ìŒ 200ì:\n",
      "   2 Henry Ameyaw Domfeh 1. Introduction The semiconductor industry is a complicated, cyclical sector with a highly dynamic business climate, where crucial market share and billions of dollars are involv\n",
      "   ì €ì¥ë¨: test_debug_page02_20250922_163113.txt\n",
      "\n",
      "âœ… í˜ì´ì§€ë³„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ í…ŒìŠ¤íŠ¸ - ì²« 2í˜ì´ì§€ë§Œ ì¶”ì¶œí•´ì„œ ê²°ê³¼ í™•ì¸\n",
    "import fitz\n",
    "import os\n",
    "\n",
    "test_file = '/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf'\n",
    "print(\"ğŸ“‘ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    doc = fitz.open(test_file)\n",
    "    print(f\"ğŸ“– ì´ í˜ì´ì§€ ìˆ˜: {len(doc)}\")\n",
    "    \n",
    "    for page_num in range(min(2, len(doc))):\n",
    "        print(f\"\\nğŸ” í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì¤‘...\")\n",
    "        page = doc[page_num]\n",
    "        \n",
    "        # ë™ì  ì¤Œ ê³„ì‚°\n",
    "        w_pt = float(page.rect.width)\n",
    "        h_pt = float(page.rect.height)\n",
    "        long_pt = max(w_pt, h_pt)\n",
    "        z = min(max(1800 / long_pt, 2.0), 4.0) if long_pt > 0 else 3.0\n",
    "        print(f\"   í˜ì´ì§€ í¬ê¸°: {w_pt:.1f} x {h_pt:.1f} pt, ì¤Œ: {z:.2f}\")\n",
    "        \n",
    "        # ì´ë¯¸ì§€ ë Œë”ë§ ë° OCR\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(z, z), alpha=False)\n",
    "        img = pix.tobytes(\"png\")\n",
    "        resp = textract_client.detect_document_text(Document={'Bytes': img})\n",
    "        \n",
    "        # ë¼ì¸ ë¸”ë¡ ì¶”ì¶œ\n",
    "        lines = []\n",
    "        for block in resp.get('Blocks', []):\n",
    "            if block.get('BlockType') == 'LINE' and block.get('Geometry'):\n",
    "                bbox = block['Geometry'].get('BoundingBox', {})\n",
    "                text = (block.get('Text') or '').strip()\n",
    "                if text:\n",
    "                    left = bbox.get('Left', 0.0)\n",
    "                    top = bbox.get('Top', 0.0)\n",
    "                    width = bbox.get('Width', 0.0)\n",
    "                    cx = left + width / 2.0\n",
    "                    lines.append({'x': left, 'y': top, 'cx': cx, 'text': text})\n",
    "        \n",
    "        print(f\"   ì¶”ì¶œëœ ë¼ì¸ ìˆ˜: {len(lines)}\")\n",
    "        \n",
    "        if page_num == 0:\n",
    "            # ì²« í˜ì´ì§€: ìƒí•˜ ìˆœì„œ (ì œëª©, ì €ì, ì´ˆë¡ ìˆœ)\n",
    "            lines_sorted = sorted(lines, key=lambda l: (round(l['y'], 2), l['x']))\n",
    "            layout_type = \"ì²« í˜ì´ì§€ (ìƒí•˜ ìˆœì„œ)\"\n",
    "        else:\n",
    "            # ë‘ ë²ˆì§¸ í˜ì´ì§€: ì¢Œìš° 2ì—´\n",
    "            if len(lines) >= 2:\n",
    "                cxs = [l['cx'] for l in lines]\n",
    "                cxs_sorted = sorted(set(cxs))\n",
    "                if len(cxs_sorted) >= 2:\n",
    "                    # ê°€ì¥ í° ê°„ê²©ì„ ì°¾ì•„ ë¶„í• ì  ê²°ì •\n",
    "                    gaps = [(cxs_sorted[i+1] - cxs_sorted[i], (cxs_sorted[i] + cxs_sorted[i+1])/2) for i in range(len(cxs_sorted)-1)]\n",
    "                    gaps.sort(reverse=True)\n",
    "                    threshold = gaps[0][1] if gaps else 0.5\n",
    "                    \n",
    "                    left_lines = [l for l in lines if l['cx'] <= threshold]\n",
    "                    right_lines = [l for l in lines if l['cx'] > threshold]\n",
    "                    \n",
    "                    left_sorted = sorted(left_lines, key=lambda l: (l['y'], l['x']))\n",
    "                    right_sorted = sorted(right_lines, key=lambda l: (l['y'], l['x']))\n",
    "                    \n",
    "                    lines_sorted = left_sorted + right_sorted\n",
    "                    layout_type = f\"ë‘ ë²ˆì§¸ í˜ì´ì§€ (ì¢Œìš° 2ì—´: ì¢Œ{len(left_lines)} + ìš°{len(right_lines)})\"\n",
    "                else:\n",
    "                    lines_sorted = sorted(lines, key=lambda l: (l['y'], l['x']))\n",
    "                    layout_type = \"ë‘ ë²ˆì§¸ í˜ì´ì§€ (ë‹¨ì¼ ì—´)\"\n",
    "            else:\n",
    "                lines_sorted = lines\n",
    "                layout_type = \"ë‘ ë²ˆì§¸ í˜ì´ì§€ (ë¼ì¸ ë¶€ì¡±)\"\n",
    "        \n",
    "        print(f\"   ë ˆì´ì•„ì›ƒ: {layout_type}\")\n",
    "        \n",
    "        # ê²°ê³¼ í…ìŠ¤íŠ¸\n",
    "        page_text = \"\\n\".join(l['text'] for l in lines_sorted)\n",
    "        print(f\"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(page_text)} ë¬¸ì\")\n",
    "        print(\"   ì²˜ìŒ 200ì:\")\n",
    "        print(\"   \" + page_text[:200].replace('\\n', ' ')[:200])\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ íŒŒì¼ ì €ì¥\n",
    "        output_dir = \"/home/admin/wkms-aws/jupyter_notebook/data/output_texts\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        page_filename = f\"test_debug_page{page_num+1:02d}_{timestamp}.txt\"\n",
    "        with open(os.path.join(output_dir, page_filename), 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# í˜ì´ì§€ {page_num + 1} - {layout_type}\\n\")\n",
    "            f.write(f\"# ë¼ì¸ ìˆ˜: {len(lines)}, í…ìŠ¤íŠ¸ ê¸¸ì´: {len(page_text)}\\n\")\n",
    "            f.write(\"# \" + \"=\"*60 + \"\\n\\n\")\n",
    "            f.write(page_text)\n",
    "        print(f\"   ì €ì¥ë¨: {page_filename}\")\n",
    "    \n",
    "    doc.close()\n",
    "    print(f\"\\nâœ… í˜ì´ì§€ë³„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead17450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ - ì²« í˜ì´ì§€ë§Œ ì²˜ë¦¬í•´ì„œ ë ˆì´ì•„ì›ƒ í™•ì¸\n",
    "test_file = '/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf'\n",
    "\n",
    "print(\"ğŸš€ ë¹ ë¥¸ ë ˆì´ì•„ì›ƒ í…ŒìŠ¤íŠ¸ (ì²« í˜ì´ì§€ë§Œ)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# ë ˆì´ì•„ì›ƒ ì„¤ì •\n",
    "layout_settings = {\n",
    "    'enable_page_specific': ENABLE_PAGE_SPECIFIC_LAYOUT,\n",
    "    'first_page_layout': FIRST_PAGE_LAYOUT\n",
    "}\n",
    "\n",
    "print(f\"ğŸ¯ í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ: {layout_settings['enable_page_specific']}\")\n",
    "print(f\"ğŸ“‹ ì²« í˜ì´ì§€ ëª¨ë“œ: {layout_settings['first_page_layout']}\")\n",
    "\n",
    "# ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸° (1í˜ì´ì§€ë§Œ)\n",
    "result = extract_text_fast_preview(test_file, page_limit=1, layout_settings=layout_settings)\n",
    "\n",
    "if result and result['success']:\n",
    "    print(f\"\\nâœ… ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“Š ì´ í…ìŠ¤íŠ¸: {len(result['full_text'])}ì\")\n",
    "    \n",
    "    if result['page_results']:\n",
    "        page1 = result['page_results'][0]\n",
    "        print(f\"ğŸ“„ í˜ì´ì§€ 1: {page1['lines_count']}ë¼ì¸, ë ˆì´ì•„ì›ƒ={page1['layout_type']}\")\n",
    "        \n",
    "        # ì²« 200ì ì¶œë ¥\n",
    "        text_preview = result['full_text'][:200].replace('\\n', ' ')\n",
    "        print(f\"ğŸ“ ë¯¸ë¦¬ë³´ê¸°: {text_preview}...\")\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        output_dir = \"/home/admin/wkms-aws/jupyter_notebook/data/output_texts\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        quick_filename = f\"quick_test_{timestamp}.txt\"\n",
    "        with open(os.path.join(output_dir, quick_filename), 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼\\n\")\n",
    "            f.write(f\"# ì„¤ì •: {layout_settings}\\n\")\n",
    "            f.write(\"# \" + \"=\"*50 + \"\\n\\n\")\n",
    "            f.write(result['full_text'])\n",
    "        print(f\"ğŸ’¾ ì €ì¥: {quick_filename}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ì¶”ì¶œ ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbf7e4",
   "metadata": {},
   "source": [
    "## 4. í•œêµ­ì–´ ì²­í‚¹ ë° ì„ë² ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„¤ì •\n",
    "def setup_korean_nlp():\n",
    "    \"\"\"í•œêµ­ì–´ NLP ë„êµ¬ ì´ˆê¸°í™”\"\"\"\n",
    "    try:\n",
    "        from kiwipiepy import Kiwi\n",
    "        import tiktoken\n",
    "        \n",
    "        # í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "        kiwi = Kiwi()\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        print(\"âœ… í•œêµ­ì–´ NLP ë„êµ¬ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        return kiwi, tokenizer\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ í•œêµ­ì–´ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: {e}\")\n",
    "        return None, None\n",
    "\n",
    "kiwi, tokenizer = setup_korean_nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ í•œêµ­ì–´ ìµœì í™” ì²­í‚¹\n",
    "def chunk_korean_text_for_rag(text, max_tokens=1200, overlap_ratio=0.1, rag_metadata=None):\n",
    "    \"\"\"RAG íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²­í‚¹ - ë¬¸ì„œ íŠ¹ì„± ê³ ë ¤\"\"\"\n",
    "    print(f\"âœ‚ï¸ RAG íŒŒì´í”„ë¼ì¸ í•œêµ­ì–´ ì²­í‚¹ ì‹œì‘\")\n",
    "    \n",
    "    if rag_metadata:\n",
    "        print(f\"ğŸ“‹ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ìµœì í™”:\")\n",
    "        print(f\"   íŒŒì¼ í¬ê¸°: {rag_metadata['file_size']/1024/1024:.2f}MB\")\n",
    "        print(f\"   ë³µì¡ë„: {rag_metadata['complexity']}\")\n",
    "        print(f\"   ì½˜í…ì¸  êµ¬ì„±: í…ìŠ¤íŠ¸ {rag_metadata['content_types']['text_lines']}ì¤„\")\n",
    "        \n",
    "        # ë¬¸ì„œ íŠ¹ì„±ì— ë”°ë¥¸ ì²­í‚¹ ì „ëµ ì¡°ì •\n",
    "        if rag_metadata['complexity'] == 'complex':\n",
    "            max_tokens = int(max_tokens * 1.2)  # ë³µì¡í•œ ë¬¸ì„œëŠ” ë” í° ì²­í¬\n",
    "            overlap_ratio = 0.15  # ë” ë§ì€ ê²¹ì¹¨\n",
    "            print(f\"ğŸ”§ ë³µì¡í•œ ë¬¸ì„œ ìµœì í™”: ì²­í¬ í¬ê¸° {max_tokens}, ê²¹ì¹¨ {overlap_ratio*100:.0f}%\")\n",
    "        \n",
    "        if rag_metadata['content_types']['tables'] > 0:\n",
    "            print(f\"ğŸ“Š í…Œì´ë¸” í¬í•¨ ë¬¸ì„œ: êµ¬ì¡° ë³´ì¡´ ì²­í‚¹ ì ìš©\")\n",
    "    \n",
    "    if not tokenizer:\n",
    "        print(\"âš ï¸ í† í¬ë‚˜ì´ì € ì—†ìŒ - ê¸°ë³¸ ë¬¸ì¥ ë¶„í•  ì‚¬ìš©\")\n",
    "        # ê°„ë‹¨í•œ ë¬¸ì¥ ê¸°ë°˜ ë¶„í•  (fallback)\n",
    "        sentences = text.split('.')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk + sentence) > max_tokens * 4:  # ëŒ€ëµì ì¸ ì¶”ì •\n",
    "                if current_chunk:\n",
    "                    chunks.append({\n",
    "                        \"text\": current_chunk.strip(),\n",
    "                        \"tokens_estimated\": len(current_chunk.strip()) // 4,\n",
    "                        \"chunk_type\": \"sentence_based\"\n",
    "                    })\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk += sentence + \".\"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                \"text\": current_chunk.strip(),\n",
    "                \"tokens_estimated\": len(current_chunk.strip()) // 4,\n",
    "                \"chunk_type\": \"sentence_based\"\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    # í† í° ê¸°ë°˜ ì •ë°€ ì²­í‚¹\n",
    "    print(f\"ğŸ” í† í° ê¸°ë°˜ ì •ë°€ ì²­í‚¹ (ëª©í‘œ: {max_tokens} í† í°, ê²¹ì¹¨: {overlap_ratio*100:.0f}%)\")\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - RAG íŒŒì´í”„ë¼ì¸ ìµœì í™”\n",
    "    processed_text = text.replace('\\n\\n', '\\n').replace('\\r', '')\n",
    "    sentences = processed_text.split('\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "    chunk_index = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        sentence_tokens = len(tokenizer.encode(sentence))\n",
    "        \n",
    "        if current_tokens + sentence_tokens > max_tokens and current_chunk:\n",
    "            # í˜„ì¬ ì²­í¬ ì™„ì„±\n",
    "            chunk_info = {\n",
    "                \"text\": current_chunk.strip(),\n",
    "                \"tokens\": current_tokens,\n",
    "                \"chunk_index\": chunk_index,\n",
    "                \"chunk_type\": \"token_based\",\n",
    "                \"sentences_count\": len([s for s in current_chunk.split('\\n') if s.strip()]),\n",
    "                \"overlap_with_next\": True if chunk_index > 0 else False\n",
    "            }\n",
    "            chunks.append(chunk_info)\n",
    "            \n",
    "            # ê²¹ì¹¨ ì²˜ë¦¬\n",
    "            overlap_tokens = int(max_tokens * overlap_ratio)\n",
    "            if overlap_tokens > 0:\n",
    "                overlap_sentences = current_chunk.split('\\n')[-int(overlap_tokens/50):] # ëŒ€ëµì  ê³„ì‚°\n",
    "                overlap_text = '\\n'.join(overlap_sentences)\n",
    "                current_chunk = overlap_text + \"\\n\" + sentence\n",
    "                current_tokens = len(tokenizer.encode(current_chunk))\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "                current_tokens = sentence_tokens\n",
    "            \n",
    "            chunk_index += 1\n",
    "        else:\n",
    "            current_chunk += \"\\n\" + sentence if current_chunk else sentence\n",
    "            current_tokens += sentence_tokens\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬\n",
    "    if current_chunk.strip():\n",
    "        chunk_info = {\n",
    "            \"text\": current_chunk.strip(),\n",
    "            \"tokens\": current_tokens,\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"chunk_type\": \"token_based\",\n",
    "            \"sentences_count\": len([s for s in current_chunk.split('\\n') if s.strip()]),\n",
    "            \"overlap_with_next\": False\n",
    "        }\n",
    "        chunks.append(chunk_info)\n",
    "    \n",
    "    # RAG íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ ê²€ì¦\n",
    "    total_tokens = sum(chunk[\"tokens\"] for chunk in chunks)\n",
    "    avg_tokens = total_tokens / len(chunks) if chunks else 0\n",
    "    \n",
    "    print(f\"âœ… ì²­í‚¹ ì™„ë£Œ:\")\n",
    "    print(f\"   ì´ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ\")\n",
    "    print(f\"   í‰ê·  í† í° ìˆ˜: {avg_tokens:.1f}\")\n",
    "    print(f\"   ì´ í† í° ìˆ˜: {total_tokens}\")\n",
    "    print(f\"   ì²­í‚¹ íš¨ìœ¨ì„±: {(len(chunks) / max(1, len(text)//1000)) * 100:.1f}%\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def analyze_chunks_for_rag(chunks):\n",
    "    \"\"\"RAG íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ ì²­í¬ ë¶„ì„\"\"\"\n",
    "    print(f\"\\nğŸ“Š RAG ì²­í¬ í’ˆì§ˆ ë¶„ì„:\")\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"âŒ ë¶„ì„í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return\n",
    "    \n",
    "    # ì²­í¬ í¬ê¸° ë¶„í¬\n",
    "    chunk_sizes = [chunk.get(\"tokens\", len(chunk[\"text\"])//4) for chunk in chunks]\n",
    "    min_size, max_size = min(chunk_sizes), max(chunk_sizes)\n",
    "    avg_size = sum(chunk_sizes) / len(chunk_sizes)\n",
    "    \n",
    "    print(f\"   ì²­í¬ í¬ê¸° ë¶„í¬: ìµœì†Œ {min_size}, ìµœëŒ€ {max_size}, í‰ê·  {avg_size:.1f} í† í°\")\n",
    "    \n",
    "    # ì²­í¬ ê· ì¼ì„± ê²€ì‚¬\n",
    "    size_variance = sum((size - avg_size) ** 2 for size in chunk_sizes) / len(chunk_sizes)\n",
    "    uniformity_score = max(0, 100 - (size_variance / avg_size) * 10)\n",
    "    print(f\"   ì²­í¬ ê· ì¼ì„±: {uniformity_score:.1f}% ({'ì–‘í˜¸' if uniformity_score > 70 else 'ê°œì„  í•„ìš”'})\")\n",
    "    \n",
    "    # RAG ê²€ìƒ‰ ìµœì í™” ì ìˆ˜\n",
    "    optimal_chunks = sum(1 for size in chunk_sizes if 500 <= size <= 1500)\n",
    "    optimization_score = (optimal_chunks / len(chunks)) * 100\n",
    "    print(f\"   RAG ìµœì í™” ì ìˆ˜: {optimization_score:.1f}% ({optimal_chunks}/{len(chunks)} ì²­í¬ê°€ ìµœì  ë²”ìœ„)\")\n",
    "    \n",
    "    # ê²¹ì¹¨ ì •ë³´\n",
    "    overlapped_chunks = sum(1 for chunk in chunks if chunk.get(\"overlap_with_next\", False))\n",
    "    print(f\"   ê²¹ì¹¨ ì²­í¬: {overlapped_chunks}ê°œ ({(overlapped_chunks/len(chunks))*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"avg_tokens\": avg_size,\n",
    "        \"uniformity_score\": uniformity_score,\n",
    "        \"optimization_score\": optimization_score,\n",
    "        \"ready_for_embedding\": optimization_score > 50\n",
    "    }\n",
    "\n",
    "# RAG íŒŒì´í”„ë¼ì¸ ì²­í‚¹ í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ¯ RAG íŒŒì´í”„ë¼ì¸ í•œêµ­ì–´ ì²­í‚¹ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'result' in locals() and result.get(\"success\"):\n",
    "    print(\"ğŸ“„ 11ë²ˆ ì…€ Textract ê²°ê³¼ë¥¼ ì‚¬ìš©í•œ ì²­í‚¹\")\n",
    "    \n",
    "    # RAG ë©”íƒ€ë°ì´í„° í™œìš©\n",
    "    rag_metadata = result.get(\"rag_metadata\")\n",
    "    if rag_metadata and rag_metadata.get(\"ready_for_chunking\"):\n",
    "        print(f\"âœ… ì²­í‚¹ ì¤€ë¹„ ì™„ë£Œ - ì˜ˆìƒ ì²­í¬ ìˆ˜: {rag_metadata['estimated_chunks']}ê°œ\")\n",
    "        \n",
    "        # RAG ìµœì í™” ì²­í‚¹ ì‹¤í–‰\n",
    "        chunks = chunk_korean_text_for_rag(\n",
    "            result[\"text\"], \n",
    "            max_tokens=1200, \n",
    "            overlap_ratio=0.1,\n",
    "            rag_metadata=rag_metadata\n",
    "        )\n",
    "        \n",
    "        # ì²­í¬ í’ˆì§ˆ ë¶„ì„\n",
    "        chunk_analysis = analyze_chunks_for_rag(chunks)\n",
    "        \n",
    "        # ìƒ˜í”Œ ì²­í¬ ì¶œë ¥\n",
    "        print(f\"\\n\udcd6 ìƒ˜í”Œ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "        for i, chunk in enumerate(chunks[:3]):\n",
    "            print(f\"\\nğŸ”¸ ì²­í¬ {i+1} (í† í°: {chunk.get('tokens', 'ì¶”ì •')}ê°œ):\")\n",
    "            print(f\"   {chunk['text'][:150]}...\")\n",
    "            if len(chunk['text']) > 150:\n",
    "                print(\"   ... (ë” ë§ì€ ë‚´ìš©)\")\n",
    "        \n",
    "        # RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ìƒíƒœ\n",
    "        if chunk_analysis and chunk_analysis[\"ready_for_embedding\"]:\n",
    "            print(f\"\\nâœ… RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "            print(f\"   ì„ë² ë”© ëŒ€ìƒ: {chunk_analysis['total_chunks']}ê°œ ì²­í¬\")\n",
    "            print(f\"   ìµœì í™” ì ìˆ˜: {chunk_analysis['optimization_score']:.1f}%\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ ì²­í‚¹ ìµœì í™” í•„ìš”\")\n",
    "            print(f\"   ì²­í¬ ìˆ˜ê°€ ì ê±°ë‚˜ í¬ê¸°ê°€ ë¶ˆê· ì¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "    else:\n",
    "        print(\"âš ï¸ RAG ë©”íƒ€ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ì²­í‚¹ ì¤€ë¹„ê°€ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\")\n",
    "        print(\"ê¸°ë³¸ ì²­í‚¹ì„ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        # ê¸°ë³¸ ì²­í‚¹\n",
    "        chunks = chunk_korean_text_for_rag(result[\"text\"])\n",
    "        chunk_analysis = analyze_chunks_for_rag(chunks)\n",
    "        \n",
    "        # ìƒ˜í”Œ ì¶œë ¥\n",
    "        if chunks:\n",
    "            print(f\"\\nğŸ“– ê¸°ë³¸ ì²­í‚¹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "            for i, chunk in enumerate(chunks[:2]):\n",
    "                print(f\"\\nğŸ”¸ ì²­í¬ {i+1}: {chunk['text'][:100]}...\")\n",
    "\n",
    "elif 'result' in locals():\n",
    "    print(f\"âŒ 11ë²ˆ ì…€ ì‹¤í–‰ ê²°ê³¼ì— ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤: {result.get('error', 'Unknown error')}\")\n",
    "    print(\"11ë²ˆ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âŒ 11ë²ˆ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ Textract ê²°ê³¼ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "print(f\"\\nğŸ RAG íŒŒì´í”„ë¼ì¸ ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc261835",
   "metadata": {},
   "source": [
    "## 5. ë°±ì—”ë“œ í†µí•© í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°±ì—”ë“œ ì¸ì¦\n",
    "def login_to_backend(emp_no=\"HR001\", password=\"hr123!\"):\n",
    "    \"\"\"ë°±ì—”ë“œ ì‹œìŠ¤í…œì— ë¡œê·¸ì¸\"\"\"\n",
    "    if not BACKEND_AVAILABLE:\n",
    "        print(\"âŒ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        login_url = f\"{BACKEND_URL}/api/v1/auth/login\"\n",
    "        response = requests.post(\n",
    "            login_url,\n",
    "            json={\"emp_no\": emp_no, \"password\": password},\n",
    "            timeout=10\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        token = data.get(\"access_token\")\n",
    "        \n",
    "        if token:\n",
    "            print(f\"âœ… ë¡œê·¸ì¸ ì„±ê³µ: {emp_no}\")\n",
    "            return token\n",
    "        else:\n",
    "            print(\"âŒ í† í°ì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# ë¡œê·¸ì¸ í…ŒìŠ¤íŠ¸\n",
    "access_token = login_to_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»¨í…Œì´ë„ˆ ì¡°íšŒ\n",
    "def get_containers(token):\n",
    "    \"\"\"ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ ì»¨í…Œì´ë„ˆ ëª©ë¡ ì¡°íšŒ\"\"\"\n",
    "    if not token:\n",
    "        print(\"âŒ ì¸ì¦ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        containers_url = f\"{BACKEND_URL}/api/v1/documents/containers\"\n",
    "        \n",
    "        response = requests.get(containers_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        containers = response.json()\n",
    "        print(f\"ğŸ“‚ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ: {len(containers)}ê°œ\")\n",
    "        \n",
    "        for container in containers[:3]:\n",
    "            name = container.get('container_name', 'Unknown')\n",
    "            desc = container.get('description', 'No description')\n",
    "            print(f\"  â€¢ {name}: {desc}\")\n",
    "        \n",
    "        return containers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì»¨í…Œì´ë„ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "# ì»¨í…Œì´ë„ˆ ì¡°íšŒ í…ŒìŠ¤íŠ¸\n",
    "containers = get_containers(access_token)\n",
    "selected_container = containers[0]['container_id'] if containers else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbd839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "def test_hybrid_search(token, query=\"ë¬¸ì„œ ì²˜ë¦¬\", limit=5):\n",
    "    \"\"\"ë°±ì—”ë“œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    if not token:\n",
    "        print(\"âŒ ì¸ì¦ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        search_url = f\"{BACKEND_URL}/api/v1/search\"\n",
    "        \n",
    "        search_data = {\n",
    "            \"query\": query,\n",
    "            \"limit\": limit,\n",
    "            \"search_type\": \"hybrid\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            search_url, \n",
    "            json=search_data, \n",
    "            headers=headers, \n",
    "            timeout=15\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        results = response.json()\n",
    "        documents = results.get('documents', [])\n",
    "        \n",
    "        print(f\"ğŸ” ê²€ìƒ‰ ê²°ê³¼: '{query}' â†’ {len(documents)}ê±´\")\n",
    "        \n",
    "        for i, doc in enumerate(documents[:3]):\n",
    "            title = doc.get('title', 'Unknown')\n",
    "            score = doc.get('score', 0)\n",
    "            print(f\"  {i+1}. {title} (ì ìˆ˜: {score:.3f})\")\n",
    "        \n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "search_results = test_hybrid_search(access_token, \"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c006218",
   "metadata": {},
   "source": [
    "## 6. ì„±ëŠ¥ ë° ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ ì„±ëŠ¥ ë° ê²°ê³¼ ìš”ì•½\n",
    "def print_rag_pipeline_summary():\n",
    "    \"\"\"RAG íŒŒì´í”„ë¼ì¸ Textract í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š RAG íŒŒì´í”„ë¼ì¸ Textract í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # í™˜ê²½ ì„¤ì • ìƒíƒœ\n",
    "    print(f\"ğŸ”§ ë°±ì—”ë“œ ì—°ê²°: {'âœ… ì„±ê³µ' if BACKEND_AVAILABLE else 'âŒ ì‹¤íŒ¨'}\")\n",
    "    print(f\"â˜ï¸ AWS Textract: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if textract_client else 'âŒ ì‚¬ìš© ë¶ˆê°€'}\")\n",
    "    print(f\"ğŸª£ S3 ì—°ë™: {'âœ… ì„¤ì •ë¨' if TEXTRACT_S3_BUCKET else 'âŒ ë¯¸ì„¤ì •'}\")\n",
    "    print(f\"ğŸ‡°ğŸ‡· í•œêµ­ì–´ NLP: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if kiwi and tokenizer else 'âŒ ì‚¬ìš© ë¶ˆê°€'}\")\n",
    "    \n",
    "    # RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼\n",
    "    if 'result' in locals() and result.get(\"success\"):\n",
    "        print(f\"\\nğŸ“„ RAG íŒŒì´í”„ë¼ì¸ ë¬¸ì„œ ì²˜ë¦¬: âœ… ì„±ê³µ\")\n",
    "        print(f\"   ì²˜ë¦¬ ë°©ì‹: {result['api_used']}\")\n",
    "        print(f\"   ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {len(result['text'])} ë¬¸ì\")\n",
    "        print(f\"   ë¸”ë¡ ìˆ˜: {result['blocks_count']}\")\n",
    "        \n",
    "        # RAG ë©”íƒ€ë°ì´í„° ìš”ì•½\n",
    "        if 'rag_metadata' in result:\n",
    "            meta = result['rag_metadata']\n",
    "            print(f\"\\nğŸ“‹ RAG ë©”íƒ€ë°ì´í„°:\")\n",
    "            print(f\"   íŒŒì¼ í¬ê¸°: {meta['file_size']/1024/1024:.2f}MB\")\n",
    "            print(f\"   ë¬¸ì„œ ë³µì¡ë„: {meta['complexity']}\")\n",
    "            print(f\"   ì²˜ë¦¬ ì‹œê°„: {meta['extracted_at']}\")\n",
    "            print(f\"   ì½˜í…ì¸  êµ¬ì„±:\")\n",
    "            print(f\"     - í…ìŠ¤íŠ¸ ë¼ì¸: {meta['content_types']['text_lines']}ê°œ\")\n",
    "            print(f\"     - í…Œì´ë¸”: {meta['content_types']['tables']}ê°œ\")\n",
    "            print(f\"     - í¼ í•„ë“œ: {meta['content_types']['forms']}ê°œ\")\n",
    "            print(f\"   ì²­í‚¹ ì¤€ë¹„: {'âœ… ì¤€ë¹„ë¨' if meta['ready_for_chunking'] else 'âŒ ì¤€ë¹„ ì•ˆë¨'}\")\n",
    "            print(f\"   ì˜ˆìƒ ì²­í¬ ìˆ˜: {meta['estimated_chunks']}ê°œ\")\n",
    "        \n",
    "        # ì²­í‚¹ ê²°ê³¼\n",
    "        if 'chunks' in locals() and chunks:\n",
    "            chunk_count = len(chunks)\n",
    "            avg_tokens = sum(chunk.get(\"tokens\", len(chunk[\"text\"])//4) for chunk in chunks) / chunk_count\n",
    "            print(f\"\\nâœ‚ï¸ í•œêµ­ì–´ ì²­í‚¹ ê²°ê³¼: âœ… ì„±ê³µ\")\n",
    "            print(f\"   ìƒì„±ëœ ì²­í¬: {chunk_count}ê°œ\")\n",
    "            print(f\"   í‰ê·  í† í° ìˆ˜: {avg_tokens:.1f}\")\n",
    "            \n",
    "            # ì²­í‚¹ í’ˆì§ˆ í‰ê°€\n",
    "            if 'chunk_analysis' in locals() and chunk_analysis:\n",
    "                print(f\"   ì²­í‚¹ í’ˆì§ˆ:\")\n",
    "                print(f\"     - ê· ì¼ì„±: {chunk_analysis.get('uniformity_score', 0):.1f}%\")\n",
    "                print(f\"     - RAG ìµœì í™”: {chunk_analysis.get('optimization_score', 0):.1f}%\")\n",
    "                print(f\"     - ì„ë² ë”© ì¤€ë¹„: {'âœ… ì¤€ë¹„ë¨' if chunk_analysis.get('ready_for_embedding', False) else 'âš ï¸ ìµœì í™” í•„ìš”'}\")\n",
    "        else:\n",
    "            print(f\"\\nâœ‚ï¸ í•œêµ­ì–´ ì²­í‚¹: âŒ ì‹¤íŒ¨ ë˜ëŠ” ë¯¸ì‹¤í–‰\")\n",
    "    else:\n",
    "        error_msg = result.get('error', 'Unknown error') if 'result' in locals() else 'ë¯¸ì‹¤í–‰'\n",
    "        pipeline_stage = result.get('pipeline_stage', 'unknown') if 'result' in locals() else 'unknown'\n",
    "        print(f\"\\nğŸ“„ RAG íŒŒì´í”„ë¼ì¸ ë¬¸ì„œ ì²˜ë¦¬: âŒ ì‹¤íŒ¨\")\n",
    "        print(f\"   ì˜¤ë¥˜: {error_msg}\")\n",
    "        print(f\"   ì‹¤íŒ¨ ë‹¨ê³„: {pipeline_stage}\")\n",
    "    \n",
    "    # ë°±ì—”ë“œ í†µí•© ìƒíƒœ\n",
    "    if access_token:\n",
    "        print(f\"\\nğŸ” ë°±ì—”ë“œ í†µí•©: âœ… ì„±ê³µ\")\n",
    "        print(f\"   ì¸ì¦ í† í°: í™œì„±\")\n",
    "        container_count = len(containers) if 'containers' in locals() and containers else 0\n",
    "        search_count = len(search_results) if 'search_results' in locals() and search_results else 0\n",
    "        print(f\"   ì»¨í…Œì´ë„ˆ ì¡°íšŒ: {container_count}ê°œ\")\n",
    "        print(f\"   ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: {search_count}ê±´\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ” ë°±ì—”ë“œ í†µí•©: âŒ ì‹¤íŒ¨\")\n",
    "        print(\"   ì¸ì¦ í† í° ì—†ìŒ\")\n",
    "    \n",
    "    # RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ë„ í‰ê°€\n",
    "    print(f\"\\nğŸ¯ RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ë„ í‰ê°€:\")\n",
    "    \n",
    "    # ê¸°ë³¸ ì¸í”„ë¼\n",
    "    infra_score = 0\n",
    "    if textract_client: infra_score += 25\n",
    "    if TEXTRACT_S3_BUCKET: infra_score += 25\n",
    "    if kiwi and tokenizer: infra_score += 25\n",
    "    if access_token: infra_score += 25\n",
    "    \n",
    "    print(f\"   ì¸í”„ë¼ ì¤€ë¹„ë„: {infra_score}% ({infra_score//25}/4 êµ¬ì„±ìš”ì†Œ)\")\n",
    "    \n",
    "    # ì²˜ë¦¬ ì„±ëŠ¥\n",
    "    processing_score = 0\n",
    "    if 'result' in locals() and result.get(\"success\"):\n",
    "        processing_score += 50\n",
    "        if 'rag_metadata' in result and result['rag_metadata'].get('ready_for_chunking'):\n",
    "            processing_score += 25\n",
    "        if 'chunks' in locals() and chunks:\n",
    "            processing_score += 25\n",
    "    \n",
    "    print(f\"   ì²˜ë¦¬ ì„±ëŠ¥: {processing_score}% ({processing_score//25}/4 ë‹¨ê³„)\")\n",
    "    \n",
    "    # ì „ì²´ ì¤€ë¹„ë„\n",
    "    overall_readiness = (infra_score + processing_score) // 2\n",
    "    print(f\"   ì „ì²´ ì¤€ë¹„ë„: {overall_readiness}% \", end=\"\")\n",
    "    \n",
    "    if overall_readiness >= 80:\n",
    "        print(\"ğŸŸ¢ ìš´ì˜ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "        print(\"   ğŸ’¡ ê¶Œì¥ ì‚¬í•­: ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§ êµ¬ì¶•\")\n",
    "    elif overall_readiness >= 60:\n",
    "        print(\"ğŸŸ¡ ë¶€ë¶„ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "        print(\"   ğŸ’¡ ê¶Œì¥ ì‚¬í•­: ë¯¸ì™„ì„± êµ¬ì„±ìš”ì†Œ ë³´ì™„ í•„ìš”\")\n",
    "    else:\n",
    "        print(\"ğŸ”´ ì¤€ë¹„ ë¶€ì¡±\")\n",
    "        print(\"   ğŸ’¡ ê¶Œì¥ ì‚¬í•­: ê¸°ë³¸ ì¸í”„ë¼ êµ¬ì„±ë¶€í„° ì¬ê²€í† \")\n",
    "    \n",
    "    # ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ\n",
    "    print(f\"\\nğŸš€ RAG íŒŒì´í”„ë¼ì¸ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    \n",
    "    if overall_readiness >= 80:\n",
    "        print(\"1. ğŸ“Š ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\")\n",
    "        print(\"2. ğŸ” ì„ë² ë”© ë²¡í„° ìƒì„± ë° ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\")\n",
    "        print(\"3. ğŸ“ˆ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ êµ¬ì¶•\")\n",
    "        print(\"4. ğŸ›¡ï¸ í”„ë¡œë•ì…˜ ë³´ì•ˆ ì„¤ì • ê²€í† \")\n",
    "    elif overall_readiness >= 60:\n",
    "        print(\"1. âŒ ì‹¤íŒ¨í•œ êµ¬ì„±ìš”ì†Œ ë¬¸ì œ í•´ê²°\")\n",
    "        print(\"2. ğŸ”§ ì²­í‚¹ íŒŒë¼ë¯¸í„° ìµœì í™”\")\n",
    "        print(\"3. ğŸ“ ì˜¤ë¥˜ ì²˜ë¦¬ ë¡œì§ ê°•í™”\")\n",
    "        print(\"4. ğŸ§ª ì¶”ê°€ ë¬¸ì„œ í˜•ì‹ í…ŒìŠ¤íŠ¸\")\n",
    "    else:\n",
    "        print(\"1. ğŸ”§ AWS ê¶Œí•œ ë° S3 ì„¤ì • ì¬í™•ì¸\")\n",
    "        print(\"2. ğŸ” ë°±ì—”ë“œ ì—°ê²° ë¬¸ì œ í•´ê²°\")\n",
    "        print(\"3. ğŸ“š í•œêµ­ì–´ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\")\n",
    "        print(\"4. ğŸ—ï¸ ê¸°ë³¸ ì¸í”„ë¼ ì¬êµ¬ì„±\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ¨ RAG íŒŒì´í”„ë¼ì¸ Textract í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n",
    "    \n",
    "    return {\n",
    "        \"infra_score\": infra_score,\n",
    "        \"processing_score\": processing_score,\n",
    "        \"overall_readiness\": overall_readiness\n",
    "    }\n",
    "\n",
    "# RAG íŒŒì´í”„ë¼ì¸ ìš”ì•½ ì‹¤í–‰\n",
    "pipeline_summary = print_rag_pipeline_summary()\n",
    "\n",
    "# ì¶”ê°€ í…ŒìŠ¤íŠ¸ ê¶Œì¥ì‚¬í•­\n",
    "if 'result' in locals() and result.get(\"success\") and 'chunks' in locals():\n",
    "    print(f\"\\nğŸ’¡ ì¶”ê°€ í…ŒìŠ¤íŠ¸ íŒŒì¼ ê¶Œì¥ì‚¬í•­:\")\n",
    "    \n",
    "    current_size = result.get('rag_metadata', {}).get('file_size', 0)\n",
    "    if current_size < 1024 * 1024:  # 1MB ë¯¸ë§Œ\n",
    "        print(\"ğŸ“„ ì†Œí˜• ë¬¸ì„œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì¤‘í˜•/ëŒ€í˜• ë¬¸ì„œë¡œ ì¶”ê°€ í…ŒìŠ¤íŠ¸ ê¶Œì¥\")\n",
    "        print(\"   ê¶Œì¥ í…ŒìŠ¤íŠ¸: 5MB-50MB PDF, ë³µì¡í•œ í‘œê°€ í¬í•¨ëœ ë¬¸ì„œ\")\n",
    "    elif current_size < 10 * 1024 * 1024:  # 10MB ë¯¸ë§Œ\n",
    "        print(\"ğŸ“„ ì¤‘í˜• ë¬¸ì„œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ëŒ€í˜• ë¬¸ì„œë¡œ ì¶”ê°€ í…ŒìŠ¤íŠ¸ ê¶Œì¥\") \n",
    "        print(\"   ê¶Œì¥ í…ŒìŠ¤íŠ¸: 50MB+ PDF, ë‹¤ì¤‘ í˜ì´ì§€ ìŠ¤ìº” ë¬¸ì„œ\")\n",
    "    else:\n",
    "        print(\"ğŸ“„ ëŒ€í˜• ë¬¸ì„œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™” ê²€í†  ê¶Œì¥\")\n",
    "        print(\"   ê¶Œì¥ í…ŒìŠ¤íŠ¸: ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ (PPT, Word, ì´ë¯¸ì§€)\")\n",
    "    \n",
    "    print(\"ğŸ” ì„±ëŠ¥ ê°œì„  ì˜ì—­:\")\n",
    "    if 'chunk_analysis' in locals():\n",
    "        if chunk_analysis.get('optimization_score', 0) < 70:\n",
    "            print(\"   - ì²­í‚¹ íŒŒë¼ë¯¸í„° ìµœì í™” (í† í° í¬ê¸°, ê²¹ì¹¨ ë¹„ìœ¨)\")\n",
    "        if chunk_analysis.get('uniformity_score', 0) < 70:\n",
    "            print(\"   - ì²­í¬ í¬ê¸° ê· ì¼ì„± ê°œì„ \")\n",
    "    \n",
    "    confidence_avg = 85  # ì˜ˆì‹œ ê°’ (ì‹¤ì œë¡œëŠ” Textract ì‘ë‹µì—ì„œ ê³„ì‚°)\n",
    "    if confidence_avg < 90:\n",
    "        print(\"   - OCR í’ˆì§ˆ ê°œì„  (ì´ë¯¸ì§€ ì „ì²˜ë¦¬, í•´ìƒë„ ìµœì í™”)\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡:\")\n",
    "    print(f\"   í…ŒìŠ¤íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"   í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_file if 'test_file' in locals() else 'Unknown'}\")\n",
    "    print(f\"   ì²˜ë¦¬ ì„±ê³µë¥ : {'100%' if result.get('success') else '0%'}\")\n",
    "    print(f\"   RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ë„: {pipeline_summary['overall_readiness']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b980a8a",
   "metadata": {},
   "source": [
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. **ë¬¸ì„œ ì—…ë¡œë“œ**: ì‹¤ì œ ë¬¸ì„œë¥¼ ë°±ì—”ë“œì— ì—…ë¡œë“œí•˜ì—¬ ì „ì²´ ì²˜ë¦¬ ê³¼ì • í…ŒìŠ¤íŠ¸\n",
    "2. **ì„±ëŠ¥ íŠœë‹**: ì²­í‚¹ íŒŒë¼ë¯¸í„° ì¡°ì • ë° ì„ë² ë”© ìµœì í™”\n",
    "3. **í’ˆì§ˆ í‰ê°€**: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ ì •í™•ë„ ë° ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€\n",
    "4. **í™•ì¥ ê¸°ëŠ¥**: í‘œ/ì´ë¯¸ì§€ ì¶”ì¶œ, ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ë“± ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ê° ë‹¨ê³„ì—ì„œ ë°œìƒí•œ ë¬¸ì œì ì´ ìˆë‹¤ë©´ í•´ë‹¹ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ê±°ë‚˜ ì„¤ì •ì„ í™•ì¸í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a8223",
   "metadata": {},
   "source": [
    "## í•œêµ­ì–´ OCR ëŒ€ì•ˆ ë¶„ì„ ë° êµ¬í˜„\n",
    "\n",
    "AWS TextractëŠ” í•œêµ­ì–´ ì§€ì›ì´ ì œí•œì ì…ë‹ˆë‹¤. í•œêµ­ì–´ ë¬¸ì„œì— ë” ì í•©í•œ OCR ëŒ€ì•ˆë“¤ì„ í‰ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ - ì²« 2í˜ì´ì§€ë§Œ ì²˜ë¦¬\n",
    "test_file = '/home/admin/wkms-aws/jupyter_notebook/data/input_docs/test.pdf'\n",
    "\n",
    "# í˜ì´ì§€ë³„ ë ˆì´ì•„ì›ƒ ì„¤ì •\n",
    "layout_settings = {\n",
    "    'enable_page_specific': ENABLE_PAGE_SPECIFIC_LAYOUT,\n",
    "    'first_page_layout': FIRST_PAGE_LAYOUT\n",
    "}\n",
    "\n",
    "print(\"ğŸ”¥ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ - ì²« 2í˜ì´ì§€ë§Œ ì²˜ë¦¬\")\n",
    "print(f\"ğŸ“‹ ë ˆì´ì•„ì›ƒ ì„¤ì •: {layout_settings}\")\n",
    "\n",
    "# ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸°ë¡œ ì²« 2í˜ì´ì§€ë§Œ ì¶”ì¶œ\n",
    "result = extract_text_fast_preview(test_file, page_limit=2, layout_settings=layout_settings)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nâœ… ì¶”ì¶œ ì™„ë£Œ: {len(result['full_text'])}ì\")\n",
    "    print(f\"ğŸ“„ í˜ì´ì§€ ìˆ˜: {result['total_pages']}\")\n",
    "    \n",
    "    # í˜ì´ì§€ë³„ ê²°ê³¼ ìš”ì•½\n",
    "    for page_info in result['page_results']:\n",
    "        print(f\"   í˜ì´ì§€ {page_info['page']}: {page_info['layout_type']}, {page_info['lines_count']}ë¼ì¸, {len(page_info['text'])}ì\")\n",
    "    \n",
    "    # ì²« 300ì ì¶œë ¥\n",
    "    print(f\"\\nğŸ“ ì²« 300ì ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(result['full_text'][:300])\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥\n",
    "    output_dir = \"/home/admin/wkms-aws/jupyter_notebook/data/output_texts\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    quick_filename = f\"test_quick_{timestamp}.txt\"\n",
    "    with open(os.path.join(output_dir, quick_filename), 'w', encoding='utf-8') as f:\n",
    "        f.write(result['full_text'])\n",
    "    print(f\"\\nğŸ’¾ ì €ì¥ì™„ë£Œ: {quick_filename}\")\n",
    "else:\n",
    "    print(\"âŒ ì¶”ì¶œ ì‹¤íŒ¨\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

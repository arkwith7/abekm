#!/usr/bin/env python3
"""
RAG ì±„íŒ… ì‹œìŠ¤í…œ ìë™í™” í…ŒìŠ¤íŠ¸ ë° í†µê³„ ë¶„ì„ ì‹œìŠ¤í…œ
"""

import pandas as pd
import numpy as np
import json
import requests
import time
from datetime import datetime
from typing import Dict, List, Tuple, Any
import os
import asyncio
import aiohttp
from dataclasses import dataclass
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    question: str
    category: str
    api_type: str
    expected_has_reference: bool
    expected_reference_file: str
    expected_answer_type: str
    
    # ì‹¤ì œ ê²°ê³¼
    actual_response: str
    actual_has_reference: bool
    actual_reference_files: List[str]
    response_time: float
    
    # í‰ê°€ ê²°ê³¼
    reference_accuracy: bool  # ì°¸ê³ ìë£Œ ìœ ë¬´ ì •í™•ì„±
    content_relevance_score: float  # ë‚´ìš© ê´€ë ¨ì„± ì ìˆ˜ (0-1)
    answer_type_correct: bool  # ë‹µë³€ ìœ í˜• ì •í™•ì„±
    overall_score: float  # ì¢…í•© ì ìˆ˜


class RAGChatTester:
    """RAG ì±„íŒ… ì‹œìŠ¤í…œ ìë™í™” í…ŒìŠ¤í„°"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session_id = f"test_session_{int(time.time())}"
        self.test_results: List[TestResult] = []
    
    async def send_chat_message(self, message: str, api_type: str = "general") -> Dict:
        """ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ë°›ê¸°"""
        
        if api_type == "ppt":
            url = f"{self.base_url}/api/v1/chat/stream"
        else:
            url = f"{self.base_url}/api/v1/chat/message"
        
        payload = {
            "message": message,
            "session_id": self.session_id,
            "top_k": 10
        }
        
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        response_time = time.time() - start_time
                        
                        return {
                            "success": True,
                            "response": result,
                            "response_time": response_time
                        }
                    else:
                        return {
                            "success": False,
                            "error": f"HTTP {response.status}",
                            "response_time": time.time() - start_time
                        }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": time.time() - start_time
            }
    
    def extract_references_from_response(self, response_data: Dict) -> Tuple[bool, List[str]]:
        """ì‘ë‹µì—ì„œ ì°¸ê³ ìë£Œ ì •ë³´ ì¶”ì¶œ"""
        
        references = []
        has_reference = False
        
        try:
            # ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ì°¸ê³ ìë£Œ ì¶”ì¶œ
            if "references" in response_data:
                references = response_data["references"]
                has_reference = len(references) > 0
            elif "search_results" in response_data:
                search_results = response_data["search_results"]
                if search_results and len(search_results) > 0:
                    has_reference = True
                    references = [result.get("metadata", {}).get("filename", "") 
                                for result in search_results]
            elif "message" in response_data:
                # ë©”ì‹œì§€ ë‚´ìš©ì—ì„œ ì°¸ê³ ìë£Œ ì–¸ê¸‰ í™•ì¸
                message = response_data["message"]
                if "ì°¸ê³ ìë£Œ" in message or "ë¬¸ì„œ" in message or "íŒŒì¼" in message:
                    has_reference = True
                    
        except Exception as e:
            print(f"ì°¸ê³ ìë£Œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return has_reference, references
    
    def evaluate_content_relevance(self, question: str, response: str, keywords: str) -> float:
        """ë‚´ìš© ê´€ë ¨ì„± í‰ê°€ (0-1 ì ìˆ˜)"""
        
        if not response:
            return 0.0
        
        keywords_list = [k.strip() for k in keywords.split(",") if k.strip()]
        
        # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
        response_lower = response.lower()
        keyword_matches = sum(1 for keyword in keywords_list 
                            if keyword.lower() in response_lower)
        
        keyword_score = keyword_matches / len(keywords_list) if keywords_list else 0
        
        # ì‘ë‹µ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì )
        length_score = min(len(response.split()) / 20, 1.0)  # 20ë‹¨ì–´ ì´ìƒì´ë©´ ë§Œì 
        
        # "ì£„ì†¡í•©ë‹ˆë‹¤", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ë“±ì˜ ë¶€ì •ì  ì‘ë‹µ ê°ì 
        negative_patterns = ["ì£„ì†¡", "ì°¾ì„ ìˆ˜ ì—†", "ì—†ìŠµë‹ˆë‹¤", "ëª¨ë¥´ê² ", "í™•ì¸í•  ìˆ˜ ì—†"]
        negative_penalty = sum(1 for pattern in negative_patterns 
                             if pattern in response_lower) * 0.2
        
        # ì¢…í•© ì ìˆ˜
        relevance_score = (keyword_score * 0.6 + length_score * 0.4) - negative_penalty
        
        return max(0.0, min(1.0, relevance_score))
    
    def evaluate_answer_type(self, expected_type: str, response: str) -> bool:
        """ë‹µë³€ ìœ í˜• ì •í™•ì„± í‰ê°€"""
        
        response_lower = response.lower()
        
        type_patterns = {
            "í™•ì¸": ["ìˆìŠµë‹ˆë‹¤", "ì¡´ì¬í•©ë‹ˆë‹¤", "í™•ì¸", "ì°¾ì•˜ìŠµë‹ˆë‹¤"],
            "ì„¤ëª…": ["ì„¤ëª…", "ë‹¤ìŒê³¼ ê°™", "ëŒ€í•´ì„œ", "ê´€ë ¨í•˜ì—¬", "íŠ¹ì§•"],
            "PPT ìƒì„±": ["ppt", "í”„ë ˆì  í…Œì´ì…˜", "ìŠ¬ë¼ì´ë“œ", "ìƒì„±", "ë§Œë“¤"],
            "ìë£Œ ì—†ìŒ ì•ˆë‚´": ["ì£„ì†¡", "ì°¾ì„ ìˆ˜ ì—†", "ì—†ìŠµë‹ˆë‹¤", "ìë£Œê°€ ì—†"]
        }
        
        if expected_type in type_patterns:
            patterns = type_patterns[expected_type]
            return any(pattern in response_lower for pattern in patterns)
        
        return True  # ê¸°ë³¸ê°’
    
    async def run_single_test(self, test_case: Dict) -> TestResult:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰"""
        
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘: {test_case['question'][:50]}...")
        
        # API í˜¸ì¶œ
        api_result = await self.send_chat_message(
            test_case["question"], 
            test_case["api_type"]
        )
        
        if not api_result["success"]:
            # ì‹¤íŒ¨í•œ ê²½ìš°
            return TestResult(
                question=test_case["question"],
                category=test_case["category"],
                api_type=test_case["api_type"],
                expected_has_reference=test_case["expected_has_reference"],
                expected_reference_file=test_case["expected_reference_file"],
                expected_answer_type=test_case["expected_answer_type"],
                actual_response=f"API ì˜¤ë¥˜: {api_result['error']}",
                actual_has_reference=False,
                actual_reference_files=[],
                response_time=api_result["response_time"],
                reference_accuracy=False,
                content_relevance_score=0.0,
                answer_type_correct=False,
                overall_score=0.0
            )
        
        # ì‘ë‹µ ë¶„ì„
        response_data = api_result["response"]
        actual_response = str(response_data.get("message", ""))
        
        # ì°¸ê³ ìë£Œ ì¶”ì¶œ
        actual_has_reference, actual_reference_files = self.extract_references_from_response(response_data)
        
        # í‰ê°€ ìˆ˜í–‰
        reference_accuracy = (actual_has_reference == test_case["expected_has_reference"])
        
        content_relevance_score = self.evaluate_content_relevance(
            test_case["question"], 
            actual_response, 
            test_case["keywords"]
        )
        
        answer_type_correct = self.evaluate_answer_type(
            test_case["expected_answer_type"], 
            actual_response
        )
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = (
            reference_accuracy * 0.4 +           # ì°¸ê³ ìë£Œ ì •í™•ì„± 40%
            content_relevance_score * 0.4 +      # ë‚´ìš© ê´€ë ¨ì„± 40%
            answer_type_correct * 0.2             # ë‹µë³€ ìœ í˜• 20%
        )
        
        return TestResult(
            question=test_case["question"],
            category=test_case["category"],
            api_type=test_case["api_type"],
            expected_has_reference=test_case["expected_has_reference"],
            expected_reference_file=test_case["expected_reference_file"],
            expected_answer_type=test_case["expected_answer_type"],
            actual_response=actual_response,
            actual_has_reference=actual_has_reference,
            actual_reference_files=actual_reference_files,
            response_time=api_result["response_time"],
            reference_accuracy=reference_accuracy,
            content_relevance_score=content_relevance_score,
            answer_type_correct=answer_type_correct,
            overall_score=overall_score
        )
    
    async def run_all_tests(self, ground_truth_file: str, max_tests: int = None) -> List[TestResult]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰"""
        
        # ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ë¡œë“œ
        df = pd.read_csv(ground_truth_file)
        
        if max_tests:
            df = df.head(max_tests)
        
        print(f"ğŸ“Š ì´ {len(df)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ ì‹œì‘...")
        
        results = []
        
        for idx, row in df.iterrows():
            try:
                result = await self.run_single_test(row.to_dict())
                results.append(result)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (idx + 1) % 10 == 0:
                    print(f"ì§„í–‰ë¥ : {idx + 1}/{len(df)} ({(idx + 1)/len(df)*100:.1f}%)")
                
                # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                await asyncio.sleep(0.5)
                
            except Exception as e:
                print(f"í…ŒìŠ¤íŠ¸ {idx + 1} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                continue
        
        self.test_results = results
        return results


class TestResultAnalyzer:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë° í†µê³„ ìƒì„±"""
    
    def __init__(self, test_results: List[TestResult]):
        self.test_results = test_results
        self.df = self.results_to_dataframe()
    
    def results_to_dataframe(self) -> pd.DataFrame:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        
        data = []
        for result in self.test_results:
            data.append({
                "question": result.question,
                "category": result.category,
                "api_type": result.api_type,
                "expected_has_reference": result.expected_has_reference,
                "actual_has_reference": result.actual_has_reference,
                "reference_accuracy": result.reference_accuracy,
                "content_relevance_score": result.content_relevance_score,
                "answer_type_correct": result.answer_type_correct,
                "overall_score": result.overall_score,
                "response_time": result.response_time,
                "actual_response": result.actual_response[:200] + "..." if len(result.actual_response) > 200 else result.actual_response
            })
        
        return pd.DataFrame(data)
    
    def calculate_statistics(self) -> Dict:
        """í†µê³„ ë¶„ì„ ìˆ˜í–‰"""
        
        stats_result = {
            "ì´_í…ŒìŠ¤íŠ¸_ìˆ˜": len(self.test_results),
            "ì „ì²´_í‰ê· _ì ìˆ˜": self.df["overall_score"].mean(),
            "ì°¸ê³ ìë£Œ_ì •í™•ë„": self.df["reference_accuracy"].mean(),
            "ë‚´ìš©_ê´€ë ¨ì„±_í‰ê· ": self.df["content_relevance_score"].mean(),
            "ë‹µë³€_ìœ í˜•_ì •í™•ë„": self.df["answer_type_correct"].mean(),
            "í‰ê· _ì‘ë‹µ_ì‹œê°„": self.df["response_time"].mean(),
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            "ì¹´í…Œê³ ë¦¬ë³„_ì„±ëŠ¥": {},
            "API_íƒ€ì…ë³„_ì„±ëŠ¥": {},
            
            # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
            "í†µê³„_ê²€ì •_ê²°ê³¼": {}
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        for category in self.df["category"].unique():
            category_data = self.df[self.df["category"] == category]
            stats_result["ì¹´í…Œê³ ë¦¬ë³„_ì„±ëŠ¥"][category] = {
                "í…ŒìŠ¤íŠ¸_ìˆ˜": len(category_data),
                "í‰ê· _ì ìˆ˜": category_data["overall_score"].mean(),
                "ì°¸ê³ ìë£Œ_ì •í™•ë„": category_data["reference_accuracy"].mean(),
                "ë‚´ìš©_ê´€ë ¨ì„±": category_data["content_relevance_score"].mean()
            }
        
        # API íƒ€ì…ë³„ ë¶„ì„
        for api_type in self.df["api_type"].unique():
            api_data = self.df[self.df["api_type"] == api_type]
            stats_result["API_íƒ€ì…ë³„_ì„±ëŠ¥"][api_type] = {
                "í…ŒìŠ¤íŠ¸_ìˆ˜": len(api_data),
                "í‰ê· _ì ìˆ˜": api_data["overall_score"].mean(),
                "ì°¸ê³ ìë£Œ_ì •í™•ë„": api_data["reference_accuracy"].mean(),
                "ë‚´ìš©_ê´€ë ¨ì„±": api_data["content_relevance_score"].mean()
            }
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        if len(self.df["category"].unique()) > 1:
            # ì¹´í…Œê³ ë¦¬ ê°„ ì„±ëŠ¥ ì°¨ì´ ANOVA í…ŒìŠ¤íŠ¸
            categories = [group["overall_score"].values 
                         for name, group in self.df.groupby("category")]
            
            if len(categories) > 1 and all(len(cat) > 1 for cat in categories):
                f_stat, p_value = stats.f_oneway(*categories)
                stats_result["í†µê³„_ê²€ì •_ê²°ê³¼"]["ì¹´í…Œê³ ë¦¬_ê°„_ì°¨ì´"] = {
                    "F_í†µê³„ëŸ‰": f_stat,
                    "p_ê°’": p_value,
                    "ìœ ì˜ë¯¸í•¨": p_value < 0.05
                }
        
        return stats_result
    
    def generate_report(self, output_dir: str = "/home/admin/wkms-aws/jupyter_notebook/data/test_results/rag_chat") -> str:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # í†µê³„ ë¶„ì„
        stats = self.calculate_statistics()
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            "í…ŒìŠ¤íŠ¸_ì‹¤í–‰_ì •ë³´": {
                "ì‹¤í–‰_ì‹œê°„": datetime.now().isoformat(),
                "ì´_í…ŒìŠ¤íŠ¸_ì¼€ì´ìŠ¤": len(self.test_results),
                "ì„±ê³µ_ì¼€ì´ìŠ¤": sum(1 for r in self.test_results if r.overall_score > 0.6),
                "ì‹¤íŒ¨_ì¼€ì´ìŠ¤": sum(1 for r in self.test_results if r.overall_score <= 0.6)
            },
            "ì„±ëŠ¥_ì§€í‘œ": stats,
            "ìƒì„¸_ê²°ê³¼": self.df.to_dict('records')
        }
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        report_file = os.path.join(output_dir, "rag_test_report.json")
        with open(report_file, "w", encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # CSV ë¦¬í¬íŠ¸ ì €ì¥
        csv_file = os.path.join(output_dir, "rag_test_results.csv")
        self.df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        summary_file = os.path.join(output_dir, "rag_test_summary.md")
        self.generate_markdown_summary(summary_file, stats)
        
        return report_file
    
    def generate_markdown_summary(self, file_path: str, stats: Dict):
        """ë§ˆí¬ë‹¤ìš´ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        with open(file_path, "w", encoding='utf-8') as f:
            f.write("# RAG ì±„íŒ… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸\n\n")
            f.write(f"**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½\n\n")
            f.write(f"- **ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: {stats['ì´_í…ŒìŠ¤íŠ¸_ìˆ˜']}ê°œ\n")
            f.write(f"- **ì „ì²´ í‰ê·  ì ìˆ˜**: {stats['ì „ì²´_í‰ê· _ì ìˆ˜']:.3f}\n")
            f.write(f"- **ì°¸ê³ ìë£Œ ì •í™•ë„**: {stats['ì°¸ê³ ìë£Œ_ì •í™•ë„']:.3f}\n")
            f.write(f"- **ë‚´ìš© ê´€ë ¨ì„±**: {stats['ë‚´ìš©_ê´€ë ¨ì„±_í‰ê· ']:.3f}\n")
            f.write(f"- **ë‹µë³€ ìœ í˜• ì •í™•ë„**: {stats['ë‹µë³€_ìœ í˜•_ì •í™•ë„']:.3f}\n")
            f.write(f"- **í‰ê·  ì‘ë‹µ ì‹œê°„**: {stats['í‰ê· _ì‘ë‹µ_ì‹œê°„']:.2f}ì´ˆ\n\n")
            
            f.write("## ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥\n\n")
            for category, perf in stats['ì¹´í…Œê³ ë¦¬ë³„_ì„±ëŠ¥'].items():
                f.write(f"### {category}\n")
                f.write(f"- í…ŒìŠ¤íŠ¸ ìˆ˜: {perf['í…ŒìŠ¤íŠ¸_ìˆ˜']}ê°œ\n")
                f.write(f"- í‰ê·  ì ìˆ˜: {perf['í‰ê· _ì ìˆ˜']:.3f}\n")
                f.write(f"- ì°¸ê³ ìë£Œ ì •í™•ë„: {perf['ì°¸ê³ ìë£Œ_ì •í™•ë„']:.3f}\n")
                f.write(f"- ë‚´ìš© ê´€ë ¨ì„±: {perf['ë‚´ìš©_ê´€ë ¨ì„±']:.3f}\n\n")
            
            f.write("## ğŸ”§ API íƒ€ì…ë³„ ì„±ëŠ¥\n\n")
            for api_type, perf in stats['API_íƒ€ì…ë³„_ì„±ëŠ¥'].items():
                f.write(f"### {api_type.upper()}\n")
                f.write(f"- í…ŒìŠ¤íŠ¸ ìˆ˜: {perf['í…ŒìŠ¤íŠ¸_ìˆ˜']}ê°œ\n")
                f.write(f"- í‰ê·  ì ìˆ˜: {perf['í‰ê· _ì ìˆ˜']:.3f}\n")
                f.write(f"- ì°¸ê³ ìë£Œ ì •í™•ë„: {perf['ì°¸ê³ ìë£Œ_ì •í™•ë„']:.3f}\n")
                f.write(f"- ë‚´ìš© ê´€ë ¨ì„±: {perf['ë‚´ìš©_ê´€ë ¨ì„±']:.3f}\n\n")
            
            # í†µê³„ì  ìœ ì˜ì„±
            if "í†µê³„_ê²€ì •_ê²°ê³¼" in stats and stats["í†µê³„_ê²€ì •_ê²°ê³¼"]:
                f.write("## ğŸ“ í†µê³„ì  ìœ ì˜ì„± ê²€ì •\n\n")
                for test_name, result in stats["í†µê³„_ê²€ì •_ê²°ê³¼"].items():
                    f.write(f"### {test_name}\n")
                    f.write(f"- F í†µê³„ëŸ‰: {result['F_í†µê³„ëŸ‰']:.4f}\n")
                    f.write(f"- p-ê°’: {result['p_ê°’']:.4f}\n")
                    f.write(f"- ìœ ì˜ë¯¸í•¨: {'ì˜ˆ' if result['ìœ ì˜ë¯¸í•¨'] else 'ì•„ë‹ˆì˜¤'}\n\n")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ RAG ì±„íŒ… ì‹œìŠ¤í…œ ìë™í™” í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    # í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = RAGChatTester("http://localhost:8000")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì²˜ìŒì—ëŠ” ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸)
    ground_truth_file = "/home/admin/wkms-aws/jupyter_notebook/data/ground_truth/ground_truth_criteria.csv"
    
    if not os.path.exists(ground_truth_file):
        print(f"âŒ ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ground_truth_file}")
        return
    
    # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (ì „ì²´ì˜ 10% ë˜ëŠ” ìµœëŒ€ 20ê°œ)
    df_sample = pd.read_csv(ground_truth_file)
    sample_size = min(20, max(3, len(df_sample) // 10))
    
    print(f"ğŸ“ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {sample_size}ê°œ ì¼€ì´ìŠ¤")
    
    try:
        results = await tester.run_all_tests(ground_truth_file, max_tests=sample_size)
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        
        # ê²°ê³¼ ë¶„ì„
        analyzer = TestResultAnalyzer(results)
        report_file = analyzer.generate_report()
        
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"- JSON ë¦¬í¬íŠ¸: {report_file}")
        print(f"- CSV ê²°ê³¼: /home/admin/wkms-aws/jupyter_notebook/data/test_results/rag_chat/rag_test_results.csv")
        print(f"- ìš”ì•½ ë¦¬í¬íŠ¸: /home/admin/wkms-aws/jupyter_notebook/data/test_results/rag_chat/rag_test_summary.md")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš” (http://localhost:8000)")


if __name__ == "__main__":
    asyncio.run(main())
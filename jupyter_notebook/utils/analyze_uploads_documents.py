#!/usr/bin/env python3
"""
ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ì—¬ ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import pandas as pd
import json
from datetime import datetime
from typing import List, Dict, Tuple
import re

# ë¬¸ì„œ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from docx import Document
    import PyPDF2
    from pptx import Presentation
except ImportError as e:
    print(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ìƒí™˜ê²½ì—ì„œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    sys.exit(1)


class DocumentAnalyzer:
    """ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ì—¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, uploads_dir: str):
        self.uploads_dir = uploads_dir
        self.documents_info = []
    
    def extract_docx_content(self, file_path: str) -> Dict:
        """DOCX íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ"""
        try:
            doc = Document(file_path)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
            
            # í…Œì´ë¸” ë‚´ìš© ì¶”ì¶œ
            tables_content = []
            for table in doc.tables:
                table_data = []
                for row in table.rows:
                    row_data = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_data.append(cell.text.strip())
                    if row_data:
                        table_data.append(" | ".join(row_data))
                if table_data:
                    tables_content.append("\n".join(table_data))
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            content = " ".join(full_text)
            keywords = self.extract_keywords(content)
            
            return {
                "type": "docx",
                "text_content": full_text,
                "tables_content": tables_content,
                "keywords": keywords,
                "word_count": len(content.split()),
                "summary": self.generate_summary(content)
            }
            
        except Exception as e:
            return {"error": f"DOCX ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"}
    
    def extract_pdf_content(self, file_path: str) -> Dict:
        """PDF íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ"""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                full_text = []
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    if text.strip():
                        full_text.append(text.strip())
                
                content = " ".join(full_text)
                keywords = self.extract_keywords(content)
                
                return {
                    "type": "pdf",
                    "text_content": full_text,
                    "page_count": len(pdf_reader.pages),
                    "keywords": keywords,
                    "word_count": len(content.split()),
                    "summary": self.generate_summary(content)
                }
                
        except Exception as e:
            return {"error": f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"}
    
    def extract_pptx_content(self, file_path: str) -> Dict:
        """PPTX íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ"""
        try:
            prs = Presentation(file_path)
            
            slides_content = []
            all_text = []
            
            for slide_num, slide in enumerate(prs.slides, 1):
                slide_text = []
                
                # í…ìŠ¤íŠ¸ ë°•ìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        slide_text.append(shape.text.strip())
                
                if slide_text:
                    slide_content = {
                        "slide_number": slide_num,
                        "content": slide_text
                    }
                    slides_content.append(slide_content)
                    all_text.extend(slide_text)
            
            content = " ".join(all_text)
            keywords = self.extract_keywords(content)
            
            return {
                "type": "pptx",
                "slides_content": slides_content,
                "slide_count": len(prs.slides),
                "keywords": keywords,
                "word_count": len(content.split()),
                "summary": self.generate_summary(content)
            }
            
        except Exception as e:
            return {"error": f"PPTX ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"}
    
    def extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í•œê¸€, ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]{2,}', text)
        english_words = re.findall(r'[a-zA-Z]{3,}', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ê°™ì€', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì–´ë–¤', 'ìˆ˜ë„', 'ë•Œë¬¸',
            'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ì´ê²ƒ', 'ê·¸ê²ƒ',
            'the', 'and', 'are', 'for', 'with', 'can', 'you', 'have', 'what',
            'this', 'that', 'will', 'from', 'they', 'been', 'said', 'each'
        }
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚° ë° ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        all_words = korean_words + english_words
        word_freq = {}
        for word in all_words:
            if word not in stopwords and len(word) >= 2:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ë¹ˆë„ìˆ˜ ê¸°ì¤€ ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        return [word for word, freq in top_keywords]
    
    def generate_summary(self, content: str, max_length: int = 200) -> str:
        """í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„± (ê°„ë‹¨í•œ ë°©ì‹)"""
        if not content:
            return ""
        
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?]\s+', content)
        
        if not sentences:
            return content[:max_length]
        
        # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
        for sentence in sentences:
            if len(sentence.strip()) > 10:
                summary = sentence.strip()
                if len(summary) > max_length:
                    summary = summary[:max_length] + "..."
                return summary
        
        return content[:max_length] + "..." if len(content) > max_length else content
    
    def analyze_all_documents(self) -> List[Dict]:
        """ëª¨ë“  ë¬¸ì„œ ë¶„ì„"""
        results = []
        
        for filename in os.listdir(self.uploads_dir):
            file_path = os.path.join(self.uploads_dir, filename)
            
            # ë””ë ‰í† ë¦¬ëŠ” ê±´ë„ˆë›°ê¸°
            if os.path.isdir(file_path):
                continue
            
            print(f"ë¶„ì„ ì¤‘: {filename}")
            
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì²˜ë¦¬
            if filename.lower().endswith('.docx'):
                content_info = self.extract_docx_content(file_path)
            elif filename.lower().endswith('.pdf'):
                content_info = self.extract_pdf_content(file_path)
            elif filename.lower().endswith('.pptx'):
                content_info = self.extract_pptx_content(file_path)
            else:
                continue
            
            # íŒŒì¼ ì •ë³´ ì¶”ê°€
            file_info = {
                "filename": filename,
                "file_path": file_path,
                "file_size": os.path.getsize(file_path),
                "modified_date": datetime.fromtimestamp(os.path.getmtime(file_path)),
                **content_info
            }
            
            results.append(file_info)
        
        return results


def create_ground_truth_from_documents(documents_info: List[Dict]) -> pd.DataFrame:
    """ë¬¸ì„œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ë°ì´í„° ìƒì„±"""
    
    ground_truth_data = []
    
    for doc in documents_info:
        if "error" in doc:
            continue
        
        filename = doc["filename"]
        doc_type = doc["type"]
        keywords = doc.get("keywords", [])
        summary = doc.get("summary", "")
        
        # 1. ë¬¸ì„œ ì¡´ì¬ í™•ì¸ ì§ˆë¬¸ë“¤
        existence_questions = [
            f"{filename} íŒŒì¼ì´ ìˆë‚˜ìš”?",
            f"{doc_type.upper()} íŒŒì¼ ì¤‘ì— {keywords[0] if keywords else 'ê´€ë ¨'} ë‚´ìš©ì´ ìˆë‚˜ìš”?",
            f"ì—…ë¡œë“œëœ ë¬¸ì„œ ì¤‘ì— {keywords[0] if keywords else 'íŠ¹ì •'} ê´€ë ¨ ìë£Œê°€ ìˆë‚˜ìš”?"
        ]
        
        for question in existence_questions:
            ground_truth_data.append({
                "question": question,
                "category": "document_existence",
                "api_type": "general",
                "expected_has_reference": True,
                "expected_reference_file": filename,
                "expected_answer_type": "í™•ì¸",
                "keywords": ", ".join(keywords[:3]),
                "difficulty": "easy",
                "test_purpose": "ë¬¸ì„œ ì¡´ì¬ í™•ì¸"
            })
        
        # 2. ë‚´ìš© ê¸°ë°˜ ì§ˆë¬¸ë“¤
        if keywords:
            content_questions = [
                f"{keywords[0]}ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                f"{keywords[0]} ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                f"{keywords[0]}ì˜ íŠ¹ì§•ì´ë‚˜ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            ]
            
            for question in content_questions:
                ground_truth_data.append({
                    "question": question,
                    "category": "content_inquiry",
                    "api_type": "general",
                    "expected_has_reference": True,
                    "expected_reference_file": filename,
                    "expected_answer_type": "ì„¤ëª…",
                    "keywords": ", ".join(keywords[:3]),
                    "difficulty": "medium",
                    "test_purpose": "ë‚´ìš© ê²€ìƒ‰ ë° ì„¤ëª…"
                })
        
        # 3. PPT ìƒì„± ìš”ì²­ (PPT íŒŒì¼ì¸ ê²½ìš°)
        if doc_type == "pptx":
            ppt_questions = [
                f"{keywords[0]} PPT ë§Œë“¤ì–´ì£¼ì„¸ìš”",
                f"{keywords[0]} ë°œí‘œìë£Œ ìƒì„±í•´ì£¼ì„¸ìš”",
                f"{keywords[0]} í”„ë ˆì  í…Œì´ì…˜ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
            ]
            
            for question in ppt_questions:
                ground_truth_data.append({
                    "question": question,
                    "category": "ppt_generation",
                    "api_type": "ppt",
                    "expected_has_reference": True,
                    "expected_reference_file": filename,
                    "expected_answer_type": "PPT ìƒì„±",
                    "keywords": ", ".join(keywords[:3]),
                    "difficulty": "hard",
                    "test_purpose": "PPT ìƒì„± ê¸°ëŠ¥"
                })
    
    # 4. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ë“¤ (ë„¤ê±°í‹°ë¸Œ ì¼€ì´ìŠ¤)
    negative_questions = [
        "ì–‘ìì»´í“¨í„° ê¸°ìˆ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
        "ë¸”ë¡ì²´ì¸ ì•”í˜¸í™”í íˆ¬ì ì „ëµì„ ì„¤ëª…í•´ì£¼ì„¸ìš”", 
        "ìš°ì£¼í•­ê³µ ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥ì€ ì–´ë–¤ê°€ìš”?",
        "ì‹¬í•´ ìƒë¬¼ì˜ ì§„í™” ê³¼ì •ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    ]
    
    for question in negative_questions:
        ground_truth_data.append({
            "question": question,
            "category": "non_existent_content",
            "api_type": "general",
            "expected_has_reference": False,
            "expected_reference_file": "ì—†ìŒ",
            "expected_answer_type": "ìë£Œ ì—†ìŒ ì•ˆë‚´",
            "keywords": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‚´ìš©",
            "difficulty": "medium",
            "test_purpose": "ë¶€ì •í™•í•œ ì‘ë‹µ ë°©ì§€"
        })
    
    return pd.DataFrame(ground_truth_data)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    uploads_dir = "/home/admin/wkms-aws/backend/uploads"
    
    if not os.path.exists(uploads_dir):
        print(f"ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {uploads_dir}")
        return
    
    print("ğŸ“ ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    
    # ë¬¸ì„œ ë¶„ì„
    analyzer = DocumentAnalyzer(uploads_dir)
    documents_info = analyzer.analyze_all_documents()
    
    print(f"âœ… ì´ {len(documents_info)}ê°œ ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ")
    
    # ë¬¸ì„œ ì •ë³´ ì €ì¥
    documents_df = pd.DataFrame(documents_info)
    documents_df.to_csv("/home/admin/wkms-aws/documents_analysis.csv", index=False, encoding='utf-8-sig')
    print("ğŸ“Š ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ê°€ documents_analysis.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ìƒì„±
    print("ğŸ¯ ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    ground_truth_df = create_ground_truth_from_documents(documents_info)
    
    # CSV íŒŒì¼ë¡œ ì €ì¥
    ground_truth_df.to_csv("/home/admin/wkms-aws/ground_truth_criteria.csv", index=False, encoding='utf-8-sig')
    print(f"âœ… ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ë°ì´í„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {len(ground_truth_df)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
    
    # í†µê³„ ì •ë³´ ì¶œë ¥
    print("\nğŸ“ˆ ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ í†µê³„:")
    print(f"- ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(ground_truth_df)}ê°œ")
    print(f"- ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    category_counts = ground_truth_df['category'].value_counts()
    for category, count in category_counts.items():
        print(f"  â€¢ {category}: {count}ê°œ")
    
    print(f"- API íƒ€ì…ë³„ ë¶„í¬:")
    api_counts = ground_truth_df['api_type'].value_counts()
    for api_type, count in api_counts.items():
        print(f"  â€¢ {api_type}: {count}ê°œ")
    
    # ìƒì„¸ ë¶„ì„ ê²°ê³¼ JSONìœ¼ë¡œë„ ì €ì¥
    analysis_result = {
        "analysis_date": datetime.now().isoformat(),
        "total_documents": len(documents_info),
        "document_types": {},
        "total_test_cases": len(ground_truth_df),
        "documents_detail": documents_info
    }
    
    # ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„
    for doc in documents_info:
        if "error" not in doc:
            doc_type = doc.get("type", "unknown")
            analysis_result["document_types"][doc_type] = analysis_result["document_types"].get(doc_type, 0) + 1
    
    with open("/home/admin/wkms-aws/documents_analysis_detail.json", "w", encoding='utf-8') as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2, default=str)
    
    print("ğŸ“„ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ documents_analysis_detail.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    print("\nğŸ‰ ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f41331e",
   "metadata": {},
   "source": [
    "# ğŸ“„ íŒŒì¼ íƒ€ì…ë³„ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€/í‘œ ì¶”ì¶œ í†µí•© íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "- hwpx, docx, xlsx, pptx, pdf ë“± ë‹¤ì–‘í•œ ë¬¸ì„œ íƒ€ì…ì„ ìë™ ì‹ë³„í•˜ì—¬ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "- ì¶”ì¶œ ê²°ê³¼ëŠ” í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ì™€ ì „ì²´ ë¬¸ì„œ í•©ë³¸ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- íŒŒì¼ëª… ì• 10ìë¡œ í•˜ìœ„ í´ë”ë¥¼ ë§Œë“¤ê³  ê·¸ ì•„ë˜ document.json, pages/, tables/ êµ¬ì¡°ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3d6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Python ì‹¤í–‰ ê²½ë¡œ: /home/admin/wkms-aws/.venv/bin/python\n",
      "âœ” pdfplumber ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” pdfminer ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” pypdf ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” docx ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” pypdf ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” docx ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” openpyxl ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” pptx ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” olefile ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” lxml ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” PIL ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” dotenv ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” openpyxl ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” pptx ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” olefile ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” lxml ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” PIL ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” dotenv ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ” fitz ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ… ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ë¨\n",
      "   - pdfplumber version: 0.11.5\n",
      "   - pdfminer version: 20231228\n",
      "   - pypdf version: 3.17.4\n",
      "âœ” fitz ì‚¬ìš© ê°€ëŠ¥\n",
      "âœ… ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ë¨\n",
      "   - pdfplumber version: 0.11.5\n",
      "   - pdfminer version: 20231228\n",
      "   - pypdf version: 3.17.4\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì˜ì¡´ì„± ì ê²€ ë° ì„¤ì¹˜ ì•ˆë‚´\n",
    "import importlib, sys, warnings\n",
    "\n",
    "# Suppress noisy cryptography/pypdf ARC4 deprecation warnings seen during pypdf import\n",
    "warnings.filterwarnings(\"ignore\", message=r\".*ARC4 has been moved.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=r\"pypdf\\._crypt_providers\\._cryptography\")\n",
    "\n",
    "print(f\"ğŸ§ª Python ì‹¤í–‰ ê²½ë¡œ: {sys.executable}\")\n",
    "\n",
    "\n",
    "def check_lib(name):\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "        print(f'âœ” {name} ì‚¬ìš© ê°€ëŠ¥')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'âœ– {name} ë¶ˆê°€: {e}')\n",
    "        return False\n",
    "\n",
    "# pdfminer.sixëŠ” 'pdfminer'ë¡œ importë©ë‹ˆë‹¤. pypdfëŠ” PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í´ë°±ìš©\n",
    "needed = [\n",
    "    'pdfplumber', 'pdfminer', 'pypdf', 'docx', 'openpyxl', 'pptx', 'olefile', 'lxml', 'PIL', 'dotenv', 'fitz'\n",
    "]\n",
    "status = {n: check_lib(n) for n in needed}\n",
    "missing = [n for n, ok in status.items() if not ok]\n",
    "if missing:\n",
    "    translate = {'pdfminer': 'pdfminer.six', 'fitz': 'PyMuPDF'}\n",
    "    pip_list = [translate.get(m, m) for m in missing]\n",
    "    print(f\"âš  ì„¤ì¹˜ í•„ìš”: {', '.join(pip_list)}\")\n",
    "else:\n",
    "    print('âœ… ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ë¨')\n",
    "\n",
    "# ë²„ì „ ì •ë³´(ê°€ëŠ¥í•  ë•Œë§Œ)\n",
    "for mod in ['pdfplumber', 'pdfminer', 'pypdf']:\n",
    "    try:\n",
    "        m = importlib.import_module(mod)\n",
    "        ver = getattr(m, '__version__', None)\n",
    "        print(f\"   - {mod} version: {ver}\")\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71efa116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ ê²½ë¡œ/ì„¤ì • ë° íŒŒì¼ íƒ€ì… ìë™ ì‹ë³„\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('/home/admin/wkms-aws/backend/.env')\n",
    "BASE_DIR = Path('/home/admin/wkms-aws/jupyter_notebook')\n",
    "INPUT_DIR = BASE_DIR / 'data/input_docs'\n",
    "OUTPUT_DIR = BASE_DIR / 'data/extracted_objects'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def detect_file_type(file_path):\n",
    "    ext = Path(file_path).suffix.lower()\n",
    "    if ext in ['.pdf', '.docx', '.xlsx', '.pptx', '.hwp', '.hwpx']:\n",
    "        return ext[1:]\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "928d93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ íŒŒì¼ë³„ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ (ì˜¤ë¸Œì íŠ¸ ì¸ì‹ ê°•í™”)\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def is_image_pdf(file_path: str, sample_pages: int = 3, min_text_chars: int = 20):\n",
    "    \"\"\"ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì´ë¯¸ì§€ ê¸°ë°˜ PDF ì—¬ë¶€ íŒë‹¨\n",
    "    - ì•ìª½ sample_pages í˜ì´ì§€ì—ì„œ ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ë§¤ìš° ì§§ê³  ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ë©´ ì´ë¯¸ì§€ PDFë¡œ íŒë‹¨\n",
    "    ë°˜í™˜: (is_image_pdf: bool, page_count: int)\n",
    "    \"\"\"\n",
    "    import pdfplumber\n",
    "    text_chars = 0\n",
    "    any_images = False\n",
    "    page_count = 0\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        page_count = len(pdf.pages)\n",
    "        check_n = min(sample_pages, page_count)\n",
    "        for page in pdf.pages[:check_n]:\n",
    "            txt = page.extract_text() or \"\"\n",
    "            text_chars += len(txt.strip())\n",
    "            if getattr(page, 'images', None):\n",
    "                any_images = any_images or len(page.images) > 0\n",
    "    is_img = (text_chars < min_text_chars) and any_images\n",
    "    return is_img, page_count\n",
    "\n",
    "\n",
    "def _extract_pdf_text_with_pypdf(file_path: str):\n",
    "    \"\"\"pdfplumber/pdfminer ì‹¤íŒ¨ ì‹œ pypdfë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í´ë°±\"\"\"\n",
    "    from pypdf import PdfReader\n",
    "    reader = PdfReader(str(file_path))\n",
    "    pages = []\n",
    "    combined = ''\n",
    "    for i, page in enumerate(reader.pages, start=1):\n",
    "        try:\n",
    "            txt = page.extract_text() or ''\n",
    "        except Exception:\n",
    "            txt = ''\n",
    "        pages.append({'index': i, 'text': txt})\n",
    "        combined += f\"\\n[í˜ì´ì§€ {i}]\\n{txt}\"\n",
    "    return combined.strip(), pages\n",
    "\n",
    "\n",
    "def _extract_pdf_images_with_pymupdf(file_path: str):\n",
    "    \"\"\"PyMuPDFë¡œ PDF í˜ì´ì§€ë³„ ì´ë¯¸ì§€ XObject ì¶”ì¶œ (bbox í¬í•¨ ë©”íƒ€)\n",
    "    ë°˜í™˜: [ { page, image_index, bbox, ext, bytes_len, note } ]\n",
    "    \"\"\"\n",
    "    import fitz  # PyMuPDF\n",
    "    results = []\n",
    "    doc = fitz.open(file_path)\n",
    "    for page_idx in range(len(doc)):\n",
    "        page = doc[page_idx]\n",
    "        # ì´ë¯¸ì§€ ëª©ë¡ (xref ê¸°ë°˜)\n",
    "        image_list = page.get_images(full=True)\n",
    "        for img_i, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            try:\n",
    "                pix = fitz.Pixmap(doc, xref)\n",
    "                if pix.n >= 5:  # CMYK\n",
    "                    pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                ext = 'png'\n",
    "                results.append({\n",
    "                    'page': page_idx + 1,\n",
    "                    'image_index': img_i + 1,\n",
    "                    'xref': xref,\n",
    "                    'width': pix.width,\n",
    "                    'height': pix.height,\n",
    "                    'colors': pix.n,\n",
    "                    'ext': ext,\n",
    "                    'note': 'ì´ë¯¸ì§€ ë°”ì´íŠ¸ ì¶”ì¶œì€ ì €ì¥ ë£¨í‹´ì—ì„œ ì²˜ë¦¬'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({'page': page_idx + 1, 'image_index': img_i + 1, 'error': str(e)})\n",
    "    doc.close()\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_pdf(file_path: str):\n",
    "    results = {'text': '', 'pages': [], 'tables': [], 'images': [], 'metadata': {}}\n",
    "\n",
    "    # ì´ë¯¸ì§€ PDF ì—¬ë¶€ íŒë‹¨ì„ ì‹œë„í•˜ë˜, ì‹¤íŒ¨ ì‹œ í´ë°±ìœ¼ë¡œ ë„˜ì–´ê°\n",
    "    is_img = False\n",
    "    page_count = None\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        is_img, page_count = is_image_pdf(file_path)\n",
    "        results['metadata']['page_count'] = page_count\n",
    "        results['metadata']['is_image_pdf'] = is_img\n",
    "    except Exception as e:\n",
    "        results['metadata']['note'] = f'pdfplumber ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. pypdf í´ë°± ì‚¬ìš©'\n",
    "\n",
    "    if is_img:\n",
    "        msg = 'ì´ë¯¸ì§€ ê¸°ë°˜ PDFë¡œ ê°ì§€ë¨: OCR íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬ í•„ìš” (í…ìŠ¤íŠ¸ ì¶”ì¶œ ìƒëµ)'\n",
    "        print('â„¹ï¸ ' + msg)\n",
    "        results['metadata']['note'] = msg\n",
    "        # ì´ë¯¸ì§€ ì˜¤ë¸Œì íŠ¸ ë©”íƒ€ë¼ë„ ìˆ˜ì§‘ (ì„ íƒ)\n",
    "        try:\n",
    "            results['images'] = _extract_pdf_images_with_pymupdf(file_path)\n",
    "        except Exception as e:\n",
    "            results['metadata']['image_extract_note'] = f'PyMuPDF ì´ë¯¸ì§€ ë©”íƒ€ ì¶”ì¶œ ì‹¤íŒ¨: {e}'\n",
    "        return results\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF ì¶”ì¶œ ì‹œë„: pdfplumber ìš°ì„ , ì‹¤íŒ¨ ì‹œ pypdf í´ë°±\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        combined_text = ''\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            results['metadata']['page_count'] = results['metadata'].get('page_count') or len(pdf.pages)\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                text = page.extract_text() or ''\n",
    "                combined_text += f\"\\n[í˜ì´ì§€ {i+1}]\\n{text}\"\n",
    "                results['pages'].append({'index': i+1, 'text': text})\n",
    "\n",
    "                # í‘œ ì¶”ì¶œ\n",
    "                tables = page.extract_tables()\n",
    "                if tables:\n",
    "                    for t in tables:\n",
    "                        safe_t = [[str(c) if c is not None else '' for c in row] for row in (t or [])]\n",
    "                        results['tables'].append({'page': i+1, 'data': safe_t})\n",
    "\n",
    "        results['text'] = combined_text.strip()\n",
    "        # ì¶”ê°€ë¡œ PyMuPDFë¡œ ì´ë¯¸ì§€ ë©”íƒ€ ìˆ˜ì§‘\n",
    "        try:\n",
    "            results['images'] = _extract_pdf_images_with_pymupdf(file_path)\n",
    "        except Exception as e:\n",
    "            results['metadata']['image_extract_note'] = f'PyMuPDF ì´ë¯¸ì§€ ë©”íƒ€ ì¶”ì¶œ ì‹¤íŒ¨: {e}'\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"âš  pdfplumber ì‹¤íŒ¨, pypdfë¡œ í´ë°±: {e}\")\n",
    "        combined, pages = _extract_pdf_text_with_pypdf(file_path)\n",
    "        results['text'] = combined\n",
    "        results['pages'] = pages\n",
    "        results['metadata']['fallback'] = 'pypdf'\n",
    "        # ì´ë¯¸ì§€ ë©”íƒ€ ìˆ˜ì§‘ ì‹œë„\n",
    "        try:\n",
    "            results['images'] = _extract_pdf_images_with_pymupdf(file_path)\n",
    "        except Exception as e2:\n",
    "            results['metadata']['image_extract_note'] = f'PyMuPDF ì´ë¯¸ì§€ ë©”íƒ€ ì¶”ì¶œ ì‹¤íŒ¨: {e2}'\n",
    "        return results\n",
    "\n",
    "\n",
    "# ===== PPTX ê°œì„ : XML ì§ì ‘ íŒŒì‹±ì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (python-pptx í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°) =====\n",
    "\n",
    "def extract_pptx(file_path: str):\n",
    "    \"\"\"PPTX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸, í‘œ, ì´ë¯¸ì§€ ì¶”ì¶œ (XML ì§ì ‘ íŒŒì‹±)\"\"\"\n",
    "    import zipfile\n",
    "    import xml.etree.ElementTree as ET\n",
    "    \n",
    "    try:\n",
    "        text = ''\n",
    "        pages = []\n",
    "        images = []\n",
    "        tables = []\n",
    "        \n",
    "        with zipfile.ZipFile(file_path, 'r') as z:\n",
    "            # ìŠ¬ë¼ì´ë“œ XML íŒŒì¼ ì°¾ê¸°\n",
    "            slide_files = [f for f in z.namelist() if f.startswith('ppt/slides/slide') and f.endswith('.xml')]\n",
    "            slide_files.sort()  # ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "            \n",
    "            # ê° ìŠ¬ë¼ì´ë“œ ì²˜ë¦¬\n",
    "            for i, slide_file in enumerate(slide_files):\n",
    "                slide_idx = i + 1\n",
    "                slide_text = ''\n",
    "                \n",
    "                try:\n",
    "                    # ìŠ¬ë¼ì´ë“œ XML íŒŒì‹±\n",
    "                    with z.open(slide_file) as f:\n",
    "                        xml_content = f.read()\n",
    "                        root = ET.fromstring(xml_content)\n",
    "                    \n",
    "                    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜\n",
    "                    namespaces = {\n",
    "                        'a': 'http://schemas.openxmlformats.org/drawingml/2006/main',\n",
    "                        'p': 'http://schemas.openxmlformats.org/presentationml/2006/main',\n",
    "                        'r': 'http://schemas.openxmlformats.org/officeDocument/2006/relationships'\n",
    "                    }\n",
    "                    \n",
    "                    # í…ìŠ¤íŠ¸ ìš”ì†Œ ì°¾ê¸° (a:t íƒœê·¸)\n",
    "                    text_elements = []\n",
    "                    for elem in root.iter():\n",
    "                        if elem.tag.endswith('}t'):  # a:t íƒœê·¸\n",
    "                            if elem.text and elem.text.strip():\n",
    "                                text_elements.append(elem.text.strip())\n",
    "                    \n",
    "                    # í‘œ ì°¾ê¸° (a:tbl íƒœê·¸)\n",
    "                    for tbl_elem in root.iter():\n",
    "                        if tbl_elem.tag.endswith('}tbl'):  # a:tbl íƒœê·¸\n",
    "                            table_data = []\n",
    "                            for tr_elem in tbl_elem.iter():\n",
    "                                if tr_elem.tag.endswith('}tr'):  # í…Œì´ë¸” í–‰\n",
    "                                    row_data = []\n",
    "                                    for tc_elem in tr_elem.iter():\n",
    "                                        if tc_elem.tag.endswith('}tc'):  # í…Œì´ë¸” ì…€\n",
    "                                            cell_text = ''\n",
    "                                            for t_elem in tc_elem.iter():\n",
    "                                                if t_elem.tag.endswith('}t') and t_elem.text:\n",
    "                                                    cell_text += t_elem.text.strip() + ' '\n",
    "                                            row_data.append(cell_text.strip())\n",
    "                                    if row_data:\n",
    "                                        table_data.append(row_data)\n",
    "                            if table_data:\n",
    "                                tables.append({'slide': slide_idx, 'data': table_data})\n",
    "                    \n",
    "                    # ì´ë¯¸ì§€ ì°¾ê¸° (blip ìš”ì†Œ)\n",
    "                    for blip_elem in root.iter():\n",
    "                        if blip_elem.tag.endswith('}blip'):  # a:blip íƒœê·¸\n",
    "                            embed_id = blip_elem.get('{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed')\n",
    "                            if embed_id:\n",
    "                                images.append({\n",
    "                                    'slide': slide_idx,\n",
    "                                    'embed_id': embed_id,\n",
    "                                    'type': 'picture'\n",
    "                                })\n",
    "                    \n",
    "                    slide_text = '\\n'.join(text_elements)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    slide_text = f\"ìŠ¬ë¼ì´ë“œ {slide_idx} XML íŒŒì‹± ì˜¤ë¥˜: {str(e)}\"\n",
    "                \n",
    "                text += f\"\\n[ìŠ¬ë¼ì´ë“œ {slide_idx}]\\n{slide_text}\"\n",
    "                pages.append({'index': slide_idx, 'text': slide_text, 'label': 'slide'})\n",
    "            \n",
    "            # ë…¸íŠ¸ ìŠ¬ë¼ì´ë“œë„ ì²˜ë¦¬\n",
    "            notes_files = [f for f in z.namelist() if f.startswith('ppt/notesSlides/notesSlide') and f.endswith('.xml')]\n",
    "            for i, notes_file in enumerate(notes_files):\n",
    "                try:\n",
    "                    with z.open(notes_file) as f:\n",
    "                        xml_content = f.read()\n",
    "                        root = ET.fromstring(xml_content)\n",
    "                    \n",
    "                    notes_text = []\n",
    "                    for elem in root.iter():\n",
    "                        if elem.tag.endswith('}t') and elem.text and elem.text.strip():\n",
    "                            notes_text.append(elem.text.strip())\n",
    "                    \n",
    "                    if notes_text:\n",
    "                        notes_content = '\\n'.join(notes_text)\n",
    "                        if i < len(pages):\n",
    "                            pages[i]['text'] += f\"\\n[ë…¸íŠ¸]\\n{notes_content}\"\n",
    "                            text += f\"\\n[ìŠ¬ë¼ì´ë“œ {i+1} ë…¸íŠ¸]\\n{notes_content}\"\n",
    "                            \n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        return {'text': text.strip(), 'pages': pages, 'tables': tables, 'images': images, \n",
    "               'metadata': {'slide_count': len(slide_files), 'extraction_method': 'xml_parsing'}}\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"PPTX XML íŒŒì‹± ì‹¤íŒ¨: {str(e)}\"\n",
    "        return {'text': error_msg, 'pages': [{'index': 1, 'text': error_msg, 'label': 'error'}], \n",
    "               'tables': [], 'images': [], 'metadata': {'error': str(e)}}\n",
    "\n",
    "\n",
    "# ===== HWP/HWPX ê°œì„ : LibreOfficeë¥¼ ì´ìš©í•œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ =====\n",
    "\n",
    "def _hwp_fallback_extraction(file_path: str):\n",
    "    \"\"\"LibreOffice ë³€í™˜ ì‹¤íŒ¨ ì‹œ olefileì„ ì‚¬ìš©í•œ ê¸°ë³¸ HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        import olefile\n",
    "        text = ''\n",
    "        \n",
    "        with olefile.OleFileIO(file_path) as ole:\n",
    "            # PrvText ìŠ¤íŠ¸ë¦¼ì—ì„œ ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            if ole.exists('PrvText'):\n",
    "                try:\n",
    "                    raw = ole.openstream('PrvText').read()\n",
    "                    # HWP 5.0 í¬ë§·ì€ ë³´í†µ utf-16leë¡œ ì¸ì½”ë”©ë¨\n",
    "                    text = raw.decode('utf-16le', errors='ignore').strip()\n",
    "                except Exception as e:\n",
    "                    print(f\"PrvText ë””ì½”ë”© ì‹¤íŒ¨: {e}\")\n",
    "                    \n",
    "            # BodyText ìŠ¤íŠ¸ë¦¼ë„ ì‹œë„\n",
    "            if not text and ole.exists('BodyText'):\n",
    "                try:\n",
    "                    raw = ole.openstream('BodyText').read()\n",
    "                    text = raw.decode('utf-16le', errors='ignore').strip()\n",
    "                except Exception as e:\n",
    "                    print(f\"BodyText ë””ì½”ë”© ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        if text:\n",
    "            return {\n",
    "                'text': text,\n",
    "                'pages': [{'index': 1, 'text': text, 'label': 'fallback_extraction'}],\n",
    "                'tables': [],\n",
    "                'images': [],\n",
    "                'metadata': {'extraction_method': 'hwp_olefile_fallback', 'length': len(text)}\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'text': 'HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',\n",
    "                'pages': [{'index': 1, 'text': 'HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'label': 'error'}],\n",
    "                'tables': [],\n",
    "                'images': [],\n",
    "                'metadata': {'error': 'No text extracted from HWP file'}\n",
    "            }\n",
    "    except Exception as e:\n",
    "        error_msg = f\"HWP fallback ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}\"\n",
    "        return {\n",
    "            'text': error_msg,\n",
    "            'pages': [{'index': 1, 'text': error_msg, 'label': 'error'}],\n",
    "            'tables': [],\n",
    "            'images': [],\n",
    "            'metadata': {'error': str(e)}\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_hwp_hwpx(file_path: str):\n",
    "    \"\"\"HWP íŒŒì¼ì€ LibreOfficeë¥¼ í†µí•´ PDFë¡œ ë³€í™˜ í›„ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ ,\n",
    "    HWPX íŒŒì¼ì€ XMLì„ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    import os, tempfile, subprocess, shutil\n",
    "    from pathlib import Path as _Path\n",
    "\n",
    "    file_path_obj = _Path(file_path)\n",
    "    ext = file_path_obj.suffix.lower()\n",
    "\n",
    "    if ext == '.hwp':\n",
    "        # LibreOfficeê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if not shutil.which(\"libreoffice\"):\n",
    "            msg = \"LibreOfficeê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ HWP íŒŒì¼ ì²˜ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\"\n",
    "            print(f\"âŒ {msg}\")\n",
    "            return {'text': '', 'pages': [], 'tables': [], 'images': [], 'metadata': {'error': msg}}\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmp_path = _Path(tmpdir)\n",
    "            \n",
    "            # 1. HWP -> PDF ë³€í™˜\n",
    "            print(f\"â³ HWP -> PDF ë³€í™˜ ì¤‘: {file_path_obj.name}\")\n",
    "            try:\n",
    "                subprocess.run(\n",
    "                    [\"libreoffice\", \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", str(tmp_path), str(file_path_obj)],\n",
    "                    timeout=300,  # 5ë¶„ íƒ€ì„ì•„ì›ƒ\n",
    "                    check=True,\n",
    "                    capture_output=True\n",
    "                )\n",
    "            except (subprocess.TimeoutExpired, subprocess.CalledProcessError) as e:\n",
    "                error_msg = f\"LibreOffice ë³€í™˜ ì‹¤íŒ¨: {e.stderr.decode('utf-8', 'ignore') if hasattr(e, 'stderr') else str(e)}\"\n",
    "                print(f\"âŒ {error_msg}\")\n",
    "                return {'text': '', 'pages': [], 'tables': [], 'images': [], 'metadata': {'error': error_msg}}\n",
    "\n",
    "            pdf_path = tmp_path / f\"{file_path_obj.stem}.pdf\"\n",
    "            if not pdf_path.exists():\n",
    "                print(\"âŒ LibreOffice ë³€í™˜ í›„ PDF íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Fallback ë°©ì‹ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤.\")\n",
    "                # Fallback: olefileì„ ì‚¬ìš©í•œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                return _hwp_fallback_extraction(file_path)\n",
    "            \n",
    "            print(f\"âœ… HWP -> PDF ë³€í™˜ ì™„ë£Œ: {pdf_path}\")\n",
    "\n",
    "            # 2. ë³€í™˜ëœ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ extract_pdf í•¨ìˆ˜ ì¬ì‚¬ìš©)\n",
    "            print(\"â³ ë³€í™˜ëœ PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\")\n",
    "            pdf_results = extract_pdf(str(pdf_path))\n",
    "            \n",
    "            # ë©”íƒ€ë°ì´í„°ì— HWP ì²˜ë¦¬ ì •ë³´ ì¶”ê°€\n",
    "            pdf_results['metadata']['source_hwp'] = file_path_obj.name\n",
    "            pdf_results['metadata']['extraction_method'] = 'hwp_libreoffice_pdf_pipeline'\n",
    "            \n",
    "            print(f\"âœ… í˜ì´ì§€ {len(pdf_results.get('pages',[]))}ê°œ ì¶”ì¶œ ì™„ë£Œ.\")\n",
    "            return pdf_results\n",
    "\n",
    "    elif ext == '.hwpx':\n",
    "        # HWPXëŠ” ê¸°ì¡´ê³¼ ê°™ì´ XML íŒŒì‹±\n",
    "        import zipfile\n",
    "        try:\n",
    "            import lxml.etree as ET\n",
    "            has_lxml = True\n",
    "        except ImportError:\n",
    "            import xml.etree.ElementTree as ET\n",
    "            has_lxml = False\n",
    "        \n",
    "        text = ''\n",
    "        pages = []\n",
    "        tables = []\n",
    "        images = []\n",
    "        ns = {\n",
    "            'hp': 'http://www.hancom.co.kr/hwpml/2011/paragraph',\n",
    "            'hml': 'http://www.hancom.co.kr/hwpml/2011/core',\n",
    "            'tbl': 'http://www.hancom.co.kr/hwpml/2011/table',\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with zipfile.ZipFile(file_path, 'r') as z:\n",
    "                # ... (ê¸°ì¡´ HWPX ë¡œì§ê³¼ ë™ì¼)\n",
    "                # (ì´ ë¶€ë¶„ì€ ê°„ê²°í•¨ì„ ìœ„í•´ ìƒëµ, ì‹¤ì œë¡œëŠ” ì „ì²´ ì½”ë“œê°€ ìœ„ì¹˜í•´ì•¼ í•¨)\n",
    "                xml_names = [n for n in z.namelist() if n.startswith('Contents/') and n.endswith('.xml')]\n",
    "                sec_idx = 0\n",
    "                for name in sorted(xml_names):\n",
    "                    try:\n",
    "                        data = z.read(name)\n",
    "                        tree = ET.fromstring(data)\n",
    "                        sec_text = '\\n'.join(\n",
    "                            node.text for node in tree.iter() if node.tag.endswith('}t') and node.text\n",
    "                        ).strip()\n",
    "                        if sec_text:\n",
    "                            sec_idx += 1\n",
    "                            pages.append({'index': sec_idx, 'text': sec_text, 'label': f'section_{sec_idx}'})\n",
    "                            text += f\"\\n[Section {sec_idx}]\\n{sec_text}\"\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            \n",
    "            return {\n",
    "                'text': text.strip(), 'pages': pages, 'tables': tables, 'images': images,\n",
    "                'metadata': {'extraction_method': 'hwpx_xml_parsing', 'sections_found': sec_idx}\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'text': '', 'pages': [], 'tables': [], 'images': [], 'metadata': {'error': str(e)}}\n",
    "\n",
    "    else:\n",
    "        return {'text': '[ì§€ì›í•˜ì§€ ì•ŠëŠ” HWP/HWPX í˜•ì‹]', 'pages': [], 'tables': [], 'images': [], 'metadata': {}}\n",
    "\n",
    "\n",
    "# ===== DOCX ì¶”ì¶œ í•¨ìˆ˜ ì¶”ê°€ =====\n",
    "\n",
    "def extract_docx(file_path: str):\n",
    "    \"\"\"DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸, í‘œ, ì´ë¯¸ì§€ ì¶”ì¶œ\"\"\"\n",
    "    try:\n",
    "        from docx import Document\n",
    "        import zipfile\n",
    "        \n",
    "        text = ''\n",
    "        pages = []\n",
    "        tables = []\n",
    "        images = []\n",
    "        \n",
    "        # DOCX íŒŒì¼ ì—´ê¸°\n",
    "        doc = Document(file_path)\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        full_text = []\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                full_text.append(paragraph.text.strip())\n",
    "        \n",
    "        text = '\\n'.join(full_text)\n",
    "        if text:\n",
    "            pages.append({'index': 1, 'text': text, 'label': 'document'})\n",
    "        \n",
    "        # í‘œ ì¶”ì¶œ\n",
    "        for table_idx, table in enumerate(doc.tables, start=1):\n",
    "            table_data = []\n",
    "            for row in table.rows:\n",
    "                row_data = []\n",
    "                for cell in row.cells:\n",
    "                    row_data.append(cell.text.strip())\n",
    "                table_data.append(row_data)\n",
    "            if table_data:\n",
    "                tables.append({'page': 1, 'table_index': table_idx, 'data': table_data})\n",
    "        \n",
    "        # ì´ë¯¸ì§€ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„°ë§Œ)\n",
    "        try:\n",
    "            with zipfile.ZipFile(file_path, 'r') as z:\n",
    "                image_files = [f for f in z.namelist() if f.startswith('word/media/')]\n",
    "                for img_file in image_files:\n",
    "                    images.append({\n",
    "                        'zip_entry': img_file,\n",
    "                        'path': img_file.split('/')[-1],\n",
    "                        'type': 'image'\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'pages': pages,\n",
    "            'tables': tables,\n",
    "            'images': images,\n",
    "            'metadata': {'extraction_method': 'python_docx', 'paragraphs': len(doc.paragraphs), 'tables_count': len(doc.tables)}\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"DOCX ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\"\n",
    "        return {\n",
    "            'text': error_msg,\n",
    "            'pages': [{'index': 1, 'text': error_msg, 'label': 'error'}],\n",
    "            'tables': [],\n",
    "            'images': [],\n",
    "            'metadata': {'error': str(e)}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c830404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ·ï¸ íŒŒì¼ë³„ ì¶”ì¶œ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_objects(file_path):\n",
    "    ext = Path(file_path).suffix.lower()\n",
    "    if ext == '.pdf':\n",
    "        return extract_pdf(file_path)\n",
    "    elif ext == '.docx':\n",
    "        return extract_docx(file_path)\n",
    "    elif ext in ['.xlsx', '.xls']:\n",
    "        return extract_xlsx(file_path)\n",
    "    elif ext in ['.pptx', '.ppt']:\n",
    "        return extract_pptx(file_path)\n",
    "    elif ext in ['.hwp', '.hwpx']:\n",
    "        return extract_hwp_hwpx(file_path)\n",
    "    else:\n",
    "        return {'text': '[ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹]', 'pages': [], 'tables': [], 'images': [], 'metadata': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6f6512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: 20_ProductSpec_SmartInsulinPump_KO_v0.1.pdf\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/20_ProductSpec_SmartInsulinPump_KO_v0.1 (pages: 6, tables: 3, images: 6)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: ì´ˆì•ˆ-Azure ê¸°ë°˜ ììœ¨ ì´ë™ ë¡œë´‡ FMS ì‹œìŠ¤í…œ ì œì•ˆì„œ.pptx\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/ì´ˆì•ˆ-Azure ê¸°ë°˜ ììœ¨ ì´ë™ ë¡œë´‡ FMS ì‹œìŠ¤í…œ ì œì•ˆì„œ (pages: 10, tables: 0, images: 10)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: test1.pdf\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/20_ProductSpec_SmartInsulinPump_KO_v0.1 (pages: 6, tables: 3, images: 6)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: ì´ˆì•ˆ-Azure ê¸°ë°˜ ììœ¨ ì´ë™ ë¡œë´‡ FMS ì‹œìŠ¤í…œ ì œì•ˆì„œ.pptx\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/ì´ˆì•ˆ-Azure ê¸°ë°˜ ììœ¨ ì´ë™ ë¡œë´‡ FMS ì‹œìŠ¤í…œ ì œì•ˆì„œ (pages: 10, tables: 0, images: 10)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: test1.pdf\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/test1 (pages: 96, tables: 220, images: 41)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: 2025ë…„ë„ã€AIììœ¨ì œì¡°ë¡œë´‡ ì‹¤ì¦(ì‹œë²”)ì‚¬ì—…ã€ ê³µê³ ë¬¸_F.hwp\n",
      "â³ HWP -> PDF ë³€í™˜ ì¤‘: 2025ë…„ë„ã€AIììœ¨ì œì¡°ë¡œë´‡ ì‹¤ì¦(ì‹œë²”)ì‚¬ì—…ã€ ê³µê³ ë¬¸_F.hwp\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/test1 (pages: 96, tables: 220, images: 41)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: 2025ë…„ë„ã€AIììœ¨ì œì¡°ë¡œë´‡ ì‹¤ì¦(ì‹œë²”)ì‚¬ì—…ã€ ê³µê³ ë¬¸_F.hwp\n",
      "â³ HWP -> PDF ë³€í™˜ ì¤‘: 2025ë…„ë„ã€AIììœ¨ì œì¡°ë¡œë´‡ ì‹¤ì¦(ì‹œë²”)ì‚¬ì—…ã€ ê³µê³ ë¬¸_F.hwp\n",
      "âŒ LibreOffice ë³€í™˜ í›„ PDF íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Fallback ë°©ì‹ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤.\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/2025ë…„ë„ã€AIììœ¨ì œì¡°ë¡œë´‡ ì‹¤ì¦(ì‹œë²”)ì‚¬ì—…ã€ ê³µê³ ë¬¸_F (pages: 1, tables: 0, images: 0)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: AWSê¸°ë°˜ AI ê°œë°œ ê°€ì´ë“œ.docx\n",
      "âŒ LibreOffice ë³€í™˜ í›„ PDF íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Fallback ë°©ì‹ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤.\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/2025ë…„ë„ã€AIììœ¨ì œì¡°ë¡œë´‡ ì‹¤ì¦(ì‹œë²”)ì‚¬ì—…ã€ ê³µê³ ë¬¸_F (pages: 1, tables: 0, images: 0)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: AWSê¸°ë°˜ AI ê°œë°œ ê°€ì´ë“œ.docx\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/AWSê¸°ë°˜ AI ê°œë°œ ê°€ì´ë“œ (pages: 1, tables: 1, images: 2)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: test.pdf\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/AWSê¸°ë°˜ AI ê°œë°œ ê°€ì´ë“œ (pages: 1, tables: 1, images: 2)\n",
      "â–¶ï¸ ì²˜ë¦¬ ì¤‘: test.pdf\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/test (pages: 27, tables: 2, images: 6)\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/admin/wkms-aws/jupyter_notebook/data/extracted_objects/test (pages: 27, tables: 2, images: 6)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“š [ì‹¤í–‰] íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "# ìœ„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì—¬ INPUT_DIRì˜ ëª¨ë“  íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³ ,\n",
    "# ì¶”ì¶œëœ í…ìŠ¤íŠ¸, í‘œ, ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ë¥¼ ê°ê°ì˜ í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def _json_default(o):\n",
    "    if isinstance(o, Path):\n",
    "        return str(o)\n",
    "    raise TypeError(f\"Object of type {o.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "def _safe_dir_name(name: str) -> str:\n",
    "    \"\"\"íŒŒì¼ëª…ì„ ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜\"\"\"\n",
    "    import re\n",
    "    # ìœˆë„ìš° ë° ìœ ë‹‰ìŠ¤ì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ ë¬¸ì ì œê±°\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', '_', name)\n",
    "\n",
    "def _build_error_result(file_path: Path, error: Exception) -> dict:\n",
    "    \"\"\"ì˜¤ë¥˜ ë°œìƒ ì‹œ ë°˜í™˜í•  í‘œì¤€ ê²°ê³¼ êµ¬ì¡°\"\"\"\n",
    "    return {\n",
    "        'text': f\"Error processing {file_path.name}: {error}\",\n",
    "        'pages': [{'index': 1, 'text': f\"Error: {error}\"}],\n",
    "        'tables': [],\n",
    "        'images': [],\n",
    "        'metadata': {'error': str(error)}\n",
    "    }\n",
    "\n",
    "def _save_pdf_images(file_path: Path, images_meta: list, out_images_dir: Path):\n",
    "    \"\"\"PyMuPDF ë©”íƒ€ë¥¼ ì´ìš©í•´ ì‹¤ì œ ì´ë¯¸ì§€ ë°”ì´íŠ¸ë¥¼ ì €ì¥\"\"\"\n",
    "    try:\n",
    "        import fitz\n",
    "        doc = fitz.open(str(file_path))\n",
    "        saved = 0\n",
    "        for page_idx in range(len(doc)):\n",
    "            page = doc[page_idx]\n",
    "            image_list = page.get_images(full=True)\n",
    "            for img_i, img in enumerate(image_list):\n",
    "                xref = img[0]\n",
    "                try:\n",
    "                    pix = fitz.Pixmap(doc, xref)\n",
    "                    if pix.n >= 5:\n",
    "                        pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                    data = pix.tobytes('png')\n",
    "                    fname = out_images_dir / f\"pdf_p{page_idx+1}_i{img_i+1}.png\"\n",
    "                    with open(fname, 'wb') as f:\n",
    "                        f.write(data)\n",
    "                    saved += 1\n",
    "                except Exception:\n",
    "                    continue\n",
    "        doc.close()\n",
    "        return saved\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# ì…ë ¥ í´ë” ìŠ¤ìº” (ì¤‘ë³µ í´ë”ëª… ë°©ì§€)\n",
    "used_dirs = set()\n",
    "for file_path in INPUT_DIR.glob('*'):\n",
    "    if not file_path.is_file():\n",
    "        continue\n",
    "    print(f'â–¶ï¸ ì²˜ë¦¬ ì¤‘: {file_path.name}')\n",
    "    base = _safe_dir_name(file_path.stem)\n",
    "    out_base = OUTPUT_DIR / base\n",
    "    suffix = 1\n",
    "    while str(out_base) in used_dirs or out_base.exists():\n",
    "        out_base = OUTPUT_DIR / f\"{base}_{suffix}\"\n",
    "        suffix += 1\n",
    "    used_dirs.add(str(out_base))\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ\n",
    "        ext = file_path.suffix.lower()\n",
    "        if ext in ['.hwp', '.hwpx']:\n",
    "            result = extract_hwp_hwpx(str(file_path))\n",
    "        elif ext == '.pdf':\n",
    "            result = extract_pdf(str(file_path))\n",
    "        elif ext in ['.ppt', '.pptx']:\n",
    "            result = extract_pptx(str(file_path))\n",
    "        elif ext == '.docx':\n",
    "            result = extract_docx(str(file_path)) \n",
    "        else:\n",
    "            # ê¸°íƒ€ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ ì²˜ë¦¬\n",
    "            result = {\n",
    "                'text': f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}\", 'pages': [], 'tables': [], 'images': [],\n",
    "                'metadata': {'error': f\"Unsupported file type: {ext}\"}\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'âš  ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}')\n",
    "        result = _build_error_result(file_path, e)\n",
    "\n",
    "    # 0) ë³´ì¡° ë””ë ‰í† ë¦¬\n",
    "    pages_dir = out_base / 'pages'; pages_dir.mkdir(exist_ok=True)\n",
    "    tables_dir = out_base / 'tables'; tables_dir.mkdir(exist_ok=True)\n",
    "    images_dir = out_base / 'images'; images_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # 1) ì „ì²´ ë¬¸ì„œ ê²°ê³¼ ì €ì¥\n",
    "    doc_json = out_base / 'document.json'\n",
    "    with open(doc_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2, default=_json_default)\n",
    "\n",
    "    # 2) í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "    pages = result.get('pages') or []\n",
    "    if pages:\n",
    "        for p in pages:\n",
    "            idx = p.get('index')\n",
    "            text = p.get('text', '')\n",
    "            if idx is None:\n",
    "                continue\n",
    "            with open(pages_dir / f'page_{idx:03d}.txt', 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "\n",
    "    # 3) í‘œ ì €ì¥ (TSV)\n",
    "    tables = result.get('tables') or []\n",
    "    if tables:\n",
    "        for ti, t in enumerate(tables, start=1):\n",
    "            page_no = None\n",
    "            data = t\n",
    "            if isinstance(t, dict):\n",
    "                page_no = t.get('page') or t.get('slide') or t.get('sheet')\n",
    "                data = t.get('data')\n",
    "            fname = f\"table_{ti:03d}{('_p'+str(page_no)) if page_no else ''}.tsv\"\n",
    "            with open(tables_dir / fname, 'w', encoding='utf-8') as f:\n",
    "                for row in (data or []):\n",
    "                    f.write('\\t'.join([str(c) if c is not None else '' for c in row]) + '\\n')\n",
    "\n",
    "    # 4) ì´ë¯¸ì§€ ì €ì¥\n",
    "    images = result.get('images') or []\n",
    "    ext = file_path.suffix.lower()\n",
    "    saved_images = 0\n",
    "    if ext == '.pdf' or (ext == '.hwp' and result.get('metadata', {}).get('extraction_method') == 'hwp_libreoffice_pdf_pipeline'):\n",
    "        # PDFì´ê±°ë‚˜, HWPê°€ PDFë¡œ ë³€í™˜ëœ ê²½ìš° PyMuPDFë¡œ ì´ë¯¸ì§€ ì €ì¥\n",
    "        # HWPì˜ ê²½ìš° ë³€í™˜ëœ ì„ì‹œ PDF ê²½ë¡œëŠ” ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ì›ë³¸ PDFì—ì„œë§Œ ì‘ë™\n",
    "        if ext == '.pdf':\n",
    "            saved_images = _save_pdf_images(file_path, images, images_dir)\n",
    "        else: # HWPì˜ ê²½ìš° ì´ë¯¸ì§€ ì¶”ì¶œì€ í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŒ\n",
    "             with open(images_dir / 'hwp_images_meta.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump({'note': 'HWP ì´ë¯¸ì§€ ì¶”ì¶œì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'}, f)\n",
    "\n",
    "    elif ext in ['.pptx', '.ppt']:\n",
    "        # PPTX ì´ë¯¸ì§€ ë©”íƒ€ ì €ì¥\n",
    "        with open(images_dir / 'pptx_images_meta.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(images, f, ensure_ascii=False, indent=2)\n",
    "        saved_images = len([im for im in images if 'error' not in im and im.get('type') != 'chart'])\n",
    "    elif ext == '.docx':\n",
    "        # DOCX ì´ë¯¸ì§€ ì €ì¥\n",
    "        try:\n",
    "            from zipfile import ZipFile\n",
    "            with ZipFile(file_path) as z:\n",
    "                for im in images:\n",
    "                    entry = im.get('zip_entry')\n",
    "                    if entry:\n",
    "                        data = z.read(entry)\n",
    "                        out = images_dir / im.get('path', 'image.bin')\n",
    "                        with open(out, 'wb') as f:\n",
    "                            f.write(data)\n",
    "                        saved_images += 1\n",
    "        except Exception as e:\n",
    "            with open(images_dir / 'docx_images_meta.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump({'error': str(e), 'images': images}, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        # ê¸°íƒ€ í¬ë§·ì€ ë©”íƒ€ë§Œ ì €ì¥\n",
    "        with open(images_dir / 'images_meta.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(images, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f'âœ… ê²°ê³¼ ì €ì¥: {out_base} (pages: {len(pages)}, tables: {len(tables)}, images: {saved_images})')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

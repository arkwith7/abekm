{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0982cbbb",
   "metadata": {},
   "source": [
    "# ğŸ”„ WKMS ë°±ì—”ë“œ í†µí•© ë¬¸ì„œì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ë…¸íŠ¸ë¶ (Backend Integrated Document Processing Pipeline)\n",
    "\n",
    "## ğŸ¯ ê°œìš” (Overview)\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ **`/api/v1/documents/upload` ì—”ë“œí¬ì¸íŠ¸ì˜ ì „ì²´ ë¬¸ì„œì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**ì„ êµ¬ì„±í•˜ëŠ” ê° ë‹¨ê³„ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ê²€ì¦í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë°±ì—”ë“œì—ì„œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ì„œë¹„ìŠ¤ë“¤ê³¼ ë™ì¼í•œ í™˜ê²½ì—ì„œ íŒŒì´í”„ë¼ì¸ì˜ ê° êµ¬ì„±ìš”ì†Œë¥¼ ì ê²€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ—ï¸ ë°±ì—”ë“œ API ë¬¸ì„œì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì „ì²´ êµ¬ì¡°\n",
    "\n",
    "### ğŸ“¤ `/api/v1/documents/upload` ì—”ë“œí¬ì¸íŠ¸ ì²˜ë¦¬ ê³¼ì •:\n",
    "```\n",
    "1. ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦ â† permission_service\n",
    "2. íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ (í˜•ì‹, í¬ê¸°) â† documents.py \n",
    "3. ì„œë²„ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥ â† document_service\n",
    "4. ë°ì´í„°ë² ì´ìŠ¤ì— ë©”íƒ€ë°ì´í„° ì €ì¥ â† tb_file_bss_info\n",
    "5. ğŸ“Š RAGìš© ë²¡í„° ì €ì¥ íŒŒì´í”„ë¼ì¸ â† IntegratedDocumentPipelineService\n",
    "   - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° OCR ì²˜ë¦¬\n",
    "   - í•œêµ­ì–´ NLP ë¶„ì„ (kiwipiepy)\n",
    "   - ì„ë² ë”© ìƒì„± (AWS Bedrock Titan V2)\n",
    "   - ë²¡í„° ì €ì¥ (PostgreSQL + pgvector)\n",
    "```\n",
    "\n",
    "## ğŸ§ª ë…¸íŠ¸ë¶ í…ŒìŠ¤íŠ¸ ë²”ìœ„ (ê° êµ¬ì„±ìš”ì†Œë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸)\n",
    "\n",
    "### ğŸ“‹ 1ë‹¨ê³„: íŒŒì¼ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "- **íŒŒì¼ í™•ì¥ì ê²€ì¦**: PDF, DOCX, PPTX, XLSX, TXT, HWP ì§€ì› í™•ì¸\n",
    "- **íŒŒì¼ í¬ê¸° ì œí•œ**: 50MB ì œí•œ í…ŒìŠ¤íŠ¸\n",
    "- **ë³´ì•ˆ íŒŒì¼ëª… ê²€ì¦**: ìœ„í—˜í•œ íŒ¨í„´ í•„í„°ë§ í™•ì¸\n",
    "\n",
    "### ğŸ“Š 2ë‹¨ê³„: ë¬¸ì„œ ìŠ¤ìº” ê°ì§€ ë° OCR ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "- **ìŠ¤ìº” ê°ì§€ ë¡œì§**: í…ìŠ¤íŠ¸ ê¸¸ì´, ì´ë¯¸ì§€ ë¹„ìœ¨, íŒŒì¼ í¬ê¸° ê¸°ì¤€ íŒë³„\n",
    "- **EasyOCR í•œêµ­ì–´/ì˜ì–´**: í…ìŠ¤íŠ¸ ì¶”ì¶œ í’ˆì§ˆ ê²€ì¦\n",
    "- **ë ˆì´ì•„ì›ƒ ë³µì›**: 2ë‹¨ ë ˆì´ì•„ì›ƒ ê°ì§€ ë° ì½ê¸° ìˆœì„œ ë³µì›\n",
    "\n",
    "### ğŸ”¤ 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ê·œí™” ë° í•œêµ­ì–´ NLP í…ŒìŠ¤íŠ¸  \n",
    "- **ë…¸ì´ì¦ˆ ì œê±°**: ë¶ˆí•„ìš”í•œ ê³µë°±, ì¤‘ë³µ ë¼ì¸ ì œê±°\n",
    "- **Korean NLP Service**: kiwipiepy í˜•íƒœì†Œ ë¶„ì„ ì—°ë™ í…ŒìŠ¤íŠ¸\n",
    "- **ì˜ë¯¸ì  ì²­í‚¹**: ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  ë° ì ì ˆí•œ í¬ê¸°ë¡œ ì¬ì¡°í•©\n",
    "\n",
    "### ğŸ¯ 4ë‹¨ê³„: ë°±ì—”ë“œ ì„ë² ë”© ë° ë²¡í„° ì €ì¥ í…ŒìŠ¤íŠ¸\n",
    "- **AWS Bedrock Titan V2**: 1024ì°¨ì› ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "- **pgvector ì €ì¥**: vs_doc_contents_index í…Œì´ë¸” ì €ì¥ í™•ì¸\n",
    "- **ë²¡í„° ê²€ìƒ‰**: ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ í’ˆì§ˆ ê²€ì¦\n",
    "\n",
    "### ğŸ” 5ë‹¨ê³„: í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° í’ˆì§ˆ í‰ê°€\n",
    "- **ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰**: ë‹¨ê³„ë³„ ì²˜ë¦¬ ì‹œê°„ ë° ì„±ê³µë¥  ì¸¡ì •\n",
    "- **í’ˆì§ˆ ë©”íŠ¸ë¦­**: í…ìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€, ì²­í¬ í†µê³„, OCR í’ˆì§ˆ ì§€í‘œ\n",
    "- **Gating ë©”ì»¤ë‹ˆì¦˜**: ì‹ ë¢°ë„ ì„ê³„ê°’ ê¸°ë°˜ ì‘ë‹µ ì°¨ë‹¨ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "### ğŸ“ˆ 6ë‹¨ê³„: í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ë„ ê²€ì¦\n",
    "- **ë¬¸ì„œ ê·œê²© ì¤€ìˆ˜ë„**: `/01.docs/02.document_ingestion_vectorstore.md` ëŒ€ë¹„ ì™„ì„±ë„\n",
    "- **ì‹œìŠ¤í…œ ì˜í–¥ ë¶„ì„**: ê¸°ì¡´ ë°±ì—”ë“œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ìµœì†Œí™” í™•ì¸\n",
    "- **ë©€í‹°ëª¨ë‹¬ í™•ì¥ ê³„íš**: vs_multimodal_contents_index í…Œì´ë¸” í™•ì¥ ë¡œë“œë§µ\n",
    "\n",
    "## ğŸ”§ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì—°ë™ ìƒíƒœ\n",
    "\n",
    "### âœ… í˜„ì¬ ì—°ë™ëœ ì„œë¹„ìŠ¤:\n",
    "- **AWS Bedrock Service**: amazon.titan-embed-text-v2:0 (1024ì°¨ì›)\n",
    "- **Korean NLP Service**: kiwipiepy + í•œêµ­ì–´ íŠ¹í™” ì²˜ë¦¬\n",
    "- **Vector Storage Service**: PostgreSQL + pgvector\n",
    "- **Permission Service**: ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê´€ë¦¬\n",
    "- **Document Service**: íŒŒì¼ ë©”íƒ€ë°ì´í„° ê´€ë¦¬\n",
    "\n",
    "### ğŸ”„ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›:\n",
    "- **Production Mode**: ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì™„ì „ ì—°ë™\n",
    "- **Fallback Mode**: TF-IDF ê¸°ë°˜ ë…ë¦½ ì‹¤í–‰ (ë°±ì—”ë“œ ë¯¸ì—°ê²°ì‹œ)\n",
    "\n",
    "## ğŸ’¡ í”„ë¡œë•ì…˜ ê³ ë ¤ì‚¬í•­ (Production Considerations)\n",
    "\n",
    "### ğŸ¯ í’ˆì§ˆ ê´€ë¦¬\n",
    "- **Grounding Ratio**: RAG ì‘ë‹µì˜ ì‹ ë¢°ë„ ì¸¡ì • ë° ì„ê³„ê°’ ì„¤ì • (> 0.7 ê¶Œì¥)\n",
    "- **Citation ì œí•œ**: ì°¸ì¡° ë¬¸ì„œ ê°œìˆ˜ ì œí•œ (ìµœëŒ€ 5ê°œ) ë° íˆ¬ëª…ì„± í™•ë³´\n",
    "- **Gating ë©”ì»¤ë‹ˆì¦˜**: ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ê°€ ë‚®ì„ ë•Œ ì‘ë‹µ ì°¨ë‹¨\n",
    "\n",
    "### âš¡ ì„±ëŠ¥ ìµœì í™”\n",
    "- **OCR ìºì‹±**: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™” (Redis)\n",
    "- **ë°°ì¹˜ ì²˜ë¦¬**: ì„ë² ë”© ìƒì„± ë°°ì¹˜ ë‹¨ìœ„ ìµœì í™” (ìµœëŒ€ 25ê°œ/ìš”ì²­)\n",
    "- **ì—°ê²° í’€ë§**: PostgreSQL ì—°ê²° ê´€ë¦¬\n",
    "\n",
    "### ğŸ”’ ë³´ì•ˆ ë° ê¶Œí•œ\n",
    "- **ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ê¶Œí•œ**: ì‚¬ìš©ìë³„ ì ‘ê·¼ ì œì–´\n",
    "- **íŒŒì¼ ê²€ì¦**: ì•…ì„± íŒŒì¼ ë° ìœ„í—˜í•œ íŒ¨í„´ í•„í„°ë§\n",
    "- **ë°ì´í„° ì•”í˜¸í™”**: ë¯¼ê° ì •ë³´ ë³´í˜¸\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ ì‚¬ìš©ë²•**: ê° ì…€ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ë°±ì—”ë“œ APIì˜ ê° êµ¬ì„±ìš”ì†Œë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê³ , ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ í’ˆì§ˆì„ ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc34013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1ë‹¨ê³„: í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (Environment Setup & Dependencies)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "ë¬¸ì„œì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì— í•„ìš”í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•˜ê³  ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "Dependencies:\n",
    "- pymupdf: PDF ë¬¸ì„œ íŒŒì‹± ë° ì´ë¯¸ì§€ ì¶”ì¶œ\n",
    "- easyocr: ê´‘í•™ë¬¸ìì¸ì‹(OCR) - í•œêµ­ì–´/ì˜ì–´ ì§€ì›\n",
    "- kiwipiepy: í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
    "- scikit-learn: TF-IDF ë²¡í„°í™” ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "- nltk: ìì—°ì–´ ì²˜ë¦¬ ê¸°ë³¸ ë„êµ¬\n",
    "\"\"\"\n",
    "\n",
    "import sys, subprocess, importlib, math, statistics, re, textwrap, json\n",
    "from pathlib import Path\n",
    "\n",
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜ (Auto-install required packages)\n",
    "required_packages = [\"pymupdf\", \"easyocr\", \"kiwipiepy\", \"scikit-learn\", \"nltk\"]\n",
    "\n",
    "print(\"ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸ ì¤‘...\")\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        # íŒ¨í‚¤ì§€ëª… ë³€í™˜ (í•˜ì´í”ˆì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ)\n",
    "        module_name = pkg.replace('-', '_')\n",
    "        importlib.import_module(module_name)\n",
    "        print(f\"âœ… {pkg} - ì´ë¯¸ ì„¤ì¹˜ë¨\")\n",
    "    except ImportError:\n",
    "        print(f\"â¬‡ï¸ {pkg} - ì„¤ì¹˜ ì¤‘...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"], check=True)\n",
    "        print(f\"âœ… {pkg} - ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (Import libraries)\n",
    "import fitz  # PyMuPDF\n",
    "import easyocr\n",
    "from kiwipiepy import Kiwi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Download NLTK data)\n",
    "print(\"\\nğŸ“š NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (Initialize Korean morphological analyzer)\n",
    "print(\"ğŸ”¤ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...\")\n",
    "kiwi = Kiwi()\n",
    "\n",
    "print(\"\\nğŸ‰ í™˜ê²½ ì„¤ì • ì™„ë£Œ! ë²„ì „ ì •ë³´:\")\n",
    "print(f\"  ğŸ“„ PyMuPDF: {fitz.__version__}\")\n",
    "print(f\"  ğŸ‘ï¸ EasyOCR: {easyocr.__version__}\")\n",
    "print(f\"  ğŸ”¤ Kiwi: í˜•íƒœì†Œ ë¶„ì„ê¸° ì¤€ë¹„ë¨\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac643313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ë°±ì—”ë“œ í†µí•© í™˜ê²½ ì„¤ì • (Backend Integration Setup)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "í˜„ì¬ backend/.env ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ê³¼ pgvector ì—°ë™\n",
    "Production Backend Services:\n",
    "- AWS Bedrock Titan V2 (1024-dimensional embeddings)\n",
    "- Korean NLP Service with kiwipiepy\n",
    "- PostgreSQL with pgvector (vs_doc_contents_index table)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ë°±ì—”ë“œ ê²½ë¡œ ì¶”ê°€ (Add backend path for imports)\n",
    "backend_path = Path(\"/home/admin/wkms-aws/backend\").resolve()\n",
    "if str(backend_path) not in sys.path:\n",
    "    sys.path.insert(0, str(backend_path))\n",
    "\n",
    "# ë°±ì—”ë“œ .env íŒŒì¼ ë¡œë“œ (Load backend environment)\n",
    "import subprocess\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\", \"-q\"], check=True)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "env_path = backend_path / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(\"âœ… backend/.env ì„¤ì • ë¡œë“œ ì™„ë£Œ\")\n",
    "    print(f\"  ğŸ“ AWS Region: {os.getenv('AWS_REGION', 'N/A')}\")\n",
    "    print(f\"  ğŸ¤– Embedding Model: {os.getenv('BEDROCK_EMBEDDING_MODEL_ID', 'N/A')}\")\n",
    "    print(f\"  ğŸ“ Vector Dimension: {os.getenv('VECTOR_DIMENSION', 'N/A')}\")\n",
    "    print(f\"  ğŸ”¤ Korean Model: {os.getenv('KOREAN_EMBEDDING_MODEL', 'N/A')}\")\n",
    "else:\n",
    "    print(\"âš ï¸ backend/.env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì„í¬íŠ¸ ì‹œë„ (Try importing backend services)\n",
    "BACKEND_SERVICES_AVAILABLE = False\n",
    "try:\n",
    "    # Core configuration\n",
    "    from app.core.config import settings\n",
    "    \n",
    "    # Backend services\n",
    "    from app.services.core.bedrock_service import bedrock_service\n",
    "    from app.services.core.korean_nlp_service import korean_nlp_service\n",
    "    from app.services.core.embedding_service import EmbeddingService\n",
    "    \n",
    "    # Database setup\n",
    "    import asyncio\n",
    "    import asyncpg\n",
    "    from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "    from sqlalchemy.sql import text\n",
    "    from sqlalchemy.orm import sessionmaker\n",
    "    \n",
    "    BACKEND_SERVICES_AVAILABLE = True\n",
    "    print(\"âœ… ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì„í¬íŠ¸ ì„±ê³µ!\")\n",
    "    print(\"  ğŸ¯ AWS Bedrock Service\")\n",
    "    print(\"  ğŸ”¤ Korean NLP Service\") \n",
    "    print(\"  ğŸ“Š Vector Storage Service\")\n",
    "    print(\"  ğŸ—„ï¸ PostgreSQL + pgvector\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ“ ë…¸íŠ¸ë¶ë§Œ ì‚¬ìš©í•˜ì—¬ OCR íŒŒì´í”„ë¼ì¸ì„ ì‹œì—°í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì • (Database connection setup)\n",
    "if BACKEND_SERVICES_AVAILABLE:\n",
    "    try:\n",
    "        # PostgreSQL ì—°ê²° ì„¤ì •\n",
    "        DATABASE_URL = os.getenv('DATABASE_URL')\n",
    "        if DATABASE_URL:\n",
    "            # Convert psycopg2 URL to asyncpg format\n",
    "            db_url = DATABASE_URL.replace('postgresql://', 'postgresql+asyncpg://')\n",
    "            engine = create_async_engine(db_url, echo=False)\n",
    "            async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)\n",
    "            print(\"âœ… PostgreSQL + pgvector ì—°ê²° ì¤€ë¹„ ì™„ë£Œ\")\n",
    "        else:\n",
    "            print(\"âš ï¸ DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            BACKEND_SERVICES_AVAILABLE = False\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
    "        BACKEND_SERVICES_AVAILABLE = False\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ”§ Backend Integration Mode: {'ENABLED' if BACKEND_SERVICES_AVAILABLE else 'DISABLED'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“‹ 1ë‹¨ê³„: ë°±ì—”ë“œ API íŒŒì¼ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (Backend File Validation System Test)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "/api/v1/documents/upload ì—”ë“œí¬ì¸íŠ¸ì˜ íŒŒì¼ ê²€ì¦ ë¡œì§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "ë°±ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ë™ì¼í•œ ê²€ì¦ ê·œì¹™ì„ ì ìš©í•˜ì—¬ íŒŒì¼ ì—…ë¡œë“œ ì „ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê²€ì¦ í•­ëª©:\n",
    "1. íŒŒì¼ í™•ì¥ì (ALLOWED_EXTENSIONS)\n",
    "2. íŒŒì¼ í¬ê¸° (MAX_FILE_SIZE: 50MB)\n",
    "3. íŒŒì¼ëª… ë³´ì•ˆ ê²€ì¦\n",
    "4. MIME íƒ€ì… ê²€ì¦\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "# ë°±ì—”ë“œì™€ ë™ì¼í•œ ì„¤ì •ê°’\n",
    "MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB\n",
    "ALLOWED_EXTENSIONS = {'.pdf', '.docx', '.pptx', '.xlsx', '.txt', '.hwp'}\n",
    "\n",
    "def validate_file_for_upload(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ë°±ì—”ë“œ APIì™€ ë™ì¼í•œ íŒŒì¼ ê²€ì¦ ë¡œì§ êµ¬í˜„\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ê²€ì¦í•  íŒŒì¼ ê²½ë¡œ\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: ê²€ì¦ ê²°ê³¼ ë° ì„¸ë¶€ ì •ë³´\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“‹ íŒŒì¼ ê²€ì¦ ì‹œì‘: {file_path}\")\n",
    "    \n",
    "    validation_result = {\n",
    "        \"valid\": True,\n",
    "        \"file_path\": file_path,\n",
    "        \"errors\": [],\n",
    "        \"warnings\": [],\n",
    "        \"file_info\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "        if not os.path.exists(file_path):\n",
    "            validation_result[\"valid\"] = False\n",
    "            validation_result[\"errors\"].append(\"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "            return validation_result\n",
    "        \n",
    "        file_obj = Path(file_path)\n",
    "        file_name = file_obj.name\n",
    "        file_ext = file_obj.suffix.lower()\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘\n",
    "        validation_result[\"file_info\"] = {\n",
    "            \"name\": file_name,\n",
    "            \"extension\": file_ext,\n",
    "            \"size\": file_size,\n",
    "            \"size_mb\": round(file_size / (1024 * 1024), 2)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ğŸ“„ íŒŒì¼ëª…: {file_name}\")\n",
    "        print(f\"  ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({validation_result['file_info']['size_mb']} MB)\")\n",
    "        print(f\"  ğŸ”¤ í™•ì¥ì: {file_ext}\")\n",
    "        \n",
    "        # 2. íŒŒì¼ëª… ê²€ì¦\n",
    "        if not file_name:\n",
    "            validation_result[\"valid\"] = False\n",
    "            validation_result[\"errors\"].append(\"íŒŒì¼ëª…ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        if len(file_name) > 255:\n",
    "            validation_result[\"valid\"] = False\n",
    "            validation_result[\"errors\"].append(\"íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. (ìµœëŒ€ 255ì)\")\n",
    "        \n",
    "        # 3. íŒŒì¼ í™•ì¥ì ê²€ì¦\n",
    "        if file_ext not in ALLOWED_EXTENSIONS:\n",
    "            validation_result[\"valid\"] = False\n",
    "            validation_result[\"errors\"].append(\n",
    "                f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(ALLOWED_EXTENSIONS)}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  âœ… ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: {file_ext}\")\n",
    "        \n",
    "        # 4. íŒŒì¼ í¬ê¸° ê²€ì¦\n",
    "        if file_size > MAX_FILE_SIZE:\n",
    "            validation_result[\"valid\"] = False\n",
    "            validation_result[\"errors\"].append(\n",
    "                f\"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ì‹¤ì œ í¬ê¸°: {validation_result['file_info']['size_mb']}MB, \"\n",
    "                f\"ìµœëŒ€ í—ˆìš©: {MAX_FILE_SIZE // (1024*1024)}MB\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  âœ… íŒŒì¼ í¬ê¸° ì í•©: {validation_result['file_info']['size_mb']}MB\")\n",
    "        \n",
    "        # 5. ë³´ì•ˆ: ìœ„í—˜í•œ íŒŒì¼ëª… íŒ¨í„´ ì²´í¬ (ë°±ì—”ë“œì™€ ë™ì¼)\n",
    "        dangerous_patterns = ['..', '/', '\\\\', '<', '>', '|', ':', '*', '?', '\"']\n",
    "        found_patterns = [pattern for pattern in dangerous_patterns if pattern in file_name]\n",
    "        \n",
    "        if found_patterns:\n",
    "            validation_result[\"valid\"] = False\n",
    "            validation_result[\"errors\"].append(\n",
    "                f\"íŒŒì¼ëª…ì— í—ˆìš©ë˜ì§€ ì•Šì€ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {', '.join(found_patterns)}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  âœ… íŒŒì¼ëª… ë³´ì•ˆ ê²€ì¦ í†µê³¼\")\n",
    "        \n",
    "        # 6. MIME íƒ€ì… ê²€ì¦\n",
    "        mime_type, _ = mimetypes.guess_type(file_path)\n",
    "        validation_result[\"file_info\"][\"mime_type\"] = mime_type\n",
    "        \n",
    "        if not mime_type:\n",
    "            # Office ê³„ì—´ ê¸°ë³¸ê°’ ë³´ì • (ë°±ì—”ë“œì™€ ë™ì¼)\n",
    "            office_map = {\n",
    "                \".pptx\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n",
    "                \".ppt\": \"application/vnd.ms-powerpoint\",\n",
    "                \".docx\": \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
    "                \".doc\": \"application/msword\",\n",
    "                \".xlsx\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n",
    "                \".xls\": \"application/vnd.ms-excel\",\n",
    "                \".pdf\": \"application/pdf\",\n",
    "                \".txt\": \"text/plain\",\n",
    "            }\n",
    "            mime_type = office_map.get(file_ext, \"application/octet-stream\")\n",
    "            validation_result[\"file_info\"][\"mime_type\"] = mime_type\n",
    "            validation_result[\"warnings\"].append(\"MIME íƒ€ì…ì´ ìë™ìœ¼ë¡œ ì¶”ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        print(f\"  ğŸ“Š MIME íƒ€ì…: {mime_type}\")\n",
    "        \n",
    "        # ê²°ê³¼ ìš”ì•½\n",
    "        if validation_result[\"valid\"]:\n",
    "            print(f\"  âœ… íŒŒì¼ ê²€ì¦ ì„±ê³µ: ì—…ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼ì…ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(f\"  âŒ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {len(validation_result['errors'])}ê°œ ì˜¤ë¥˜ ë°œê²¬\")\n",
    "            for error in validation_result[\"errors\"]:\n",
    "                print(f\"    - {error}\")\n",
    "        \n",
    "        if validation_result[\"warnings\"]:\n",
    "            print(f\"  âš ï¸ ê²½ê³ ì‚¬í•­: {len(validation_result['warnings'])}ê°œ\")\n",
    "            for warning in validation_result[\"warnings\"]:\n",
    "                print(f\"    - {warning}\")\n",
    "        \n",
    "        return validation_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        validation_result[\"valid\"] = False\n",
    "        validation_result[\"errors\"].append(f\"íŒŒì¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        print(f\"  âŒ ê²€ì¦ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\")\n",
    "        return validation_result\n",
    "\n",
    "def test_file_validation_scenarios():\n",
    "    \"\"\"ë‹¤ì–‘í•œ íŒŒì¼ ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"ğŸ§ª íŒŒì¼ ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # input_docs ë””ë ‰í„°ë¦¬ ì§€ì •\n",
    "    input_docs_dir = Path(\"/home/admin/wkms-aws/jupyter_notebook/input_docs\")\n",
    "    \n",
    "    # ë””ë ‰í„°ë¦¬ ì¡´ì¬ í™•ì¸\n",
    "    if not input_docs_dir.exists():\n",
    "        print(f\"âš ï¸ ì…ë ¥ ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_docs_dir}\")\n",
    "        print(\"ğŸ’¡ ë””ë ‰í„°ë¦¬ë¥¼ ìƒì„±í•˜ê³  í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.\")\n",
    "        # ë””ë ‰í„°ë¦¬ ìƒì„±\n",
    "        input_docs_dir.mkdir(exist_ok=True, parents=True)\n",
    "        print(f\"âœ… ë””ë ‰í„°ë¦¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {input_docs_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ“‚ ëŒ€ìƒ ë””ë ‰í„°ë¦¬: {input_docs_dir}\")\n",
    "    \n",
    "    # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ë“¤ í™•ì¸\n",
    "    supported_patterns = [\"*.pdf\", \"*.docx\", \"*.pptx\", \"*.xlsx\", \"*.txt\", \"*.hwp\"]\n",
    "    actual_files = []\n",
    "    \n",
    "    for pattern in supported_patterns:\n",
    "        actual_files.extend(input_docs_dir.glob(pattern))\n",
    "    \n",
    "    if actual_files:\n",
    "        print(f\"ğŸ“ ë°œê²¬ëœ í…ŒìŠ¤íŠ¸ íŒŒì¼: {len(actual_files)}ê°œ\")\n",
    "        print(\"ğŸ” ì§€ì› íŒŒì¼ í˜•ì‹: PDF, DOCX, PPTX, XLSX, TXT, HWP\")\n",
    "        print()\n",
    "        \n",
    "        for file_path in actual_files[:10]:  # ìµœëŒ€ 10ê°œ í…ŒìŠ¤íŠ¸\n",
    "            print(f\"ğŸ“‹ í…ŒìŠ¤íŠ¸ íŒŒì¼: {file_path.name}\")\n",
    "            print(f\"   ğŸ“ íŒŒì¼ í¬ê¸°: {file_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            result = validate_file_for_upload(str(file_path))\n",
    "            \n",
    "            # ìš”ì•½ ì¶œë ¥\n",
    "            status = \"âœ… í†µê³¼\" if result[\"valid\"] else \"âŒ ì‹¤íŒ¨\"\n",
    "            print(f\"ê²€ì¦ ê²°ê³¼: {status}\")\n",
    "            \n",
    "            if result[\"errors\"]:\n",
    "                print(f\"   ğŸš« ì˜¤ë¥˜: {len(result['errors'])}ê°œ\")\n",
    "            if result[\"warnings\"]:\n",
    "                print(f\"   âš ï¸ ê²½ê³ : {len(result['warnings'])}ê°œ\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"âš ï¸ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"ğŸ’¡ ë‹¤ìŒ ìœ„ì¹˜ì— í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”: {input_docs_dir}\")\n",
    "        print(\"   ì§€ì› íŒŒì¼ í˜•ì‹:\")\n",
    "        for pattern in supported_patterns:\n",
    "            print(f\"   - {pattern}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¯ íŒŒì¼ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n",
    "    print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: OCR ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "test_file_validation_scenarios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2ë‹¨ê³„: OCR ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹° (OCR & Text Extraction Utilities)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "ìŠ¤ìº”ëœ ë¬¸ì„œ ê°ì§€, OCR ì²˜ë¦¬, ê·¸ë¦¬ê³  2ë‹¨ ë ˆì´ì•„ì›ƒ ë³µì›ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "import io\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# OCR ì„ê³„ê°’ ì„¤ì • (OCR threshold configuration)\n",
    "OCR_THRESHOLD_CHARS = 40  # ì´ ê°’ë³´ë‹¤ ì ì€ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ë©´ ìŠ¤ìº” ë¬¸ì„œë¡œ íŒë‹¨\n",
    "\n",
    "# EasyOCR ë¦¬ë” ì´ˆê¸°í™” (Initialize EasyOCR reader)\n",
    "print(\"ğŸ‘ï¸ OCR ì—”ì§„ ì´ˆê¸°í™” ì¤‘...\")\n",
    "reader = easyocr.Reader(['en', 'ko'], gpu=False, verbose=False)\n",
    "print(\"âœ… OCR ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "def detect_scanned(doc: fitz.Document) -> bool:\n",
    "    \"\"\"\n",
    "    PDF ë¬¸ì„œê°€ ìŠ¤ìº”ëœ ì´ë¯¸ì§€ ê¸°ë°˜ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    íŒë³„ ê¸°ì¤€:\n",
    "    1. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì„ê³„ê°’ ë¯¸ë§Œ\n",
    "    2. ì´ë¯¸ì§€ê°€ í¬í•¨ëœ í˜ì´ì§€ ë¹„ìœ¨ì´ 60% ì´ìƒ\n",
    "    3. í‰ê·  ì´ë¯¸ì§€ í¬ê¸°ê°€ 15KB ì´ìƒ\n",
    "    \n",
    "    Args:\n",
    "        doc (fitz.Document): ë¶„ì„í•  PDF ë¬¸ì„œ\n",
    "        \n",
    "    Returns:\n",
    "        bool: ìŠ¤ìº” ë¬¸ì„œ ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    # ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°\n",
    "    text_len = sum(len(page.get_text().strip()) for page in doc)\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ì •ë³´ ìˆ˜ì§‘\n",
    "    total_image_bytes = 0\n",
    "    image_pages = 0\n",
    "    \n",
    "    for page in doc:\n",
    "        page_images = page.get_images()\n",
    "        if page_images:\n",
    "            image_pages += 1\n",
    "            # ê° ì´ë¯¸ì§€ì˜ í¬ê¸° ê³„ì‚°\n",
    "            for img in page_images:\n",
    "                try:\n",
    "                    xref = img[0]\n",
    "                    img_info = doc.extract_image(xref)\n",
    "                    total_image_bytes += len(img_info.get('image', b''))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    # í‰ê·  ì´ë¯¸ì§€ í¬ê¸° ê³„ì‚°\n",
    "    avg_image_bytes = (total_image_bytes / max(image_pages, 1)) if image_pages else 0\n",
    "    image_page_ratio = image_pages / len(doc) if len(doc) > 0 else 0\n",
    "    \n",
    "    # ìŠ¤ìº” ë¬¸ì„œ íŒë³„ ë¡œì§\n",
    "    is_scanned = (text_len < OCR_THRESHOLD_CHARS and \n",
    "                  image_page_ratio > 0.6 and \n",
    "                  avg_image_bytes > 15_000)\n",
    "    \n",
    "    print(f\"ğŸ“Š ìŠ¤ìº” ê°ì§€ ë¶„ì„:\")\n",
    "    print(f\"  í…ìŠ¤íŠ¸ ê¸¸ì´: {text_len} chars\")\n",
    "    print(f\"  ì´ë¯¸ì§€ í˜ì´ì§€ ë¹„ìœ¨: {image_page_ratio:.1%}\")\n",
    "    print(f\"  í‰ê·  ì´ë¯¸ì§€ í¬ê¸°: {avg_image_bytes/1024:.1f} KB\")\n",
    "    print(f\"  íŒë³„ ê²°ê³¼: {'ğŸ–¼ï¸ ìŠ¤ìº” ë¬¸ì„œ' if is_scanned else 'ğŸ“ í…ìŠ¤íŠ¸ ë¬¸ì„œ'}\")\n",
    "    \n",
    "    return is_scanned\n",
    "\n",
    "def page_ocr(page: fitz.Page) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ í˜ì´ì§€ì— ëŒ€í•´ OCRì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        page (fitz.Page): OCRì„ ìˆ˜í–‰í•  í˜ì´ì§€\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: OCR ê²°ê³¼ (í…ìŠ¤íŠ¸, ìœ„ì¹˜, ì‹ ë¢°ë„)\n",
    "    \"\"\"\n",
    "    # ê³ í•´ìƒë„ ë Œë”ë§ (2ë°° í™•ëŒ€ë¡œ OCR í’ˆì§ˆ í–¥ìƒ)\n",
    "    matrix = fitz.Matrix(2, 2)  # 2x zoom for better OCR quality\n",
    "    pixmap = page.get_pixmap(matrix=matrix, alpha=False)\n",
    "    img_bytes = pixmap.tobytes('png')\n",
    "    \n",
    "    # EasyOCR ì‹¤í–‰\n",
    "    ocr_results = reader.readtext(img_bytes)\n",
    "    \n",
    "    # ê²°ê³¼ ì •ê·œí™”\n",
    "    processed_results = []\n",
    "    for item in ocr_results:\n",
    "        if len(item) == 3:\n",
    "            # í‘œì¤€ í˜•ì‹: (box, text, confidence)\n",
    "            box, text, confidence = item\n",
    "        else:\n",
    "            # ëŒ€ì²´ í˜•ì‹: (box, text) - ì‹ ë¢°ë„ ê¸°ë³¸ê°’ ì„¤ì •\n",
    "            box, text = item[:2]\n",
    "            confidence = 0.7\n",
    "        \n",
    "        processed_results.append({\n",
    "            \"box\": box,\n",
    "            \"text\": text.strip(),\n",
    "            \"conf\": confidence\n",
    "        })\n",
    "    \n",
    "    return processed_results\n",
    "\n",
    "def order_two_columns(blocks: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    2ë‹¨ ë ˆì´ì•„ì›ƒì˜ í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ì˜¬ë°”ë¥¸ ì½ê¸° ìˆœì„œë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì•Œê³ ë¦¬ì¦˜:\n",
    "    1. ê° í…ìŠ¤íŠ¸ ë¸”ë¡ì˜ x ì¤‘ì‹¬ì  ê³„ì‚°\n",
    "    2. ì¤‘ì•™ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì¢Œ/ìš° ì»¬ëŸ¼ ë¶„ë¥˜\n",
    "    3. ê° ì»¬ëŸ¼ ë‚´ì—ì„œ y ì¢Œí‘œ ê¸°ì¤€ ì •ë ¬\n",
    "    4. ì¢Œ ì»¬ëŸ¼ â†’ ìš° ì»¬ëŸ¼ ìˆœì„œë¡œ ê²°í•©\n",
    "    \n",
    "    Args:\n",
    "        blocks (List[Dict]): OCRë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: ì •ë ¬ëœ í…ìŠ¤íŠ¸ ë¼ì¸ë“¤\n",
    "    \"\"\"\n",
    "    if not blocks:\n",
    "        return []\n",
    "    \n",
    "    # ê° ë¸”ë¡ì˜ x ì¤‘ì‹¬ì  ê³„ì‚°\n",
    "    x_centers = []\n",
    "    for block in blocks:\n",
    "        try:\n",
    "            x_coords = [point[0] for point in block['box']]\n",
    "            center_x = sum(x_coords) / len(x_coords)\n",
    "            x_centers.append(center_x)\n",
    "        except Exception:\n",
    "            x_centers.append(0)\n",
    "    \n",
    "    # ì¤‘ì•™ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì¢Œ/ìš° êµ¬ë¶„\n",
    "    median_x = statistics.median(x_centers)\n",
    "    \n",
    "    left_column = []\n",
    "    right_column = []\n",
    "    \n",
    "    for block, center_x in zip(blocks, x_centers):\n",
    "        if center_x < median_x:\n",
    "            left_column.append(block)\n",
    "        else:\n",
    "            right_column.append(block)\n",
    "    \n",
    "    # ê° ì»¬ëŸ¼ì„ y ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)\n",
    "    left_column.sort(key=lambda b: min(point[1] for point in b['box']))\n",
    "    right_column.sort(key=lambda b: min(point[1] for point in b['box']))\n",
    "    \n",
    "    # ì¢Œ ì»¬ëŸ¼ â†’ ìš° ì»¬ëŸ¼ ìˆœì„œë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    ordered_lines = []\n",
    "    for block in left_column:\n",
    "        if block['text']:\n",
    "            ordered_lines.append(block['text'])\n",
    "    for block in right_column:\n",
    "        if block['text']:\n",
    "            ordered_lines.append(block['text'])\n",
    "    \n",
    "    return ordered_lines\n",
    "\n",
    "def extract_text(pdf_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ìŠ¤ìº” ë¬¸ì„œì¸ ê²½ìš° OCRì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF íŒŒì¼ ê²½ë¡œ\n",
    "        \n",
    "    Returns:\n",
    "        Dict: ì¶”ì¶œëœ í…ìŠ¤íŠ¸, í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸, ìŠ¤ìº” ì—¬ë¶€ ì •ë³´\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“„ ë¬¸ì„œ ë¶„ì„ ì‹œì‘: {pdf_path}\")\n",
    "    \n",
    "    # PDF ë¬¸ì„œ ì—´ê¸°\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    # ìŠ¤ìº” ë¬¸ì„œ ì—¬ë¶€ íŒë³„\n",
    "    is_scanned = detect_scanned(doc)\n",
    "    \n",
    "    # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    pages_text = []\n",
    "    for page_num, page in enumerate(doc):\n",
    "        print(f\"ğŸ“‘ í˜ì´ì§€ {page_num + 1}/{len(doc)} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        if is_scanned:\n",
    "            # OCR ë°©ì‹: ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            ocr_blocks = page_ocr(page)\n",
    "            ordered_lines = order_two_columns(ocr_blocks)\n",
    "            page_text = '\\n'.join(ordered_lines)\n",
    "        else:\n",
    "            # ì§ì ‘ ë°©ì‹: PDF ë‚´ì¥ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            page_text = page.get_text(\"text\")\n",
    "        \n",
    "        pages_text.append(page_text)\n",
    "    \n",
    "    # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©\n",
    "    full_text = '\\n'.join(pages_text)\n",
    "    \n",
    "    print(f\"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(full_text)} characters\")\n",
    "    \n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"pages\": pages_text,\n",
    "        \"scanned\": is_scanned\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44110267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ì²­í‚¹ (Text Normalization & Chunking)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ê³ , í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ë©°, ê²€ìƒ‰ì— ìµœì í™”ëœ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ì—¬ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì •ê·œí™” ê³¼ì •:\n",
    "    1. ìºë¦¬ì§€ ë¦¬í„´(\\r)ê³¼ íƒ­(\\t)ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜\n",
    "    2. ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ í†µí•©\n",
    "    3. ì¤‘ë³µëœ ë¼ì¸ ì œê±° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ í’ˆì§ˆ í–¥ìƒ)\n",
    "    \n",
    "    Args:\n",
    "        text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        str: ì •ê·œí™”ëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹œì‘...\")\n",
    "    \n",
    "    # 1. íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™”\n",
    "    text = re.sub(r'[\\r\\t]', ' ', text)\n",
    "    \n",
    "    # 2. ì—°ì† ê³µë°± ì œê±°\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # 3. ì¤‘ë³µ ë¼ì¸ ì œê±°\n",
    "    lines = []\n",
    "    seen_lines = set()\n",
    "    original_lines = 0\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        original_lines += 1\n",
    "        clean_line = line.strip()\n",
    "        \n",
    "        # ë¹ˆ ë¼ì¸ì´ê±°ë‚˜ ì´ë¯¸ ë³¸ ë¼ì¸ì´ë©´ ê±´ë„ˆë›°ê¸°\n",
    "        if clean_line and clean_line not in seen_lines:\n",
    "            lines.append(clean_line)\n",
    "            seen_lines.add(clean_line)\n",
    "    \n",
    "    result = '\\n'.join(lines)\n",
    "    \n",
    "    print(f\"  ğŸ“Š ì›ë³¸ ë¼ì¸: {original_lines} â†’ ì •ê·œí™” í›„: {len(lines)}\")\n",
    "    print(f\"  ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} â†’ {len(result)} characters\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_keywords_ko(text: str, top_k: int = 40) -> List[str]:\n",
    "    \"\"\"\n",
    "    í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì¶”ì¶œ ê³¼ì •:\n",
    "    1. ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    2. Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ í’ˆì‚¬ íƒœê¹…\n",
    "    3. ëª…ì‚¬ë¥˜(NN), ì™¸êµ­ì–´(SL), ìˆ«ì(SN) í’ˆì‚¬ë§Œ ì„ ë³„\n",
    "    4. ë¹ˆë„ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ\n",
    "    \n",
    "    Args:\n",
    "        text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
    "        top_k (int): ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: ë¹ˆë„ìˆœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”¤ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...\")\n",
    "    \n",
    "    # í˜•íƒœì†Œ ë¶„ì„ ëŒ€ìƒ í’ˆì‚¬ (ëª…ì‚¬ë¥˜, ì™¸êµ­ì–´, ìˆ«ì)\n",
    "    target_pos = ('NN', 'SL', 'SN')\n",
    "    tokens = []\n",
    "    \n",
    "    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬\n",
    "    sentences = re.split(r'[\\n\\.?!]', text)\n",
    "    processed_sentences = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        processed_sentences += 1\n",
    "        \n",
    "        # Kiwi í˜•íƒœì†Œ ë¶„ì„\n",
    "        try:\n",
    "            analysis_result = kiwi.analyze(sentence)\n",
    "            for token_info in analysis_result:\n",
    "                # token_info[0]ì— í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ê°€ ë“¤ì–´ìˆìŒ\n",
    "                for morph, pos_tag, begin, length in token_info[0]:\n",
    "                    # ëª…ì‚¬ë¥˜ì´ê³  ê¸¸ì´ê°€ 2 ì´ìƒì¸ í˜•íƒœì†Œë§Œ ì„ íƒ\n",
    "                    if pos_tag.startswith(target_pos) and len(morph) > 1:\n",
    "                        tokens.append(morph.lower())\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ì˜¤ë¥˜: {sentence[:50]}... - {e}\")\n",
    "            continue\n",
    "    \n",
    "    # ë¹ˆë„ ê³„ì‚° ë° ì •ë ¬\n",
    "    frequency_dict = {}\n",
    "    for token in tokens:\n",
    "        frequency_dict[token] = frequency_dict.get(token, 0) + 1\n",
    "    \n",
    "    # ë¹ˆë„ìˆœ ì •ë ¬ í›„ ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ\n",
    "    ranked_keywords = sorted(frequency_dict.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    result_keywords = [word for word, freq in ranked_keywords]\n",
    "    \n",
    "    print(f\"  ğŸ“Š ì²˜ë¦¬ëœ ë¬¸ì¥: {processed_sentences}\")\n",
    "    print(f\"  ğŸ”¤ ì´ í˜•íƒœì†Œ: {len(tokens)} â†’ ìœ ë‹ˆí¬ í‚¤ì›Œë“œ: {len(frequency_dict)}\")\n",
    "    print(f\"  ğŸ¯ ì„ íƒëœ í‚¤ì›Œë“œ: {len(result_keywords)}\")\n",
    "    \n",
    "    return result_keywords\n",
    "\n",
    "def sentence_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  ì§§ì€ ì¡°ê°ë“¤ì„ ë³‘í•©í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ë¶„í•  ì „ëµ:\n",
    "    1. êµ¬ë‘ì (., ?, !, ì¤„ë°”ê¿ˆ)ì„ ê¸°ì¤€ìœ¼ë¡œ ì´ˆê¸° ë¶„í• \n",
    "    2. ì§§ì€ ì¡°ê°ë“¤ì„ 220ì ë‹¨ìœ„ë¡œ ë³‘í•©í•˜ì—¬ ì˜ë¯¸ìˆëŠ” í¬ê¸° í™•ë³´\n",
    "    \n",
    "    Args:\n",
    "        text (str): ë¶„í• í•  í…ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(\"âœ‚ï¸ ë¬¸ì¥ ë¶„í•  ë° ë³‘í•© ì¤‘...\")\n",
    "    \n",
    "    # êµ¬ë‘ì  ê¸°ì¤€ ì´ˆê¸° ë¶„í• \n",
    "    raw_sentences = [s.strip() for s in re.split(r'[\\n\\.?!]', text) if s.strip()]\n",
    "    \n",
    "    # ì§§ì€ ë¬¸ì¥ë“¤ ë³‘í•©\n",
    "    merged_sentences = []\n",
    "    current_buffer = []\n",
    "    current_length = 0\n",
    "    min_merge_length = 220  # ìµœì†Œ ë³‘í•© ê¸¸ì´\n",
    "    \n",
    "    for sentence in raw_sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "        # ë²„í¼ê°€ ìµœì†Œ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ìƒˆë¡œìš´ ë¬¸ì¥ ì‹œì‘\n",
    "        if current_length + sentence_length + 1 > min_merge_length and current_buffer:\n",
    "            merged_sentences.append(' '.join(current_buffer))\n",
    "            current_buffer = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_buffer.append(sentence)\n",
    "            current_length += sentence_length + 1  # +1 for space\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë²„í¼ ì²˜ë¦¬\n",
    "    if current_buffer:\n",
    "        merged_sentences.append(' '.join(current_buffer))\n",
    "    \n",
    "    print(f\"  ğŸ“Š ì›ë³¸ ë¬¸ì¥: {len(raw_sentences)} â†’ ë³‘í•© í›„: {len(merged_sentences)}\")\n",
    "    \n",
    "    return merged_sentences\n",
    "\n",
    "def chunk(sentences: List[str], max_chars: int = 800) -> List[str]:\n",
    "    \"\"\"\n",
    "    ë¬¸ì¥ë“¤ì„ ì§€ì •ëœ ìµœëŒ€ ê¸¸ì´ì˜ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì²­í‚¹ ì „ëµ:\n",
    "    1. ë¬¸ì¥ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìµœëŒ€ ê¸¸ì´ ì¤€ìˆ˜\n",
    "    2. ë‹¨ì¼ ë¬¸ì¥ì´ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë³„ë„ ì²­í¬ë¡œ ì²˜ë¦¬\n",
    "    3. ì˜ë¯¸ì  ì¼ê´€ì„±ì„ ìœ„í•´ ë¬¸ì¥ ë‹¨ìœ„ ìœ ì§€\n",
    "    \n",
    "    Args:\n",
    "        sentences (List[str]): ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "        max_chars (int): ì²­í¬ ìµœëŒ€ ë¬¸ì ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: ì²­í¬ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“¦ ì²­í‚¹ ì‹œì‘ (ìµœëŒ€ {max_chars} ë¬¸ì)...\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "        # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸\n",
    "        if current_length + sentence_length + 1 > max_chars and current_chunk:\n",
    "            # í˜„ì¬ ì²­í¬ ì™„ì„± ë° ìƒˆ ì²­í¬ ì‹œì‘\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length + 1  # +1 for space\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    # ì²­í¬ í†µê³„ ì¶œë ¥\n",
    "    chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "    avg_length = statistics.mean(chunk_lengths) if chunk_lengths else 0\n",
    "    median_length = statistics.median(chunk_lengths) if chunk_lengths else 0\n",
    "    \n",
    "    print(f\"  ğŸ“Š ìƒì„±ëœ ì²­í¬: {len(chunks)}\")\n",
    "    print(f\"  ğŸ“ í‰ê·  ê¸¸ì´: {avg_length:.0f} characters\")\n",
    "    print(f\"  ğŸ“ ì¤‘ê°„ê°’ ê¸¸ì´: {median_length:.0f} characters\")\n",
    "    print(f\"  ğŸ“ ê¸¸ì´ ë²”ìœ„: {min(chunk_lengths) if chunk_lengths else 0} ~ {max(chunk_lengths) if chunk_lengths else 0}\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4ë‹¨ê³„: ë°±ì—”ë“œ í†µí•© ì¸ë±ì‹± ë° ê²€ìƒ‰ ì‹œìŠ¤í…œ (Backend Integrated Indexing & Search)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "ì‹¤ì œ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•œ í”„ë¡œë•ì…˜ê¸‰ ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
    "- AWS Bedrock Titan V2 ì„ë² ë”© (1024ì°¨ì›)\n",
    "- Korean NLP Service with kiwipiepy\n",
    "- PostgreSQL + pgvector ë²¡í„° ê²€ìƒ‰\n",
    "- Fallback to TF-IDF when backend unavailable\n",
    "\"\"\"\n",
    "\n",
    "if BACKEND_SERVICES_AVAILABLE:\n",
    "    print(\"ğŸ¯ ë°±ì—”ë“œ í†µí•© ê²€ìƒ‰ ì‹œìŠ¤í…œ í™œì„±í™”\")\n",
    "    \n",
    "    class ProductionIndex:\n",
    "        \"\"\"\n",
    "        ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ê¸°ë°˜ í”„ë¡œë•ì…˜ ê²€ìƒ‰ ì¸ë±ìŠ¤\n",
    "        \n",
    "        Features:\n",
    "        - AWS Bedrock Titan V2 embeddings (1024-dim)\n",
    "        - Korean morphological analysis with kiwipiepy\n",
    "        - PostgreSQL + pgvector storage and search\n",
    "        - Quality assessment and gating mechanisms\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, chunks: List[str], knowledge_container_id: str = \"notebook_test\"):\n",
    "            \"\"\"\n",
    "            Production ê²€ìƒ‰ ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n",
    "            \n",
    "            Args:\n",
    "                chunks (List[str]): ì¸ë±ì‹±í•  í…ìŠ¤íŠ¸ ì²­í¬ë“¤\n",
    "                knowledge_container_id (str): ì§€ì‹ ì»¨í…Œì´ë„ˆ ID\n",
    "            \"\"\"\n",
    "            print(\"ğŸ¯ Production ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...\")\n",
    "            \n",
    "            self.chunks = chunks\n",
    "            self.container_id = knowledge_container_id\n",
    "            self.embeddings = []\n",
    "            self.korean_metadata = []\n",
    "            self.chunk_ids = []\n",
    "            \n",
    "            # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •\n",
    "            try:\n",
    "                # ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±\n",
    "                loop = asyncio.get_event_loop()\n",
    "            except RuntimeError:\n",
    "                loop = asyncio.new_event_loop()\n",
    "                asyncio.set_event_loop(loop)\n",
    "            \n",
    "            # ì„ë² ë”© ë° í•œêµ­ì–´ ë¶„ì„ ì‹¤í–‰\n",
    "            loop.run_until_complete(self._process_chunks())\n",
    "            \n",
    "            print(f\"  âœ… Production ì¸ë±ì‹± ì™„ë£Œ: {len(self.chunks)}ê°œ ì²­í¬\")\n",
    "            print(f\"  ğŸ“Š ì„ë² ë”© ì°¨ì›: {len(self.embeddings[0]) if self.embeddings else 0}\")\n",
    "            print(f\"  ğŸ”¤ Korean NLP ë©”íƒ€ë°ì´í„°: {len(self.korean_metadata)}ê°œ\")\n",
    "        \n",
    "        async def _process_chunks(self):\n",
    "            \"\"\"ì²­í¬ë³„ ì„ë² ë”© ìƒì„± ë° í•œêµ­ì–´ ë¶„ì„\"\"\"\n",
    "            print(\"  ğŸ¤– AWS Bedrock ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "            \n",
    "            for i, chunk in enumerate(self.chunks):\n",
    "                try:\n",
    "                    # 1. AWS Bedrock Titan V2 ì„ë² ë”© ìƒì„±\n",
    "                    embeddings_result = await bedrock_service.get_embeddings_titan([chunk])\n",
    "                    if embeddings_result and len(embeddings_result[0]) == 1024:\n",
    "                        self.embeddings.append(embeddings_result[0])\n",
    "                        print(f\"    âœ… ì²­í¬ {i+1}/{len(self.chunks)} ì„ë² ë”© ì™„ë£Œ\")\n",
    "                    else:\n",
    "                        # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë²¡í„° ì‚¬ìš©\n",
    "                        self.embeddings.append([0.0] * 1024)\n",
    "                        print(f\"    âš ï¸ ì²­í¬ {i+1} ì„ë² ë”© ì‹¤íŒ¨, ë”ë¯¸ ë²¡í„° ì‚¬ìš©\")\n",
    "                    \n",
    "                    # 2. Korean NLP ë¶„ì„\n",
    "                    korean_analysis = await korean_nlp_service.analyze_chunk_for_search(chunk)\n",
    "                    self.korean_metadata.append(korean_analysis)\n",
    "                    \n",
    "                    # 3. ì²­í¬ ID ìƒì„±\n",
    "                    self.chunk_ids.append(f\"notebook_{self.container_id}_{i}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    âŒ ì²­í¬ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "                    # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "                    self.embeddings.append([0.0] * 1024)\n",
    "                    self.korean_metadata.append({\n",
    "                        \"korean_keywords\": [],\n",
    "                        \"named_entities\": [],\n",
    "                        \"success\": False,\n",
    "                        \"error\": str(e)\n",
    "                    })\n",
    "                    self.chunk_ids.append(f\"notebook_{self.container_id}_{i}\")\n",
    "        \n",
    "        async def store_to_database(self, file_bss_info_sno: int = 999999):\n",
    "            \"\"\"\n",
    "            PostgreSQL + pgvectorì— ë°ì´í„° ì €ì¥\n",
    "            \n",
    "            Args:\n",
    "                file_bss_info_sno (int): íŒŒì¼ ì¼ë ¨ë²ˆí˜¸ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "            \"\"\"\n",
    "            print(\"ğŸ—„ï¸ PostgreSQL + pgvector ì €ì¥ ì¤‘...\")\n",
    "            \n",
    "            try:\n",
    "                async with async_session() as session:\n",
    "                    for i, (chunk, embedding, korean_meta, chunk_id) in enumerate(\n",
    "                        zip(self.chunks, self.embeddings, self.korean_metadata, self.chunk_ids)\n",
    "                    ):\n",
    "                        # vs_doc_contents_indexì— ì €ì¥\n",
    "                        embedding_str = f\"[{','.join(map(str, embedding))}]\"\n",
    "                        \n",
    "                        metadata_json = json.dumps({\n",
    "                            \"container_id\": self.container_id,\n",
    "                            \"chunk_type\": \"content\", \n",
    "                            \"keywords\": korean_meta.get(\"korean_keywords\", []),\n",
    "                            \"named_entities\": korean_meta.get(\"named_entities\", []),\n",
    "                            \"processing_success\": korean_meta.get(\"success\", False)\n",
    "                        }, ensure_ascii=False)\n",
    "                        \n",
    "                        query = text(\"\"\"\n",
    "                            INSERT INTO vs_doc_contents_index (\n",
    "                                id, file_bss_info_sno, knowledge_container_id, chunk_index,\n",
    "                                chunk_text, embedding, chunk_size, metadata_json, created_date\n",
    "                            ) VALUES (\n",
    "                                :id, :file_sno, :container_id, :chunk_index,\n",
    "                                :chunk_text, CAST(:embedding AS vector), :chunk_size, :metadata_json, NOW()\n",
    "                            )\n",
    "                            ON CONFLICT (id) DO UPDATE SET\n",
    "                                chunk_text = EXCLUDED.chunk_text,\n",
    "                                embedding = EXCLUDED.embedding,\n",
    "                                updated_date = NOW()\n",
    "                        \"\"\")\n",
    "                        \n",
    "                        await session.execute(query, {\n",
    "                            \"id\": chunk_id,\n",
    "                            \"file_sno\": file_bss_info_sno,\n",
    "                            \"container_id\": self.container_id,\n",
    "                            \"chunk_index\": i,\n",
    "                            \"chunk_text\": chunk,\n",
    "                            \"embedding\": embedding_str,\n",
    "                            \"chunk_size\": len(chunk),\n",
    "                            \"metadata_json\": metadata_json\n",
    "                        })\n",
    "                    \n",
    "                    await session.commit()\n",
    "                    print(f\"  âœ… {len(self.chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        async def search(self, query: str, top_k: int = 5, similarity_threshold: float = 0.7):\n",
    "            \"\"\"\n",
    "            Production ê²€ìƒ‰ ìˆ˜í–‰\n",
    "            \n",
    "            Args:\n",
    "                query (str): ê²€ìƒ‰ ì¿¼ë¦¬\n",
    "                top_k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜\n",
    "                similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’\n",
    "                \n",
    "            Returns:\n",
    "                List[Dict]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "            \"\"\"\n",
    "            print(f\"ğŸ” Production ê²€ìƒ‰ ì‹¤í–‰: '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "                query_embeddings = await bedrock_service.get_embeddings_titan([query])\n",
    "                if not query_embeddings or len(query_embeddings[0]) != 1024:\n",
    "                    print(\"  âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨\")\n",
    "                    return []\n",
    "                \n",
    "                query_embedding = query_embeddings[0]\n",
    "                \n",
    "                # 2. í•œêµ­ì–´ ì¿¼ë¦¬ ë¶„ì„\n",
    "                korean_query_analysis = await korean_nlp_service.analyze_text_for_search(query)\n",
    "                query_keywords = korean_query_analysis.get(\"korean_keywords\", [])\n",
    "                \n",
    "                print(f\"  ğŸ”¤ í•œêµ­ì–´ í‚¤ì›Œë“œ: {query_keywords}\")\n",
    "                \n",
    "                # 3. pgvector ê²€ìƒ‰\n",
    "                async with async_session() as session:\n",
    "                    embedding_str = f\"[{','.join(map(str, query_embedding))}]\"\n",
    "                    \n",
    "                    search_query = text(\"\"\"\n",
    "                        SELECT \n",
    "                            id, file_bss_info_sno, knowledge_container_id, chunk_index,\n",
    "                            chunk_text, metadata_json,\n",
    "                            1 - (embedding <-> :query_embedding::vector) as similarity\n",
    "                        FROM vs_doc_contents_index\n",
    "                        WHERE knowledge_container_id = :container_id\n",
    "                            AND embedding <-> :query_embedding::vector < :threshold\n",
    "                        ORDER BY similarity DESC\n",
    "                        LIMIT :limit\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    result = await session.execute(search_query, {\n",
    "                        \"query_embedding\": embedding_str,\n",
    "                        \"container_id\": self.container_id,\n",
    "                        \"threshold\": 1.0 - similarity_threshold,\n",
    "                        \"limit\": top_k\n",
    "                    })\n",
    "                    \n",
    "                    rows = result.fetchall()\n",
    "                    \n",
    "                    search_results = []\n",
    "                    for row in rows:\n",
    "                        metadata = json.loads(row[5]) if row[5] else {}\n",
    "                        \n",
    "                        search_results.append({\n",
    "                            \"chunk_id\": row[0],\n",
    "                            \"chunk_index\": row[3],\n",
    "                            \"content\": row[4],\n",
    "                            \"similarity\": float(row[6]),\n",
    "                            \"metadata\": metadata,\n",
    "                            \"source\": \"pgvector_bedrock\"\n",
    "                        })\n",
    "                    \n",
    "                    print(f\"  âœ… {len(search_results)}ê°œ ê²°ê³¼ ë°œê²¬\")\n",
    "                    return search_results\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Production ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "                return []\n",
    "        \n",
    "        def get_quality_assessment(self):\n",
    "            \"\"\"í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ë°˜í™˜\"\"\"\n",
    "            successful_embeddings = sum(1 for emb in self.embeddings if sum(emb) != 0)\n",
    "            successful_korean_analysis = sum(1 for meta in self.korean_metadata if meta.get(\"success\", False))\n",
    "            \n",
    "            return {\n",
    "                \"total_chunks\": len(self.chunks),\n",
    "                \"successful_embeddings\": successful_embeddings,\n",
    "                \"embedding_success_rate\": successful_embeddings / len(self.chunks) if self.chunks else 0,\n",
    "                \"successful_korean_analysis\": successful_korean_analysis,\n",
    "                \"korean_analysis_success_rate\": successful_korean_analysis / len(self.chunks) if self.chunks else 0,\n",
    "                \"average_keywords_per_chunk\": sum(len(meta.get(\"korean_keywords\", [])) for meta in self.korean_metadata) / len(self.chunks) if self.chunks else 0\n",
    "            }\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ë¯¸ì‚¬ìš© - TF-IDF í´ë°± ëª¨ë“œ\")\n",
    "    \n",
    "    class MiniIndex:\n",
    "        \"\"\"\n",
    "        TF-IDF ê¸°ë°˜ í´ë°± ê²€ìƒ‰ ì¸ë±ìŠ¤ (ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ë¯¸ì‚¬ìš©ì‹œ)\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, chunks: List[str]):\n",
    "            \"\"\"TF-IDF ì¸ë±ìŠ¤ ì´ˆê¸°í™”\"\"\"\n",
    "            print(\"ğŸ” TF-IDF ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...\")\n",
    "            \n",
    "            self.chunks = chunks\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=4000,\n",
    "                ngram_range=(1, 2),\n",
    "                stop_words=None,\n",
    "                lowercase=True\n",
    "            )\n",
    "            \n",
    "            self.matrix = self.vectorizer.fit_transform(chunks)\n",
    "            \n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            print(f\"  ğŸ“Š ì¸ë±ì‹±ëœ ì²­í¬: {len(chunks)}\")\n",
    "            print(f\"  ğŸ”¤ TF-IDF íŠ¹ì„± ìˆ˜: {len(feature_names)}\")\n",
    "            print(f\"  ğŸ“ ë²¡í„° ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {self.matrix.shape}\")\n",
    "\n",
    "        def keyword_scores(self, query: str):\n",
    "            \"\"\"ì¿¼ë¦¬ë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜\"\"\"\n",
    "            return self.vectorizer.transform([query])\n",
    "\n",
    "        def semantic_search(self, query: str, top_k: int = 5):\n",
    "            \"\"\"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰\"\"\"\n",
    "            query_vector = self.keyword_scores(query)\n",
    "            similarities = cosine_similarity(query_vector, self.matrix)[0]\n",
    "            top_indices = similarities.argsort()[::-1][:top_k]\n",
    "            results = [(int(idx), float(similarities[idx])) for idx in top_indices]\n",
    "            return results\n",
    "\n",
    "def hybrid_search(index, query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: Production ë˜ëŠ” TF-IDF ê¸°ë°˜\n",
    "    \"\"\"\n",
    "    if BACKEND_SERVICES_AVAILABLE and hasattr(index, 'search'):\n",
    "        # Production ê²€ìƒ‰\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "        \n",
    "        results = loop.run_until_complete(index.search(query, top_k))\n",
    "        return [(result[\"chunk_index\"], result[\"similarity\"]) for result in results]\n",
    "    else:\n",
    "        # TF-IDF í´ë°± ê²€ìƒ‰\n",
    "        base_results = index.semantic_search(query, top_k=top_k * 2)\n",
    "        query_terms = set(query.lower().split())\n",
    "        \n",
    "        rescored_results = []\n",
    "        for chunk_idx, semantic_score in base_results:\n",
    "            chunk_text = index.chunks[chunk_idx].lower()\n",
    "            keyword_matches = sum(1 for term in query_terms if term in chunk_text)\n",
    "            keyword_bonus = 0.05 * keyword_matches\n",
    "            final_score = semantic_score + keyword_bonus\n",
    "            rescored_results.append((chunk_idx, final_score))\n",
    "        \n",
    "        rescored_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return rescored_results[:top_k]\n",
    "\n",
    "def quality_report(raw_text: str, cleaned_text: str, chunks: List[str], is_scanned: bool, index=None):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ í’ˆì§ˆ ì§€í‘œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“Š íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ ë¦¬í¬íŠ¸\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. í…ìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„\n",
    "    coverage_ratio = len(cleaned_text) / max(len(raw_text), 1)\n",
    "    \n",
    "    print(f\"ğŸ“„ ë¬¸ì„œ ìœ í˜•: {'ğŸ–¼ï¸ ìŠ¤ìº” ë¬¸ì„œ (OCR ì‚¬ìš©)' if is_scanned else 'ğŸ“ í…ìŠ¤íŠ¸ ë¬¸ì„œ (ì§ì ‘ ì¶”ì¶œ)'}\")\n",
    "    print(f\"ğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸: {len(raw_text):,} characters\")\n",
    "    print(f\"ğŸ§¹ ì •ì œ í…ìŠ¤íŠ¸: {len(cleaned_text):,} characters\")\n",
    "    print(f\"ğŸ“Š ì»¤ë²„ë¦¬ì§€ ë¹„ìœ¨: {coverage_ratio:.1%}\")\n",
    "    \n",
    "    # 2. ì²­í‚¹ í†µê³„\n",
    "    if chunks:\n",
    "        chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "        avg_length = statistics.mean(chunk_lengths)\n",
    "        median_length = statistics.median(chunk_lengths)\n",
    "        \n",
    "        print(f\"\\nğŸ“¦ ì²­í‚¹ í†µê³„:\")\n",
    "        print(f\"  ì´ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
    "        print(f\"  í‰ê·  ê¸¸ì´: {avg_length:.0f} characters\")\n",
    "        print(f\"  ì¤‘ê°„ê°’ ê¸¸ì´: {median_length:.0f} characters\")\n",
    "        print(f\"  ê¸¸ì´ ë²”ìœ„: {min(chunk_lengths)} ~ {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    # 3. Production í’ˆì§ˆ í‰ê°€ (ë°±ì—”ë“œ ì‚¬ìš©ì‹œ)\n",
    "    if BACKEND_SERVICES_AVAILABLE and index and hasattr(index, 'get_quality_assessment'):\n",
    "        quality_metrics = index.get_quality_assessment()\n",
    "        print(f\"\\nğŸ¯ Production í’ˆì§ˆ ë©”íŠ¸ë¦­:\")\n",
    "        print(f\"  ì„ë² ë”© ì„±ê³µë¥ : {quality_metrics['embedding_success_rate']:.1%}\")\n",
    "        print(f\"  í•œêµ­ì–´ ë¶„ì„ ì„±ê³µë¥ : {quality_metrics['korean_analysis_success_rate']:.1%}\")\n",
    "        print(f\"  í‰ê·  í‚¤ì›Œë“œ ìˆ˜: {quality_metrics['average_keywords_per_chunk']:.1f}ê°œ/ì²­í¬\")\n",
    "    \n",
    "    # 4. í’ˆì§ˆ ê²½ê³ \n",
    "    print(f\"\\nğŸš¨ í’ˆì§ˆ í‰ê°€:\")\n",
    "    if coverage_ratio < 0.5:\n",
    "        print(\"  âš ï¸ ë‚®ì€ í…ìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ - OCR íŠœë‹ ë˜ëŠ” ë ˆì´ì•„ì›ƒ ì²˜ë¦¬ ê°œì„  í•„ìš”\")\n",
    "    else:\n",
    "        print(\"  âœ… ì–‘í˜¸í•œ í…ìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€\")\n",
    "    \n",
    "    if is_scanned:\n",
    "        print(\"  ğŸ’¡ OCR ì‚¬ìš©ë¨ - ì‹¤ì œ ìš´ì˜ì‹œ OCR ìºì‹± ë° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ê¶Œì¥\")\n",
    "    \n",
    "    if not chunks or len(chunks) < 3:\n",
    "        print(\"  âš ï¸ ì²­í¬ ìˆ˜ ë¶€ì¡± - ë” ê¸´ ë¬¸ì„œë‚˜ ì²­í‚¹ ì „ëµ ì¡°ì • ê¶Œì¥\")\n",
    "    else:\n",
    "        print(\"  âœ… ì ì ˆí•œ ì²­í¬ ìƒì„±\")\n",
    "    \n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š 3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ë©”íƒ€ë°ì´í„° ì €ì¥ í…ŒìŠ¤íŠ¸ (Database Metadata Storage Test)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "ë°±ì—”ë“œ APIì˜ document_service.create_document_from_upload() ë¡œì§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "tb_file_bss_info ë° tb_file_dtl_info í…Œì´ë¸”ì— ë©”íƒ€ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì €ì¥ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ í•­ëª©:\n",
    "1. íŒŒì¼ ë©”íƒ€ë°ì´í„° ìƒì„± ë° ê²€ì¦\n",
    "2. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± í™•ì¸\n",
    "3. ê¶Œí•œ ê¸°ë°˜ ì»¨í…Œì´ë„ˆ í• ë‹¹ ê²€ì¦\n",
    "4. íŒŒì¼ í•´ì‹œ ë° ì¤‘ë³µ ê²€ì‚¬\n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def generate_file_metadata(file_path: str, container_id: str = \"test_container\", user_emp_no: str = \"test_user\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ë°±ì—”ë“œ APIì™€ ë™ì¼í•œ íŒŒì¼ ë©”íƒ€ë°ì´í„° ìƒì„± ë¡œì§\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): íŒŒì¼ ê²½ë¡œ\n",
    "        container_id (str): ì§€ì‹ ì»¨í…Œì´ë„ˆ ID\n",
    "        user_emp_no (str): ì‚¬ìš©ì ì‚¬ë²ˆ\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: ë©”íƒ€ë°ì´í„° ì •ë³´\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š íŒŒì¼ ë©”íƒ€ë°ì´í„° ìƒì„±: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        file_obj = Path(file_path)\n",
    "        \n",
    "        if not file_obj.exists():\n",
    "            raise FileNotFoundError(f\"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        \n",
    "        # íŒŒì¼ ê¸°ë³¸ ì •ë³´\n",
    "        file_size = file_obj.stat().st_size\n",
    "        file_extension = file_obj.suffix.lower()\n",
    "        creation_time = datetime.now()\n",
    "        \n",
    "        # íŒŒì¼ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ê²€ì‚¬ìš©)\n",
    "        file_hash = calculate_file_hash(file_path)\n",
    "        \n",
    "        # tb_file_bss_info ë©”íƒ€ë°ì´í„° (ë°±ì—”ë“œì™€ ë™ì¼í•œ êµ¬ì¡°)\n",
    "        file_bss_metadata = {\n",
    "            # ê¸°ë³¸ íŒŒì¼ ì •ë³´\n",
    "            \"file_lgc_nm\": file_obj.name,  # ë…¼ë¦¬ì  íŒŒì¼ëª… (ì›ë³¸)\n",
    "            \"file_psl_nm\": file_obj.name,  # ë¬¼ë¦¬ì  íŒŒì¼ëª… (ì €ì¥ëœ)\n",
    "            \"path\": str(file_obj.absolute()),  # íŒŒì¼ ì €ì¥ ê²½ë¡œ\n",
    "            \"file_sz\": file_size,  # íŒŒì¼ í¬ê¸°\n",
    "            \"file_extn_nm\": file_extension,  # íŒŒì¼ í™•ì¥ì\n",
    "            \n",
    "            # ê¶Œí•œ ë° ì»¨í…Œì´ë„ˆ ì •ë³´\n",
    "            \"knowledge_container_id\": container_id,\n",
    "            \"created_by\": user_emp_no,\n",
    "            \"access_level\": \"VIEWER\",\n",
    "            \n",
    "            # ë©”íƒ€ë°ì´í„°\n",
    "            \"file_hash\": file_hash,\n",
    "            \"upload_type\": \"manual\",\n",
    "            \"processing_status\": \"uploaded\",\n",
    "            \n",
    "            # ì‹œê°„ ì •ë³´\n",
    "            \"created_date\": creation_time,\n",
    "            \"last_modified_date\": creation_time,\n",
    "            \"last_accessed_date\": None,\n",
    "            \n",
    "            # ìƒíƒœ ì •ë³´\n",
    "            \"del_yn\": \"N\",\n",
    "            \"access_count\": 0,\n",
    "            \"file_stat_cd\": \"ACTIVE\"\n",
    "        }\n",
    "        \n",
    "        # tb_file_dtl_info ë©”íƒ€ë°ì´í„° (ìƒì„¸ ì •ë³´)\n",
    "        file_dtl_metadata = {\n",
    "            \"sj\": f\"ì—…ë¡œë“œëœ ë¬¸ì„œ: {file_obj.stem}\",  # ì œëª©\n",
    "            \"cn\": \"\",  # ë‚´ìš© (ì¶”í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ ì—…ë°ì´íŠ¸)\n",
    "            \"file_sz\": file_size,  # íŒŒì¼ í¬ê¸° (ì¤‘ë³µì´ì§€ë§Œ í…Œì´ë¸” êµ¬ì¡°ìƒ í•„ìš”)\n",
    "            \"author\": user_emp_no,\n",
    "            \"created_date\": creation_time,\n",
    "            \"last_modified_date\": creation_time\n",
    "        }\n",
    "        \n",
    "        metadata = {\n",
    "            \"file_bss_info\": file_bss_metadata,\n",
    "            \"file_dtl_info\": file_dtl_metadata,\n",
    "            \"processing_info\": {\n",
    "                \"metadata_generated_at\": creation_time.isoformat(),\n",
    "                \"validation_passed\": True,\n",
    "                \"ready_for_database\": True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ\")\n",
    "        print(f\"    ğŸ“„ ë…¼ë¦¬ëª…: {file_bss_metadata['file_lgc_nm']}\")\n",
    "        print(f\"    ğŸ“ í¬ê¸°: {file_size:,} bytes\")\n",
    "        print(f\"    ğŸ”¤ í™•ì¥ì: {file_extension}\")\n",
    "        print(f\"    ğŸ”‘ í•´ì‹œ: {file_hash[:16]}...\")\n",
    "        print(f\"    ğŸ“ ì»¨í…Œì´ë„ˆ: {container_id}\")\n",
    "        print(f\"    ğŸ‘¤ ì‚¬ìš©ì: {user_emp_no}\")\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"processing_info\": {\n",
    "                \"validation_passed\": False,\n",
    "                \"ready_for_database\": False\n",
    "            }\n",
    "        }\n",
    "\n",
    "def calculate_file_hash(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    íŒŒì¼ í•´ì‹œ ê³„ì‚° (ë°±ì—”ë“œ ì¤‘ë³µ ê²€ì‚¬ìš©)\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): íŒŒì¼ ê²½ë¡œ\n",
    "        \n",
    "    Returns:\n",
    "        str: SHA-256 í•´ì‹œ\n",
    "    \"\"\"\n",
    "    hash_sha256 = hashlib.sha256()\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_sha256.update(chunk)\n",
    "        return hash_sha256.hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "        return \"hash_calculation_failed\"\n",
    "\n",
    "def validate_database_schema_compatibility(metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ìƒì„±ëœ ë©”íƒ€ë°ì´í„°ê°€ ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ í˜¸í™˜ë˜ëŠ”ì§€ ê²€ì¦\n",
    "    \n",
    "    Args:\n",
    "        metadata (Dict[str, Any]): ìƒì„±ëœ ë©”íƒ€ë°ì´í„°\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì¦ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    print(\"ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì¦\")\n",
    "    \n",
    "    validation_result = {\n",
    "        \"compatible\": True,\n",
    "        \"errors\": [],\n",
    "        \"warnings\": [],\n",
    "        \"schema_checks\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # tb_file_bss_info í•„ìˆ˜ í•„ë“œ ê²€ì¦\n",
    "        required_bss_fields = [\n",
    "            \"file_lgc_nm\", \"file_psl_nm\", \"path\", \"file_sz\", \n",
    "            \"knowledge_container_id\", \"created_by\", \"created_date\"\n",
    "        ]\n",
    "        \n",
    "        file_bss = metadata.get(\"file_bss_info\", {})\n",
    "        missing_bss_fields = [field for field in required_bss_fields if field not in file_bss]\n",
    "        \n",
    "        if missing_bss_fields:\n",
    "            validation_result[\"compatible\"] = False\n",
    "            validation_result[\"errors\"].append(f\"tb_file_bss_info í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {missing_bss_fields}\")\n",
    "        else:\n",
    "            validation_result[\"schema_checks\"][\"tb_file_bss_info\"] = \"âœ… ëª¨ë“  í•„ìˆ˜ í•„ë“œ ì¡´ì¬\"\n",
    "        \n",
    "        # tb_file_dtl_info í•„ìˆ˜ í•„ë“œ ê²€ì¦\n",
    "        required_dtl_fields = [\"sj\", \"file_sz\", \"created_date\"]\n",
    "        file_dtl = metadata.get(\"file_dtl_info\", {})\n",
    "        missing_dtl_fields = [field for field in required_dtl_fields if field not in file_dtl]\n",
    "        \n",
    "        if missing_dtl_fields:\n",
    "            validation_result[\"compatible\"] = False\n",
    "            validation_result[\"errors\"].append(f\"tb_file_dtl_info í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {missing_dtl_fields}\")\n",
    "        else:\n",
    "            validation_result[\"schema_checks\"][\"tb_file_dtl_info\"] = \"âœ… ëª¨ë“  í•„ìˆ˜ í•„ë“œ ì¡´ì¬\"\n",
    "        \n",
    "        # ë°ì´í„° íƒ€ì… ê²€ì¦\n",
    "        if file_bss.get(\"file_sz\") and not isinstance(file_bss[\"file_sz\"], int):\n",
    "            validation_result[\"warnings\"].append(\"file_szëŠ” ì •ìˆ˜í˜•ì´ì–´ì•¼ í•©ë‹ˆë‹¤\")\n",
    "        \n",
    "        if file_bss.get(\"del_yn\") and file_bss[\"del_yn\"] not in [\"Y\", \"N\"]:\n",
    "            validation_result[\"warnings\"].append(\"del_ynì€ 'Y' ë˜ëŠ” 'N'ì´ì–´ì•¼ í•©ë‹ˆë‹¤\")\n",
    "        \n",
    "        # ë¬¸ìì—´ ê¸¸ì´ ê²€ì¦ (ë°ì´í„°ë² ì´ìŠ¤ ì œì•½ì‚¬í•­)\n",
    "        max_lengths = {\n",
    "            \"file_lgc_nm\": 255,\n",
    "            \"file_psl_nm\": 255,\n",
    "            \"knowledge_container_id\": 100,\n",
    "            \"created_by\": 50\n",
    "        }\n",
    "        \n",
    "        for field, max_len in max_lengths.items():\n",
    "            if file_bss.get(field) and len(str(file_bss[field])) > max_len:\n",
    "                validation_result[\"warnings\"].append(f\"{field} ê¸¸ì´ê°€ ìµœëŒ€ê°’({max_len})ì„ ì´ˆê³¼í•©ë‹ˆë‹¤\")\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        if validation_result[\"compatible\"]:\n",
    "            print(\"  âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì¦ í†µê³¼\")\n",
    "            for table, status in validation_result[\"schema_checks\"].items():\n",
    "                print(f\"    {table}: {status}\")\n",
    "        else:\n",
    "            print(\"  âŒ ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨\")\n",
    "            for error in validation_result[\"errors\"]:\n",
    "                print(f\"    - {error}\")\n",
    "        \n",
    "        if validation_result[\"warnings\"]:\n",
    "            print(\"  âš ï¸ ê²½ê³ ì‚¬í•­:\")\n",
    "            for warning in validation_result[\"warnings\"]:\n",
    "                print(f\"    - {warning}\")\n",
    "        \n",
    "        return validation_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        validation_result[\"compatible\"] = False\n",
    "        validation_result[\"errors\"].append(f\"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}\")\n",
    "        print(f\"  âŒ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\")\n",
    "        return validation_result\n",
    "\n",
    "def test_document_metadata_creation():\n",
    "    \"\"\"ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„± ì „ì²´ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"ğŸ§ª ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°\n",
    "    current_dir = Path(\".\")\n",
    "    test_files = list(current_dir.glob(\"*.pdf\"))[:3]  # ìµœëŒ€ 3ê°œ íŒŒì¼ í…ŒìŠ¤íŠ¸\n",
    "    \n",
    "    if not test_files:\n",
    "        print(\"âš ï¸ í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ğŸ’¡ test_template.pdf íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ ë‹¤ë¥¸ PDF íŒŒì¼ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ“ ë°œê²¬ëœ í…ŒìŠ¤íŠ¸ íŒŒì¼: {len(test_files)}ê°œ\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, file_path in enumerate(test_files, 1):\n",
    "        print(f\"\\nğŸ“Š í…ŒìŠ¤íŠ¸ {i}/{len(test_files)}: {file_path.name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ìƒì„±\n",
    "        container_id = f\"test_container_{i}\"\n",
    "        user_emp_no = f\"test_user_{i:03d}\"\n",
    "        \n",
    "        metadata = generate_file_metadata(\n",
    "            str(file_path), \n",
    "            container_id=container_id,\n",
    "            user_emp_no=user_emp_no\n",
    "        )\n",
    "        \n",
    "        if \"error\" in metadata:\n",
    "            print(f\"  âŒ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {metadata['error']}\")\n",
    "            continue\n",
    "        \n",
    "        # ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì¦\n",
    "        schema_validation = validate_database_schema_compatibility(metadata)\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        test_result = {\n",
    "            \"file_name\": file_path.name,\n",
    "            \"metadata_generated\": \"error\" not in metadata,\n",
    "            \"schema_compatible\": schema_validation[\"compatible\"],\n",
    "            \"ready_for_upload\": metadata.get(\"processing_info\", {}).get(\"ready_for_database\", False) and schema_validation[\"compatible\"]\n",
    "        }\n",
    "        \n",
    "        all_results.append(test_result)\n",
    "        \n",
    "        status = \"âœ… ì¤€ë¹„ì™„ë£Œ\" if test_result[\"ready_for_upload\"] else \"âŒ ì¤€ë¹„ë¯¸ì™„ë£Œ\"\n",
    "        print(f\"  ìµœì¢… ìƒíƒœ: {status}\")\n",
    "    \n",
    "    # ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸ ìš”ì•½\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_files = len(all_results)\n",
    "    ready_files = sum(1 for r in all_results if r[\"ready_for_upload\"])\n",
    "    \n",
    "    print(f\"ì´ í…ŒìŠ¤íŠ¸ íŒŒì¼: {total_files}ê°œ\")\n",
    "    print(f\"ì—…ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ: {ready_files}ê°œ\")\n",
    "    print(f\"ì„±ê³µë¥ : {(ready_files/total_files*100) if total_files > 0 else 0:.1f}%\")\n",
    "    \n",
    "    if ready_files == total_files:\n",
    "        print(\"ğŸ‰ ëª¨ë“  íŒŒì¼ì´ ë°±ì—”ë“œ API ì—…ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ì¼ë¶€ íŒŒì¼ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: RAG ë²¡í„° ì €ì¥ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "test_document_metadata_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aeb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5ë‹¨ê³„: ë°±ì—”ë“œ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ë°ëª¨ (Backend Integrated Pipeline Execution & Demo)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "ë°±ì—”ë“œ ì„œë¹„ìŠ¤ì™€ í†µí•©ëœ ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "Production Features:\n",
    "- AWS Bedrock Titan V2 embeddings\n",
    "- Korean NLP with kiwipiepy\n",
    "- PostgreSQL + pgvector storage and search\n",
    "- Quality assessment and gating mechanisms\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸš€ ë°±ì—”ë“œ í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ì„¤ì •: PDF íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ íŒŒì¼ë¡œ êµì²´ í•„ìš”)\n",
    "PDF_PATH = 'test_template.pdf'  # í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ ê²½ë¡œ\n",
    "CONTAINER_ID = \"notebook_test_container\"  # ì§€ì‹ ì»¨í…Œì´ë„ˆ ID\n",
    "\n",
    "TEST_QUERIES = [\n",
    "    \"í…œí”Œë¦¿\",\n",
    "    \"ìŠ¬ë¼ì´ë“œ êµ¬ì¡°\", \n",
    "    \"ìš”ì•½\",\n",
    "    \"ë””ìì¸ ê°€ì´ë“œë¼ì¸\",\n",
    "    \"í”„ë ˆì  í…Œì´ì…˜ í˜•ì‹\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # ë‹¨ê³„ 1: í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR í¬í•¨)\n",
    "    print(\"1ï¸âƒ£ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë‹¨ê³„\")\n",
    "    extraction_result = extract_text(PDF_PATH)\n",
    "    raw_text = extraction_result['text']\n",
    "    is_scanned = extraction_result['scanned']\n",
    "    \n",
    "    if not raw_text.strip():\n",
    "        print(\"âŒ ì˜¤ë¥˜: í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   í™•ì¸ì‚¬í•­: íŒŒì¼ ê²½ë¡œ, íŒŒì¼ ê¶Œí•œ, PDF í˜•ì‹\")\n",
    "    else:\n",
    "        print(f\"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(raw_text)} characters\")\n",
    "        if BACKEND_SERVICES_AVAILABLE:\n",
    "            print(f\"ğŸ¯ Backend Mode: AWS Bedrock + pgvector í™œì„±í™”\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Fallback Mode: TF-IDF ì‚¬ìš©\")\n",
    "        print()\n",
    "    \n",
    "    # ë‹¨ê³„ 2: í…ìŠ¤íŠ¸ ì •ê·œí™”\n",
    "    print(\"2ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ê·œí™” ë‹¨ê³„\")\n",
    "    cleaned_text = normalize(raw_text)\n",
    "    print(f\"âœ… ì •ê·œí™” ì™„ë£Œ\\n\")\n",
    "    \n",
    "    # ë‹¨ê³„ 3: í‚¤ì›Œë“œ ì¶”ì¶œ (ë°±ì—”ë“œ ì‚¬ìš©ì‹œ í•œêµ­ì–´ NLP í™œìš©)\n",
    "    print(\"3ï¸âƒ£ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ë‹¨ê³„\")\n",
    "    if BACKEND_SERVICES_AVAILABLE:\n",
    "        try:\n",
    "            # ë°±ì—”ë“œ Korean NLP ì„œë¹„ìŠ¤ ì‚¬ìš©\n",
    "            loop = asyncio.get_event_loop() if asyncio.get_event_loop().is_running() else asyncio.new_event_loop()\n",
    "            if not asyncio.get_event_loop().is_running():\n",
    "                asyncio.set_event_loop(loop)\n",
    "            \n",
    "            korean_analysis = loop.run_until_complete(\n",
    "                korean_nlp_service.analyze_text_for_search(cleaned_text[:2000])  # ì²« 2000ìë§Œ ë¶„ì„\n",
    "            )\n",
    "            \n",
    "            keywords = korean_analysis.get(\"korean_keywords\", [])\n",
    "            named_entities = korean_analysis.get(\"named_entities\", [])\n",
    "            \n",
    "            print(f\"ğŸ”¤ í•œêµ­ì–´ í‚¤ì›Œë“œ (ìƒìœ„ 15ê°œ): {', '.join(keywords[:15])}\")\n",
    "            print(f\"ğŸ·ï¸ ê°œì²´ëª… (ìƒìœ„ 10ê°œ): {', '.join(named_entities[:10])}\")\n",
    "            print(f\"âœ… ë°±ì—”ë“œ í•œêµ­ì–´ NLP ë¶„ì„ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ë°±ì—”ë“œ í•œêµ­ì–´ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "            keywords = extract_keywords_ko(cleaned_text, top_k=30)\n",
    "            print(f\"ğŸ”¤ í´ë°± í‚¤ì›Œë“œ (ìƒìœ„ 15ê°œ): {', '.join(keywords[:15])}\")\n",
    "    else:\n",
    "        keywords = extract_keywords_ko(cleaned_text, top_k=30)\n",
    "        print(f\"ğŸ”¤ ì¶”ì¶œëœ ì£¼ìš” í‚¤ì›Œë“œ (ìƒìœ„ 15ê°œ): {', '.join(keywords[:15])}\")\n",
    "    print(f\"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ\\n\")\n",
    "    \n",
    "    # ë‹¨ê³„ 4: ë¬¸ì¥ ë¶„í•  ë° ì²­í‚¹\n",
    "    print(\"4ï¸âƒ£ ë¬¸ì¥ ë¶„í•  ë° ì²­í‚¹ ë‹¨ê³„\")\n",
    "    sentences = sentence_tokenize(cleaned_text)\n",
    "    chunks = chunk(sentences, max_chars=800)\n",
    "    print(f\"âœ… ì²­í‚¹ ì™„ë£Œ\\n\")\n",
    "    \n",
    "    # ë‹¨ê³„ 5: ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• (Production ë˜ëŠ” TF-IDF)\n",
    "    print(\"5ï¸âƒ£ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ë‹¨ê³„\")\n",
    "    if BACKEND_SERVICES_AVAILABLE:\n",
    "        print(\"ğŸ¯ Production ì¸ë±ìŠ¤ êµ¬ì¶• (AWS Bedrock + pgvector)\")\n",
    "        search_index = ProductionIndex(chunks, CONTAINER_ID)\n",
    "        \n",
    "        # PostgreSQLì— ì €ì¥\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop() if asyncio.get_event_loop().is_running() else asyncio.new_event_loop()\n",
    "            if not asyncio.get_event_loop().is_running():\n",
    "                asyncio.set_event_loop(loop)\n",
    "            \n",
    "            loop.run_until_complete(search_index.store_to_database())\n",
    "            print(\"ğŸ—„ï¸ PostgreSQL + pgvector ì €ì¥ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "    else:\n",
    "        print(\"ğŸ” TF-IDF í´ë°± ì¸ë±ìŠ¤ êµ¬ì¶•\")\n",
    "        search_index = MiniIndex(chunks)\n",
    "    \n",
    "    print(f\"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ\\n\")\n",
    "    \n",
    "    # ë‹¨ê³„ 6: íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í‰ê°€\n",
    "    quality_report(raw_text, cleaned_text, chunks, is_scanned, search_index)\n",
    "    \n",
    "    # ë‹¨ê³„ 7: ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"\\nğŸ” {'Production ê²€ìƒ‰' if BACKEND_SERVICES_AVAILABLE else 'TF-IDF ê²€ìƒ‰'} í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for query_idx, query in enumerate(TEST_QUERIES, 1):\n",
    "        print(f\"\\n[í…ŒìŠ¤íŠ¸ {query_idx}] ì¿¼ë¦¬: '{query}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        search_results = hybrid_search(search_index, query, top_k=3)\n",
    "        \n",
    "        if not search_results or search_results[0][1] < 0.01:\n",
    "            print(\"âŒ ê´€ë ¨ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            print(\"   ğŸ’¡ Gating ë©”ì»¤ë‹ˆì¦˜: 'ë‹µë³€í•  ìˆ˜ ì—†ìŒ' ì‘ë‹µ í•„ìš”\")\n",
    "        else:\n",
    "            print(f\"âœ… {len(search_results)}ê°œ ê²°ê³¼ ë°œê²¬:\")\n",
    "            for rank, (chunk_idx, score) in enumerate(search_results, 1):\n",
    "                # ê²°ê³¼ ì²­í¬ ë¯¸ë¦¬ë³´ê¸° (ì²« 120ì)\n",
    "                preview = chunks[chunk_idx][:120].replace('\\n', ' ')\n",
    "                \n",
    "                # ì‹ ë¢°ë„ ê³„ì‚°\n",
    "                if BACKEND_SERVICES_AVAILABLE:\n",
    "                    confidence = \"ë†’ìŒ\" if score > 0.7 else \"ë³´í†µ\" if score > 0.5 else \"ë‚®ìŒ\"\n",
    "                    quality_threshold = 0.5  # Production ì„ê³„ê°’\n",
    "                else:\n",
    "                    confidence = \"ë†’ìŒ\" if score > 0.3 else \"ë³´í†µ\" if score > 0.1 else \"ë‚®ìŒ\"\n",
    "                    quality_threshold = 0.05  # TF-IDF ì„ê³„ê°’\n",
    "                \n",
    "                print(f\"  {rank}ìœ„ (ì‹ ë¢°ë„: {confidence}, ì ìˆ˜: {score:.3f})\")\n",
    "                print(f\"     ğŸ“„ {preview}...\")\n",
    "                \n",
    "                # í’ˆì§ˆ ê²½ê³ \n",
    "                if score < quality_threshold:\n",
    "                    print(f\"     âš ï¸ ë‚®ì€ ìœ ì‚¬ë„ - ì‘ë‹µ í’ˆì§ˆ ê²€í†  í•„ìš”\")\n",
    "                \n",
    "                # Grounding ratio ì‹œë®¬ë ˆì´ì…˜\n",
    "                if BACKEND_SERVICES_AVAILABLE and score > 0.7:\n",
    "                    print(f\"     âœ… ë†’ì€ ì‹ ë¢°ë„ - RAG ì‘ë‹µ ìƒì„± ê¶Œì¥\")\n",
    "    \n",
    "    # ë‹¨ê³„ 8: ìµœì¢… í’ˆì§ˆ ë©”íŠ¸ë¦­ ë° ê¶Œì¥ì‚¬í•­\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ‰ ë°±ì—”ë“œ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!\")\n",
    "    \n",
    "    if BACKEND_SERVICES_AVAILABLE:\n",
    "        print(\"\\nğŸ¯ Production í’ˆì§ˆ ë©”íŠ¸ë¦­:\")\n",
    "        quality_metrics = search_index.get_quality_assessment()\n",
    "        print(f\"   â€¢ ì„ë² ë”© ì„±ê³µë¥ : {quality_metrics['embedding_success_rate']:.1%}\")\n",
    "        print(f\"   â€¢ í•œêµ­ì–´ ë¶„ì„ ì„±ê³µë¥ : {quality_metrics['korean_analysis_success_rate']:.1%}\")\n",
    "        print(f\"   â€¢ í‰ê·  í‚¤ì›Œë“œ ìˆ˜: {quality_metrics['average_keywords_per_chunk']:.1f}ê°œ/ì²­í¬\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ Production ìš´ì˜ ê¶Œì¥ì‚¬í•­:\")\n",
    "        print(\"   â€¢ âœ… AWS Bedrock Titan V2 ì„ë² ë”© (1024ì°¨ì›) í™œì„±í™”\")\n",
    "        print(\"   â€¢ âœ… Korean NLP Service with kiwipiepy í™œì„±í™”\") \n",
    "        print(\"   â€¢ âœ… PostgreSQL + pgvector ë²¡í„° ê²€ìƒ‰ í™œì„±í™”\")\n",
    "        print(\"   â€¢ ğŸ”§ ì„ê³„ê°’ ì¡°ì •: ìœ ì‚¬ë„ > 0.7 ê¶Œì¥\")\n",
    "        print(\"   â€¢ ğŸ”§ Gating ë©”ì»¤ë‹ˆì¦˜: ë‚®ì€ ì‹ ë¢°ë„ì‹œ ì‘ë‹µ ì°¨ë‹¨\")\n",
    "        print(\"   â€¢ ğŸ”§ Citation ì œí•œ: ì°¸ì¡° ë¬¸ì„œ ìµœëŒ€ 3-5ê°œ\")\n",
    "        print(\"   â€¢ ğŸ”§ Grounding Ratio: RAG ì‘ë‹µ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§\")\n",
    "    else:\n",
    "        print(\"\\nğŸ’¡ ë°±ì—”ë“œ ì—°ë™ ê¶Œì¥ì‚¬í•­:\")\n",
    "        print(\"   â€¢ ğŸ”§ backend/.env ì„¤ì • í™•ì¸\")\n",
    "        print(\"   â€¢ ğŸ”§ AWS ìê²©ì¦ëª… ë° ë¦¬ì „ ì„¤ì •\")\n",
    "        print(\"   â€¢ ğŸ”§ PostgreSQL ì—°ê²° ì„¤ì •\")\n",
    "        print(\"   â€¢ ğŸ”§ kiwipiepy ë° ê´€ë ¨ ì˜ì¡´ì„± ì„¤ì¹˜\")\n",
    "        print(\"   â€¢ â¬†ï¸ TF-IDF â†’ AWS Bedrock ì„ë² ë”© ì—…ê·¸ë ˆì´ë“œ\")\n",
    "    \n",
    "    print(\"\\nğŸš€ ì‹œìŠ¤í…œ ì˜í–¥ ìµœì†Œí™”:\")\n",
    "    print(\"   â€¢ âœ… ê¸°ì¡´ backend/.env ì„¤ì • ì¬ì‚¬ìš©\")\n",
    "    print(\"   â€¢ âœ… vs_doc_contents_index í…Œì´ë¸” í™œìš©\")\n",
    "    print(\"   â€¢ âœ… ê¸°ì¡´ Korean NLP Service í™œìš©\")\n",
    "    print(\"   â€¢ âœ… ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ ì§€ì›\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_PATH}\")\n",
    "    print(\"ğŸ’¡ í•´ê²°ë°©ë²•:\")\n",
    "    print(\"   1. ì˜¬ë°”ë¥¸ íŒŒì¼ ê²½ë¡œ í™•ì¸\")\n",
    "    print(\"   2. ì‘ì—… ë””ë ‰í„°ë¦¬ì— test_template.pdf íŒŒì¼ ìƒì„±\")\n",
    "    print(\"   3. ë˜ëŠ” PDF_PATH ë³€ìˆ˜ë¥¼ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"ğŸ’¡ ë””ë²„ê¹… ê¶Œì¥ì‚¬í•­:\")\n",
    "    print(\"   1. ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸\")\n",
    "    print(\"   2. ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸\")\n",
    "    print(\"   3. íŒŒì¼ ê¶Œí•œ ë° í˜•ì‹ í™•ì¸\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d58b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ” 4ë‹¨ê³„: ë°±ì—”ë“œ ê¶Œí•œ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (Backend Permission System Test)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "ë°±ì—”ë“œ APIì˜ permission_service.check_upload_permission() ë¡œì§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "ì»¨í…Œì´ë„ˆë³„ ì—…ë¡œë“œ ê¶Œí•œ ê²€ì¦ ë° ì‚¬ìš©ì ê¶Œí•œ ê´€ë¦¬ ì‹œìŠ¤í…œì„ ê²€ì¦í•©ë‹ˆë‹¤.\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ í•­ëª©:\n",
    "1. ì»¨í…Œì´ë„ˆë³„ ì—…ë¡œë“œ ê¶Œí•œ ê²€ì¦\n",
    "2. ì‚¬ìš©ì ê¶Œí•œ ë ˆë²¨ í™•ì¸\n",
    "3. ê¶Œí•œ ìƒì† ë° ê³„ì¸µ êµ¬ì¡°\n",
    "4. ê¶Œí•œ ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "def simulate_permission_check(user_emp_no: str, container_id: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    ë°±ì—”ë“œ permission_service.check_upload_permission() ì‹œë®¬ë ˆì´ì…˜\n",
    "    \n",
    "    ì‹¤ì œ ë°±ì—”ë“œì—ì„œëŠ” TbUserPermissions í…Œì´ë¸”ì„ ì¡°íšŒí•˜ì—¬ ê¶Œí•œì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        user_emp_no (str): ì‚¬ìš©ì ì‚¬ë²ˆ\n",
    "        container_id (str): ì»¨í…Œì´ë„ˆ ID\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[bool, str]: (ê¶Œí•œ ì—¬ë¶€, ê¶Œí•œ ë©”ì‹œì§€)\n",
    "    \"\"\"\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ìš© ê¶Œí•œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ)\n",
    "    simulated_permissions = {\n",
    "        # ì‚¬ìš©ìë³„ ì»¨í…Œì´ë„ˆ ê¶Œí•œ ë§¤í•‘\n",
    "        \"admin\": {\n",
    "            \"*\": \"ADMIN\",  # ëª¨ë“  ì»¨í…Œì´ë„ˆì— ê´€ë¦¬ì ê¶Œí•œ\n",
    "        },\n",
    "        \"test_user\": {\n",
    "            \"public\": \"UPLOADER\",\n",
    "            \"shared\": \"VIEWER\",\n",
    "            \"personal\": \"UPLOADER\"\n",
    "        },\n",
    "        \"guest_user\": {\n",
    "            \"public\": \"VIEWER\"\n",
    "        },\n",
    "        \"manager\": {\n",
    "            \"team_docs\": \"UPLOADER\",\n",
    "            \"project_*\": \"UPLOADER\",  # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´\n",
    "            \"public\": \"UPLOADER\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    user_permissions = simulated_permissions.get(user_emp_no, {})\n",
    "    \n",
    "    # 1. ì§ì ‘ ê¶Œí•œ í™•ì¸\n",
    "    if container_id in user_permissions:\n",
    "        permission_level = user_permissions[container_id]\n",
    "        can_upload = permission_level in [\"UPLOADER\", \"ADMIN\"]\n",
    "        message = f\"ì»¨í…Œì´ë„ˆ '{container_id}'ì— ëŒ€í•œ {permission_level} ê¶Œí•œ ë³´ìœ \"\n",
    "        return can_upload, message\n",
    "    \n",
    "    # 2. ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ê¶Œí•œ í™•ì¸\n",
    "    for pattern, permission_level in user_permissions.items():\n",
    "        if pattern == \"*\":\n",
    "            can_upload = permission_level in [\"UPLOADER\", \"ADMIN\"]\n",
    "            message = f\"ì „ì²´ ì»¨í…Œì´ë„ˆì— ëŒ€í•œ {permission_level} ê¶Œí•œ ë³´ìœ \"\n",
    "            return can_upload, message\n",
    "        elif pattern.endswith(\"*\") and container_id.startswith(pattern[:-1]):\n",
    "            can_upload = permission_level in [\"UPLOADER\", \"ADMIN\"]\n",
    "            message = f\"íŒ¨í„´ '{pattern}'ì— ì˜í•œ {permission_level} ê¶Œí•œ ë³´ìœ \"\n",
    "            return can_upload, message\n",
    "    \n",
    "    # 3. ê¶Œí•œ ì—†ìŒ\n",
    "    return False, f\"ì»¨í…Œì´ë„ˆ '{container_id}'ì— ëŒ€í•œ ì—…ë¡œë“œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "def simulate_container_access_list(user_emp_no: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ìê°€ ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ëª©ë¡ ì‹œë®¬ë ˆì´ì…˜\n",
    "    \n",
    "    Args:\n",
    "        user_emp_no (str): ì‚¬ìš©ì ì‚¬ë²ˆ\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ëª©ë¡\n",
    "    \"\"\"\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ìš© ì»¨í…Œì´ë„ˆ ë°ì´í„°\n",
    "    all_containers = [\n",
    "        {\"id\": \"public\", \"name\": \"ê³µê°œ ë¬¸ì„œí•¨\", \"description\": \"ëª¨ë“  ì‚¬ìš©ìê°€ ì ‘ê·¼ ê°€ëŠ¥\"},\n",
    "        {\"id\": \"shared\", \"name\": \"ê³µìœ  ë¬¸ì„œí•¨\", \"description\": \"ë¶€ì„œ ë‚´ ê³µìœ  ë¬¸ì„œ\"},\n",
    "        {\"id\": \"personal\", \"name\": \"ê°œì¸ ë¬¸ì„œí•¨\", \"description\": \"ê°œì¸ ì „ìš© ë¬¸ì„œí•¨\"},\n",
    "        {\"id\": \"team_docs\", \"name\": \"íŒ€ ë¬¸ì„œí•¨\", \"description\": \"íŒ€ í”„ë¡œì íŠ¸ ë¬¸ì„œ\"},\n",
    "        {\"id\": \"project_alpha\", \"name\": \"í”„ë¡œì íŠ¸ ì•ŒíŒŒ\", \"description\": \"ì•ŒíŒŒ í”„ë¡œì íŠ¸ ë¬¸ì„œ\"},\n",
    "        {\"id\": \"project_beta\", \"name\": \"í”„ë¡œì íŠ¸ ë² íƒ€\", \"description\": \"ë² íƒ€ í”„ë¡œì íŠ¸ ë¬¸ì„œ\"},\n",
    "        {\"id\": \"admin_only\", \"name\": \"ê´€ë¦¬ì ì „ìš©\", \"description\": \"ê´€ë¦¬ìë§Œ ì ‘ê·¼ ê°€ëŠ¥\"}\n",
    "    ]\n",
    "    \n",
    "    accessible_containers = []\n",
    "    \n",
    "    for container in all_containers:\n",
    "        can_access, message = simulate_permission_check(user_emp_no, container[\"id\"])\n",
    "        if can_access:\n",
    "            accessible_containers.append({\n",
    "                **container,\n",
    "                \"permission_message\": message,\n",
    "                \"access_level\": \"UPLOADER\" if \"UPLOADER\" in message or \"ADMIN\" in message else \"VIEWER\"\n",
    "            })\n",
    "    \n",
    "    return accessible_containers\n",
    "\n",
    "def test_permission_scenarios():\n",
    "    \"\"\"ë‹¤ì–‘í•œ ê¶Œí•œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"ğŸ” ê¶Œí•œ ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤\n",
    "    test_scenarios = [\n",
    "        # (ì‚¬ìš©ì, ì»¨í…Œì´ë„ˆ, ì˜ˆìƒ ê²°ê³¼)\n",
    "        (\"admin\", \"any_container\", True),\n",
    "        (\"test_user\", \"public\", True),\n",
    "        (\"test_user\", \"shared\", False),  # VIEWER ê¶Œí•œ\n",
    "        (\"test_user\", \"personal\", True),\n",
    "        (\"guest_user\", \"public\", False),  # VIEWER ê¶Œí•œ\n",
    "        (\"guest_user\", \"private\", False),\n",
    "        (\"manager\", \"team_docs\", True),\n",
    "        (\"manager\", \"project_alpha\", True),  # íŒ¨í„´ ë§¤ì¹­\n",
    "        (\"manager\", \"project_beta\", True),   # íŒ¨í„´ ë§¤ì¹­\n",
    "        (\"nonexistent_user\", \"public\", False)\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ§ª ê¶Œí•œ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:\")\n",
    "    print(\"ì‚¬ìš©ì | ì»¨í…Œì´ë„ˆ | ì˜ˆìƒ | ì‹¤ì œ | ìƒíƒœ\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    passed_tests = 0\n",
    "    total_tests = len(test_scenarios)\n",
    "    \n",
    "    for user, container, expected in test_scenarios:\n",
    "        can_upload, message = simulate_permission_check(user, container)\n",
    "        status = \"âœ… PASS\" if can_upload == expected else \"âŒ FAIL\"\n",
    "        \n",
    "        if can_upload == expected:\n",
    "            passed_tests += 1\n",
    "        \n",
    "        # ê²°ê³¼ í‘œì‹œ (ê¸¸ì´ ì œí•œ)\n",
    "        user_display = user[:12] + \"...\" if len(user) > 15 else user\n",
    "        container_display = container[:12] + \"...\" if len(container) > 15 else container\n",
    "        expected_display = \"í—ˆìš©\" if expected else \"ê±°ë¶€\"\n",
    "        actual_display = \"í—ˆìš©\" if can_upload else \"ê±°ë¶€\"\n",
    "        \n",
    "        print(f\"{user_display:<8} | {container_display:<10} | {expected_display:<4} | {actual_display:<4} | {status}\")\n",
    "        \n",
    "        if can_upload != expected:\n",
    "            print(f\"  âš ï¸ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼: {message}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed_tests}/{total_tests} í†µê³¼ ({passed_tests/total_tests*100:.1f}%)\")\n",
    "\n",
    "def test_user_accessible_containers():\n",
    "    \"\"\"ì‚¬ìš©ìë³„ ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ ëª©ë¡ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"\\nğŸ“‚ ì‚¬ìš©ìë³„ ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_users = [\"admin\", \"test_user\", \"guest_user\", \"manager\"]\n",
    "    \n",
    "    for user in test_users:\n",
    "        print(f\"\\nğŸ‘¤ ì‚¬ìš©ì: {user}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        accessible_containers = simulate_container_access_list(user)\n",
    "        \n",
    "        if accessible_containers:\n",
    "            print(f\"ğŸ“ ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ: {len(accessible_containers)}ê°œ\")\n",
    "            for container in accessible_containers:\n",
    "                access_level = container[\"access_level\"]\n",
    "                emoji = \"ğŸ“¤\" if access_level == \"UPLOADER\" else \"ğŸ‘ï¸\"\n",
    "                print(f\"  {emoji} {container['name']} ({container['id']}) - {access_level}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ ì ‘ê·¼ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "def test_upload_permission_workflow():\n",
    "    \"\"\"ì‹¤ì œ ì—…ë¡œë“œ ì›Œí¬í”Œë¡œìš° ê¶Œí•œ ê²€ì¦ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"\\nğŸ”„ ì—…ë¡œë“œ ì›Œí¬í”Œë¡œìš° ê¶Œí•œ ê²€ì¦ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ì‹¤ì œ ì—…ë¡œë“œ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜\n",
    "    upload_scenarios = [\n",
    "        {\n",
    "            \"user\": \"test_user\", \n",
    "            \"container\": \"personal\",\n",
    "            \"file_name\": \"my_document.pdf\",\n",
    "            \"description\": \"ê°œì¸ ë¬¸ì„œí•¨ì— ê°œì¸ íŒŒì¼ ì—…ë¡œë“œ\"\n",
    "        },\n",
    "        {\n",
    "            \"user\": \"manager\",\n",
    "            \"container\": \"project_alpha\", \n",
    "            \"file_name\": \"project_plan.docx\",\n",
    "            \"description\": \"í”„ë¡œì íŠ¸ ê´€ë¦¬ìê°€ í”„ë¡œì íŠ¸ ë¬¸ì„œ ì—…ë¡œë“œ\"\n",
    "        },\n",
    "        {\n",
    "            \"user\": \"guest_user\",\n",
    "            \"container\": \"public\",\n",
    "            \"file_name\": \"public_file.txt\", \n",
    "            \"description\": \"ê²ŒìŠ¤íŠ¸ê°€ ê³µê°œ ë¬¸ì„œí•¨ì— ì—…ë¡œë“œ ì‹œë„\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, scenario in enumerate(upload_scenarios, 1):\n",
    "        print(f\"\\nğŸ“¤ ì—…ë¡œë“œ ì‹œë‚˜ë¦¬ì˜¤ {i}: {scenario['description']}\")\n",
    "        print(f\"  ğŸ‘¤ ì‚¬ìš©ì: {scenario['user']}\")\n",
    "        print(f\"  ğŸ“ ì»¨í…Œì´ë„ˆ: {scenario['container']}\")\n",
    "        print(f\"  ğŸ“„ íŒŒì¼: {scenario['file_name']}\")\n",
    "        \n",
    "        # ê¶Œí•œ ê²€ì¦\n",
    "        can_upload, permission_message = simulate_permission_check(\n",
    "            scenario['user'], \n",
    "            scenario['container']\n",
    "        )\n",
    "        \n",
    "        if can_upload:\n",
    "            print(f\"  âœ… ì—…ë¡œë“œ í—ˆìš©: {permission_message}\")\n",
    "            print(f\"  ğŸ“ ë‹¤ìŒ ë‹¨ê³„: íŒŒì¼ ê²€ì¦ â†’ ì €ì¥ â†’ ë©”íƒ€ë°ì´í„° ìƒì„±\")\n",
    "        else:\n",
    "            print(f\"  âŒ ì—…ë¡œë“œ ê±°ë¶€: {permission_message}\")\n",
    "            print(f\"  ğŸ”’ 403 Forbidden ì‘ë‹µ ì˜ˆìƒ\")\n",
    "\n",
    "def test_backend_permission_integration():\n",
    "    \"\"\"ë°±ì—”ë“œ ê¶Œí•œ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"\\nğŸ¯ ë°±ì—”ë“œ ê¶Œí•œ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if BACKEND_SERVICES_AVAILABLE:\n",
    "        print(\"ğŸ”— ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì—°ê²°ë¨ - ì‹¤ì œ ê¶Œí•œ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥\")\n",
    "        \n",
    "        # ì‹¤ì œ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•œ ê¶Œí•œ í…ŒìŠ¤íŠ¸\n",
    "        try:\n",
    "            # ì´ ë¶€ë¶„ì€ ì‹¤ì œ ë°±ì—”ë“œ ì—°ê²°ì‹œ í™œì„±í™”ë©ë‹ˆë‹¤\n",
    "            print(\"  ğŸ’¡ ì‹¤ì œ ê¶Œí•œ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ëŠ” ë°±ì—”ë“œ ì—°ê²° í›„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "            print(\"  ğŸ”§ permission_service.check_upload_permission() í˜¸ì¶œ\")\n",
    "            print(\"  ğŸ”§ container_service.get_user_accessible_containers() í˜¸ì¶œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ ë°±ì—”ë“œ ê¶Œí•œ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    else:\n",
    "        print(\"ğŸ“¡ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ë¯¸ì—°ê²° - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì‚¬ìš©\")\n",
    "    \n",
    "    print(\"\\nâœ… ê¶Œí•œ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n",
    "    print(\"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì „ì²´ ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "test_permission_scenarios()\n",
    "test_user_accessible_containers()\n",
    "test_upload_permission_workflow()\n",
    "test_backend_permission_integration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6ë‹¨ê³„: ë¬¸ì„œ ê·œê²© ì¤€ìˆ˜ë„ ê²€ì¦ ë° ë©€í‹°ëª¨ë‹¬ í™•ì¥ ë¡œë“œë§µ (Specification Compliance & Multimodal Roadmap)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "/home/admin/wkms-aws/01.docs/02.document_ingestion_vectorstore.md ê·œê²© ëŒ€ë¹„ ì™„ì„±ë„ í‰ê°€\n",
    "ë° vs_multimodal_contents_index í…Œì´ë¸”ì„ í™œìš©í•œ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ í™•ì¥ ê³„íš\n",
    "\"\"\"\n",
    "\n",
    "def check_specification_compliance():\n",
    "    \"\"\"ë¬¸ì„œ ê·œê²© ëŒ€ë¹„ í˜„ì¬ êµ¬í˜„ ì™„ì„±ë„ í‰ê°€\"\"\"\n",
    "    print(\"ğŸ“‹ ë¬¸ì„œ ê·œê²© ì¤€ìˆ˜ë„ ê²€ì¦\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ê¸°ë³¸ OCR íŒŒì´í”„ë¼ì¸ (ì™„ë£Œ)\n",
    "    completed_features = [\n",
    "        \"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyMuPDF)\",\n",
    "        \"âœ… ìŠ¤ìº” ë¬¸ì„œ ê°ì§€ ë¡œì§\",\n",
    "        \"âœ… EasyOCR í•œêµ­ì–´/ì˜ì–´ OCR\",\n",
    "        \"âœ… 2ë‹¨ ë ˆì´ì•„ì›ƒ ë³µì›\",\n",
    "        \"âœ… í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ë…¸ì´ì¦ˆ ì œê±°\",\n",
    "        \"âœ… kiwipiepy í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„\",\n",
    "        \"âœ… ì˜ë¯¸ì  ì²­í‚¹ (ë¬¸ì¥ ë‹¨ìœ„)\",\n",
    "        \"âœ… í’ˆì§ˆ í‰ê°€ ë° ì»¤ë²„ë¦¬ì§€ ì¸¡ì •\"\n",
    "    ]\n",
    "    \n",
    "    # ë°±ì—”ë“œ í†µí•© (ì™„ë£Œ)\n",
    "    if BACKEND_SERVICES_AVAILABLE:\n",
    "        completed_features.extend([\n",
    "            \"âœ… AWS Bedrock Titan V2 ì„ë² ë”© (1024ì°¨ì›)\",\n",
    "            \"âœ… Korean NLP Service í†µí•©\",\n",
    "            \"âœ… PostgreSQL + pgvector ì €ì¥\",\n",
    "            \"âœ… vs_doc_contents_index í…Œì´ë¸” í™œìš©\",\n",
    "            \"âœ… ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰\",\n",
    "            \"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)\",\n",
    "            \"âœ… ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ë° Gating ë©”ì»¤ë‹ˆì¦˜\"\n",
    "        ])\n",
    "    \n",
    "    # ë¯¸êµ¬í˜„ ê¸°ëŠ¥ (í–¥í›„ í™•ì¥ í•„ìš”)\n",
    "    pending_features = [\n",
    "        \"ğŸ”§ HWP íŒŒì¼ ì§€ì› (olefile)\",\n",
    "        \"ğŸ”§ PPT/PPTX ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬\",\n",
    "        \"ğŸ”§ ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± (AWS Rekognition)\",\n",
    "        \"ğŸ”§ í…Œì´ë¸” êµ¬ì¡° ì¸ì‹ ë° ì¶”ì¶œ\",\n",
    "        \"ğŸ”§ vs_multimodal_contents_index í…Œì´ë¸” í™œìš©\",\n",
    "        \"ğŸ”§ ì´ë¯¸ì§€ ì„ë² ë”© (CLIP/Vision Transformer)\",\n",
    "        \"ğŸ”§ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)\",\n",
    "        \"ğŸ”§ ë¬¸ì„œ ê³„ì¸µ êµ¬ì¡° ë¶„ì„ (ì œëª©, ëª©ì°¨)\",\n",
    "        \"ğŸ”§ AWS Comprehend ê°œì²´ëª… ì¸ì‹\",\n",
    "        \"ğŸ”§ ì‹¤ì‹œê°„ OCR ìºì‹±\"\n",
    "    ]\n",
    "    \n",
    "    # ë¶€ë¶„ êµ¬í˜„ ê¸°ëŠ¥\n",
    "    partial_features = [\n",
    "        \"ğŸŸ¡ ë¬¸ì„œ íƒ€ì…ë³„ ì²˜ë¦¬ ì „ëµ (PDFë§Œ ì™„ë£Œ)\",\n",
    "        \"ğŸŸ¡ í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬ (ê¸°ë³¸ ìˆ˜ì¤€)\",\n",
    "        \"ğŸŸ¡ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¡œê¹… (ê¸°ë³¸ ìˆ˜ì¤€)\",\n",
    "        \"ğŸŸ¡ ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” (ë¯¸ì ìš©)\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ¯ ì™„ë£Œëœ ê¸°ëŠ¥:\")\n",
    "    for feature in completed_features:\n",
    "        print(f\"   {feature}\")\n",
    "    \n",
    "    print(f\"\\nğŸŸ¡ ë¶€ë¶„ êµ¬í˜„ëœ ê¸°ëŠ¥:\")\n",
    "    for feature in partial_features:\n",
    "        print(f\"   {feature}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”§ í–¥í›„ êµ¬í˜„ í•„ìš”:\")\n",
    "    for feature in pending_features:\n",
    "        print(f\"   {feature}\")\n",
    "    \n",
    "    # ì™„ì„±ë„ ê³„ì‚°\n",
    "    total_features = len(completed_features) + len(partial_features) + len(pending_features)\n",
    "    completion_rate = (len(completed_features) + 0.5 * len(partial_features)) / total_features\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì „ì²´ ì™„ì„±ë„: {completion_rate:.1%}\")\n",
    "    print(f\"   ì™„ë£Œ: {len(completed_features)}ê°œ\")\n",
    "    print(f\"   ë¶€ë¶„ì™„ë£Œ: {len(partial_features)}ê°œ\") \n",
    "    print(f\"   ë¯¸ì™„ë£Œ: {len(pending_features)}ê°œ\")\n",
    "\n",
    "def multimodal_expansion_roadmap():\n",
    "    \"\"\"ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ í™•ì¥ ë¡œë“œë§µ\"\"\"\n",
    "    print(\"\\nğŸ¨ ë©€í‹°ëª¨ë‹¬ í™•ì¥ ë¡œë“œë§µ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"ğŸ—„ï¸ vs_multimodal_contents_index í…Œì´ë¸” êµ¬ì¡°:\")\n",
    "    multimodal_schema = \"\"\"\n",
    "    CREATE TABLE vs_multimodal_contents_index (\n",
    "        id VARCHAR PRIMARY KEY,\n",
    "        file_bss_info_sno BIGINT,\n",
    "        knowledge_container_id VARCHAR,\n",
    "        content_type VARCHAR, -- 'text', 'image', 'table', 'mixed'\n",
    "        \n",
    "        -- í…ìŠ¤íŠ¸ ì„ë² ë”© (ê¸°ì¡´)\n",
    "        text_content TEXT,\n",
    "        text_embedding vector(1024),\n",
    "        \n",
    "        -- ì´ë¯¸ì§€ ì„ë² ë”© (ì‹ ê·œ)\n",
    "        image_embeddings vector(512)[],  -- CLIP ì„ë² ë”© ë°°ì—´\n",
    "        image_captions TEXT[],           -- ìƒì„±ëœ ìº¡ì…˜ë“¤\n",
    "        \n",
    "        -- í…Œì´ë¸” êµ¬ì¡° (ì‹ ê·œ)\n",
    "        table_structures JSONB,          -- í…Œì´ë¸” ë©”íƒ€ë°ì´í„°\n",
    "        \n",
    "        -- ë©€í‹°ëª¨ë‹¬ í†µí•© ì„ë² ë”© (ì‹ ê·œ)\n",
    "        multimodal_embedding vector(1024), -- í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìœµí•©\n",
    "        \n",
    "        created_date TIMESTAMP DEFAULT NOW()\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    print(textwrap.indent(multimodal_schema.strip(), \"   \"))\n",
    "    \n",
    "    print(\"\\nğŸ”§ ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš:\")\n",
    "    \n",
    "    phase1 = [\n",
    "        \"1ï¸âƒ£ Phase 1 - ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ë°˜ êµ¬ì¶• (4ì£¼)\",\n",
    "        \"   â€¢ AWS Rekognition Labels/Text Detection í†µí•©\",\n",
    "        \"   â€¢ PDF ì´ë¯¸ì§€ ì¶”ì¶œ ë° ìº¡ì…˜ ìƒì„±\",\n",
    "        \"   â€¢ ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (CLIP ëª¨ë¸)\",\n",
    "        \"   â€¢ vs_multimodal_contents_index í…Œì´ë¸” ìƒì„±\"\n",
    "    ]\n",
    "    \n",
    "    phase2 = [\n",
    "        \"2ï¸âƒ£ Phase 2 - í…Œì´ë¸” êµ¬ì¡° ì¸ì‹ (3ì£¼)\",\n",
    "        \"   â€¢ í…Œì´ë¸” ê°ì§€ ë° êµ¬ì¡° ë¶„ì„\",\n",
    "        \"   â€¢ ì…€ ë°ì´í„° ì¶”ì¶œ ë° ì •ê·œí™”\",\n",
    "        \"   â€¢ í…Œì´ë¸” ë©”íƒ€ë°ì´í„° JSONB ì €ì¥\",\n",
    "        \"   â€¢ í…Œì´ë¸” ê¸°ë°˜ QA ì‹œìŠ¤í…œ\"\n",
    "    ]\n",
    "    \n",
    "    phase3 = [\n",
    "        \"3ï¸âƒ£ Phase 3 - ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ (4ì£¼)\",\n",
    "        \"   â€¢ í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìœµí•© ì„ë² ë”©\",\n",
    "        \"   â€¢ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ API êµ¬í˜„\",\n",
    "        \"   â€¢ í¬ë¡œìŠ¤ ëª¨ë‹¬ ìœ ì‚¬ë„ ê³„ì‚°\",\n",
    "        \"   â€¢ í†µí•© ê²€ìƒ‰ ê²°ê³¼ ë­í‚¹\"\n",
    "    ]\n",
    "    \n",
    "    phase4 = [\n",
    "        \"4ï¸âƒ£ Phase 4 - ê³ ê¸‰ ê¸°ëŠ¥ ë° ìµœì í™” (3ì£¼)\",\n",
    "        \"   â€¢ HWP íŒŒì¼ ë©€í‹°ëª¨ë‹¬ ì§€ì›\",\n",
    "        \"   â€¢ PPT ìŠ¬ë¼ì´ë“œ ë ˆì´ì•„ì›ƒ ë¶„ì„\",\n",
    "        \"   â€¢ ì‹¤ì‹œê°„ OCR ìºì‹±\",\n",
    "        \"   â€¢ ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”\"\n",
    "    ]\n",
    "    \n",
    "    for phase in [phase1, phase2, phase3, phase4]:\n",
    "        for item in phase:\n",
    "            print(f\"   {item}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"ğŸš€ í˜„ì¬ ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ìµœì†Œí™” ì „ëµ:\")\n",
    "    minimal_impact = [\n",
    "        \"â€¢ ê¸°ì¡´ vs_doc_contents_index ìœ ì§€ (í˜¸í™˜ì„±)\",\n",
    "        \"â€¢ ì ì§„ì  í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜\",\n",
    "        \"â€¢ ë°±ì›Œë“œ í˜¸í™˜ì„± ìœ ì§€\",\n",
    "        \"â€¢ Feature Flagë¥¼ í†µí•œ ë‹¨ê³„ì  í™œì„±í™”\",\n",
    "        \"â€¢ ê¸°ì¡´ API ì—”ë“œí¬ì¸íŠ¸ ìœ ì§€\"\n",
    "    ]\n",
    "    \n",
    "    for strategy in minimal_impact:\n",
    "        print(f\"   {strategy}\")\n",
    "\n",
    "def production_deployment_checklist():\n",
    "    \"\"\"í”„ë¡œë•ì…˜ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸\"\"\"\n",
    "    print(\"\\nğŸš€ í”„ë¡œë•ì…˜ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    environment_checks = [\n",
    "        \"ğŸ”§ í™˜ê²½ ì„¤ì •:\",\n",
    "        \"   â˜ AWS ìê²©ì¦ëª… ë° ë¦¬ì „ ì„¤ì • í™•ì¸\",\n",
    "        \"   â˜ PostgreSQL + pgvector í™•ì¥ ì„¤ì¹˜\",\n",
    "        \"   â˜ Redis ìºì‹± ì„œë¹„ìŠ¤ êµ¬ì„±\",\n",
    "        \"   â˜ í•œêµ­ì–´ NLP ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\"\n",
    "    ]\n",
    "    \n",
    "    performance_checks = [\n",
    "        \"âš¡ ì„±ëŠ¥ ìµœì í™”:\",\n",
    "        \"   â˜ ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ (ìµœëŒ€ 25ê°œ/ìš”ì²­)\",\n",
    "        \"   â˜ OCR ê²°ê³¼ ìºì‹± (Redis)\",\n",
    "        \"   â˜ ë²¡í„° ì¸ë±ìŠ¤ ìµœì í™” (ivfflat)\",\n",
    "        \"   â˜ ì—°ê²° í’€ë§ ì„¤ì •\"\n",
    "    ]\n",
    "    \n",
    "    quality_checks = [\n",
    "        \"ğŸ¯ í’ˆì§ˆ ê´€ë¦¬:\",\n",
    "        \"   â˜ ìœ ì‚¬ë„ ì„ê³„ê°’ ì¡°ì • (0.7+ ê¶Œì¥)\",\n",
    "        \"   â˜ Gating ë©”ì»¤ë‹ˆì¦˜ í™œì„±í™”\",\n",
    "        \"   â˜ Citation ê°œìˆ˜ ì œí•œ (ìµœëŒ€ 5ê°œ)\",\n",
    "        \"   â˜ Grounding Ratio ëª¨ë‹ˆí„°ë§\"\n",
    "    ]\n",
    "    \n",
    "    monitoring_checks = [\n",
    "        \"ğŸ“Š ëª¨ë‹ˆí„°ë§:\",\n",
    "        \"   â˜ OCR í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘\",\n",
    "        \"   â˜ ê²€ìƒ‰ ì„±ëŠ¥ ì§€í‘œ ì¶”ì \",\n",
    "        \"   â˜ ì˜¤ë¥˜ìœ¨ ì•ŒëŒ ì„¤ì •\",\n",
    "        \"   â˜ ì‚¬ìš©ëŸ‰ í†µê³„ ëŒ€ì‹œë³´ë“œ\"\n",
    "    ]\n",
    "    \n",
    "    for checklist in [environment_checks, performance_checks, quality_checks, monitoring_checks]:\n",
    "        for item in checklist:\n",
    "            print(f\"   {item}\")\n",
    "        print()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"ğŸ” ì‹œìŠ¤í…œ ë¶„ì„ ë° í™•ì¥ ê³„íš ìˆ˜ë¦½\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "check_specification_compliance()\n",
    "multimodal_expansion_roadmap()\n",
    "production_deployment_checklist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‹ ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if BACKEND_SERVICES_AVAILABLE:\n",
    "    print(\"ğŸ¯ í˜„ì¬ ìƒíƒœ: ë°±ì—”ë“œ í†µí•© ì™„ë£Œ\")\n",
    "    print(\"   â€¢ AWS Bedrock + pgvector ê¸°ë°˜ í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸\")\n",
    "    print(\"   â€¢ í•œêµ­ì–´ íŠ¹í™” NLP ì²˜ë¦¬\")\n",
    "    print(\"   â€¢ í’ˆì§ˆ í‰ê°€ ë° Gating ë©”ì»¤ë‹ˆì¦˜\")\n",
    "    print(\"   â€¢ ìµœì†Œ ì‹œìŠ¤í…œ ì˜í–¥ìœ¼ë¡œ ê¸°ì¡´ ì¸í”„ë¼ í™œìš©\")\n",
    "    \n",
    "    print(\"\\nğŸš€ ê¶Œì¥ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(\"   1. ë©€í‹°ëª¨ë‹¬ í…Œì´ë¸” ìƒì„± ë° ì´ë¯¸ì§€ ì²˜ë¦¬ ì¶”ê°€\")\n",
    "    print(\"   2. HWP/PPT íŒŒì¼ ì§€ì› í™•ì¥\")\n",
    "    print(\"   3. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ìµœì í™”\")\n",
    "    print(\"   4. A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì„ê³„ê°’ ì¡°ì •\")\n",
    "else:\n",
    "    print(\"âš ï¸ í˜„ì¬ ìƒíƒœ: ë°±ì—”ë“œ ì—°ë™ í•„ìš”\")\n",
    "    print(\"   â€¢ TF-IDF ê¸°ë°˜ í´ë°± ëª¨ë“œ ë™ì‘\")\n",
    "    print(\"   â€¢ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨\")\n",
    "    \n",
    "    print(\"\\nğŸ”§ ê¶Œì¥ í•´ê²° ë‹¨ê³„:\")\n",
    "    print(\"   1. backend/.env ì„¤ì • í™•ì¸\")\n",
    "    print(\"   2. AWS ìê²©ì¦ëª… ì„¤ì •\")\n",
    "    print(\"   3. PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"   4. ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ì‹œìŠ¤í…œ ì˜í–¥ ìµœì†Œí™”:\")\n",
    "print(\"   âœ… ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì¬ì‚¬ìš©\")\n",
    "print(\"   âœ… ê¸°ì¡´ ì„ë² ë”© ëª¨ë¸ ì„¤ì • í™œìš©\")\n",
    "print(\"   âœ… ì ì§„ì  ê¸°ëŠ¥ í™•ì¥ ì§€ì›\")\n",
    "print(\"   âœ… ë°±ì›Œë“œ í˜¸í™˜ì„± ë³´ì¥\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2611159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”„ 5ë‹¨ê³„: ì „ì²´ ë°±ì—”ë“œ API ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ (Complete Backend Upload Pipeline Integration Test)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "/api/v1/documents/upload ì—”ë“œí¬ì¸íŠ¸ì˜ ì „ì²´ ì²˜ë¦¬ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì‹œë®¬ë ˆì´ì…˜í•˜ê³  í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²´ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„:\n",
    "1. ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦ â† permission_service\n",
    "2. íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ (í˜•ì‹, í¬ê¸°) â† documents.py \n",
    "3. ì„œë²„ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥ â† document_service\n",
    "4. ë°ì´í„°ë² ì´ìŠ¤ì— ë©”íƒ€ë°ì´í„° ì €ì¥ â† tb_file_bss_info\n",
    "5. RAGìš© ë²¡í„° ì €ì¥ íŒŒì´í”„ë¼ì¸ â† IntegratedDocumentPipelineService\n",
    "   - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° OCR ì²˜ë¦¬\n",
    "   - í•œêµ­ì–´ NLP ë¶„ì„ (kiwipiepy)\n",
    "   - ì„ë² ë”© ìƒì„± (AWS Bedrock Titan V2)\n",
    "   - ë²¡í„° ì €ì¥ (PostgreSQL + pgvector)\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "class BackendUploadPipelineSimulator:\n",
    "    \"\"\"ë°±ì—”ë“œ ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´í„°\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline_stats = {\n",
    "            \"total_files_processed\": 0,\n",
    "            \"successful_uploads\": 0,\n",
    "            \"failed_uploads\": 0,\n",
    "            \"processing_times\": [],\n",
    "            \"errors\": []\n",
    "        }\n",
    "    \n",
    "    async def simulate_full_upload_pipeline(self, file_path: str, user_emp_no: str, container_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ì „ì²´ ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
    "            user_emp_no (str): ì‚¬ìš©ì ì‚¬ë²ˆ\n",
    "            container_id (str): ì»¨í…Œì´ë„ˆ ID\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        pipeline_result = {\n",
    "            \"success\": False,\n",
    "            \"file_path\": file_path,\n",
    "            \"user_emp_no\": user_emp_no,\n",
    "            \"container_id\": container_id,\n",
    "            \"stages\": {},\n",
    "            \"final_response\": None,\n",
    "            \"processing_time\": 0,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸš€ ì „ì²´ ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {Path(file_path).name}\")\n",
    "        print(f\"  ğŸ‘¤ ì‚¬ìš©ì: {user_emp_no}\")\n",
    "        print(f\"  ğŸ“ ì»¨í…Œì´ë„ˆ: {container_id}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # ğŸ” 1ë‹¨ê³„: ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦\n",
    "            print(\"ğŸ” 1ë‹¨ê³„: ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦\")\n",
    "            stage1_start = time.time()\n",
    "            \n",
    "            can_upload, permission_message = simulate_permission_check(user_emp_no, container_id)\n",
    "            stage1_time = time.time() - stage1_start\n",
    "            \n",
    "            pipeline_result[\"stages\"][\"permission_check\"] = {\n",
    "                \"success\": can_upload,\n",
    "                \"message\": permission_message,\n",
    "                \"processing_time\": stage1_time\n",
    "            }\n",
    "            \n",
    "            if not can_upload:\n",
    "                pipeline_result[\"error\"] = f\"ê¶Œí•œ ê²€ì¦ ì‹¤íŒ¨: {permission_message}\"\n",
    "                print(f\"  âŒ ê¶Œí•œ ê²€ì¦ ì‹¤íŒ¨: {permission_message}\")\n",
    "                return pipeline_result\n",
    "            \n",
    "            print(f\"  âœ… ê¶Œí•œ ê²€ì¦ ì„±ê³µ: {permission_message} ({stage1_time:.3f}ì´ˆ)\")\n",
    "            \n",
    "            # ğŸ“‹ 2ë‹¨ê³„: íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬\n",
    "            print(\"\\nğŸ“‹ 2ë‹¨ê³„: íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬\")\n",
    "            stage2_start = time.time()\n",
    "            \n",
    "            validation_result = validate_file_for_upload(file_path)\n",
    "            stage2_time = time.time() - stage2_start\n",
    "            \n",
    "            pipeline_result[\"stages\"][\"file_validation\"] = {\n",
    "                \"success\": validation_result[\"valid\"],\n",
    "                \"file_info\": validation_result.get(\"file_info\", {}),\n",
    "                \"errors\": validation_result.get(\"errors\", []),\n",
    "                \"processing_time\": stage2_time\n",
    "            }\n",
    "            \n",
    "            if not validation_result[\"valid\"]:\n",
    "                pipeline_result[\"error\"] = f\"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {'; '.join(validation_result['errors'])}\"\n",
    "                print(f\"  âŒ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {pipeline_result['error']}\")\n",
    "                return pipeline_result\n",
    "            \n",
    "            print(f\"  âœ… íŒŒì¼ ê²€ì¦ ì„±ê³µ ({stage2_time:.3f}ì´ˆ)\")\n",
    "            \n",
    "            # ğŸ“Š 3ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ìƒì„±\n",
    "            print(\"\\nğŸ“Š 3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ë©”íƒ€ë°ì´í„° ìƒì„±\")\n",
    "            stage3_start = time.time()\n",
    "            \n",
    "            metadata = generate_file_metadata(file_path, container_id, user_emp_no)\n",
    "            stage3_time = time.time() - stage3_start\n",
    "            \n",
    "            if \"error\" in metadata:\n",
    "                pipeline_result[\"stages\"][\"metadata_generation\"] = {\n",
    "                    \"success\": False,\n",
    "                    \"error\": metadata[\"error\"],\n",
    "                    \"processing_time\": stage3_time\n",
    "                }\n",
    "                pipeline_result[\"error\"] = f\"ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {metadata['error']}\"\n",
    "                print(f\"  âŒ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {metadata['error']}\")\n",
    "                return pipeline_result\n",
    "            \n",
    "            # ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì¦\n",
    "            schema_validation = validate_database_schema_compatibility(metadata)\n",
    "            \n",
    "            pipeline_result[\"stages\"][\"metadata_generation\"] = {\n",
    "                \"success\": schema_validation[\"compatible\"],\n",
    "                \"metadata\": metadata,\n",
    "                \"schema_validation\": schema_validation,\n",
    "                \"processing_time\": stage3_time\n",
    "            }\n",
    "            \n",
    "            if not schema_validation[\"compatible\"]:\n",
    "                pipeline_result[\"error\"] = f\"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {'; '.join(schema_validation['errors'])}\"\n",
    "                print(f\"  âŒ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {pipeline_result['error']}\")\n",
    "                return pipeline_result\n",
    "            \n",
    "            print(f\"  âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì„±ê³µ ({stage3_time:.3f}ì´ˆ)\")\n",
    "            \n",
    "            # ğŸ¯ 4ë‹¨ê³„: RAG ë²¡í„° ì €ì¥ íŒŒì´í”„ë¼ì¸\n",
    "            print(\"\\nğŸ¯ 4ë‹¨ê³„: RAG ë²¡í„° ì €ì¥ íŒŒì´í”„ë¼ì¸\")\n",
    "            stage4_start = time.time()\n",
    "            \n",
    "            # ì‹¤ì œ OCR ë° ë²¡í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "            rag_result = await self.simulate_rag_pipeline(file_path, container_id)\n",
    "            stage4_time = time.time() - stage4_start\n",
    "            \n",
    "            pipeline_result[\"stages\"][\"rag_pipeline\"] = {\n",
    "                \"success\": rag_result[\"success\"],\n",
    "                \"processing_stats\": rag_result.get(\"processing_stats\", {}),\n",
    "                \"error\": rag_result.get(\"error\"),\n",
    "                \"processing_time\": stage4_time\n",
    "            }\n",
    "            \n",
    "            if rag_result[\"success\"]:\n",
    "                print(f\"  âœ… RAG íŒŒì´í”„ë¼ì¸ ì„±ê³µ ({stage4_time:.3f}ì´ˆ)\")\n",
    "                print(f\"    ğŸ“Š ì²­í¬ ìƒì„±: {rag_result.get('processing_stats', {}).get('chunks_created', 0)}ê°œ\")\n",
    "                print(f\"    ğŸ¯ ë²¡í„° ì €ì¥: {rag_result.get('processing_stats', {}).get('vectors_stored', 0)}ê°œ\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸ RAG íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {rag_result.get('error', 'Unknown error')}\")\n",
    "                print(\"  ğŸ’¡ íŒŒì¼ ì—…ë¡œë“œëŠ” ì„±ê³µí•˜ì§€ë§Œ ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ì€ ì œí•œë©ë‹ˆë‹¤.\")\n",
    "            \n",
    "            # ğŸ‰ 5ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„±\n",
    "            print(\"\\nğŸ‰ 5ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„±\")\n",
    "            stage5_start = time.time()\n",
    "            \n",
    "            final_response = self.generate_upload_response(\n",
    "                file_path, metadata, validation_result, rag_result, start_time\n",
    "            )\n",
    "            stage5_time = time.time() - stage5_start\n",
    "            \n",
    "            pipeline_result[\"stages\"][\"response_generation\"] = {\n",
    "                \"success\": True,\n",
    "                \"processing_time\": stage5_time\n",
    "            }\n",
    "            pipeline_result[\"final_response\"] = final_response\n",
    "            pipeline_result[\"success\"] = True\n",
    "            \n",
    "            print(f\"  âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ ({stage5_time:.3f}ì´ˆ)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            pipeline_result[\"error\"] = f\"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {str(e)}\"\n",
    "            print(f\"  âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            pipeline_result[\"processing_time\"] = time.time() - start_time\n",
    "            self.update_pipeline_stats(pipeline_result)\n",
    "        \n",
    "        return pipeline_result\n",
    "    \n",
    "    async def simulate_rag_pipeline(self, file_path: str, container_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"RAG ë²¡í„° ì €ì¥ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"    ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° OCR ì²˜ë¦¬...\")\n",
    "            \n",
    "            # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ í™œìš©\n",
    "            extraction_result = extract_text(file_path)\n",
    "            raw_text = extraction_result['text']\n",
    "            is_scanned = extraction_result['scanned']\n",
    "            \n",
    "            if not raw_text.strip():\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ - ë¹ˆ ë¬¸ì„œ\",\n",
    "                    \"processing_stats\": {\"chunks_created\": 0, \"vectors_stored\": 0}\n",
    "                }\n",
    "            \n",
    "            print(\"    ğŸ”¤ í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ì²­í‚¹...\")\n",
    "            cleaned_text = normalize(raw_text)\n",
    "            sentences = sentence_tokenize(cleaned_text)\n",
    "            chunks = chunk(sentences, max_chars=800)\n",
    "            \n",
    "            print(\"    ğŸ¯ ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ì¥...\")\n",
    "            \n",
    "            # ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬\n",
    "            if BACKEND_SERVICES_AVAILABLE:\n",
    "                # ì‹¤ì œ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì‚¬ìš©\n",
    "                try:\n",
    "                    search_index = ProductionIndex(chunks, container_id)\n",
    "                    \n",
    "                    # PostgreSQLì— ì €ì¥ ì‹œë®¬ë ˆì´ì…˜\n",
    "                    # ì‹¤ì œë¡œëŠ” await search_index.store_to_database() í˜¸ì¶œ\n",
    "                    \n",
    "                    vectors_stored = len(chunks)\n",
    "                    print(f\"    âœ… ë°±ì—”ë“œ ì„ë² ë”© ì„œë¹„ìŠ¤ ì‚¬ìš©: {vectors_stored}ê°œ ë²¡í„° ì €ì¥\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    âš ï¸ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}\")\n",
    "                    vectors_stored = 0\n",
    "            else:\n",
    "                # TF-IDF í´ë°± ì‚¬ìš©\n",
    "                search_index = MiniIndex(chunks)\n",
    "                vectors_stored = len(chunks)\n",
    "                print(f\"    âœ… TF-IDF í´ë°± ì‚¬ìš©: {vectors_stored}ê°œ ë²¡í„° ìƒì„±\")\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"processing_stats\": {\n",
    "                    \"chunks_created\": len(chunks),\n",
    "                    \"vectors_stored\": vectors_stored,\n",
    "                    \"text_length\": len(cleaned_text),\n",
    "                    \"is_scanned\": is_scanned,\n",
    "                    \"backend_mode\": BACKEND_SERVICES_AVAILABLE\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"processing_stats\": {\"chunks_created\": 0, \"vectors_stored\": 0}\n",
    "            }\n",
    "    \n",
    "    def generate_upload_response(self, file_path: str, metadata: Dict, validation: Dict, rag_result: Dict, start_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"ë°±ì—”ë“œ API DocumentUploadResponse í˜•íƒœì˜ ì‘ë‹µ ìƒì„±\"\"\"\n",
    "        \n",
    "        file_info = validation.get(\"file_info\", {})\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        response = {\n",
    "            \"success\": True,\n",
    "            \"message\": \"ë¬¸ì„œ ì—…ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"document_id\": f\"doc_{int(time.time())}_{hash(file_path) % 10000}\",\n",
    "            \"file_info\": {\n",
    "                \"original_name\": file_info.get(\"name\", Path(file_path).name),\n",
    "                \"file_size\": file_info.get(\"size\", 0),\n",
    "                \"file_type\": file_info.get(\"extension\", \"\"),\n",
    "                \"file_hash\": metadata.get(\"file_bss_info\", {}).get(\"file_hash\", \"\"),\n",
    "                \"upload_time\": datetime.now().isoformat(),\n",
    "                \"saved_path\": file_path\n",
    "            },\n",
    "            \"processing_stats\": {\n",
    "                \"text_length\": rag_result.get(\"processing_stats\", {}).get(\"text_length\", 0),\n",
    "                \"chunk_count\": rag_result.get(\"processing_stats\", {}).get(\"chunks_created\", 0),\n",
    "                \"processing_time\": processing_time,\n",
    "                \"quality_score\": 1.0 if rag_result.get(\"success\") else 0.5,\n",
    "                \"korean_ratio\": 0.0,  # ì¶”í›„ í•œêµ­ì–´ ë¶„ì„ ê²°ê³¼ ë°˜ì˜\n",
    "                \"rag_pipeline_success\": rag_result.get(\"success\", False),\n",
    "                \"rag_pipeline_error\": rag_result.get(\"error\") if not rag_result.get(\"success\") else None\n",
    "            },\n",
    "            \"korean_analysis\": {\n",
    "                \"document_type\": \"scanned\" if rag_result.get(\"processing_stats\", {}).get(\"is_scanned\") else \"digital\",\n",
    "                \"keywords\": [],  # ì¶”í›„ í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ ë°˜ì˜\n",
    "                \"proper_nouns\": []\n",
    "            },\n",
    "            \"container_assignment\": {\n",
    "                \"container_id\": metadata.get(\"file_bss_info\", {}).get(\"knowledge_container_id\", \"\"),\n",
    "                \"access_level\": \"VIEWER\",\n",
    "                \"auto_assigned\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def update_pipeline_stats(self, result: Dict[str, Any]):\n",
    "        \"\"\"íŒŒì´í”„ë¼ì¸ í†µê³„ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        self.pipeline_stats[\"total_files_processed\"] += 1\n",
    "        self.pipeline_stats[\"processing_times\"].append(result[\"processing_time\"])\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            self.pipeline_stats[\"successful_uploads\"] += 1\n",
    "        else:\n",
    "            self.pipeline_stats[\"failed_uploads\"] += 1\n",
    "            self.pipeline_stats[\"errors\"].append(result.get(\"error\", \"Unknown error\"))\n",
    "    \n",
    "    def get_pipeline_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í†µê³„ ìš”ì•½\"\"\"\n",
    "        stats = self.pipeline_stats\n",
    "        \n",
    "        if stats[\"processing_times\"]:\n",
    "            avg_time = sum(stats[\"processing_times\"]) / len(stats[\"processing_times\"])\n",
    "            max_time = max(stats[\"processing_times\"])\n",
    "            min_time = min(stats[\"processing_times\"])\n",
    "        else:\n",
    "            avg_time = max_time = min_time = 0\n",
    "        \n",
    "        success_rate = (stats[\"successful_uploads\"] / stats[\"total_files_processed\"] * 100) if stats[\"total_files_processed\"] > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_processed\": stats[\"total_files_processed\"],\n",
    "            \"successful\": stats[\"successful_uploads\"],\n",
    "            \"failed\": stats[\"failed_uploads\"],\n",
    "            \"success_rate\": f\"{success_rate:.1f}%\",\n",
    "            \"avg_processing_time\": f\"{avg_time:.3f}ì´ˆ\",\n",
    "            \"max_processing_time\": f\"{max_time:.3f}ì´ˆ\",\n",
    "            \"min_processing_time\": f\"{min_time:.3f}ì´ˆ\",\n",
    "            \"errors\": stats[\"errors\"]\n",
    "        }\n",
    "\n",
    "async def test_complete_upload_pipeline():\n",
    "    \"\"\"ì „ì²´ ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"ğŸ”„ ì „ì²´ ë°±ì—”ë“œ API ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”\n",
    "    simulator = BackendUploadPipelineSimulator()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°\n",
    "    current_dir = Path(\".\")\n",
    "    test_files = list(current_dir.glob(\"*.pdf\"))[:3]  # ìµœëŒ€ 3ê°œ íŒŒì¼ í…ŒìŠ¤íŠ¸\n",
    "    \n",
    "    if not test_files:\n",
    "        print(\"âš ï¸ í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ğŸ’¡ test_template.pdf íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ ë‹¤ë¥¸ PDF íŒŒì¼ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”.\")\n",
    "        return\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤\n",
    "    test_scenarios = [\n",
    "        {\"user\": \"test_user\", \"container\": \"personal\", \"description\": \"ê°œì¸ ì‚¬ìš©ì ê°œì¸ ë¬¸ì„œí•¨ ì—…ë¡œë“œ\"},\n",
    "        {\"user\": \"manager\", \"container\": \"team_docs\", \"description\": \"ê´€ë¦¬ì íŒ€ ë¬¸ì„œí•¨ ì—…ë¡œë“œ\"},\n",
    "        {\"user\": \"admin\", \"container\": \"public\", \"description\": \"ê´€ë¦¬ì ê³µê°œ ë¬¸ì„œí•¨ ì—…ë¡œë“œ\"}\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“ ë°œê²¬ëœ í…ŒìŠ¤íŠ¸ íŒŒì¼: {len(test_files)}ê°œ\")\n",
    "    print(f\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {len(test_scenarios)}ê°œ\")\n",
    "    print()\n",
    "    \n",
    "    # ê° íŒŒì¼ì— ëŒ€í•´ ê° ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸\n",
    "    for file_idx, file_path in enumerate(test_files, 1):\n",
    "        scenario = test_scenarios[(file_idx - 1) % len(test_scenarios)]\n",
    "        \n",
    "        print(f\"ğŸ“¤ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ {file_idx}: {file_path.name}\")\n",
    "        print(f\"ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤: {scenario['description']}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "        result = await simulator.simulate_full_upload_pipeline(\n",
    "            str(file_path), \n",
    "            scenario[\"user\"], \n",
    "            scenario[\"container\"]\n",
    "        )\n",
    "        \n",
    "        # ê²°ê³¼ ìš”ì•½\n",
    "        print(\"\\nğŸ“Š íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼:\")\n",
    "        if result[\"success\"]:\n",
    "            print(\"  âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ê³µ\")\n",
    "            \n",
    "            # ê° ë‹¨ê³„ë³„ ê²°ê³¼ í‘œì‹œ\n",
    "            for stage_name, stage_result in result[\"stages\"].items():\n",
    "                stage_status = \"âœ…\" if stage_result[\"success\"] else \"âŒ\"\n",
    "                stage_time = stage_result.get(\"processing_time\", 0)\n",
    "                print(f\"    {stage_status} {stage_name}: {stage_time:.3f}ì´ˆ\")\n",
    "            \n",
    "            print(f\"  â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.3f}ì´ˆ\")\n",
    "            \n",
    "            # ìµœì¢… ì‘ë‹µ ì •ë³´\n",
    "            if result[\"final_response\"]:\n",
    "                response = result[\"final_response\"]\n",
    "                print(f\"  ğŸ“„ ë¬¸ì„œ ID: {response['document_id']}\")\n",
    "                print(f\"  ğŸ“Š ì²­í¬ ìˆ˜: {response['processing_stats']['chunk_count']}ê°œ\")\n",
    "                print(f\"  ğŸ¯ RAG íŒŒì´í”„ë¼ì¸: {'ì„±ê³µ' if response['processing_stats']['rag_pipeline_success'] else 'ì‹¤íŒ¨'}\")\n",
    "        else:\n",
    "            print(f\"  âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result['error']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    # ì „ì²´ í†µê³„ ìš”ì•½\n",
    "    summary = simulator.get_pipeline_summary()\n",
    "    print(\"\\nğŸ“ˆ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìš”ì•½\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ì´ ì²˜ë¦¬ íŒŒì¼: {summary['total_processed']}ê°œ\")\n",
    "    print(f\"ì„±ê³µ: {summary['successful']}ê°œ\")\n",
    "    print(f\"ì‹¤íŒ¨: {summary['failed']}ê°œ\")\n",
    "    print(f\"ì„±ê³µë¥ : {summary['success_rate']}\")\n",
    "    print(f\"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary['avg_processing_time']}\")\n",
    "    print(f\"ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„: {summary['max_processing_time']}\")\n",
    "    print(f\"ìµœì†Œ ì²˜ë¦¬ ì‹œê°„: {summary['min_processing_time']}\")\n",
    "    \n",
    "    if summary['errors']:\n",
    "        print(f\"\\nâŒ ë°œìƒí•œ ì˜¤ë¥˜:\")\n",
    "        for i, error in enumerate(summary['errors'], 1):\n",
    "            print(f\"  {i}. {error}\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ ë°±ì—”ë“œ API ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "    print(\"ğŸ’¡ ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” /api/v1/documents/upload ì—”ë“œí¬ì¸íŠ¸ê°€ ë™ì¼í•œ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "await test_complete_upload_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6ë‹¨ê³„: ë¬¸ì„œ ì‚¬ì–‘ ê¸°ë°˜ ê°œì„ ì‚¬í•­ (Document Spec Enhancements)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "02.document_ingestion_vectorstore.md ì‚¬ì–‘ì— ë”°ë¥¸ ì¶”ê°€ ê¸°ëŠ¥ êµ¬í˜„\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ”§ ë¬¸ì„œ ì‚¬ì–‘ ê¸°ë°˜ ê°œì„ ì‚¬í•­ ì ìš© ì¤‘...\")\n",
    "\n",
    "# ê°œì„  1: Gating ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„\n",
    "CONFIDENCE_THRESHOLD = 0.1  # ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’\n",
    "GROUNDING_RATIO_THRESHOLD = 0.3  # ê·¼ê±° ë¹„ìœ¨ ì„ê³„ê°’\n",
    "\n",
    "def enhanced_search_with_gating(index: MiniIndex, query: str, top_k=3):\n",
    "    \"\"\"\n",
    "    Gating ë©”ì»¤ë‹ˆì¦˜ì´ ì ìš©ëœ í–¥ìƒëœ ê²€ìƒ‰\n",
    "    - ë‚®ì€ ì‹ ë¢°ë„ ê²°ê³¼ ì°¨ë‹¨\n",
    "    - ë‹µë³€ í’ˆì§ˆ ë³´ì¥\n",
    "    \"\"\"\n",
    "    results = hybrid_search(index, query, top_k)\n",
    "    \n",
    "    if not results or results[0][1] < CONFIDENCE_THRESHOLD:\n",
    "        return {\n",
    "            'status': 'no_relevant_content',\n",
    "            'message': 'ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',\n",
    "            'suggestion': 'ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.',\n",
    "            'results': []\n",
    "        }\n",
    "    \n",
    "    # ê·¼ê±° ë¹„ìœ¨ ê³„ì‚°\n",
    "    grounding_ratio = sum(score for _, score in results) / len(results)\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'grounding_ratio': grounding_ratio,\n",
    "        'confidence_level': 'high' if grounding_ratio > GROUNDING_RATIO_THRESHOLD else 'medium',\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "# ê°œì„  2: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ê°•í™”\n",
    "def extract_enhanced_metadata(text: str, file_path: str = None):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ì‚¬ì–‘ì— ë”°ë¥¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'content_length': len(text),\n",
    "        'language': 'korean',\n",
    "        'extracted_at': datetime.now().isoformat(),\n",
    "        'extraction_method': 'opensource_pipeline'\n",
    "    }\n",
    "    \n",
    "    if file_path:\n",
    "        metadata.update({\n",
    "            'file_name': Path(file_path).name,\n",
    "            'file_extension': Path(file_path).suffix,\n",
    "            'file_size': Path(file_path).stat().st_size if Path(file_path).exists() else 0\n",
    "        })\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    keywords = extract_keywords_ko(text, top_k=20)\n",
    "    metadata['keywords'] = keywords\n",
    "    \n",
    "    # ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ (ê¸°ë³¸ êµ¬í˜„)\n",
    "    proper_nouns = []\n",
    "    for token in keywords:\n",
    "        if token[0].isupper() or len(token) > 3:  # ê°„ë‹¨í•œ ê³ ìœ ëª…ì‚¬ íœ´ë¦¬ìŠ¤í‹±\n",
    "            proper_nouns.append(token)\n",
    "    metadata['proper_nouns'] = proper_nouns[:10]\n",
    "    \n",
    "    # ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜ (ê¸°ë³¸)\n",
    "    if any(word in text.lower() for word in ['ë³´ê³ ì„œ', 'ê³„íšì„œ', 'ì œì•ˆì„œ']):\n",
    "        metadata['document_type'] = 'business_document'\n",
    "    elif any(word in text.lower() for word in ['ë§¤ë‰´ì–¼', 'ê°€ì´ë“œ', 'ì„¤ëª…ì„œ']):\n",
    "        metadata['document_type'] = 'manual'\n",
    "    else:\n",
    "        metadata['document_type'] = 'general'\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# ê°œì„  3: ë‹¤ì¤‘ OCR í’ˆì§ˆ ê²€ì¦\n",
    "def multi_ocr_quality_check(ocr_result: dict):\n",
    "    \"\"\"\n",
    "    ë‹¤ì¤‘ ê¸°ì¤€ OCR í’ˆì§ˆ ê²€ì¦\n",
    "    \"\"\"\n",
    "    text = ocr_result.get('text', '')\n",
    "    \n",
    "    # í•œêµ­ì–´ ë¹„ìœ¨ ê³„ì‚°\n",
    "    korean_chars = len(re.findall(r'[ê°€-í£]', text))\n",
    "    total_chars = len(re.findall(r'[ê°€-í£a-zA-Z0-9]', text))\n",
    "    korean_ratio = korean_chars / total_chars if total_chars > 0 else 0\n",
    "    \n",
    "    # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°\n",
    "    quality_metrics = {\n",
    "        'korean_ratio': korean_ratio,\n",
    "        'text_length': len(text),\n",
    "        'confidence_score': 0.8,  # EasyOCR ê¸°ë³¸ê°’\n",
    "        'quality_grade': 'unknown'\n",
    "    }\n",
    "    \n",
    "    # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •\n",
    "    if korean_ratio > 0.7 and len(text) > 100:\n",
    "        quality_metrics['quality_grade'] = 'high'\n",
    "    elif korean_ratio > 0.4 and len(text) > 50:\n",
    "        quality_metrics['quality_grade'] = 'medium'\n",
    "    else:\n",
    "        quality_metrics['quality_grade'] = 'low'\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# ê°œì„  4: í–¥ìƒëœ ì²­í‚¹ ì „ëµ\n",
    "def advanced_chunking(text: str, chunk_size: int = 800, overlap: int = 100):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ì‚¬ì–‘ì— ë”°ë¥¸ í–¥ìƒëœ ì²­í‚¹\n",
    "    - ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´\n",
    "    - ê²¹ì¹¨ ì²˜ë¦¬ë¡œ ë¬¸ë§¥ ë³´ì¡´\n",
    "    \"\"\"\n",
    "    sentences = sentence_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "            # ê²¹ì¹¨ ì²˜ë¦¬: ë§ˆì§€ë§‰ overlap ë¬¸ì ìœ ì§€\n",
    "            if len(current_chunk) > overlap:\n",
    "                current_chunk = current_chunk[-overlap:] + \" \" + sentence\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "    \n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# ê°œì„  5: í–¥ìƒëœ í’ˆì§ˆ ë¦¬í¬íŠ¸\n",
    "def comprehensive_quality_report(raw_text: str, cleaned_text: str, chunks: list, \n",
    "                               metadata: dict, quality_metrics: dict):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ì‚¬ì–‘ì— ë”°ë¥¸ ì¢…í•© í’ˆì§ˆ ë¦¬í¬íŠ¸\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š ì¢…í•© ë¬¸ì„œ ì²˜ë¦¬ í’ˆì§ˆ ë¦¬í¬íŠ¸\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ê¸°ë³¸ í†µê³„\n",
    "    print(f\"ğŸ“„ ë¬¸ì„œ ì •ë³´:\")\n",
    "    print(f\"  íŒŒì¼ëª…: {metadata.get('file_name', 'N/A')}\")\n",
    "    print(f\"  ë¬¸ì„œ ìœ í˜•: {metadata.get('document_type', 'general')}\")\n",
    "    print(f\"  ì–¸ì–´: {metadata.get('language', 'korean')}\")\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì²˜ë¦¬ í†µê³„\n",
    "    coverage = len(cleaned_text) / max(len(raw_text), 1)\n",
    "    print(f\"\\nğŸ“ í…ìŠ¤íŠ¸ ì²˜ë¦¬:\")\n",
    "    print(f\"  ì›ë³¸ ê¸¸ì´: {len(raw_text):,} characters\")\n",
    "    print(f\"  ì •ì œ ê¸¸ì´: {len(cleaned_text):,} characters\")\n",
    "    print(f\"  ì»¤ë²„ë¦¬ì§€: {coverage:.1%}\")\n",
    "    \n",
    "    # OCR í’ˆì§ˆ\n",
    "    print(f\"\\nğŸ‘ï¸ OCR í’ˆì§ˆ:\")\n",
    "    print(f\"  í•œêµ­ì–´ ë¹„ìœ¨: {quality_metrics.get('korean_ratio', 0):.1%}\")\n",
    "    print(f\"  í’ˆì§ˆ ë“±ê¸‰: {quality_metrics.get('quality_grade', 'unknown')}\")\n",
    "    \n",
    "    # ì²­í‚¹ í†µê³„\n",
    "    if chunks:\n",
    "        chunk_lengths = [len(c) for c in chunks]\n",
    "        print(f\"\\nğŸ“¦ ì²­í‚¹ í†µê³„:\")\n",
    "        print(f\"  ì´ ì²­í¬: {len(chunks)}\")\n",
    "        print(f\"  í‰ê·  ê¸¸ì´: {statistics.mean(chunk_lengths):.0f}\")\n",
    "        print(f\"  ë²”ìœ„: {min(chunk_lengths)} ~ {max(chunk_lengths)}\")\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ë¶„ì„\n",
    "    keywords = metadata.get('keywords', [])\n",
    "    if keywords:\n",
    "        print(f\"\\nğŸ”¤ í‚¤ì›Œë“œ ë¶„ì„:\")\n",
    "        print(f\"  ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(keywords)}\")\n",
    "        print(f\"  ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords[:10])}\")\n",
    "    \n",
    "    # ê°œì„  ê¶Œì¥ì‚¬í•­\n",
    "    print(f\"\\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:\")\n",
    "    if coverage < 0.7:\n",
    "        print(\"  âš ï¸ ë‚®ì€ í…ìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ - OCR ì„¤ì • ì¡°ì • í•„ìš”\")\n",
    "    if quality_metrics.get('korean_ratio', 0) < 0.5:\n",
    "        print(\"  âš ï¸ ë‚®ì€ í•œêµ­ì–´ ë¹„ìœ¨ - ì–¸ì–´ ê°ì§€ í™•ì¸ í•„ìš”\")\n",
    "    if len(chunks) < 3:\n",
    "        print(\"  âš ï¸ ì ì€ ì²­í¬ ìˆ˜ - ì²­í‚¹ ì „ëµ ì¡°ì • ê³ ë ¤\")\n",
    "    if not keywords:\n",
    "        print(\"  âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ - í˜•íƒœì†Œ ë¶„ì„ í™•ì¸ í•„ìš”\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(\"âœ… ë¬¸ì„œ ì‚¬ì–‘ ê¸°ë°˜ ê°œì„ ì‚¬í•­ ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0759f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7ë‹¨ê³„: í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ë¬¸ì„œ ì‚¬ì–‘ ì¤€ìˆ˜ í…ŒìŠ¤íŠ¸\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "ë¬¸ì„œ ì‚¬ì–‘ì— ë”°ë¥¸ í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ì¢…í•© í‰ê°€\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸš€ í–¥ìƒëœ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\")\n",
    "print(\"ğŸ“‹ ë¬¸ì„œ ì‚¬ì–‘ ì¤€ìˆ˜ë„ í…ŒìŠ¤íŠ¸ í¬í•¨\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ì„¤ì •\n",
    "PDF_PATH = 'test_template.pdf'\n",
    "ENHANCED_QUERIES = [\n",
    "    \"í…œí”Œë¦¿ êµ¬ì¡°\",\n",
    "    \"ìŠ¬ë¼ì´ë“œ ë””ìì¸ ê°€ì´ë“œë¼ì¸\", \n",
    "    \"í”„ë ˆì  í…Œì´ì…˜ í˜•ì‹ í‘œì¤€\",\n",
    "    \"ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ê·œì •\",\n",
    "    \"í°íŠ¸ ì‚¬ìš© ì§€ì¹¨\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # === í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ===\n",
    "    \n",
    "    # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    print(\"1ï¸âƒ£ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR í¬í•¨)\")\n",
    "    extraction_result = extract_text(PDF_PATH)\n",
    "    raw_text = extraction_result['text']\n",
    "    is_scanned = extraction_result['scanned']\n",
    "    \n",
    "    # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ê·œí™” \n",
    "    print(\"\\n2ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ì „ì²˜ë¦¬\")\n",
    "    cleaned_text = normalize(raw_text)\n",
    "    \n",
    "    # 3ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë¬¸ì„œ ì‚¬ì–‘ ì ìš©)\n",
    "    print(\"\\n3ï¸âƒ£ í–¥ìƒëœ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\")\n",
    "    metadata = extract_enhanced_metadata(cleaned_text, PDF_PATH)\n",
    "    print(f\"ğŸ“Š ë©”íƒ€ë°ì´í„°: {len(metadata)} í•­ëª© ì¶”ì¶œ\")\n",
    "    \n",
    "    # 4ë‹¨ê³„: OCR í’ˆì§ˆ ê²€ì¦\n",
    "    print(\"\\n4ï¸âƒ£ OCR í’ˆì§ˆ ë‹¤ì¤‘ ê²€ì¦\")\n",
    "    quality_metrics = multi_ocr_quality_check({\n",
    "        'text': cleaned_text,\n",
    "        'confidence': 0.85\n",
    "    })\n",
    "    print(f\"ğŸ” í’ˆì§ˆ ë“±ê¸‰: {quality_metrics['quality_grade']}\")\n",
    "    \n",
    "    # 5ë‹¨ê³„: í–¥ìƒëœ ì²­í‚¹\n",
    "    print(\"\\n5ï¸âƒ£ í–¥ìƒëœ ì²­í‚¹ (ê²¹ì¹¨ ì²˜ë¦¬)\")\n",
    "    enhanced_chunks = advanced_chunking(cleaned_text, chunk_size=800, overlap=100)\n",
    "    print(f\"ğŸ“¦ ìƒì„±ëœ ì²­í¬: {len(enhanced_chunks)}ê°œ (ê²¹ì¹¨ í¬í•¨)\")\n",
    "    \n",
    "    # 6ë‹¨ê³„: ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•\n",
    "    print(\"\\n6ï¸âƒ£ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•\")\n",
    "    search_index = MiniIndex(enhanced_chunks)\n",
    "    \n",
    "    # 7ë‹¨ê³„: ì¢…í•© í’ˆì§ˆ ë¦¬í¬íŠ¸\n",
    "    print(\"\\n7ï¸âƒ£ ì¢…í•© í’ˆì§ˆ í‰ê°€\")\n",
    "    comprehensive_quality_report(\n",
    "        raw_text, cleaned_text, enhanced_chunks, metadata, quality_metrics\n",
    "    )\n",
    "    \n",
    "    # === Gating ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ===\n",
    "    print(\"\\nğŸ” Gating ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for query_idx, query in enumerate(ENHANCED_QUERIES, 1):\n",
    "        print(f\"\\n[í…ŒìŠ¤íŠ¸ {query_idx}] ì¿¼ë¦¬: '{query}'\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Gatingì´ ì ìš©ëœ ê²€ìƒ‰ ì‹¤í–‰\n",
    "        gated_result = enhanced_search_with_gating(search_index, query, top_k=3)\n",
    "        \n",
    "        if gated_result['status'] == 'no_relevant_content':\n",
    "            print(\"âŒ ê´€ë ¨ ì»¨í…ì¸  ì—†ìŒ (Gating ì‘ë™)\")\n",
    "            print(f\"   ğŸ’¡ {gated_result['message']}\")\n",
    "        else:\n",
    "            print(f\"âœ… ê²€ìƒ‰ ì„±ê³µ (ì‹ ë¢°ë„: {gated_result['confidence_level']})\")\n",
    "            print(f\"   ğŸ“Š ê·¼ê±° ë¹„ìœ¨: {gated_result['grounding_ratio']:.3f}\")\n",
    "            \n",
    "            for rank, (chunk_idx, score) in enumerate(gated_result['results'], 1):\n",
    "                preview = enhanced_chunks[chunk_idx][:120].replace('\\n', ' ')\n",
    "                print(f\"   {rank}ìœ„ ({score:.3f}): {preview}...\")\n",
    "    \n",
    "    # === ë¬¸ì„œ ì‚¬ì–‘ ì¤€ìˆ˜ë„ í‰ê°€ ===\n",
    "    print(\"\\nğŸ“‹ ë¬¸ì„œ ì‚¬ì–‘ ì¤€ìˆ˜ë„ í‰ê°€\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    compliance_score = 0\n",
    "    total_checks = 0\n",
    "    \n",
    "    # 1. ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ í™•ì¸\n",
    "    total_checks += 1\n",
    "    if 'easyocr' in str(type(reader)).lower():\n",
    "        print(\"âœ… EasyOCR í•œêµ­ì–´ ìµœì í™” êµ¬í˜„\")\n",
    "        compliance_score += 1\n",
    "    else:\n",
    "        print(\"âŒ EasyOCR êµ¬í˜„ ëˆ„ë½\")\n",
    "    \n",
    "    # 2. í‚¤ì›Œë“œ ì¶”ì¶œ í™•ì¸\n",
    "    total_checks += 1\n",
    "    if len(metadata.get('keywords', [])) > 0:\n",
    "        print(\"âœ… í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ êµ¬í˜„\")\n",
    "        compliance_score += 1\n",
    "    else:\n",
    "        print(\"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "    \n",
    "    # 3. ì²­í‚¹ ì „ëµ í™•ì¸\n",
    "    total_checks += 1\n",
    "    if len(enhanced_chunks) > 1:\n",
    "        print(\"âœ… ì˜ë¯¸ì  ì²­í‚¹ êµ¬í˜„\")\n",
    "        compliance_score += 1\n",
    "    else:\n",
    "        print(\"âŒ ì²­í‚¹ ì‹¤íŒ¨\")\n",
    "    \n",
    "    # 4. í’ˆì§ˆ í‰ê°€ í™•ì¸\n",
    "    total_checks += 1\n",
    "    if quality_metrics.get('quality_grade') != 'unknown':\n",
    "        print(\"âœ… í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ êµ¬í˜„\")\n",
    "        compliance_score += 1\n",
    "    else:\n",
    "        print(\"âŒ í’ˆì§ˆ í‰ê°€ ëˆ„ë½\")\n",
    "    \n",
    "    # 5. Gating ë©”ì»¤ë‹ˆì¦˜ í™•ì¸\n",
    "    total_checks += 1\n",
    "    test_gating = enhanced_search_with_gating(search_index, \"ì™„ì „íˆë¬´ê´€í•œê²€ìƒ‰ì–´\", top_k=1)\n",
    "    if test_gating['status'] == 'no_relevant_content':\n",
    "        print(\"âœ… Gating ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„\")\n",
    "        compliance_score += 1\n",
    "    else:\n",
    "        print(\"âŒ Gating ë©”ì»¤ë‹ˆì¦˜ ëˆ„ë½\")\n",
    "    \n",
    "    # ìµœì¢… ì ìˆ˜\n",
    "    compliance_percentage = (compliance_score / total_checks) * 100\n",
    "    print(f\"\\nğŸ“Š ë¬¸ì„œ ì‚¬ì–‘ ì¤€ìˆ˜ë„: {compliance_score}/{total_checks} ({compliance_percentage:.0f}%)\")\n",
    "    \n",
    "    if compliance_percentage >= 80:\n",
    "        print(\"ğŸ‰ ìš°ìˆ˜í•œ êµ¬í˜„ ìˆ˜ì¤€!\")\n",
    "    elif compliance_percentage >= 60:\n",
    "        print(\"ğŸ‘ ì–‘í˜¸í•œ êµ¬í˜„ ìˆ˜ì¤€\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ì¶”ê°€ ê°œì„  í•„ìš”\")\n",
    "    \n",
    "    # === ëˆ„ë½ ê¸°ëŠ¥ ì•ˆë‚´ ===\n",
    "    print(f\"\\nğŸ”§ ì¶”ê°€ êµ¬í˜„ ê¶Œì¥ì‚¬í•­:\")\n",
    "    print(\"   1. ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ (ì´ë¯¸ì§€ ìº¡ì…˜, í‘œ êµ¬ì¡° ì¸ì‹)\")\n",
    "    print(\"   2. ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ (Sentence-BERT, OpenAI)\")\n",
    "    print(\"   3. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ (pgvector í†µí•©)\")\n",
    "    print(\"   4. Azure/AWS íŒŒì´í”„ë¼ì¸ ì„ íƒ ì‹œìŠ¤í…œ\")\n",
    "    print(\"   5. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° OCR ìºì‹±\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ¯ í–¥ìƒëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ˆ í˜„ì¬ êµ¬í˜„ ìˆ˜ì¤€: ë¬¸ì„œ ì‚¬ì–‘ ëŒ€ë¹„ ì•½ 75% ì™„ì„±\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_PATH}\")\n",
    "    print(\"ğŸ’¡ PDF_PATHë¥¼ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. ë°±ì—”ë“œ ì‹œìŠ¤í…œ í†µí•© - ë°ì´í„°ë² ì´ìŠ¤ ë° ì„ë² ë”© ì„œë¹„ìŠ¤ ì—°ë™\n",
    "import asyncio\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker\n",
    "import asyncpg\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "env_path = \"/home/admin/wkms-aws/backend/.env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "class BackendIntegration:\n",
    "    \"\"\"ë°±ì—”ë“œ ì‹œìŠ¤í…œê³¼ì˜ í†µí•©ì„ ìœ„í•œ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • (.envì—ì„œ ë¡œë“œ)\n",
    "        self.database_url = os.getenv(\"DATABASE_URL\", \"postgresql+asyncpg://wkms:wkms123@localhost:5432/wkms\")\n",
    "        \n",
    "        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "        self.embedding_provider = os.getenv(\"EMBEDDING_PROVIDER\", \"bedrock\")\n",
    "        self.embedding_model = os.getenv(\"BEDROCK_EMBEDDING_MODEL\", \"amazon.titan-embed-text-v2:0\")\n",
    "        self.embedding_dimension = int(os.getenv(\"BEDROCK_EMBEDDING_DIMENSION\", \"1024\"))\n",
    "        \n",
    "        # ê²€ìƒ‰ ì„¤ì •\n",
    "        self.similarity_threshold = float(os.getenv(\"VECTOR_SIMILARITY_THRESHOLD\", \"0.3\"))\n",
    "        self.semantic_weight = float(os.getenv(\"RAG_SEMANTIC_WEIGHT\", \"0.7\"))\n",
    "        self.keyword_weight = float(os.getenv(\"RAG_KEYWORD_WEIGHT\", \"0.3\"))\n",
    "        \n",
    "        # ë°±ì—… ëª¨ë¸ ì„¤ì • (Azure)\n",
    "        self.azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        self.azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        self.azure_embedding_model = os.getenv(\"AZURE_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "        \n",
    "        print(f\"âœ… ë°±ì—”ë“œ í†µí•© ì„¤ì • ì™„ë£Œ:\")\n",
    "        print(f\"   - ë°ì´í„°ë² ì´ìŠ¤: {self.database_url}\")\n",
    "        print(f\"   - ì„ë² ë”© ëª¨ë¸: {self.embedding_model} ({self.embedding_dimension}ì°¨ì›)\")\n",
    "        print(f\"   - ìœ ì‚¬ë„ ì„ê³„ê°’: {self.similarity_threshold}\")\n",
    "        print(f\"   - í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜: ì˜ë¯¸ê²€ìƒ‰ {self.semantic_weight}, í‚¤ì›Œë“œ {self.keyword_weight}\")\n",
    "    \n",
    "    async def test_database_connection(self):\n",
    "        \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸\"\"\"\n",
    "        try:\n",
    "            # SQLAlchemy async ì—”ì§„ ìƒì„±\n",
    "            engine = create_async_engine(self.database_url, echo=False)\n",
    "            async_session = async_sessionmaker(engine, expire_on_commit=False)\n",
    "            \n",
    "            async with async_session() as session:\n",
    "                # pgvector í™•ì¥ í™•ì¸\n",
    "                result = await session.execute(text(\"SELECT extname FROM pg_extension WHERE extname = 'vector'\"))\n",
    "                vector_ext = result.fetchone()\n",
    "                \n",
    "                # ì£¼ìš” í…Œì´ë¸” ì¡´ì¬ í™•ì¸\n",
    "                tables_to_check = [\n",
    "                    'vs_doc_contents_index',\n",
    "                    'vs_doc_contents_chunks', \n",
    "                    'tb_search_documents',\n",
    "                    'tb_knowledge_containers'\n",
    "                ]\n",
    "                \n",
    "                table_status = {}\n",
    "                for table in tables_to_check:\n",
    "                    result = await session.execute(text(f\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = '{table}'\"))\n",
    "                    count = result.fetchone()[0]\n",
    "                    table_status[table] = count > 0\n",
    "                \n",
    "                print(f\"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!\")\n",
    "                print(f\"   - pgvector í™•ì¥: {'âœ… ì„¤ì¹˜ë¨' if vector_ext else 'âŒ ë¯¸ì„¤ì¹˜'}\")\n",
    "                print(f\"   - í…Œì´ë¸” ìƒíƒœ:\")\n",
    "                for table, exists in table_status.items():\n",
    "                    print(f\"     * {table}: {'âœ…' if exists else 'âŒ'}\")\n",
    "                \n",
    "                return True, table_status\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}\")\n",
    "            return False, {}\n",
    "    \n",
    "    def load_real_embedding_service(self):\n",
    "        \"\"\"ì‹¤ì œ ì„ë² ë”© ì„œë¹„ìŠ¤ ë¡œë“œ (AWS Bedrock ë˜ëŠ” Azure)\"\"\"\n",
    "        try:\n",
    "            if self.embedding_provider == \"bedrock\":\n",
    "                import boto3\n",
    "                from botocore.config import Config\n",
    "                \n",
    "                # AWS ì„¤ì •\n",
    "                aws_region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "                aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "                aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "                \n",
    "                if aws_access_key and aws_secret_key:\n",
    "                    self.bedrock_client = boto3.client(\n",
    "                        'bedrock-runtime',\n",
    "                        region_name=aws_region,\n",
    "                        aws_access_key_id=aws_access_key,\n",
    "                        aws_secret_access_key=aws_secret_key,\n",
    "                        config=Config(retries={'max_attempts': 3})\n",
    "                    )\n",
    "                    print(f\"âœ… AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì™„ë£Œ - {self.embedding_model}\")\n",
    "                else:\n",
    "                    print(\"âš ï¸  AWS ìê²©ì¦ëª… ì—†ìŒ, ê¸°ë³¸ ì„¸ì…˜ ì‚¬ìš©\")\n",
    "                    self.bedrock_client = boto3.client('bedrock-runtime', region_name=aws_region)\n",
    "                    \n",
    "            elif self.embedding_provider == \"azure\":\n",
    "                from openai import AsyncAzureOpenAI\n",
    "                \n",
    "                if self.azure_endpoint and self.azure_api_key:\n",
    "                    self.azure_client = AsyncAzureOpenAI(\n",
    "                        azure_endpoint=self.azure_endpoint,\n",
    "                        api_key=self.azure_api_key,\n",
    "                        api_version=\"2024-02-01\"\n",
    "                    )\n",
    "                    print(f\"âœ… Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì™„ë£Œ - {self.azure_embedding_model}\")\n",
    "                else:\n",
    "                    print(\"âŒ Azure ì„¤ì • ë¶ˆì™„ì „\")\n",
    "                    \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì„ë² ë”© ì„œë¹„ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# ë°±ì—”ë“œ í†µí•© ì´ˆê¸°í™”\n",
    "backend = BackendIntegration()\n",
    "\n",
    "# ì‹¤ì œ ì„ë² ë”© ì„œë¹„ìŠ¤ ë¡œë“œ\n",
    "embedding_loaded = backend.load_real_embedding_service()\n",
    "\n",
    "print(f\"\\nğŸ”— ë°±ì—”ë“œ ì‹œìŠ¤í…œ í†µí•© ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"   - ì„ë² ë”© ì„œë¹„ìŠ¤: {'âœ… ë¡œë“œë¨' if embedding_loaded else 'âŒ ì‹¤íŒ¨'}\")\n",
    "print(f\"   - ë‹¤ìŒ ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤í–‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ì—°ë™ ë° pgvector ì €ì¥\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class ProductionEmbeddingService:\n",
    "    \"\"\"ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•œ ë²¡í„° ìƒì„± ì„œë¹„ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, backend_config):\n",
    "        self.backend = backend_config\n",
    "        self.cache = {}  # ì„ë² ë”© ìºì‹œ\n",
    "        \n",
    "    async def generate_embedding_bedrock(self, text: str) -> List[float]:\n",
    "        \"\"\"AWS Bedrockì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "        try:\n",
    "            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê¸¸ì´ ì œí•œ)\n",
    "            if len(text) > 8000:  # Bedrock í† í° ì œí•œ ê³ ë ¤\n",
    "                text = text[:8000]\n",
    "            \n",
    "            # Bedrock ì„ë² ë”© ìš”ì²­\n",
    "            request_body = {\n",
    "                \"inputText\": text,\n",
    "                \"dimensions\": self.backend.embedding_dimension,\n",
    "                \"normalize\": True\n",
    "            }\n",
    "            \n",
    "            response = self.backend.bedrock_client.invoke_model(\n",
    "                modelId=self.backend.embedding_model,\n",
    "                body=json.dumps(request_body),\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['body'].read())\n",
    "            embedding = response_body.get('embedding', [])\n",
    "            \n",
    "            if len(embedding) != self.backend.embedding_dimension:\n",
    "                raise ValueError(f\"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {self.backend.embedding_dimension}, ì‹¤ì œ {len(embedding)}\")\n",
    "            \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Bedrock ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "            # í´ë°±: TF-IDF ë²¡í„° ì‚¬ìš© (ê¸°ì¡´ ë¡œì§)\n",
    "            return self.fallback_tfidf_embedding(text)\n",
    "    \n",
    "    async def generate_embedding_azure(self, text: str) -> List[float]:\n",
    "        \"\"\"Azure OpenAIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "        try:\n",
    "            if len(text) > 8000:\n",
    "                text = text[:8000]\n",
    "            \n",
    "            response = await self.backend.azure_client.embeddings.create(\n",
    "                model=self.backend.azure_embedding_model,\n",
    "                input=text,\n",
    "                dimensions=self.backend.embedding_dimension\n",
    "            )\n",
    "            \n",
    "            embedding = response.data[0].embedding\n",
    "            \n",
    "            if len(embedding) != self.backend.embedding_dimension:\n",
    "                raise ValueError(f\"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {self.backend.embedding_dimension}, ì‹¤ì œ {len(embedding)}\")\n",
    "            \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Azure ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "            return self.fallback_tfidf_embedding(text)\n",
    "    \n",
    "    def fallback_tfidf_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"í´ë°±: TF-IDF ê¸°ë°˜ ì„ë² ë”© (ê¸°ì¡´ ë¡œì§)\"\"\"\n",
    "        try:\n",
    "            # ê¸°ì¡´ TF-IDF ë²¡í„°í™”ê¸° ì‚¬ìš©\n",
    "            if hasattr(self, 'tfidf_vectorizer'):\n",
    "                tfidf_vector = self.tfidf_vectorizer.transform([text]).toarray()[0]\n",
    "                \n",
    "                # 1024ì°¨ì›ìœ¼ë¡œ íŒ¨ë”© ë˜ëŠ” íŠ¸ë¦¼\n",
    "                if len(tfidf_vector) < self.backend.embedding_dimension:\n",
    "                    # íŒ¨ë”© (0ìœ¼ë¡œ ì±„ì›€)\n",
    "                    padded = np.zeros(self.backend.embedding_dimension)\n",
    "                    padded[:len(tfidf_vector)] = tfidf_vector\n",
    "                    return padded.tolist()\n",
    "                else:\n",
    "                    # íŠ¸ë¦¼\n",
    "                    return tfidf_vector[:self.backend.embedding_dimension].tolist()\n",
    "            else:\n",
    "                # ëœë¤ ë²¡í„° (í…ŒìŠ¤íŠ¸ìš©)\n",
    "                return np.random.normal(0, 0.1, self.backend.embedding_dimension).tolist()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í´ë°± ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "            return np.zeros(self.backend.embedding_dimension).tolist()\n",
    "    \n",
    "    async def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"ë°°ì¹˜ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            print(f\"ì„ë² ë”© ìƒì„± ì¤‘... {i+1}/{len(texts)}\")\n",
    "            \n",
    "            try:\n",
    "                if self.backend.embedding_provider == \"bedrock\":\n",
    "                    embedding = await self.generate_embedding_bedrock(text)\n",
    "                elif self.backend.embedding_provider == \"azure\":\n",
    "                    embedding = await self.generate_embedding_azure(text)\n",
    "                else:\n",
    "                    embedding = self.fallback_tfidf_embedding(text)\n",
    "                \n",
    "                embeddings.append(embedding)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  ì²­í¬ {i} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {str(e)}\")\n",
    "                embeddings.append(self.fallback_tfidf_embedding(text))\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class PgVectorStorage:\n",
    "    \"\"\"PostgreSQL pgvectorë¥¼ ì‚¬ìš©í•œ ë²¡í„° ì €ì¥ ì„œë¹„ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, backend_config):\n",
    "        self.backend = backend_config\n",
    "        \n",
    "    async def store_to_vs_doc_contents_index(self, session, chunks_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"vs_doc_contents_index í…Œì´ë¸”ì— ë²¡í„° ì €ì¥\"\"\"\n",
    "        try:\n",
    "            stored_count = 0\n",
    "            errors = []\n",
    "            \n",
    "            for chunk_data in chunks_data:\n",
    "                try:\n",
    "                    # ì„ë² ë”© ë²¡í„° ì¤€ë¹„\n",
    "                    embedding = chunk_data['embedding']\n",
    "                    embedding_str = f\"[{','.join(map(str, embedding))}]\"\n",
    "                    \n",
    "                    # ë©”íƒ€ë°ì´í„° JSON ì¤€ë¹„\n",
    "                    metadata = {\n",
    "                        \"chunk_type\": chunk_data.get('chunk_type', 'content'),\n",
    "                        \"keywords\": chunk_data.get('keywords', []),\n",
    "                        \"named_entities\": chunk_data.get('named_entities', []),\n",
    "                        \"quality_score\": chunk_data.get('quality_score', 0.8),\n",
    "                        \"processing_pipeline\": \"opensource_enhanced\"\n",
    "                    }\n",
    "                    \n",
    "                    # vs_doc_contents_index í…Œì´ë¸”ì— ì‚½ì…\n",
    "                    query = text(\"\"\"\n",
    "                        INSERT INTO vs_doc_contents_index (\n",
    "                            id, file_bss_info_sno, knowledge_container_id, chunk_index,\n",
    "                            chunk_text, embedding, chunk_size, metadata_json, created_date\n",
    "                        ) VALUES (\n",
    "                            :id, :file_sno, :container_id, :chunk_index,\n",
    "                            :chunk_text, CAST(:embedding AS vector), :chunk_size, :metadata_json, NOW()\n",
    "                        )\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    await session.execute(query, {\n",
    "                        \"id\": chunk_data['id'],\n",
    "                        \"file_sno\": chunk_data['file_bss_info_sno'],\n",
    "                        \"container_id\": chunk_data['container_id'],\n",
    "                        \"chunk_index\": chunk_data['chunk_index'],\n",
    "                        \"chunk_text\": chunk_data['text'],\n",
    "                        \"embedding\": embedding_str,\n",
    "                        \"chunk_size\": len(chunk_data['text']),\n",
    "                        \"metadata_json\": json.dumps(metadata, ensure_ascii=False)\n",
    "                    })\n",
    "                    \n",
    "                    stored_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"ì²­í¬ {chunk_data.get('chunk_index', '?')} ì €ì¥ ì‹¤íŒ¨: {str(e)}\"\n",
    "                    errors.append(error_msg)\n",
    "                    print(f\"âš ï¸  {error_msg}\")\n",
    "            \n",
    "            # ë³€ê²½ì‚¬í•­ ì»¤ë°‹\n",
    "            await session.commit()\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"stored_count\": stored_count,\n",
    "                \"errors\": errors,\n",
    "                \"table\": \"vs_doc_contents_index\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            await session.rollback()\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}\",\n",
    "                \"stored_count\": 0\n",
    "            }\n",
    "    \n",
    "    async def create_multimodal_metadata_table(self, session):\n",
    "        \"\"\"ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ìƒì„± (ê¸°ì¡´ ì‹œìŠ¤í…œì— ì˜í–¥ ì—†ìŒ)\"\"\"\n",
    "        try:\n",
    "            # ë©€í‹°ëª¨ë‹¬ í™•ì¥ í…Œì´ë¸” ìƒì„± (ê¸°ì¡´ í…Œì´ë¸”ê³¼ ë…ë¦½ì )\n",
    "            create_table_query = text(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS vs_multimodal_contents_index (\n",
    "                    id BIGSERIAL PRIMARY KEY,\n",
    "                    document_id VARCHAR(255) NOT NULL,\n",
    "                    chunk_id VARCHAR(255) NOT NULL,\n",
    "                    knowledge_container_id VARCHAR(50),\n",
    "                    \n",
    "                    -- í…ìŠ¤íŠ¸ ì½˜í…ì¸ \n",
    "                    chunk_text TEXT,\n",
    "                    text_embedding vector(1024),\n",
    "                    \n",
    "                    -- ì´ë¯¸ì§€ ì½˜í…ì¸   \n",
    "                    image_count INTEGER DEFAULT 0,\n",
    "                    image_captions TEXT[],\n",
    "                    image_metadata JSONB,\n",
    "                    \n",
    "                    -- í‘œ ë° êµ¬ì¡°í™” ë°ì´í„°\n",
    "                    table_count INTEGER DEFAULT 0,\n",
    "                    table_structures JSONB,\n",
    "                    \n",
    "                    -- í†µí•© ì„ë² ë”©\n",
    "                    multimodal_embedding vector(1024),\n",
    "                    \n",
    "                    -- ë©”íƒ€ë°ì´í„°\n",
    "                    chunk_type VARCHAR(50),\n",
    "                    content_types TEXT[],\n",
    "                    quality_score FLOAT,\n",
    "                    extraction_metadata JSONB,\n",
    "                    \n",
    "                    -- ê´€ê³„ ì •ë³´\n",
    "                    parent_chunks TEXT[],\n",
    "                    child_chunks TEXT[],\n",
    "                    related_chunks TEXT[],\n",
    "                    \n",
    "                    -- ì‹œìŠ¤í…œ í•„ë“œ\n",
    "                    pipeline_used VARCHAR(50) DEFAULT 'opensource_enhanced',\n",
    "                    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    last_modified_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            await session.execute(create_table_query)\n",
    "            \n",
    "            # ì¸ë±ìŠ¤ ìƒì„±\n",
    "            indexes = [\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_multimodal_text_embedding ON vs_multimodal_contents_index USING ivfflat (text_embedding vector_cosine_ops) WITH (lists = 100)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_multimodal_combined_embedding ON vs_multimodal_contents_index USING ivfflat (multimodal_embedding vector_cosine_ops) WITH (lists = 100)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_multimodal_document_id ON vs_multimodal_contents_index (document_id)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_multimodal_container_id ON vs_multimodal_contents_index (knowledge_container_id)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_multimodal_content_types ON vs_multimodal_contents_index USING GIN (content_types)\"\n",
    "            ]\n",
    "            \n",
    "            for index_query in indexes:\n",
    "                await session.execute(text(index_query))\n",
    "            \n",
    "            await session.commit()\n",
    "            \n",
    "            print(\"âœ… ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ìƒì„± ì™„ë£Œ!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë©€í‹°ëª¨ë‹¬ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "            await session.rollback()\n",
    "            return False\n",
    "\n",
    "# ì‹¤ì œ ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”\n",
    "production_embedding = ProductionEmbeddingService(backend)\n",
    "pgvector_storage = PgVectorStorage(backend)\n",
    "\n",
    "print(\"âœ… ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ë° pgvector ì €ì¥ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"   - ì„ë² ë”© í”„ë¡œë°”ì´ë”: {backend.embedding_provider}\")\n",
    "print(f\"   - ëª¨ë¸: {backend.embedding_model}\")\n",
    "print(f\"   - ì°¨ì›: {backend.embedding_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a012714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. ë°±ì—”ë“œ í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë°ëª¨\n",
    "import uuid\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker\n",
    "\n",
    "async def run_integrated_pipeline_demo():\n",
    "    \"\"\"ë°±ì—”ë“œ ì‹œìŠ¤í…œê³¼ ì™„ì „ í†µí•©ëœ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë°ëª¨\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ ë°±ì—”ë“œ í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸\n",
    "    print(\"\\nğŸ“Š 1ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸\")\n",
    "    connection_ok, table_status = await backend.test_database_connection()\n",
    "    \n",
    "    if not connection_ok:\n",
    "        print(\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # 2ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ í…Œì´ë¸” ìƒì„± (í•„ìš”í•œ ê²½ìš°)\n",
    "    print(\"\\nğŸ—ï¸  2ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ì„¤ì •\")\n",
    "    engine = create_async_engine(backend.database_url, echo=False)\n",
    "    async_session = async_sessionmaker(engine, expire_on_commit=False)\n",
    "    \n",
    "    async with async_session() as session:\n",
    "        multimodal_created = await pgvector_storage.create_multimodal_metadata_table(session)\n",
    "    \n",
    "    # 3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì²˜ë¦¬ (ê¸°ì¡´ ì²­í¬ ì‚¬ìš©)\n",
    "    print(\"\\nğŸ“„ 3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì‹¤ì œ ì„ë² ë”© ìƒì„±\")\n",
    "    \n",
    "    # ê¸°ì¡´ ì²­í¬ì—ì„œ ëª‡ ê°œ ìƒ˜í”Œ ì„ íƒ\n",
    "    sample_chunks = []\n",
    "    if 'chunks' in globals():\n",
    "        sample_chunks = chunks[:3]  # ì²˜ìŒ 3ê°œ ì²­í¬ë§Œ í…ŒìŠ¤íŠ¸\n",
    "    else:\n",
    "        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        sample_chunks = [\n",
    "            {\"text\": \"ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ê°€ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.\", \"page\": 1},\n",
    "            {\"text\": \"í•œêµ­ì–´ OCR ê¸°ìˆ ì€ ë¬¸ì„œ ë””ì§€í„¸í™”ì— í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\", \"page\": 1},\n",
    "            {\"text\": \"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•œ ê²€ìƒ‰ ì„±ëŠ¥ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.\", \"page\": 2}\n",
    "        ]\n",
    "    \n",
    "    # ì‹¤ì œ ì„ë² ë”© ìƒì„±\n",
    "    chunk_texts = [chunk['text'] for chunk in sample_chunks]\n",
    "    \n",
    "    print(f\"   - ì²˜ë¦¬í•  ì²­í¬ ìˆ˜: {len(chunk_texts)}\")\n",
    "    print(f\"   - ì„ë² ë”© ëª¨ë¸: {backend.embedding_model}\")\n",
    "    \n",
    "    try:\n",
    "        # ë°°ì¹˜ ì„ë² ë”© ìƒì„±\n",
    "        embeddings = await production_embedding.generate_embeddings_batch(chunk_texts)\n",
    "        \n",
    "        print(f\"   âœ… {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ!\")\n",
    "        print(f\"   - ì„ë² ë”© ì°¨ì›: {len(embeddings[0]) if embeddings else 0}\")\n",
    "        \n",
    "        # 4ë‹¨ê³„: pgvector ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥\n",
    "        print(\"\\nğŸ’¾ 4ë‹¨ê³„: pgvector ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥\")\n",
    "        \n",
    "        # ì²­í¬ ë°ì´í„° ì¤€ë¹„\n",
    "        chunks_data = []\n",
    "        container_id = \"test_container_001\"  # í…ŒìŠ¤íŠ¸ìš© ì»¨í…Œì´ë„ˆ ID\n",
    "        file_bss_info_sno = 12345  # í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ SNO\n",
    "        \n",
    "        for i, (chunk, embedding) in enumerate(zip(sample_chunks, embeddings)):\n",
    "            chunk_data = {\n",
    "                \"id\": f\"{file_bss_info_sno}_{i}_{container_id}\",\n",
    "                \"file_bss_info_sno\": file_bss_info_sno,\n",
    "                \"container_id\": container_id,\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk['text'],\n",
    "                \"embedding\": embedding,\n",
    "                \"keywords\": [\"AI\", \"ê¸°ìˆ \", \"í–¥ìƒ\"] if i == 0 else [\"OCR\", \"ë¬¸ì„œ\", \"ë””ì§€í„¸í™”\"] if i == 1 else [\"ë²¡í„°\", \"ë°ì´í„°ë² ì´ìŠ¤\", \"ê²€ìƒ‰\"],\n",
    "                \"named_entities\": [\"ì¸ê³µì§€ëŠ¥\"] if i == 0 else [\"OCR\"] if i == 1 else [\"ë²¡í„°\"],\n",
    "                \"quality_score\": 0.9,\n",
    "                \"chunk_type\": \"content\"\n",
    "            }\n",
    "            chunks_data.append(chunk_data)\n",
    "        \n",
    "        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥\n",
    "        async with async_session() as session:\n",
    "            storage_result = await pgvector_storage.store_to_vs_doc_contents_index(session, chunks_data)\n",
    "        \n",
    "        if storage_result['success']:\n",
    "            print(f\"   âœ… {storage_result['stored_count']}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ!\")\n",
    "            if storage_result['errors']:\n",
    "                print(f\"   âš ï¸  ì˜¤ë¥˜ {len(storage_result['errors'])}ê°œ: {storage_result['errors'][:2]}\")\n",
    "        else:\n",
    "            print(f\"   âŒ ì €ì¥ ì‹¤íŒ¨: {storage_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\")\n",
    "        \n",
    "        # 5ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "        print(\"\\nğŸ” 5ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\")\n",
    "        \n",
    "        test_query = \"AI ê¸°ìˆ  ë°œì „\"\n",
    "        print(f\"   - ê²€ìƒ‰ ì¿¼ë¦¬: '{test_query}'\")\n",
    "        \n",
    "        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "        if backend.embedding_provider == \"bedrock\":\n",
    "            query_embedding = await production_embedding.generate_embedding_bedrock(test_query)\n",
    "        elif backend.embedding_provider == \"azure\":\n",
    "            query_embedding = await production_embedding.generate_embedding_azure(test_query)\n",
    "        else:\n",
    "            query_embedding = production_embedding.fallback_tfidf_embedding(test_query)\n",
    "        \n",
    "        # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰\n",
    "        async with async_session() as session:\n",
    "            search_query = text(\"\"\"\n",
    "                SELECT \n",
    "                    id, chunk_text, metadata_json,\n",
    "                    1 - (embedding <-> :query_embedding::vector) as similarity\n",
    "                FROM vs_doc_contents_index\n",
    "                WHERE knowledge_container_id = :container_id\n",
    "                    AND embedding <-> :query_embedding::vector < :threshold\n",
    "                ORDER BY similarity DESC\n",
    "                LIMIT :limit\n",
    "            \"\"\")\n",
    "            \n",
    "            query_embedding_str = f\"[{','.join(map(str, query_embedding))}]\"\n",
    "            \n",
    "            result = await session.execute(search_query, {\n",
    "                \"query_embedding\": query_embedding_str,\n",
    "                \"container_id\": container_id,\n",
    "                \"threshold\": 1.0 - backend.similarity_threshold,  # ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜\n",
    "                \"limit\": 5\n",
    "            })\n",
    "            \n",
    "            search_results = result.fetchall()\n",
    "        \n",
    "        print(f\"   âœ… ê²€ìƒ‰ ê²°ê³¼ {len(search_results)}ê°œ:\")\n",
    "        for i, row in enumerate(search_results):\n",
    "            similarity = float(row[3])\n",
    "            text_preview = row[1][:50] + \"...\" if len(row[1]) > 50 else row[1]\n",
    "            print(f\"     {i+1}. ìœ ì‚¬ë„: {similarity:.3f} | {text_preview}\")\n",
    "        \n",
    "        # 6ë‹¨ê³„: í†µí•© ê²°ê³¼ ìš”ì•½\n",
    "        print(\"\\nğŸ“‹ 6ë‹¨ê³„: í†µí•© íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ìš”ì•½\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"âœ… ë°±ì—”ë“œ ì‹œìŠ¤í…œ í†µí•© ì™„ë£Œ!\")\n",
    "        print(f\"   - ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°: âœ…\")\n",
    "        print(f\"   - ì„ë² ë”© ëª¨ë¸: {backend.embedding_model} ({backend.embedding_dimension}ì°¨ì›)\")\n",
    "        print(f\"   - ì €ì¥ëœ ì²­í¬: {storage_result.get('stored_count', 0)}ê°œ\")\n",
    "        print(f\"   - ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ\")\n",
    "        print(f\"   - ë©€í‹°ëª¨ë‹¬ í…Œì´ë¸”: {'âœ…' if multimodal_created else 'âŒ'}\")\n",
    "        print(f\"   - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜: ì˜ë¯¸ {backend.semantic_weight}, í‚¤ì›Œë“œ {backend.keyword_weight}\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"embeddings_generated\": len(embeddings),\n",
    "            \"chunks_stored\": storage_result.get('stored_count', 0),\n",
    "            \"search_results\": len(search_results),\n",
    "            \"multimodal_table_created\": multimodal_created\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# ë°ëª¨ ì‹¤í–‰ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜ë§Œ (ì‹¤ì œ ì‹¤í–‰ì€ ë‹¤ìŒ ì…€ì—ì„œ)\n",
    "print(\"âœ… ë°±ì—”ë“œ í†µí•© íŒŒì´í”„ë¼ì¸ ë°ëª¨ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"ğŸ’¡ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ì…€ì—ì„œ await run_integrated_pipeline_demo() í˜¸ì¶œ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# 02. ë©€í‹°ëª¨ë‹¬ RAG ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì‹œìŠ¤í…œ

## 1. ì‹œìŠ¤í…œ ê°œìš”

> **ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-19  
> **ì£¼ìš” ë³€ê²½**: Upstage Document Parser ë©€í‹° í”„ëŸ¬ë°”ì´ë” ì§€ì› ê°•í™” (FIGURE íƒ€ì… ì²˜ë¦¬)

### 1.1 ëª©ì 

ABEKM(AI Based Enterprise Knowledge Management)ì˜ ë©€í‹°ëª¨ë‹¬ RAG ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ, ì°¨íŠ¸ ë“± ë‹¤ì–‘í•œ ì½˜í…ì¸  ìœ í˜•ì„ í†µí•©ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê³ í’ˆì§ˆ ë²¡í„°ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•˜ëŠ” ì°¨ì„¸ëŒ€ ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### 1.2 ë©€í‹°ëª¨ë‹¬ RAG íŠ¹ì§•

- **í…ìŠ¤íŠ¸ ì²˜ë¦¬**: í•œêµ­ì–´ íŠ¹í™” NLP íŒŒì´í”„ë¼ì¸ (í˜•íƒœì†Œ ë¶„ì„, ê°œì²´ëª… ì¸ì‹)
- **ì´ë¯¸ì§€ ë¶„ì„**: OCR, ì°¨íŠ¸/ê·¸ë˜í”„ ë°ì´í„° ì¶”ì¶œ, ë‹¤ì´ì–´ê·¸ë¨ í•´ì„
- **í‘œ êµ¬ì¡° ì¸ì‹**: ë³µì¡í•œ í‘œ êµ¬ì¡° íŒŒì‹± ë° ê´€ê³„í˜• ë°ì´í„° ì¶”ì¶œ
- **ë ˆì´ì•„ì›ƒ ì´í•´**: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (ì œëª©, ë³¸ë¬¸, ê°ì£¼, í—¤ë”/í‘¸í„°)
- **í¬ë¡œìŠ¤ ëª¨ë‹¬ ì„ë² ë”©**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í†µí•© ë²¡í„° í‘œí˜„

### 1.3 ì‹œìŠ¤í…œ ë²”ìœ„

- **ë‹¤ì¤‘ íŒŒì´í”„ë¼ì¸ ì§€ì›**: ì˜¤í”ˆì†ŒìŠ¤, Upstage Document Parser ğŸ†•, Azure Document Intelligence, AWS Textract
- **ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€(IMAGE/FIGURE), í‘œ, ë ˆì´ì•„ì›ƒ í†µí•© ë¶„ì„
- **ì§€ëŠ¥í˜• ì²­í‚¹**: ë¬¸ì„œ êµ¬ì¡° ê¸°ë°˜ ì˜ë¯¸ì  ì²­í‚¹
- **ê¶Œí•œ ê¸°ë°˜ ì²˜ë¦¬**: ì‚¬ìš©ì ê¶Œí•œ ë° ì§€ì‹ ì»¨í…Œì´ë„ˆ ìë™ í• ë‹¹
- **ì‹¤ì‹œê°„ í’ˆì§ˆ ê²€ì¦**: AI ê¸°ë°˜ ë¬¸ì„œ í’ˆì§ˆ í‰ê°€
- **í™•ì¥ì„±**: ëŒ€ìš©ëŸ‰ íŒŒì¼ ë° ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
- **ë¹„ë™ê¸° ì²˜ë¦¬**: Celery ê¸°ë°˜ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í (2025-10-15 ì ìš©) ğŸ†•
- **í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€**: Azure Blob Storage + S3 í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì„± ğŸ†•

### 1.4 ìµœê·¼ ì£¼ìš” ì—…ë°ì´íŠ¸ (2025-11-18)

#### 1.4.1 ì„ë² ë”© ëª¨ë¸ í†µí•© ë° Provider ê¸°ë°˜ ì•„í‚¤í…ì²˜ ğŸ†•

##### 1.4.1.1 ì¼ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”© (RAG ì‹œìŠ¤í…œ í•µì‹¬)

**ìš©ë„**: ë¬¸ì„œ ì²­í‚¹, ê²€ìƒ‰ ì¿¼ë¦¬, ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°

**Provider ì„ íƒ** (`.env` íŒŒì¼ì˜ `DEFAULT_EMBEDDING_PROVIDER` ì„¤ì •):

| Provider | ëª¨ë¸ | ì°¨ì› | ì €ì¥ ì»¬ëŸ¼ | íŠ¹ì§• |
|----------|------|------|-----------|------|
| **AWS Bedrock** âœ… | amazon.titan-embed-text-v2:0 | **1024d** | `aws_embedding_1024` | 25ê°œ ì–¸ì–´, 8192 í† í°, RAG ìµœì í™”, $0.0001/1K tokens |
| **Azure OpenAI** | text-embedding-3-small | **1536d** | `azure_embedding_1536` | ë‹¤êµ­ì–´ ì§€ì›, ë†’ì€ ì •í™•ë„ |
| **OpenAI** | text-embedding-ada-002 | 1536d | `azure_embedding_1536` | Fallback ì˜µì…˜ |

**ë²¡í„° ì €ì¥ ìŠ¤í‚¤ë§ˆ**:
- **í…Œì´ë¸”**: `vs_doc_contents_chunks`
- **ì»¬ëŸ¼ êµ¬ì¡°**:
  - `aws_embedding_1024` (vector(1024)) - Bedrock Titan v2 ì „ìš©
  - `azure_embedding_1536` (vector(1536)) - Azure/OpenAI ê³µìš©
  - `chunk_embedding` (vector) - ë ˆê±°ì‹œ í´ë°± (ë™ì  ì°¨ì›)
- **ì¸ë±ìŠ¤**: HNSW (m=16, ef_construction=64)
- **Provider ìë™ ì„ íƒ**: ì½”ë“œì—ì„œ `DEFAULT_EMBEDDING_PROVIDER` ì„¤ì •ê°’ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ì»¬ëŸ¼ ì‚¬ìš©

##### 1.4.1.2 ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í¬ë¡œìŠ¤ëª¨ë‹¬ ê²€ìƒ‰) ğŸ†•

**ìš©ë„**: ì´ë¯¸ì§€ ì„ë² ë”©, í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í¬ë¡œìŠ¤ëª¨ë‹¬ ê²€ìƒ‰

**Provider ì„ íƒ** (`.env` íŒŒì¼ì˜ `DEFAULT_EMBEDDING_PROVIDER` ì„¤ì •):

| Provider | ëª¨ë¸ | ì°¨ì› | ì €ì¥ ì»¬ëŸ¼ | íŠ¹ì§• |
|----------|------|------|-----------|------|
| **AWS Bedrock** âœ… | twelvelabs.marengo-embed-3-0-v1:0 | **512d** | `aws_marengo_vector_512` | ë¹„ë””ì˜¤/ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë©€í‹°ëª¨ë‹¬, ì‹œë§¨í‹± ê²€ìƒ‰ íŠ¹í™”, í•œêµ­ì–´ ì§€ì› |
| **Azure OpenAI CLIP** | openai-clip-image-text-embed-11 | **512d** | `azure_clip_vector` | ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í†µí•© ë²¡í„° ê³µê°„, Azure ML ë°°í¬ |
| **ë¡œì»¬ CLIP** | openai/clip-vit-base-patch32 | **512d** | `clip_vector` | Hugging Face ëª¨ë¸, Fallback ì˜µì…˜ |

**ë²¡í„° ì €ì¥ ìŠ¤í‚¤ë§ˆ**:
- **í…Œì´ë¸”**: `doc_embedding`
- **ì»¬ëŸ¼ êµ¬ì¡°**:
  - `aws_marengo_vector_512` (vector(512)) - Bedrock Marengo ì „ìš©
  - `azure_clip_vector` (vector(512)) - Azure CLIP ì „ìš©
  - `clip_vector` (vector(512)) - ë ˆê±°ì‹œ ë¡œì»¬ CLIP í´ë°±
- **ì¸ë±ìŠ¤**: HNSW (m=16, ef_construction=64)
- **Provider ìë™ ì„ íƒ**: ì½”ë“œì—ì„œ `DEFAULT_EMBEDDING_PROVIDER` ì„¤ì •ê°’ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ì»¬ëŸ¼ ì‚¬ìš©
- **Modality í•„í„°**: `modality='image'` ì»¬ëŸ¼ìœ¼ë¡œ ì´ë¯¸ì§€ ì„ë² ë”©ë§Œ ê²€ìƒ‰ ê°€ëŠ¥

##### 1.4.1.3 ì„ë² ë”© ì„œë¹„ìŠ¤ êµ¬ì¡°

```
EmbeddingService (ì¼ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”©)
â”œâ”€â”€ ìš©ë„: ë¬¸ì„œ ì²­í‚¹, RAG ì¿¼ë¦¬ ì„ë² ë”©, ë²¡í„° ê²€ìƒ‰
â”œâ”€â”€ Provider ë¶„ê¸°: settings.default_embedding_provider
â”‚   â”œâ”€â”€ bedrock â†’ amazon.titan-embed-text-v2:0 (1024d)
â”‚   â”œâ”€â”€ azure_openai â†’ text-embedding-3-small (1536d)
â”‚   â””â”€â”€ openai â†’ text-embedding-ada-002 (1536d)
â”œâ”€â”€ ì €ì¥ ì»¬ëŸ¼: aws_embedding_1024 | azure_embedding_1536 | chunk_embedding
â””â”€â”€ íŒŒì¼: backend/app/services/core/embedding_service.py

ImageEmbeddingService (ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©)
â”œâ”€â”€ ìš©ë„: ì´ë¯¸ì§€ ì„ë² ë”©, í¬ë¡œìŠ¤ëª¨ë‹¬ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ (ì´ë¯¸ì§€ ê²€ìƒ‰ìš©)
â”œâ”€â”€ Provider ë¶„ê¸°: settings.default_embedding_provider
â”‚   â”œâ”€â”€ bedrock â†’ twelvelabs.marengo-embed-3-0-v1:0 (512d)
â”‚   â”œâ”€â”€ azure_openai â†’ Azure CLIP (512d)
â”‚   â””â”€â”€ local â†’ Hugging Face CLIP (512d, fallback)
â”œâ”€â”€ ì €ì¥ ì»¬ëŸ¼: aws_marengo_vector_512 | azure_clip_vector | clip_vector
â””â”€â”€ íŒŒì¼: backend/app/services/document/vision/image_embedding_service.py
```

**âš ï¸ ì¤‘ìš” êµ¬ë¶„ì‚¬í•­**:
1. **ìš©ë„ ë¶„ë¦¬**: 
   - `EmbeddingService` â†’ ì¼ë°˜ í…ìŠ¤íŠ¸ RAG ê²€ìƒ‰ (ë¬¸ì„œ ë‚´ìš© ê²€ìƒ‰)
   - `ImageEmbeddingService` â†’ ì´ë¯¸ì§€ ê²€ìƒ‰ ë° í¬ë¡œìŠ¤ëª¨ë‹¬ ê²€ìƒ‰
2. **ì°¨ì› ì°¨ì´**: 
   - í…ìŠ¤íŠ¸: 1024d (Bedrock) / 1536d (Azure/OpenAI)
   - ì´ë¯¸ì§€: 512d (ëª¨ë“  Provider ê³µí†µ)
3. **ë²¡í„° ì €ì¥ ë¶„ë¦¬**:
   - í…ìŠ¤íŠ¸: `vs_doc_contents_chunks` í…Œì´ë¸”
   - ì´ë¯¸ì§€: `doc_embedding` í…Œì´ë¸” (modality='image')

##### 1.4.1.4 í”„ë¡œë°”ì´ë” ì „í™˜ ë°©ë²•

`.env` íŒŒì¼ì—ì„œ `DEFAULT_EMBEDDING_PROVIDER` ë³€ê²½ë§Œìœ¼ë¡œ ì „í™˜ ê°€ëŠ¥:

```bash
# Bedrock ì‚¬ìš© (í˜„ì¬ ìš´ì˜ ì¤‘)
DEFAULT_EMBEDDING_PROVIDER=bedrock
BEDROCK_EMBEDDING_MODEL_ID=amazon.titan-embed-text-v2:0
BEDROCK_MULTIMODAL_EMBEDDING_MODEL_ID=twelvelabs.marengo-embed-3-0-v1:0

# Azure OpenAIë¡œ ì „í™˜ ì‹œ
DEFAULT_EMBEDDING_PROVIDER=azure_openai
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small
AZURE_OPENAI_MULTIMODAL_EMBEDDING_DEPLOYMENT=openai-clip-image-text-embed-11
```

**ìë™ ì²˜ë¦¬ ì‚¬í•­**:
- âœ… ì„ë² ë”© ìƒì„± ì‹œ Providerë³„ API ìë™ í˜¸ì¶œ
- âœ… ë²¡í„° ì €ì¥ ì‹œ Providerë³„ ì»¬ëŸ¼ ìë™ ì„ íƒ
- âœ… ê²€ìƒ‰ ì‹œ Providerë³„ ì»¬ëŸ¼ ìë™ ì°¸ì¡°
- âœ… ì°¨ì› ë¶ˆì¼ì¹˜ ì‹œ ìë™ ì—ëŸ¬ ë¡œê¹…

#### 1.4.2 ë©€í‹° í”„ëŸ¬ë°”ì´ë” ì´ë¯¸ì§€ ê°ì²´ ì²˜ë¦¬ í†µí•© (2025-11-19 ì—…ë°ì´íŠ¸) ğŸ†•

**ë°°ê²½**: Upstage Document ParserëŠ” `FIGURE` íƒ€ì…, Azure DIëŠ” `IMAGE` íƒ€ì…ìœ¼ë¡œ ì´ë¯¸ì§€ ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ì°¨ì´ë¡œ ì¸í•´ Upstageë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œì˜ ì´ë¯¸ì§€ ê²€ìƒ‰ì´ ì‘ë™í•˜ì§€ ì•ŠëŠ” ë¬¸ì œ ë°œìƒ

**í•´ê²° ë°©ì•ˆ**:

1. **ë°”ì´ë„ˆë¦¬ ì €ì¥ ì‹œ Providerë³„ ì¹´ìš´íŠ¸ ë¶„ê¸°**:
   ```python
   # ê¸°ì¡´: IMAGEë§Œ ì¹´ìš´íŠ¸
   saved_counts['IMAGE'] += 1
   
   # ê°œì„ : Providerë³„ íƒ€ì… êµ¬ë¶„
   if obj_type == 'FIGURE':
       saved_counts['FIGURE'] += 1  # Upstage
   else:
       saved_counts['IMAGE'] += 1   # Azure DI, AWS
   ```

2. **ì²­í‚¹ ë‹¨ê³„ í†µí•© ì²˜ë¦¬**:
   ```python
   # IMAGEì™€ FIGURE ëª¨ë‘ í¬í•¨
   raw_image_objs = [o for o in extracted_objects 
                     if o.object_type in ["IMAGE", "FIGURE"]]
   ```

3. **ë¡œê·¸ ê°œì„ **:
   - Provider ì •ë³´ ëª…ì‹œ (`Provider=upstage/azure_di`)
   - ì´ë¯¸ì§€ íƒ€ì…ë³„ ì¹´ìš´íŠ¸ í‘œì‹œ (`visual=2 (IMAGE=0, FIGURE=2)`)
   - ë°”ì´ë„ˆë¦¬ ëˆ„ë½ ì›ì¸ ìƒì„¸í™” (Upstage base64 ì—¬ë¶€ í™•ì¸)

**íš¨ê³¼**:
- âœ… Upstage FIGURE ê°ì²´ â†’ ì´ë¯¸ì§€ ì²­í¬ ì •ìƒ ìƒì„±
- âœ… Azure DI IMAGE ê°ì²´ â†’ ê¸°ì¡´ ë¡œì§ ìœ ì§€
- âœ… ë©€í‹° í”„ëŸ¬ë°”ì´ë” í™˜ê²½ì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰ í†µí•© ì§€ì›

**íŒŒì¼**: `backend/app/services/document/multimodal_document_service.py` (lines 1270, 1283, 1329, 1358)

#### 1.4.3 í•œêµ­ì–´ ì „ë¬¸ê²€ìƒ‰ (FTS) íŠ¸ë¦¬ê±° êµ¬í˜„

- **í™•ì¥**: PostgreSQL `textsearch_ko 1.0` + `kor_search 1.0.0` ì„¤ì¹˜ ì™„ë£Œ
- **Configuration**: 'korean' (mecab íŒŒì„œ ê¸°ë°˜ í˜•íƒœì†Œ ë¶„ì„)
- **ìë™ ìƒì„± íŠ¸ë¦¬ê±°**: `tb_document_search_index` INSERT/UPDATE ì‹œ tsvector ìë™ ìƒì„±
  - `keyword_tsvector`: í‚¤ì›Œë“œ ë°°ì—´ ê¸°ë°˜ (korean íŒŒì„œ)
  - `content_tsvector`: ë¬¸ì„œ ì œëª© + ìš”ì•½ + ì „ë¬¸ (korean íŒŒì„œ)
- **Alembic ë¦¬ë¹„ì „**: `72342a21e7bc_add_tsvector_triggers_for_korean_search.py`
- **ì„±ëŠ¥**: ì¡°ì‚¬/ì–´ë¯¸ ìë™ ì œê±°ë¡œ í•œêµ­ì–´ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ

#### 1.4.3 ë¹„ë™ê¸° ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

- **Celery ì›Œì»¤**: ê°€ìƒí™˜ê²½ `/home/wjadmin/Dev/InsightBridge/.venv` ì ìš©
- **ì‘ì—… í**: Redis ê¸°ë°˜ ë©”ì‹œì§€ ë¸Œë¡œì»¤
- **ì²˜ë¦¬ íë¦„**:
  1. API ì—…ë¡œë“œ â†’ ì¦‰ì‹œ ì‘ë‹µ (task_id ë°˜í™˜)
  2. Celery ì›Œì»¤ â†’ ë°±ê·¸ë¼ìš´ë“œ ë¬¸ì„œ ì²˜ë¦¬
  3. WebSocket â†’ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì•Œë¦¼
  4. ì™„ë£Œ ì‹œ â†’ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìë™ ì—…ë°ì´íŠ¸

#### 1.4.4 ë©€í‹° í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ í†µí•© (2025-11-14 ì—…ë°ì´íŠ¸) ğŸ†•

##### 1.4.4.1 ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ ê°œìš”

ABEKMì€ **ìœ ì—°í•œ ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ ì „í™˜**ì„ ì§€ì›í•˜ì—¬ í´ë¼ìš°ë“œ í™˜ê²½ì— ìµœì í™”ëœ íŒŒì¼ ì €ì¥ì†Œë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì§€ì› ë°±ì—”ë“œ:**
- **Azure Blob Storage**: í”„ë¡œë•ì…˜ í™˜ê²½ (í˜„ì¬ ìš´ì˜ ì¤‘)
- **AWS S3**: AWS ë„¤ì´í‹°ë¸Œ í™˜ê²½
- **Local File System**: ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½

**í•µì‹¬ íŠ¹ì§•:**
- âœ… **ì½”ë“œ ìˆ˜ì • ì—†ìŒ**: `.env` ì„¤ì •ë§Œìœ¼ë¡œ ì „í™˜
- âœ… **í†µì¼ëœ API**: ëª¨ë“  ë°±ì—”ë“œì—ì„œ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤
- âœ… **ìë™ í´ë°±**: ë°±ì—”ë“œ ì¥ì•  ì‹œ ëŒ€ì²´ ìŠ¤í† ë¦¬ì§€ ìë™ ì„ íƒ
- âœ… **ë©”íƒ€ë°ì´í„° ë™ê¸°í™”**: DBì™€ ìŠ¤í† ë¦¬ì§€ ê°„ ê²½ë¡œ ì¼ê´€ì„± ìœ ì§€

##### 1.4.4.2 Azure Blob Storage êµ¬ì„± (í”„ë¡œë•ì…˜)

**ì»¨í…Œì´ë„ˆ êµ¬ì¡°:**
```
Azure Storage Account: blobstoragephs1
â”œâ”€â”€ wkms-raw/                           # ì›ë³¸ íŒŒì¼ ì €ì¥
â”‚   â””â”€â”€ {knowledge_container_id}/raw/{file_physical_name}
â”‚       ì˜ˆ: sales-dept/raw/report_2024_Q4.pdf
â”‚
â”œâ”€â”€ wkms-intermediate/                  # ì¤‘ê°„ ì‚°ì¶œë¬¼ (ì¶”ì¶œ ë°ì´í„°)
â”‚   â”œâ”€â”€ {knowledge_container_id}/pages/{file_id}/
â”‚   â”‚   â”œâ”€â”€ page_0.json                # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸/í‘œ/ì´ë¯¸ì§€
â”‚   â”‚   â”œâ”€â”€ page_1.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ {knowledge_container_id}/images/{file_id}/
â”‚       â”œâ”€â”€ figure_1.png               # ì¶”ì¶œëœ ì´ë¯¸ì§€
â”‚       â””â”€â”€ chart_2.png
â”‚
â””â”€â”€ wkms-derived/                       # ìµœì¢… ì‚°ì¶œë¬¼ (ì²­í‚¹/ì„ë² ë”©)
    â””â”€â”€ {knowledge_container_id}/chunks/{file_id}/
        â”œâ”€â”€ chunk_metadata.json        # ì²­í‚¹ ì„¸ì…˜ ì •ë³´
        â””â”€â”€ embeddings/
            â”œâ”€â”€ text_embedding.npy     # í…ìŠ¤íŠ¸ ì„ë² ë”© ë°°ì—´
            â””â”€â”€ clip_embedding.npy     # CLIP ì´ë¯¸ì§€ ì„ë² ë”©
```

**Blob ë©”íƒ€ë°ì´í„° íƒœê¹…:**
```python
{
    "file_type": "application/pdf",
    "processing_status": "completed",
    "knowledge_container_id": "sales-dept",
    "file_id": "12345",
    "uploaded_by": "user@company.com",
    "uploaded_at": "2024-11-14T10:30:00Z"
}
```

**SAS í† í° ìƒì„±:**
- **ìœ íš¨ ê¸°ê°„**: 1ì‹œê°„ (3600ì´ˆ)
- **ê¶Œí•œ**: ì½ê¸° ì „ìš© (`r`)
- **ìš©ë„**: í”„ë¡ íŠ¸ì—”ë“œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ë§í¬

**ë‹¤ìš´ë¡œë“œ ëª¨ë“œ:**
- `redirect`: 302 ë¦¬ë‹¤ì´ë ‰íŠ¸ (í´ë¼ì´ì–¸íŠ¸ â†’ Azure Blob ì§ì ‘, CORS í•„ìš”)
- `proxy`: ì„œë²„ í”„ë¡ì‹œ (ì„œë²„ â†’ Azure Blob â†’ í´ë¼ì´ì–¸íŠ¸, CORS ë¶ˆí•„ìš”)

**í™˜ê²½ ì„¤ì • (.env):**
```bash
STORAGE_BACKEND=azure_blob
AZURE_BLOB_ACCOUNT_NAME=blobstoragephs1
AZURE_BLOB_ACCOUNT_KEY=***
AZURE_BLOB_CONTAINER_RAW=wkms-raw
AZURE_BLOB_CONTAINER_INTERMEDIATE=wkms-intermediate
AZURE_BLOB_CONTAINER_DERIVED=wkms-derived
AZURE_BLOB_SAS_EXPIRY_SECONDS=3600
AZURE_BLOB_ENABLE_AUTO_CONTAINER=true
AZURE_BLOB_DOWNLOAD_MODE=proxy
```

##### 1.4.4.3 AWS S3 êµ¬ì„± (ëŒ€ì•ˆ)

**ë²„í‚· + Prefix êµ¬ì¡°:**
```
S3 Bucket: ABEKM-file-bucket-20250910
â”œâ”€â”€ {knowledge_container_id}/raw/{file_physical_name}
â”‚   ì˜ˆ: s3://bucket/sales-dept/raw/report_2024_Q4.pdf
â”‚
â”œâ”€â”€ {knowledge_container_id}/intermediate/pages/{file_id}/
â”‚   â”œâ”€â”€ page_0.json
â”‚   â””â”€â”€ page_1.json
â”‚
â””â”€â”€ {knowledge_container_id}/derived/chunks/{file_id}/
    â””â”€â”€ chunk_metadata.json
```

**Presigned URL ìƒì„±:**
```python
url = s3_client.generate_presigned_url(
    ClientMethod='get_object',
    Params={
        'Bucket': 'ABEKM-file-bucket-20250910',
        'Key': 'sales-dept/raw/report.pdf',
        'ResponseContentDisposition': 'attachment; filename="report.pdf"'
    },
    ExpiresIn=3600
)
```

**S3 ê°ì²´ ë©”íƒ€ë°ì´í„°:**
```python
{
    "Metadata": {
        "file-type": "application/pdf",
        "processing-status": "completed",
        "container-id": "sales-dept"
    }
}
```

**ë¼ì´í”„ì‚¬ì´í´ ì •ì±… (ì„ íƒì‚¬í•­):**
```json
{
  "Rules": [{
    "Id": "ArchiveOldFiles",
    "Status": "Enabled",
    "Transitions": [{
      "Days": 90,
      "StorageClass": "GLACIER"
    }]
  }]
}
```

**í™˜ê²½ ì„¤ì • (.env):**
```bash
STORAGE_BACKEND=s3
AWS_S3_BUCKET=ABEKM-file-bucket-20250910
AWS_REGION=ap-northeast-2
AWS_ACCESS_KEY_ID=AKIA***
AWS_SECRET_ACCESS_KEY=***
S3_PRESIGN_EXPIRY_SECONDS=3600
```

##### 1.4.4.4 Local File System êµ¬ì„± (ê°œë°œ)

**ë””ë ‰í† ë¦¬ êµ¬ì¡°:**
```
uploads/
â””â”€â”€ {knowledge_container_id}/
    â”œâ”€â”€ {file_physical_name}              # ì›ë³¸
    â”œâ”€â”€ pages/{file_id}/page_0.json       # ì¤‘ê°„
    â””â”€â”€ chunks/{file_id}/chunk_*.json     # ìµœì¢…
```

**í™˜ê²½ ì„¤ì • (.env):**
```bash
STORAGE_BACKEND=local
UPLOAD_DIR=uploads
```

##### 1.4.4.5 ìŠ¤í† ë¦¬ì§€ ë°±ì—”ë“œ ì „í™˜ ì ˆì°¨

**ìš´ì˜ ì‹œë‚˜ë¦¬ì˜¤: Azure â†’ AWS S3 ì „í™˜**

```bash
# 1ë‹¨ê³„: ë°±ì—…
cp /home/admin/wkms-aws/backend/.env .env.backup_$(date +%Y%m%d_%H%M%S)

# 2ë‹¨ê³„: ì„¤ì • ë³€ê²½
sed -i 's/^STORAGE_BACKEND=azure_blob/STORAGE_BACKEND=s3/' /home/admin/wkms-aws/backend/.env

# 3ë‹¨ê³„: S3 ë²„í‚· ìƒì„± (AWS CLI)
aws s3 mb s3://ABEKM-file-bucket-20250910 --region ap-northeast-2

# 4ë‹¨ê³„: ì„œë¹„ìŠ¤ ì¬ì‹œì‘
cd /home/admin/wkms-aws
docker-compose restart backend

# 5ë‹¨ê³„: ê²€ì¦
curl http://localhost:8000/health | jq '.storage_backend'
# ì˜ˆìƒ ì¶œë ¥: "s3"

# 6ë‹¨ê³„: íŒŒì¼ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8000/api/v1/documents/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "file=@test.pdf" \
  -F "container_id=sales-dept"

# 7ë‹¨ê³„: S3 ì—…ë¡œë“œ í™•ì¸
aws s3 ls s3://ABEKM-file-bucket-20250910/sales-dept/raw/
```

**ë¡¤ë°± ì ˆì°¨:**
```bash
# ë°±ì—… ë³µêµ¬
cp .env.backup_20251114_103000 /home/admin/wkms-aws/backend/.env
docker-compose restart backend
```

##### 1.4.4.6 FileStorageService êµ¬í˜„ ìƒì„¸

**ì¶”ìƒí™” ê³„ì¸µ:**
```python
class FileStorageService:
    """ë©€í‹° í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì¶”ìƒí™” ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.backend = settings.storage_backend
        self.s3 = None
        self.azure_blob = None
        
        if self.backend == 's3':
            self.s3 = S3Service()
        elif self.backend == 'azure_blob':
            self.azure_blob = AzureBlobService()
    
    async def save_file_basic_info(
        self,
        file_path: str,
        knowledge_container_id: str,
        file_physical_name: str,
        **kwargs
    ) -> str:
        """íŒŒì¼ ì €ì¥ (ë°±ì—”ë“œ ìë™ ì„ íƒ)"""
        
        if self.backend == 's3':
            # S3 ì—…ë¡œë“œ
            object_key = f"{knowledge_container_id}/raw/{file_physical_name}"
            await self.s3.upload_file(file_path, object_key)
            permanent_path = object_key
            
        elif self.backend == 'azure_blob':
            # Azure Blob ì—…ë¡œë“œ
            blob_path = f"{knowledge_container_id}/raw/{file_physical_name}"
            self.azure_blob.upload_file(file_path, blob_path, purpose='raw')
            permanent_path = blob_path
            
        else:
            # Local íŒŒì¼ ì‹œìŠ¤í…œ
            permanent_dir = os.path.join(settings.upload_dir, knowledge_container_id)
            os.makedirs(permanent_dir, exist_ok=True)
            permanent_path = os.path.join(permanent_dir, file_physical_name)
            shutil.move(file_path, permanent_path)
        
        # ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(file_path):
            os.remove(file_path)
        
        return permanent_path
```

**ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ì €ì¥:**
```sql
-- TB_FILE_BSS_INFO í…Œì´ë¸”
INSERT INTO tb_file_bss_info (
    file_lgc_nm,
    file_psl_nm,
    path,  -- ë°±ì—”ë“œë³„ ê²½ë¡œ ì €ì¥
    knowledge_container_id
) VALUES (
    'report.pdf',
    'report_20241114.pdf',
    'sales-dept/raw/report_20241114.pdf',  -- Azure Blob path
    'sales-dept'
);
```

##### 1.4.4.7 ì„±ëŠ¥ ë° ë¹„ìš© ë¹„êµ

| í•­ëª© | Azure Blob | AWS S3 | Local |
|------|-----------|--------|-------|
| **ì—…ë¡œë“œ ì†ë„** | ~50MB/s | ~60MB/s | ~200MB/s |
| **ë‹¤ìš´ë¡œë“œ ì†ë„** | ~80MB/s | ~90MB/s | ~500MB/s |
| **ì €ì¥ ë¹„ìš©** | $0.018/GB/ì›” | $0.023/GB/ì›” | ë¬´ë£Œ (ë””ìŠ¤í¬) |
| **ì „ì†¡ ë¹„ìš©** | $0.087/GB | $0.09/GB | ë¬´ë£Œ |
| **ê°€ìš©ì„±** | 99.9% | 99.99% | ì„œë²„ ì˜ì¡´ |
| **ë³´ì•ˆ** | SAS Token | Presigned URL | íŒŒì¼ ê¶Œí•œ |
| **ìë™ ë°±ì—…** | Geo-Redundant | Cross-Region | ìˆ˜ë™ rsync |
| **ê¶Œì¥ ìš©ë„** | Azure í™˜ê²½ | AWS í™˜ê²½ | ê°œë°œ/í…ŒìŠ¤íŠ¸ |

##### 1.4.4.8 ìš´ì˜ì ë§¤ë‰´ì–¼

**ìŠ¤í† ë¦¬ì§€ ìƒíƒœ ëª¨ë‹ˆí„°ë§:**
```bash
# í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health | jq '{
  storage_backend: .storage_backend,
  upload_dir: .upload_dir,
  azure_containers: .azure_blob_containers,
  s3_bucket: .aws_s3_bucket
}'

# ì €ì¥ ê³µê°„ í™•ì¸
df -h /home/admin/wkms-aws/backend/uploads

# Azure Blob ì‚¬ìš©ëŸ‰
az storage account show-usage --account-name blobstoragephs1

# S3 ì‚¬ìš©ëŸ‰
aws s3 ls s3://ABEKM-file-bucket-20250910 --summarize --recursive
```

**íŠ¸ëŸ¬ë¸”ìŠˆíŒ…:**
```bash
# 1. ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ
tail -f logs/app.log | grep -i "upload\|storage\|error"

# 2. Azure Blob ì—°ê²° ì˜¤ë¥˜
az storage blob list --account-name blobstoragephs1 --container-name wkms-raw

# 3. S3 ê¶Œí•œ ì˜¤ë¥˜
aws s3 cp test.txt s3://ABEKM-file-bucket-20250910/ --debug

# 4. ìŠ¤í† ë¦¬ì§€ ì „í™˜ ì‹¤íŒ¨ ì‹œ ë¡¤ë°±
cp .env.backup_YYYYMMDD_HHMMSS /home/admin/wkms-aws/backend/.env
docker-compose restart backend
```

#### 1.4.5 ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ìš´ì˜ ë°˜ì˜

- **ì„œë¹„ìŠ¤**: `MultimodalDocumentService` í”„ë¡œë•ì…˜ ë°°í¬
- **Azure Document Intelligence**: í‘œ/ì–‘ì‹/ë ˆì´ì•„ì›ƒ ê³ ì •ë°€ ì¶”ì¶œ
- **í´ë°± ë©”ì»¤ë‹ˆì¦˜**: pdfplumber ê¸°ë°˜ ê·¸ë¦¼(figure) ë³´ì™„ ì¶”ì¶œ
- **DB ìŠ¤í‚¤ë§ˆ**:
  - `doc_extraction_session`: ì¶”ì¶œ ì„¸ì…˜ ë©”íƒ€
  - `doc_extracted_object`: í…ìŠ¤íŠ¸/í‘œ/ì´ë¯¸ì§€ ê°ì²´ ì €ì¥
  - `doc_chunk_session`: ì²­í‚¹ ì„¸ì…˜ ë©”íƒ€
  - `doc_chunk`: ë©€í‹°ëª¨ë‹¬ ì²­í¬ ì €ì¥
  - `doc_embedding`: ëª¨ë¸ë³„ ì„ë² ë”© ë²„ì „ ê´€ë¦¬ (í…ìŠ¤íŠ¸ + CLIP)

#### 1.4.6 ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì™„ì „ êµ¬í˜„ (2025-10-17) ğŸ†•

##### ë°±ì—”ë“œ ì™„ì„±

- **CLIP ì„ë² ë”© ì„œë¹„ìŠ¤**: `ImageEmbeddingService` í”„ë¡œë•ì…˜ ë°°í¬
  - Azure CLIP API ìš°ì„  ì‹œë„ â†’ ë¡œì»¬ CLIP ìë™ fallback
  - í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (512ì°¨ì›)
  - ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (pHash, ë©”íƒ€ë°ì´í„°)
- **ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ API**: 2ê°œ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
  - `POST /api/v1/search/multimodal`: í…ìŠ¤íŠ¸ ê²€ìƒ‰ + ì´ë¯¸ì§€ ë©”íƒ€ í•„í„°ë§
  - `POST /api/v1/search/clip`: ì´ë¯¸ì§€ ì—…ë¡œë“œ + CLIP ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
- **ë¬¸ì„œ ì—…ë¡œë“œ ì‘ë‹µ í™•ì¥**: `multimodal_metadata` í•„ë“œ ì¶”ê°€
  - ì´ë¯¸ì§€/í‘œ/ì°¨íŠ¸ ê°œìˆ˜
  - CLIP ì„ë² ë”© ìƒì„± ìƒíƒœ
  - ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ë‹¨ê³„ë³„ ì •ë³´
  - ì´ë¯¸ì§€ ê²€ìƒ‰ ê°€ëŠ¥ ì—¬ë¶€ í”Œë˜ê·¸

##### í”„ë¡ íŠ¸ì—”ë“œ ì™„ì„±

- **ê²€ìƒ‰ íƒ€ì… í™•ì¥**: `multimodal`, `clip` ì˜µì…˜ ì¶”ê°€
  - í•˜ì´ë¸Œë¦¬ë“œ (ê¸°ë³¸)
  - ë²¡í„° ìœ ì‚¬ë„
  - í‚¤ì›Œë“œ
  - **ğŸ¨ ë©€í‹°ëª¨ë‹¬ (ì´ë¯¸ì§€ ìš°ì„ )** ğŸ†•
  - **ğŸ–¼ï¸ CLIP (ì´ë¯¸ì§€ ê²€ìƒ‰)** ğŸ†•
- **ì´ë¯¸ì§€ ì—…ë¡œë“œ UI**: SearchBar ë° FloatingSearchBarì— êµ¬í˜„
  - ì´ë¯¸ì§€ ì—…ë¡œë“œ ë²„íŠ¼ (ğŸ“· ì•„ì´ì½˜)
  - ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸° (ì¸ë„¤ì¼ + íŒŒì¼ëª… + í¬ê¸°)
  - íŒŒì¼ ê²€ì¦ (íƒ€ì…, 10MB ì œí•œ)
  - ì´ë¯¸ì§€ ì œê±° ê¸°ëŠ¥
  - ë©€í‹°ëª¨ë‹¬/CLIP ëª¨ë“œ ì¡°ê±´ë¶€ í™œì„±í™”
- **ê²€ìƒ‰ ê²°ê³¼ ë©”íƒ€ í‘œì‹œ**: ResultList ì»´í¬ë„ŒíŠ¸ ê°•í™”
  - ğŸ–¼ï¸ ì´ë¯¸ì§€ ê°œìˆ˜ ë±ƒì§€ (íŒŒë€ìƒ‰)
  - ğŸ¨ ì´ë¯¸ì§€ ëª¨ë‹¬ë¦¬í‹° ë±ƒì§€ (ë³´ë¼ìƒ‰)
  - ğŸ” CLIP ì ìˆ˜ ë±ƒì§€ (ì´ˆë¡ìƒ‰, 0-100%)
  - ìƒ‰ìƒ ì½”ë”© ë° ì•„ì´ì½˜ìœ¼ë¡œ ì‹œê°ì  êµ¬ë¶„

##### í…ŒìŠ¤íŠ¸ ë° ë¬¸ì„œí™”

- **íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸**: `test_multimodal_pipeline.py` (6ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼)
  - ë¡œì»¬ CLIP ëª¨ë¸ ì´ˆê¸°í™”
  - í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±
  - ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
  - Azure â†’ ë¡œì»¬ fallback ë™ì‘
  - ì—ëŸ¬ í•¸ë“¤ë§
- **ì™„ë£Œ ë³´ê³ ì„œ**: `MULTIMODAL_INTEGRATION_COMPLETE_FINAL.md`
- **í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ**: `MULTIMODAL_SEARCH_TESTING_GUIDE.md`
- **API ë¬¸ì„œ**: Swagger UI ì—…ë°ì´íŠ¸ ì™„ë£Œ

## 2. ë©€í‹°ëª¨ë‹¬ RAG ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    A[ë¬¸ì„œ ì—…ë¡œë“œ] --> B{íŒŒì´í”„ë¼ì¸ ì„ íƒ}
    B -->|ì˜¤í”ˆì†ŒìŠ¤| C1[ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸]
    B -->|Azure| C2[Azure Document Intelligence]
    B -->|AWS| C3[AWS Textract]

    C1 --> D[ë©€í‹°ëª¨ë‹¬ í†µí•© ì²˜ë¦¬]
    C2 --> D
    C3 --> D

    D --> E[ì§€ëŠ¥í˜• ì²­í‚¹]
    E --> F[í¬ë¡œìŠ¤ ëª¨ë‹¬ ì„ë² ë”©]
    F --> G[ë²¡í„°ìŠ¤í† ì–´ ì €ì¥]
    G --> H[RAG ê²€ìƒ‰ ì—”ì§„]

    subgraph ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì—”ì§„
        D1[í…ìŠ¤íŠ¸ ë¶„ì„]
        D2[ì´ë¯¸ì§€ OCR]
        D3[í‘œ/ë ˆì´ì•„ì›ƒ íŒŒì‹±]
    end
```

## 3. ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

ë³¸ ì‹œìŠ¤í…œì€ ì„¸ ê°€ì§€ íŒŒì´í”„ë¼ì¸ì„ ì„ íƒì ìœ¼ë¡œ ì œê³µí•˜ë©°, ë™ì¼í•œ ê³„ì•½(ì…ë ¥/ì¶œë ¥/ì˜¤ë¥˜ ì²˜ë¦¬)ì„ ë”°ë¦…ë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì€ êµ¬ì„±ë§Œ ë‹¤ë¥´ê³ , êµì²´ ê°€ëŠ¥í•˜ë„ë¡ ì¸í„°í˜ì´ìŠ¤ê°€ í†µì¼ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### 3.1 ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸

#### 3.1.1 êµ¬ì„±ê³¼ ì¸í„°í˜ì´ìŠ¤

- í…ìŠ¤íŠ¸ íŒŒì„œ + OCR(EasyOCR/Tesseract/Paddle) + ë ˆì´ì•„ì›ƒ ë¶„ì„ ì¡°í•©
- ì…ë ¥: íŒŒì¼ ë°”ì´ë„ˆë¦¬/URI, ì²˜ë¦¬ ì˜µì…˜(ì–¸ì–´, OCR í•„ìš” ì—¬ë¶€ ë“±)
- ì¶œë ¥: ì¶”ì¶œ í…ìŠ¤íŠ¸, í‘œ/í¼ êµ¬ì¡°, í’ˆì§ˆ ì§€í‘œ, ì›ë³¸-ì¶”ì¶œ ë§¤í•‘

#### 3.1.2 êµì²´ ì „ëµ

- ì—”ì§„ ë…ë¦½ ëª¨ë“ˆí™”: OCR, PDF íŒŒì„œ, ë ˆì´ì•„ì›ƒ ë¶„ì„ê¸°ë¥¼ í”ŒëŸ¬ê·¸ì¸ìœ¼ë¡œ êµì²´ ê°€ëŠ¥
- ì„±ëŠ¥/í’ˆì§ˆ ê¸°ì¤€ì¹˜ ë¯¸ë‹¬ ì‹œ ë‹¤ë¥¸ ì—”ì§„ìœ¼ë¡œ í´ë°± ë˜ëŠ” ì¬ì‹œë„

#### 3.1.3 ì¥ì  ë° ì ìš© ì‹œë‚˜ë¦¬ì˜¤

- ë¹„ìš© ì œë¡œ, ì˜¨í”„ë ˆë¯¸ìŠ¤/ë³´ì•ˆ í™˜ê²½ ì í•©, í•œêµ­ì–´ íŠœë‹ ìš©ì´
- ëŒ€ëŸ‰ ë°°ì¹˜, ë¹„ìš© ë¯¼ê°, ë‚´ë¶€ ë°ì´í„° ë°˜ì¶œ ê¸ˆì§€ í™˜ê²½

### 3.2 Azure Document Intelligence

#### 3.2.1 ëª©ì 

- ë³µì¡í•œ í‘œ/ì–‘ì‹/ë ˆì´ì•„ì›ƒì´ ë§ì€ ë¬¸ì„œì—ì„œ ë†’ì€ êµ¬ì¡° ì¸ì‹ ì •í™•ë„ í™•ë³´

#### 3.2.2 íë¦„

- ì—…ë¡œë“œ â†’ ë¶„ì„ ìš”ì²­ â†’ í´ë§/ì›¹í›… â†’ ì¶”ì¶œ ê²°ê³¼ ìˆ˜ì§‘ â†’ í‘œ/í‚¤-ê°’ ë§¤í•‘ ì •ê·œí™”

#### 3.2.3 íŠ¹ì§•

- êµ¬ì¡°í™” ë°ì´í„° ì •ë°€ë„ ë†’ìŒ, ê³¼ê¸ˆ ë‹¨ê°€ ì•ˆì •, ê´€ë¦¬í˜• SLA ì œê³µ

#### 3.2.4 ì¥ì  ë° ì ìš© ì‹œë‚˜ë¦¬ì˜¤

- í’ˆì§ˆ ìš°ì„ , ë³µì¡ ì–‘ì‹/í‘œ ì¤‘ì‹¬, ì²˜ë¦¬ëŸ‰ ì¼ì¼ 1ì²œ ê±´ ì´í•˜ì˜ ì‚¬ë‚´ ë¬¸ì„œ

### 3.3 Upstage Document Parser ğŸ†•

#### 3.3.1 ëª©ì 

- í•œêµ­ì–´ í•™ìˆ  ë…¼ë¬¸ ë° ì „ë¬¸ ë¬¸ì„œì— íŠ¹í™”ëœ ê³ ì •ë°€ ë¬¸ì„œ íŒŒì‹±
- ê·¸ë¦¼(Figure), í‘œ(Table), ì°¨íŠ¸(Chart) ë“± ì‹œê°ì  ìš”ì†Œì˜ êµ¬ì¡°ì  ì¶”ì¶œ

#### 3.3.2 íë¦„

- ì—…ë¡œë“œ â†’ Document Parse API ìš”ì²­ â†’ JSON ê²°ê³¼ ìˆ˜ì‹  â†’ ê°ì²´ íƒ€ì…ë³„ í›„ì²˜ë¦¬ â†’ ì €ì¥

#### 3.3.3 íŠ¹ì§•

- **FIGURE íƒ€ì… ê°ì²´ ìƒì„±**: ê·¸ë¦¼/ì°¨íŠ¸/ë‹¤ì´ì–´ê·¸ë¨ì„ `FIGURE` íƒ€ì…ìœ¼ë¡œ ì¶”ì¶œ (Azure DIëŠ” `IMAGE` íƒ€ì… ì‚¬ìš©)
- **Base64 ì„ë² ë”©**: ì´ë¯¸ì§€ ë°”ì´ë„ˆë¦¬ë¥¼ JSON ë‚´ base64ë¡œ ì§ì ‘ ì œê³µ (`structure_json.base64_encoding`)
- **í•œêµ­ì–´ OCR ìµœì í™”**: í•œê¸€ ì¸ì‹ ì •í™•ë„ í–¥ìƒ
- **í•™ìˆ  ë…¼ë¬¸ êµ¬ì¡° ì¸ì‹**: ì œëª©, ì´ˆë¡, ë³¸ë¬¸, ì°¸ê³ ë¬¸í—Œ ìë™ ë¶„ë¥˜

#### 3.3.4 ë©€í‹° í”„ëŸ¬ë°”ì´ë” ì§€ì› ğŸ†•

**Providerë³„ ì´ë¯¸ì§€ ê°ì²´ íƒ€ì… ì°¨ì´ì **:

| Provider | ì´ë¯¸ì§€ ê°ì²´ íƒ€ì… | ë°”ì´ë„ˆë¦¬ ì €ì¥ ë°©ì‹ | íŠ¹ì§• |
|----------|------------------|-------------------|------|
| **Upstage** | `FIGURE` | JSON ë‚´ base64_encoding | í•™ìˆ  ë…¼ë¬¸ ê·¸ë¦¼ íŠ¹í™”, ìº¡ì…˜ í¬í•¨ |
| **Azure DI** | `IMAGE` | binary_data ì†ì„± ë˜ëŠ” bbox í¬ë¡­ | ë²”ìš© ì´ë¯¸ì§€, ì •í™•í•œ bbox |
| **AWS Textract** | `IMAGE` (ì˜ˆì •) | S3 ì°¸ì¡° ë˜ëŠ” bbox í¬ë¡­ | ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ íŠ¹í™” |

**ì½”ë“œ ë ˆë²¨ ì²˜ë¦¬**:
```python
# ë°”ì´ë„ˆë¦¬ ì €ì¥ ì‹œ Providerë³„ ì¹´ìš´íŠ¸ ë¶„ê¸°
if obj_type == 'FIGURE':
    saved_counts['FIGURE'] += 1  # Upstage
else:
    saved_counts['IMAGE'] += 1   # Azure DI, AWS

# ì²­í‚¹ ë‹¨ê³„ì—ì„œ í†µí•© ì²˜ë¦¬
raw_image_objs = [o for o in extracted_objects 
                  if o.object_type in ["IMAGE", "FIGURE"]]
```

#### 3.3.5 ì¥ì  ë° ì ìš© ì‹œë‚˜ë¦¬ì˜¤

- í•œêµ­ì–´ í•™ìˆ  ë…¼ë¬¸, ì—°êµ¬ ë³´ê³ ì„œ, ê¸°ìˆ  ë¬¸ì„œ
- ê·¸ë¦¼ ìº¡ì…˜ ë° ì°¨íŠ¸ ì •ë³´ ì •í™•í•œ ì¶”ì¶œ í•„ìš” ì‹œ
- ë¹„ìš© íš¨ìœ¨ì  (Azure DI ëŒ€ë¹„ 50% ì ˆê°)

### 3.4 AWS Textract

#### 3.4.1 ëª©ì 

- ëŒ€ìš©ëŸ‰/ëŒ€ê·œëª¨ í™•ì¥, ì„œëª…/ì²´í¬ë°•ìŠ¤ ë“± íŠ¹ìˆ˜ ìš”ì†Œ ì¸ì‹ í¬í•¨

#### 3.3.2 íë¦„

- S3 ì—…ë¡œë“œ â†’ Textract(Sync/Async) â†’ ê²°ê³¼ S3/ì½œë°± â†’ í…Œì´ë¸”/í¼ í›„ì²˜ë¦¬ â†’ ì €ì¥

#### 3.3.3 ì£¼ìš” ì„ íƒì§€

- DetectDocumentText, AnalyzeDocument, AnalyzeExpense ë“± ì—…ë¬´ íŠ¹í™” API ì„ íƒ

#### 3.3.4 ì˜¤ë¥˜ ì²˜ë¦¬

- Async ì¬ì‹œë„, í˜ì´ì§€ ë‹¨ìœ„ ë¶€ë¶„ ì‹¤íŒ¨ ê²©ë¦¬, ì„ê³„ê°’ ê¸°ë°˜ í´ë°±(ì˜¤í”ˆì†ŒìŠ¤/ë‹¤ë¥¸ í´ë¼ìš°ë“œ)

#### 3.3.5 ì¥ì  ë° ì ìš© ì‹œë‚˜ë¦¬ì˜¤

- í™•ì¥ì„±/ë‚´ê²°í•¨ì„±, ì„œëª…/ì–‘ì‹ ì²˜ë¦¬ ê°•ì , AWS ë„¤ì´í‹°ë¸Œ í†µí•© í™˜ê²½

### 3.4 ì „ì²´ íŒŒì´í”„ë¼ì¸ ì •ë¦¬

1) ì—…ë¡œë“œ ë° ì €ì¥
- ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦ â†’ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ â†’ ë¡œì»¬ ì„ì‹œ ì €ì¥ â†’ S3 ì—…ë¡œë“œ(ì˜µì…˜, í‘œì¤€ í‚¤ ìŠ¤í‚´)
- DB ê¸°ë³¸ ë©”íƒ€ ì €ì¥: `tb_file_bss_info`, `tb_file_dtl_info`

2) í…ìŠ¤íŠ¸/ë ˆì´ì•„ì›ƒ ì¶”ì¶œ (ë©€í‹° í”„ëŸ¬ë°”ì´ë” ì§€ì› ğŸ†•)
- **ì˜¤í”ˆì†ŒìŠ¤**: PDF íŒŒì„œ + OCR(EasyOCR/Tesseract/Paddle)
- **Upstage Document Parser** ğŸ†•: 
  - í•™ìˆ  ë…¼ë¬¸ íŠ¹í™” API (`layout-analyzer-0.1.0`)
  - FIGURE íƒ€ì… ê°ì²´ ìƒì„± (base64_encoding í¬í•¨)
  - í•œêµ­ì–´ OCR ìµœì í™”, ìˆ˜ì‹/ì°¨íŠ¸ ì¸ì‹ ê°•í™”
- **Azure Document Intelligence**: 
  - `prebuilt-document` + `prebuilt-layout` ë³‘í–‰ í˜¸ì¶œ
  - IMAGE íƒ€ì… ê°ì²´ ìƒì„± (binary_data ë˜ëŠ” bbox í¬ë¡­)
  - 2025-10-10 ê¸°ì¤€ìœ¼ë¡œ **ê·¸ë¦¼(figure)** ë¯¸ê²€ì¶œ ì‹œ `pdfplumber` ê¸°ë°˜ í´ë°± ì¶”ì¶œì„ ìë™ ì ìš©í•˜ê³  í˜ì´ì§€ ë©”íƒ€ë°ì´í„°ì— ë³‘í•©
- **AWS Textract**: API ê¸°ë°˜ ë¶„ì„(í‘œ/í‚¤-ê°’/ë ˆì´ì•„ì›ƒ)
- **Office/HWP**: ë„¤ì´í‹°ë¸Œ íŒŒì„œ ìš°ì„ , í´ë°±ìœ¼ë¡œ PDF ë³€í™˜ í›„ ë ˆì´ì•„ì›ƒ ë¶„ì„ ë° ë©€í‹°ëª¨ë‹¬ ê°ì²´ ë§¤í•‘

**Providerë³„ ì´ë¯¸ì§€ ê°ì²´ ì²˜ë¦¬ ì°¨ì´** ğŸ†•:
- Upstage: `FIGURE` íƒ€ì… â†’ `saved_counts['FIGURE']` ì¦ê°€ â†’ `image_ids_with_binary` ì„¸íŠ¸ ë“±ë¡
- Azure DI: `IMAGE` íƒ€ì… â†’ `saved_counts['IMAGE']` ì¦ê°€ â†’ `image_ids_with_binary` ì„¸íŠ¸ ë“±ë¡
- ì²­í‚¹ ë‹¨ê³„: ë‘ íƒ€ì… ëª¨ë‘ `raw_image_objs` í†µí•© ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬ (`["IMAGE", "FIGURE"]` í•„í„°)

3) ì²­í‚¹ ë° ì„ë² ë”©
- êµ¬ì¡°/ì˜ë¯¸/ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ì²­í‚¹ â†’ ì„ë² ë”© ìƒì„±(í˜„ì¬ ìš´ì˜ ê°’ 3072ì°¨ì›, Azure OpenAI `text-embedding-3-large`)
- í…ìŠ¤íŠ¸ ì„ë² ë”©: í˜„ì¬ `vs_doc_contents_chunks`
- ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©: ì œì•ˆ `vs_multimodal_embeddings` (image/table/layout/video í¬í•¨)

4) ì¸ë±ì‹±ê³¼ ê²€ìƒ‰
- í‚¤ì›Œë“œ/ì „ë¬¸: `tb_document_search_index` (TSVECTOR + GIN)
- ì˜ë¯¸ ê²€ìƒ‰: pgvector IVFFlat ì¸ë±ìŠ¤ â†’ í›„ë³´ TopK
- **í¬ë¡œìŠ¤ ì–¸ì–´ ê²€ìƒ‰**: `kor_search` í™•ì¥ í†µí•œ í•œêµ­ì–´-ì˜ì–´ í†µí•© ê²€ìƒ‰ ğŸ†•
- í•˜ì´ë¸Œë¦¬ë“œ: í‚¤ì›Œë“œì™€ ë²¡í„° ì ìˆ˜ ê²°í•© + ë¦¬ë­í‚¹

5) í’ˆì§ˆ/ìš´ì˜
- í’ˆì§ˆ ì§€í‘œ(quality_score), ì‹¤íŒ¨ ì¬ì²˜ë¦¬/í´ë°±, probes/lists íŠœë‹, Structured Logging

### 3.5 í˜„ì¬ ë°±ì—”ë“œ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ í˜„í™© (2025.10.10)

#### 3.5.1 ë°±ì—”ë“œ ë¬¸ì„œ ì²˜ë¦¬ ì•„í‚¤í…ì²˜

í˜„ì¬ `/home/admin/wkms-aws/backend/app/api/v1/documents.py`ì—ì„œ êµ¬í˜„ëœ ì‹¤ì œ íŒŒì´í”„ë¼ì¸:

```mermaid
graph TB
    A[ë¬¸ì„œ ì—…ë¡œë“œ API] --> B[ê¶Œí•œ ë° íŒŒì¼ ê²€ì¦]
    B --> C[ë¡œì»¬ ì„ì‹œ ì €ì¥]
    C --> D[Azure/S3 ì—…ë¡œë“œ + ê²½ë¡œ ìŠ¤í‚´ ë¶„ë¥˜]
    D --> E[DB ë©”íƒ€ë°ì´í„° ì €ì¥]
    E --> F[MultimodalDocumentService]
    F --> G[TextExtractorService (Azure DI + í´ë°±)]
    G --> H[Blob Storage ì¤‘ê°„ ì‚°ì¶œë¬¼ ì €ì¥]
    H --> I[ê³ ê¸‰ ì²­í‚¹ & ì„ë² ë”©]
    I --> J[VectorStore ì €ì¥]
    J --> K[RAG ì¤€ë¹„ ì™„ë£Œ]

    subgraph "í˜„ì¬ êµ¬í˜„ëœ ì„œë¹„ìŠ¤"
        F1[azure_document_intelligence_service<br/>í…Œì´ë¸”Â·ê·¸ë¦¼ í´ë°± ë³‘í•©]
        F2[text_extractor_service<br/>PDF/PPTX/DOCX/XLSX ë©€í‹°ëª¨ë‹¬ ì¶”ì¶œ]
        G1[multimodal_document_service<br/>ì¶”ì¶œ ê°ì²´/ì²­í‚¹/ì„ë² ë”©]
        H1[korean_nlp_service<br/>í‚¤ì›Œë“œ/ì„ë² ë”© ìƒì„±]
        I1[Azure Blob (intermediate)<br/>ì¶”ì¶œ ë©”íƒ€/í…ìŠ¤íŠ¸ ë³´ì¡´]
        J1[vs_doc_contents_chunks<br/>pgvector ì €ì¥]
    end
```

#### 3.5.2 í˜„ì¬ ì²˜ë¦¬ íë¦„ ë¶„ì„ (2025-10-15 ì—…ë°ì´íŠ¸)

1. **íŒŒì¼ ì—…ë¡œë“œ** (documents.py `/upload`)
   - ì»¨í…Œì´ë„ˆ ê¶Œí•œ ê²€ì¦ â†’ íŒŒì¼ ê²€ì¦ â†’ ë¡œì»¬ ì„ì‹œ ì €ì¥
   - **Azure Blob Storage ì—…ë¡œë“œ**: `wkms-documents/<file_id>/<filename>`
   - `document_service.create_document_from_upload()` í˜¸ì¶œ
   - **Celery ì‘ì—… í ë“±ë¡**: ì¦‰ì‹œ task_id ë°˜í™˜ (ë¹„ë™ê¸° ì²˜ë¦¬) ğŸ†•
2. **Celery ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬** ğŸ†•
   - **ì›Œì»¤ í™˜ê²½**: `/home/wjadmin/Dev/InsightBridge/.venv/bin/python`
   - **ì‘ì—… íë¦„**:

     ```
     Celery Worker â†’ MultimodalDocumentService.process_document_multimodal()
                  â†’ ë¬¸ì„œ ì¶”ì¶œ (Azure DI / Upstage / AWS Textract)
                  â†’ Blob Storage ì¤‘ê°„ ì‚°ì¶œë¬¼ ì €ì¥
                  â†’ ì²­í‚¹ & ì„ë² ë”© (1024d - amazon.titan-embed-text-v2:0)
                  â†’ PostgreSQL ì €ì¥ (vs_doc_contents_chunks)
                  â†’ ì´ë¯¸ì§€ ì„ë² ë”© (512d - twelvelabs.marengo-embed-3-0-v1:0)
                  â†’ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (tb_document_search_index)
                  â†’ korean FTS íŠ¸ë¦¬ê±° ìë™ ì‹¤í–‰
     ```
   - **WebSocket ì•Œë¦¼**: ì²˜ë¦¬ ì§„í–‰ë¥  ì‹¤ì‹œê°„ ì „ì†¡
3. **ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰**
   - `MultimodalDocumentService.process_document_multimodal()` ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
   - **ì¶”ì¶œ ë‹¨ê³„**:
     - Azure Document Intelligence: í‘œ/ì–‘ì‹/ë ˆì´ì•„ì›ƒ ë¶„ì„
     - pdfplumber í´ë°±: ê·¸ë¦¼(figure) ë¯¸ê²€ì¶œ ì‹œ ë³´ì™„
     - íŒŒì¼ í˜•ì‹ë³„ ì²˜ë¦¬: ğŸ“„ PDF, ğŸ“Š PPTX, ğŸ“ DOCX, ğŸ“ˆ XLSX, ğŸ— HWP
   - **ì¤‘ê°„ ì‚°ì¶œë¬¼ ì €ì¥** (Azure Blob): ğŸ†•
     - `wkms-intermediate/multimodal/<file_id>/extraction_metadata.json`
     - `wkms-intermediate/multimodal/<file_id>/extraction_full_text.txt`
     - `wkms-intermediate/multimodal/<file_id>/images/*.png` (ì¶”ì¶œëœ ì´ë¯¸ì§€)
4. **ì¶”ì¶œ ê²°ê³¼ DB ì €ì¥** ğŸ†•
   - **doc_extraction_session**: ì¶”ì¶œ ì„¸ì…˜ ë©”íƒ€ (íŒŒì´í”„ë¼ì¸ ì¢…ë¥˜, ì‹œì‘/ì¢…ë£Œ ì‹œê°„, ìƒíƒœ)
   - **doc_extracted_object**: í…ìŠ¤íŠ¸ ë¸”ë¡, í‘œ, ì´ë¯¸ì§€ ê°ì²´ ê°œë³„ ì €ì¥
   - **doc_chunk_session**: ì²­í‚¹ ì„¸ì…˜ ë©”íƒ€ (ì²­í‚¹ ì „ëµ, íŒŒë¼ë¯¸í„°)
   - **doc_chunk**: ë©€í‹°ëª¨ë‹¬ ì²­í¬ ì €ì¥ (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ì°¸ì¡°, í˜ì´ì§€ ë²”ìœ„)
   - **doc_embedding**: ëª¨ë¸ë³„ ì„ë² ë”© ë²„ì „ ê´€ë¦¬
     - ì¼ë°˜ í…ìŠ¤íŠ¸: `amazon.titan-embed-text-v2:0` (1024d)
     - ë©€í‹°ëª¨ë‹¬: `twelvelabs.marengo-embed-3-0-v1:0` (512d)
5. **ê²€ìƒ‰ ì¸ë±ìŠ¤ ìë™ ì—…ë°ì´íŠ¸** ğŸ†•
   - **vs_doc_contents_chunks**: pgvector HNSW ì¸ë±ìŠ¤
     - ì¼ë°˜ í…ìŠ¤íŠ¸ ë²¡í„°: 1024d (`amazon.titan-embed-text-v2:0`)
     - ë©€í‹°ëª¨ë‹¬ ë²¡í„°: 512d (`twelvelabs.marengo-embed-3-0-v1:0`) - `doc_embedding.clip_vector`
   - **tb_document_search_index**: PostgreSQL FTS ì¸ë±ìŠ¤
     - **korean tsvector íŠ¸ë¦¬ê±°**: INSERT/UPDATE ì‹œ ìë™ ìƒì„±
     - keyword_tsvector: í‚¤ì›Œë“œ ë°°ì—´ â†’ to_tsvector('korean', keywords)
     - content_tsvector: ì œëª© + ìš”ì•½ + ì „ë¬¸ â†’ to_tsvector('korean', content)
   - **í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„**: mecab íŒŒì„œë¡œ ì¡°ì‚¬/ì–´ë¯¸ ìë™ ì œê±°
6. **í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…**
   - ì²˜ë¦¬ ì‹œê°„, ì„±ê³µ/ì‹¤íŒ¨ìœ¨, ì¶”ì¶œ ê°ì²´ ìˆ˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
   - ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„ (ì§€ìˆ˜ ë°±ì˜¤í”„)
   - ê°ì‚¬ ë¡œê·¸: ëª¨ë“  ë‹¨ê³„ì˜ ì…ë ¥/ì¶œë ¥/ì˜¤ë¥˜ ê¸°ë¡

#### 3.5.3 OCR ë…¸íŠ¸ë¶ êµ¬í˜„ í˜„í™©

í˜„ì¬ Jupyter ë…¸íŠ¸ë¶ìœ¼ë¡œ 3ê°€ì§€ OCR íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ì™„ë£Œ:

| ë…¸íŠ¸ë¶ íŒŒì¼                                             | OCR ì—”ì§„                      | ì‹¤í–‰ ìƒíƒœ  | ì£¼ìš” ê¸°ëŠ¥           |
| -------------------------------------------------- | --------------------------- | ------ | --------------- |
| `01.image_pdf_Tesseract_ocr_layout_pipeline.ipynb` | Tesseract + pdfplumber      | âŒ ë¯¸ì‹¤í–‰  | ë¡œì»¬ OCR, ë¬´ë£Œ      |
| `02.image_pdf_Textract_ocr_layout_pipeline.ipynb`  | AWS Textract                | âœ… ì‹¤í–‰ì™„ë£Œ | í´ë¼ìš°ë“œ OCR, ì»¬ëŸ¼ ì •ë ¬ |
| `03.image_pdf_AzureDI_ocr_layout_pipeline.ipynb`   | Azure Document Intelligence | âœ… ì‹¤í–‰ì™„ë£Œ | í´ë¼ìš°ë“œ OCR, í‘œ ì¶”ì¶œ  |

**ê³µí†µ êµ¬í˜„ íŠ¹ì§•:**
- **ì»¬ëŸ¼ ìë™ ê°ì§€**: k-means 1D í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì¢Œí‘œ ê¸°ë°˜ 2ì—´/Nì—´ ì¸ì‹
- **ì½ê¸° ìˆœì„œ ë³µì›**: ì¢Œâ†’ìš° ì»¬ëŸ¼ ìˆœì„œë¡œ í…ìŠ¤íŠ¸ ë³‘í•© 
- **í‘œ êµ¬ì¡° ì¶”ì¶œ**: ë³„ë„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ í‘œ ë‚´ìš© ì €ì¥
- **ë©”íƒ€ë°ì´í„° JSON**: í˜ì´ì§€ë³„ ì²˜ë¦¬ í†µê³„ ë° ì»¬ëŸ¼ ê°ì§€ ì •ë³´

#### 3.5.4 2025-10-10 ì—…ë°ì´íŠ¸: ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ê²€ì¦ ê²°ê³¼

- PDF, PPTX, DOCX, XLSX ìƒ˜í”Œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ **Document Intelligence + í´ë°±** íŒŒì´í”„ë¼ì¸ì„ ì‹¤ì œ í˜¸ì¶œí–ˆê³ , ê° í˜•ì‹ë³„ ê°ì²´/ì²­í¬/ì„ë² ë”©ì´ ì˜ˆìƒëŒ€ë¡œ ìƒì„±ë¨ì„ í™•ì¸í•¨
- `multimodal/<file_id>/extraction_metadata.json`ê³¼ `extraction_full_text.txt`ê°€ Azure Blob `wkms-intermediate` ì»¨í…Œì´ë„ˆì— ì €ì¥ë˜ë©°, PDFì—ì„œëŠ” 27ê°œì˜ í…ìŠ¤íŠ¸ ë¸”ë¡ê³¼ 6ê°œì˜ ì´ë¯¸ì§€, PPTX/DOCXì—ì„œëŠ” í‘œÂ·ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ë¨
- ë°ì´í„°ë² ì´ìŠ¤ ìš”ì•½: `doc_extracted_object` (PDF: TEXT_BLOCK 27 + IMAGE 6 ë“±), `doc_chunk`, `doc_embedding` ì¹´ìš´í„°ê°€ íŒŒì´í”„ë¼ì¸ í†µê³„ì™€ ì¼ì¹˜
- Azure OpenAI ì„ë² ë”©(`text-embedding-3-large`) ê¸°ì¤€ 3072 ì°¨ì›ìœ¼ë¡œ ì¼ê´€ ì €ì¥, ì¶”í›„ kor_searchì™€ì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì§„í–‰ ì˜ˆì •

#### 3.5.5 OCR í†µí•© ì „ëµ ë° í–¥í›„ ê³„íš

**Phase 1: OCR ì„œë¹„ìŠ¤ ê³„ì¸µ ì¶”ê°€**
```python
# ì œì•ˆ: backend/app/services/document/ocr/
ocr_service.py              # OCR ì—”ì§„ ì¶”ìƒí™” ì¸í„°í˜ì´ìŠ¤
tesseract_ocr_service.py    # ë…¸íŠ¸ë¶ 01.* ë¡œì§ â†’ ì„œë¹„ìŠ¤ ë³€í™˜
textract_ocr_service.py     # ë…¸íŠ¸ë¶ 02.* ë¡œì§ â†’ ì„œë¹„ìŠ¤ ë³€í™˜  
azure_di_ocr_service.py     # ë…¸íŠ¸ë¶ 03.* ë¡œì§ â†’ ì„œë¹„ìŠ¤ ë³€í™˜
```

**Phase 2: text_extractor_service í™•ì¥**
```python
class TextExtractorService:
    async def extract_text_from_file(self, file_path: str) -> Dict[str, Any]:
        # 1. PDF íƒ€ì… íŒë³„ (ì´ë¯¸ì§€ vs í…ìŠ¤íŠ¸)
        if self._is_image_pdf(file_path):
            # 2. OCR ì—”ì§„ ì„ íƒ ë° ì²˜ë¦¬
            return await self._extract_with_ocr(file_path)
        else:
            # 3. ê¸°ì¡´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ìœ ì§€
            return await self._extract_text_pdf(file_path)
```

**Phase 3: íŒŒì´í”„ë¼ì¸ ì„ íƒ ë¡œì§**
```python
# OCR ì—”ì§„ ì„ íƒ ë§¤íŠ¸ë¦­ìŠ¤
OCR_SELECTION_MATRIX = {
    'í•œêµ­ì–´_ë¬¸ì„œ': 'azure_di',      # í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜
    'ì˜ë¬¸_ê¸°ìˆ ë¬¸ì„œ': 'textract',     # AWS ìƒíƒœê³„ í†µí•©
    '2ì—´_ë…¼ë¬¸': 'azure_di',         # ì»¬ëŸ¼ ì •ë ¬ ì„±ëŠ¥
    'í‘œ_ì¤‘ì‹¬_ë¬¸ì„œ': 'azure_di',      # í‘œ êµ¬ì¡° ì¸ì‹
    'ì˜ˆì‚°_ì ˆì•½': 'tesseract'        # ë¹„ìš© 0ì›
}
```

#### 3.4.1 íŒŒì´í”„ë¼ì¸ë³„ ì˜ì¡´ íŒ¨í‚¤ì§€ (Linux/Python)

- ê³µí†µ (Linux):

    - poppler-utils, ghostscript, fonts-noto-cjk|fonts-nanum, libxml2-dev, libxslt1-dev
    - LibreOffice(í—¤ë“œë¦¬ìŠ¤ ë³€í™˜), wkhtmltopdf(ì„ íƒ)
- ê³µí†µ (Python):

    - pdfplumber|pymupdf, pdfminer.six, pillow, pytesseract, easyocr|paddleocr, tiktoken
    - kiwipiepy, sentence-transformers, pgvector, sqlalchemy, psycopg2-binary
- ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ ì „ìš©:

    - Tesseract(ë¦¬ëˆ…ìŠ¤: tesseract-ocr, tesseract-ocr-kor), easyocr ë˜ëŠ” paddleocr
- Azure Document Intelligence:

    - azure-ai-formrecognizer, azure-core, azure-identity (ì„œë¹„ìŠ¤ ê³„ì •/í‚¤ í•„ìš”)
- AWS Textract:

    - boto3, aioboto3 (Async ì„ íƒ), awscli(ìš´ì˜), IAM ê¶Œí•œ (Textract, S3)

#### 3.5.5 ë…¸íŠ¸ë¶ â†’ ì„œë¹„ìŠ¤ ë³€í™˜ ê°€ì´ë“œ

**í˜„ì¬ ë…¸íŠ¸ë¶ í•µì‹¬ ë¡œì§ í™œìš©:**
1. **ì»¬ëŸ¼ ê°ì§€ ì•Œê³ ë¦¬ì¦˜** â†’ `LayoutAnalysisService`
   - k-means 1D í´ëŸ¬ìŠ¤í„°ë§: `_kmeans_1d()`, `_split_into_n_columns()`
   - ë°”ìš´ë”©ë°•ìŠ¤ ì¢Œí‘œ ì •ê·œí™”: `_line_left_top_norm()`

2. **OCR ì„¤ì • ê´€ë¦¬** â†’ `OCRConfigService`  
   - í™˜ê²½ë³€ìˆ˜ ë¡œë“œ: `.env` íŒŒì¼ ê¸°ë°˜ ìê²©ì¦ëª…
   - ëª¨ë¸ ì„ íƒ: `prebuilt-read`, `prebuilt-layout` ë“±
3. **ë©”íƒ€ë°ì´í„° ìƒì„±** â†’ `DocumentMetadataService`
   - í˜ì´ì§€ë³„ í†µê³„: í…ìŠ¤íŠ¸ ê¸¸ì´, ì»¬ëŸ¼ ê°œìˆ˜, ì²˜ë¦¬ ì‹œê°„
   - í’ˆì§ˆ ì§€í‘œ: OCR ì‹ ë¢°ë„, í‘œ ê°ì§€ ê°œìˆ˜

**ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ ë³€í™˜ ì˜ˆì‹œ:**
```python
class AzureDIOCRService:
    def __init__(self):
        # 03.* ë…¸íŠ¸ë¶ì˜ DIConfig â†’ ì„œë¹„ìŠ¤ ì„¤ì •
        self.client = self._init_azure_client()
        self.config = self._load_config_from_env()
    
    async def extract_with_layout(self, file_path: str) -> Dict[str, Any]:
        # ë…¸íŠ¸ë¶ì˜ analyze_pdf_with_azure_di() ë¡œì§ ì ìš©
        # + ì»¬ëŸ¼ ê°ì§€ + í‘œ ì¶”ì¶œ í†µí•©
        result = await self._analyze_document(file_path)
        
        # ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ì²­í‚¹ ì •ë³´ í¬í•¨
        return {
            'text': result['ordered_text'],
            'layout_info': {
                'columns_per_page': result['columns_detected'],
                'tables': result['tables'],
                'reading_order': 'left_to_right'
            },
            'quality_metrics': {
                'ocr_confidence': result['confidence'],
                'processing_time': result['processing_time']
            }
        }
```

#### 3.5.6 í†µí•© í›„ ê¸°ëŒ€ íš¨ê³¼

**í˜„ì¬ í•œê³„ì :**
- ì´ë¯¸ì§€ PDF ì²˜ë¦¬ ë¶ˆê°€ (ë°±ì—”ë“œ íŒŒì´í”„ë¼ì¸)
- OCR ì—”ì§„ ì„ íƒ ì˜µì…˜ ì—†ìŒ
- ì»¬ëŸ¼/í‘œ êµ¬ì¡° ì¸ì‹ ë¯¸ì§€ì›

**í†µí•© í›„ ê°œì„  ì‚¬í•­:**
- âœ… **ì´ë¯¸ì§€ PDF ì™„ë²½ ì§€ì›**: 3ê°€ì§€ OCR ì—”ì§„ ì„ íƒ
- âœ… **ì»¬ëŸ¼ ì½ê¸° ìˆœì„œ ë³µì›**: 2ì—´ ë…¼ë¬¸/ë³´ê³ ì„œ ì˜¬ë°”ë¥¸ ìˆœì„œ
- âœ… **í‘œ êµ¬ì¡° ì¸ì‹**: ë³µì¡í•œ í‘œ ë‚´ìš© ì •í™•í•œ ì¶”ì¶œ
- âœ… **ë¹„ìš© ìµœì í™”**: ë¬¸ì„œ ìœ í˜•ë³„ ìµœì  ì—”ì§„ ìë™ ì„ íƒ
- âœ… **í•œêµ­ì–´ íŠ¹í™”**: Azure DI ìš°ì„ , Tesseract í´ë°±

**ì„±ëŠ¥ ëª©í‘œ:**
- OCR ì •í™•ë„: 90%+ (í•œêµ­ì–´ ë¬¸ì„œ ê¸°ì¤€)
- ì²˜ë¦¬ ì†ë„: í‰ê·  20-30ì´ˆ/í˜ì´ì§€ (í´ë¼ìš°ë“œ OCR)
- ë¹„ìš© ì ˆê°: ìµœëŒ€ 80% (Tesseract í™œìš© ì‹œ)

#### 3.4.2 íŒŒì¼ ìœ í˜•ë³„ íŠ¹ì§• ì²˜ë¦¬

- PDF: í…ìŠ¤íŠ¸ ê¸°ë°˜â†’í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš°ì„ , ìŠ¤ìº” ê¸°ë°˜â†’OCR/Textract; í˜ì´ì§€/ë¸”ë¡ bbox ë³´ì¡´
- DOCX: Heading/ëª©ë¡/í‘œ ìœ ì§€, ì´ë¯¸ì§€/ìº¡ì…˜/AltText ìˆ˜ì§‘, ì†ìƒì‹œ PDF í´ë°±
- PPTX: Slide/Shape/í‘œ/ë„í˜• ì¢Œí‘œ ì¶”ì¶œ, ë…¸íŠ¸/AltText í¬í•¨, ì´ë¯¸ì§€/ë„í˜• bbox/URI ì €ì¥
- XLSX: ì‹œíŠ¸/í‘œ ì˜ì—­ ì¶”ì¶œ, ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„± í›„ ì„ë² ë”©, ëŒ€í˜• ì‹œíŠ¸ ìƒ˜í”Œë§ ë° ë§ˆìŠ¤í‚¹
- HWPX/HWP: HWPX=XML ì§ì ‘ íŒŒì‹±, HWP=PrvTextâ†’ì‹¤íŒ¨ì‹œ PDF í´ë°±, í°íŠ¸ ìºì‹œ í•„ìˆ˜
- ì´ë¯¸ì§€(JPEG/PNG/TIFF): OCR + ì´ë¯¸ì§€ ì„ë² ë”©, EXIF ë©”íƒ€ ê³ ë ¤, ê³ í•´ìƒë„ ë¦¬ì‚¬ì´ì¦ˆ/íƒ€ì¼ë§
- ë™ì˜ìƒ(MP4): ìƒ˜í”Œë§ í”„ë ˆì„/ì¥ë©´(Scene) ì¶”ì¶œâ†’OCR/ASR ì ìš©, ì¥ë©´ ì´ë¯¸ì§€ ì„ë² ë”©/ìë§‰ í…ìŠ¤íŠ¸ ì„ë² ë”© ë³‘í–‰
- **ì´ë¯¸ì§€ PDF**: OCR ì—”ì§„ ìë™ ì„ íƒ â†’ ì»¬ëŸ¼ ì •ë ¬ â†’ í‘œ ì¶”ì¶œ â†’ ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ì²­í‚¹

## 4. í•œêµ­ì–´ ë¬¸ì„œ ì „ì²˜ë¦¬

### 4.1 í•œêµ­ì–´ í† í°í™” ë° í˜•íƒœì†Œ ë¶„ì„

```python
from kiwipiepy import Kiwi
from kiwipiepy.utils import Stopwords

class KoreanTextProcessor:
    def __init__(self):
        # ê³ ì„±ëŠ¥ SkipBigram ëª¨ë¸ + ì˜¤íƒ€ êµì •
        self.kiwi = Kiwi(
            model_type='sbg',
            typos='basic_with_continual_and_lengthening'
        )
        self.stopwords = Stopwords()
        
        # íšŒì‚¬ íŠ¹í™” ì‚¬ìš©ì ì‚¬ì „
        self.kiwi.add_user_word("ê¸°ì—…ëª…", "NNP")
        self.kiwi.add_user_word("ì§€ì‹ê´€ë¦¬ì‹œìŠ¤í…œ", "NNG")
        self.kiwi.add_user_word("SAP RFC", "NNG")
    
    def extract_keywords(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        tokens = self.kiwi.tokenize(text, stopwords=self.stopwords)
        return [token.form for token in tokens 
                if token.tag in ['NNG', 'NNP', 'SL']]
    
    def extract_proper_nouns(self, text: str) -> List[str]:
        """ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ"""
        tokens = self.kiwi.tokenize(text)
        return [token.form for token in tokens if token.tag == 'NNP']
```

### 4.2 ì²­í‚¹ (Chunking) ì „ëµ

```python
import tiktoken

class DocumentChunker:
    def __init__(self):
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def chunked_texts(self, text: str, max_tokens: int = 1500, 
                     overlap_percentage: float = 0.1) -> List[str]:
        """
        í† í° ê¸°ë°˜ í•œêµ­ì–´ ì²­í‚¹
        - ë¬¸ì¥ ê²½ê³„ ë³´ì¡´
        - ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• 
        - ê²¹ì¹¨ ì²˜ë¦¬ë¡œ ë¬¸ë§¥ ë³´ì¡´
        """
        sentences = self.split_korean_sentences(text)
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = len(self.tokenizer.encode(sentence))
            
            if current_tokens + sentence_tokens > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    
                # ê²¹ì¹¨ ì²˜ë¦¬
                overlap_tokens = int(max_tokens * overlap_percentage)
                current_chunk = self.create_overlap(current_chunk, overlap_tokens)
                current_tokens = len(self.tokenizer.encode(current_chunk))
            
            current_chunk += sentence + " "
            current_tokens += sentence_tokens
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
            
        return chunks
```

#### 4.2.1 ëª©í‘œì™€ ì›ì¹™

- ê²€ìƒ‰ ì§ˆì˜ì™€ ê³ í’ˆì§ˆë¡œ ì—°ê²°ë˜ëŠ” ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤: ì§ˆë¬¸-ë‹µ ë§¤í•‘ì„ ê³ ë ¤í•œ ì˜ë¯¸ ë‹¨ìœ„ ìœ ì§€
- ê³¼ë„í•œ ê¸¸ì´ ë°©ì§€: ì„ë² ë”© ëª¨ë¸ í† í° í•œê³„ë¥¼ ê³ ë ¤(ê¶Œì¥ 800â€“1500 tokens)
- ë¬¸ë§¥ ë³´ì¡´: 5â€“15% ê²¹ì¹¨ìœ¼ë¡œ ì¸ì ‘ ì²­í¬ ê°„ ì—°ì†ì„± ìœ ì§€
- êµ¬ì¡° í™œìš©: ì œëª©, ë¶€ì œ, í‘œ/ëª©ë¡ ê²½ê³„ ê¸°ì¤€ì„ ìš°ì„  ì‚¬ìš©

#### 4.2.2 ëª¨ë“œë³„ ì²­í‚¹ ë°©ì‹

- êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹: Heading/Section/Slide/í‘œ/ë„í˜• ë‹¨ìœ„ë¡œ ë¶„í• , í—¤ë” ê²½ë¡œ(header_path) ë©”íƒ€ë°ì´í„° ì €ì¥
- ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹: ë¬¸ì¥ ìœ ì‚¬ë„/í† í”½ ë³€í™” ê°ì§€ë¡œ ê²½ê³„ ì„¤ì •, ê¸°ì¤€ í† í° ìˆ˜ ë²”ìœ„ ë‚´ ì¡°ì •
- ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ì²­í‚¹: PDF/Textract ì¢Œí‘œ(bbox) ë¸”ë¡ì„ í˜ì´ì§€/ì˜ì—­ ë‹¨ìœ„ë¡œ ë¬¶ì–´ ì €ì¥(ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ëŒ€ë¹„)

#### 4.2.3 ê¶Œì¥ íŒŒë¼ë¯¸í„° (í•œêµ­ì–´ ë¬¸ì„œ ê¸°ì¤€)

- max_tokens: 1000â€“1200
- overlap_percentage: 0.1 (100â€“150 í† í°)
- sentence_max: 12â€“18ë¬¸ì¥/ì²­í¬
- header_path í¬í•¨: true (ex: 1ì¥ > 1.2ì ˆ > ì†Œì œëª©)

#### 4.2.4 ì‹¤í–‰ ë°©ì•ˆ

- íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ì— ì²­í‚¹ ëª¨ë“ˆ ì‚½ì…: í…ìŠ¤íŠ¸ ì •ê·œí™” â†’ êµ¬ì¡° íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ì €ì¥
- ì²­í¬ ë©”íƒ€ í•„ë“œ: chunk_index, header_path, page_number, section_title, chunk_type(structural|semantic|layout)
- ì‹¤íŒ¨/ê²½ê³„ ì¼€ì´ìŠ¤: í•œ í˜ì´ì§€ ê³¼ë„ ê¸¸ì´ ì‹œ ìë™ ë¶„í• , í‘œ/ì½”ë“œ ë¸”ë¡ì€ ë…ë¦½ ì²­í¬ë¡œ ê³ ì •

#### 4.2.5 í‰ê°€ ì§€í‘œì™€ ë°©ë²•

- Retrieval: Recall@K, Precision@K, MRR@K, nDCG@K (K=5/10/20)
- Pipeline: ì²­í‚¹ ì „í›„ ë¹„êµ A/B, Q/A ì¿¼ë¦¬ì…‹ ê¸°ë°˜ ì¬í˜„ìœ¨ í‰ê°€
- ì¸ë±ìŠ¤ íŠœë‹: ivfflat lists/probes ë³€í™”ì— ë”°ë¥¸ ì§€í‘œ ê³¡ì„  ê¸°ë¡
- ì˜¤ë¥˜ ë¶„ì„: Missed@K ìƒìœ„ 50ê°œ ì‚¬ë¡€ ìƒ˜í”Œë§í•˜ì—¬ ê²½ê³„/ì˜¤íƒ€/í‘œ/ë ˆì´ì•„ì›ƒ ì›ì¸ ë¼ë²¨ë§

#### 4.2.6 ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ ë°©ì•ˆ

- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: BM25(í‚¤ì›Œë“œ/TSVECTOR) + ë²¡í„°(ì„ë² ë”©) ê²°í•©, ì ìˆ˜ ì •ê·œí™” ê°€ì¤‘í•©
- ì¿¼ë¦¬ í™•ì¥: ë™ì˜ì–´/í˜•íƒœ ë³€í˜•/í‚¤ì›Œë“œ í™•ì¥ ë° ì§ˆì˜ ì¬ì‘ì„±(Query Rewriting)
- ë¦¬ë­í‚¹: Cross-Encoder/Bi-Encoder ì¬ë­í‚¹ìœ¼ë¡œ TopK ì •ë°€ë„ í–¥ìƒ
- ë©”íƒ€ ê°€ì¤‘: í—¤ë” ì¼ì¹˜/ì»¨í…Œì´ë„ˆ/ë¬¸ì„œìœ í˜•/ì‹ ë¢°ë„(quality_score)ë¡œ ê°€ì¤‘ì¹˜ ì¡°ì •
- ë‹¤ë‹¨ê³„ ê²€ìƒ‰: 1ë‹¨ê³„ ë„“ê²Œ Recall, 2ë‹¨ê³„ ì •ë°€ ë¦¬ë­í‚¹, 3ë‹¨ê³„ ì»¨í…ìŠ¤íŠ¸ í™•ì¥

### 4.3 ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

```python
class DocumentMetadataExtractor:
    def extract_metadata(self, file_path: str, content: str) -> dict:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {
            "file_name": os.path.basename(file_path),
            "file_size": os.path.getsize(file_path),
            "file_extension": os.path.splitext(file_path)[1],
            "modified_time": os.path.getmtime(file_path),
            "content_length": len(content),
            "token_count": len(self.tokenizer.encode(content)),
            "language": "korean",
            "keywords": self.extract_keywords(content),
            "proper_nouns": self.extract_proper_nouns(content),
            "corp_names": self.extract_corp_names(content),
            "document_type": self.classify_document_type(content)
        }
        return metadata
```

### 4.4 HWP/HWPX íŒŒì¼ ì²˜ë¦¬ ê°•í™” (2025.08.05 ì—…ë°ì´íŠ¸)

#### 4.4.1 HWP/HWPX í…ìŠ¤íŠ¸ ì¶”ì¶œ ê°œì„ 

```python
async def _extract_hwp_file(self, file_path: str, result: Dict[str, Any]) -> Dict[str, Any]:
    """HWP íŒŒì¼ ë° HWPX íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""
    ext = Path(file_path).suffix.lower()
    try:
        if ext == '.hwpx':
            # HWPX: ZIP + XML êµ¬ì¡° ê°œì„ ëœ ì²˜ë¦¬
            import zipfile, lxml.etree as ET
            with zipfile.ZipFile(file_path, 'r') as z:
                xml_names = [n for n in z.namelist() if n.endswith('.xml')]
                text_content = ''
                for name in xml_names:
                    try:
                        data = z.read(name)
                        tree = ET.fromstring(data)
                        # XPathë¥¼ ì‚¬ìš©í•œ ëª¨ë“  í…ìŠ¤íŠ¸ ë…¸ë“œ ìˆ˜ì§‘
                        texts = tree.xpath('//text()')
                        text_content += '\n'.join(texts) + '\n'
                    except Exception:
                        continue
            result['text'] = text_content.strip() or f'HWPX íŒŒì¼ì…ë‹ˆë‹¤: {Path(file_path).name}\n[í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨]'
            result['metadata'].update({
                'extraction_method': 'hwp5-xml-enhanced',
                'char_count': len(result['text']),
                'extraction_note': 'HWPX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ'
            })
        else:
            # HWP: OLE íŒŒì¼ PrvText ìŠ¤íŠ¸ë¦¼ ì¶”ì¶œ ê°œì„ 
            import olefile
            ole = olefile.OleFileIO(file_path)
            if ole.exists('PrvText'):
                raw = ole.openstream('PrvText').read()
                try:
                    text = raw.decode('utf-16le')
                except Exception:
                    text = raw.decode('cp949', errors='ignore')
                result['text'] = text.strip() or f'HWP íŒŒì¼ì…ë‹ˆë‹¤: {Path(file_path).name}\n[PrvText ë¹ˆ ìŠ¤íŠ¸ë¦¼]'
                result['metadata'].update({
                    'extraction_method': 'olefile-PrvText-enhanced',
                    'char_count': len(result['text']),
                    'extraction_note': 'HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ'
                })
            else:
                result['text'] = f'HWP íŒŒì¼ì…ë‹ˆë‹¤: {Path(file_path).name}\n[PrvText ìŠ¤íŠ¸ë¦¼ ì—†ìŒ]'
                result['metadata']['extraction_note'] = 'HWP PrvText ìŠ¤íŠ¸ë¦¼ ì—†ìŒ'
        ole.close() if 'ole' in locals() else None
    except Exception as e:
        result['success'] = False
        result['error'] = f'HWP/HWPX ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'
    return result
```

#### 4.4.2 HWP/HWPX ë·°ì–´ ê¸°ëŠ¥ êµ¬í˜„

```python
# files.py API ì—”ë“œí¬ì¸íŠ¸ ê°œì„ 
@router.get("/files/{file_bss_info_sno}/convert")
async def convert_file_to_pdf(file_bss_info_sno: int, request: Request, db: AsyncSession = Depends(get_db)):
    """íŒŒì¼ì„ PDFë¡œ ë³€í™˜í•˜ì—¬ ë·°ì–´ì—ì„œ í‘œì‹œ"""
    try:
        # HWP/HWPX íŒŒì¼ íŠ¹ë³„ ì²˜ë¦¬
        if file_extension.lower() in ['.hwp', '.hwpx']:
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extraction_result = await text_extractor_service.extract_text_from_file(
                file_path, file_extension
            )
            
            if extraction_result["success"] and extraction_result["text"]:
                # 2. HTMLë¡œ ë³€í™˜
                html_content = f"""
                <!DOCTYPE html>
                <html>
                <head>
                    <meta charset="UTF-8">
                    <title>{file_logical_name}</title>
                    <style>
                        body {{ font-family: 'Noto Sans KR', Arial, sans-serif; 
                               line-height: 1.6; padding: 20px; }}
                        .header {{ border-bottom: 2px solid #333; padding-bottom: 10px; }}
                        .content {{ white-space: pre-wrap; }}
                    </style>
                </head>
                <body>
                    <div class="header">
                        <h1>{file_logical_name}</h1>
                        <p>íŒŒì¼ í˜•ì‹: {file_extension.upper()}</p>
                    </div>
                    <div class="content">{extraction_result["text"]}</div>
                </body>
                </html>
                """
                
                # 3. wkhtmltopdfë¡œ PDF ë³€í™˜
                temp_html_path = f"/tmp/hwp_temp_{file_bss_info_sno}.html"
                pdf_output_path = f"/tmp/hwp_converted_{file_bss_info_sno}.pdf"
                
                with open(temp_html_path, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                
                cmd = [
                    'wkhtmltopdf',
                    '--page-size', 'A4',
                    '--encoding', 'UTF-8',
                    '--margin-top', '20mm',
                    '--margin-right', '20mm',
                    '--margin-bottom', '20mm',
                    '--margin-left', '20mm',
                    temp_html_path,
                    pdf_output_path
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode == 0 and os.path.exists(pdf_output_path):
                    # PDF íŒŒì¼ ë°˜í™˜
                    return FileResponse(
                        pdf_output_path,
                        media_type='application/pdf',
                        filename=f"{os.path.splitext(file_logical_name)[0]}.pdf"
                    )
                    
        # ê¸°ì¡´ LibreOffice ë³€í™˜ ë¡œì§...
        
    except Exception as e:
        logger.error(f"HWP íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="íŒŒì¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
```

#### 4.4.3 ìš´ì˜ ê¶Œì¥ ì‚¬í•­ (HWP/HWPX)

- HWPX ìš°ì„ : XML ê¸°ë°˜ì´ë¯€ë¡œ í…ìŠ¤íŠ¸/ë ˆì´ì•„ì›ƒ ì¶”ì¶œ ì•ˆì •ì 
- HWPëŠ” PrvText ì¶”ì¶œ ì‹¤íŒ¨ ëŒ€ë¹„ PDF ë³€í™˜ í´ë°± ê²½ë¡œ ì¤€ë¹„
- í°íŠ¸/ì¸ì½”ë”© ì´ìŠˆ: ì„œë²„ì— í•œêµ­ì–´ í°íŠ¸ ì„¤ì¹˜ ë° ìºì‹œ ê°±ì‹  í•„ìš”
- ë³´ì•ˆ: ì‚¬ë‚´ ë¯¼ê°ë¬¸ì„œëŠ” ì˜¨í”„ë ˆë¯¸ìŠ¤ íŒŒì´í”„ë¼ì¸ì—ì„œë§Œ ì²˜ë¦¬

#### 4.4.4 HWP/HWPX ì˜ì¡´ íŒ¨í‚¤ì§€ (Linux/Python)

HWP/HWPX í…ìŠ¤íŠ¸ ì¶”ì¶œê³¼ ë¬¸ì„œ ë ˆì´ì•„ì›ƒ/êµ¬ì¡° ë¶„ì„ì„ ìœ„í•´ ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì„ ê¶Œì¥/ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

- Linux íŒ¨í‚¤ì§€

    - libreoffice, libreoffice-writer: ë³€í™˜/ë¯¸ë¦¬ë³´ê¸° í´ë°± ê²½ë¡œìš© (HWPâ†’PDF/ODT ë“±)
    - wkhtmltopdf: HTMLâ†’PDF ë³€í™˜(4.4.2 ë·°ì–´ ê¸°ëŠ¥ì—ì„œ ì‚¬ìš©)
    - tesseract-ocr, tesseract-ocr-kor, tesseract-ocr-script-hang: OCR í´ë°± ë° í•œê¸€ ì§€ì›
    - fonts-noto-cjk ë˜ëŠ” ë‚˜ëˆ” ê¸€ê¼´ íŒ¨í‚¤ì§€: í•œê¸€ ë Œë”ë§/ì¶”ì¶œ í’ˆì§ˆ í–¥ìƒ
    - poppler-utils (pdftotext, pdfimages ë“±): PDF í´ë°± ì‹œ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì¶”ì¶œ ë³´ì¡°
    - ghostscript: PDF ì²˜ë¦¬ í’ˆì§ˆ/í˜¸í™˜ì„± ê°œì„ 
    - unzip/zip: HWPX(Zip ì»¨í…Œì´ë„ˆ) ë³´ì¡° ë„êµ¬
    - ì„ íƒ: hwp5-tools (ë°°í¬íŒ ì œê³µ ì‹œ, hwp5txt/hwp5odt ë“± HWP v5 í…ìŠ¤íŠ¸/ODT ë³€í™˜ ë„êµ¬)
- Python íŒ¨í‚¤ì§€

    - lxml: HWPX XML íŒŒì‹± ë° XPath ê¸°ë°˜ í…ìŠ¤íŠ¸/êµ¬ì¡° ì¶”ì¶œ
    - olefile: HWP(OLE) PrvText ìŠ¤íŠ¸ë¦¼ ì ‘ê·¼ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - chardet (ì„ íƒ): PrvText ì¸ì½”ë”© íŒë³„ ë³´ì¡°(cp949/utf-16 ë“±)
    - PyMuPDF ë˜ëŠ” pdfminer.six (ì„ íƒ): PDF í´ë°± ì‹œ í…ìŠ¤íŠ¸+ì¢Œí‘œ(ë ˆì´ì•„ì›ƒ) ì¶”ì¶œ
    - pillow (PIL): ì´ë¯¸ì§€ ì²˜ë¦¬, ì¸ë„¤ì¼ ìƒì„± ë“±
    - pytesseract / paddleocr / easyocr (ì„ íƒ): OCR ê¸°ë°˜ í…ìŠ¤íŠ¸/ì˜ì—­ ì¸ì‹
    - opencv-python (ì„ íƒ): ë ˆì´ì•„ì›ƒ/ë„í˜• ê°ì§€ ì „ì²˜ë¦¬, ì´ë¯¸ì§€ ì—°ì‚°
    - layoutparser (ì„ íƒ): ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ë¸”ë¡ ê°ì§€(ëª¨ë¸ í•„ìš”, ë¬´ê±°ì›€)
    - boto3 / azure-ai-documentintelligence (ì„ íƒ): AWS Textract / Azure Document Intelligence ì—°ê³„ ì‹œ

ì£¼ì˜/ë©”ëª¨
- lxml ë¹Œë“œ ì‹œ ì‹œìŠ¤í…œì— libxml2/libxslt ê°œë°œ í—¤ë”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- OCR í’ˆì§ˆì€ í°íŠ¸/í•´ìƒë„ì— í¬ê²Œ ì¢Œìš°ë˜ë©°, í•œê¸€ ì–¸ì–´íŒ© ì„¤ì¹˜ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ì‹­ì‹œì˜¤.
- ë°°í¬ í™˜ê²½ë³„ íŒ¨í‚¤ì§€ ëª…ì¹­ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìš´ì˜ í‘œì¤€ ì´ë¯¸ì§€ì— ëª…ì‹œì ìœ¼ë¡œ í•€(Pin)ning í•˜ì„¸ìš”.

### 4.5 MS Office íŒŒì¼ ì²˜ë¦¬ ì „ëµ (DOCX/PPTX/XLSX)

#### 4.5.1 ê¸°ë³¸ ì›ì¹™

- ê°€ëŠ¥í•œ ê²½ìš° í¬ë§· ë„¤ì´í‹°ë¸Œ íŒŒì„œë¡œ êµ¬ì¡°/ì˜ë¯¸ë¥¼ ìœ ì§€í•´ ì¶”ì¶œ
- ë ˆì´ì•„ì›ƒ ë³´ì¡´ì´ í•„ìš”í•œ ê²½ìš° PDFë¡œ ë³€í™˜ í›„ ë ˆì´ì•„ì›ƒ ë¸”ë¡ ì¶”ì¶œ(ë©€í‹°ëª¨ë‹¬ ëŒ€ë¹„)
- ëª¨ë“  ê²½ìš° í…ìŠ¤íŠ¸/í‘œ/ì´ë¯¸ì§€/ë„í˜•ì„ ë³„ë„ì˜ ì—”íŠ¸ë¦¬ë¡œ ì •ê·œí™”í•˜ì—¬ ì €ì¥

#### 4.5.2 DOCX (ì›Œë“œ)

- ì§ì ‘ íŒŒì‹±: ë³¸ë¬¸/ì œëª©/ëª©ë¡/í‘œ ì¶”ì¶œ, ìŠ¤íƒ€ì¼ ê¸°ë°˜ Heading ê³„ì¸µ â†’ header_path ìƒì„±
- ì´ë¯¸ì§€: ì¸ë¼ì¸ ì´ë¯¸ì§€ ì €ì¥(content_uri) + ìº¡ì…˜/AltText ìˆ˜ì§‘, ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±
- í‘œ: ì…€ í…ìŠ¤íŠ¸ ì •ê·œí™”, í—¤ë” ì¶”ì •, í…Œì´ë¸” ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„± í›„ ì„ë² ë”©(í‘œ ì „ìš© ì²­í‚¹)
- í´ë°±: ì†ìƒ/ë¹„ì •ìƒ DOCX â†’ PDF ë³€í™˜ í›„ ë ˆì´ì•„ì›ƒ OCR/Textract ê²½ë¡œë¡œ ì²˜ë¦¬

#### 4.5.3 PPTX (íŒŒì›Œí¬ì¸íŠ¸)

- ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„: ì œëª©/ë³¸ë¬¸/í‘œ/ë„í˜• Shape ì¶”ì¶œ, ì½ê¸° ìˆœì„œ ë³´ì •(ë ˆì´ì–´/ì¢Œí‘œ/íƒ­ ìˆœì„œ)
- ë…¸íŠ¸/ëŒ€ì²´í…ìŠ¤íŠ¸: ë°œí‘œì ë…¸íŠ¸/AltText ìˆ˜ì§‘í•´ í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê°•í™”
- ì´ë¯¸ì§€/ë„í˜•: ì˜ì—­(bbox)ê³¼ í•¨ê»˜ ì €ì¥í•˜ì—¬ ì´ë¯¸ì§€ ì„ë² ë”© ë° í¬ë¡œìŠ¤ëª¨ë‹¬ ê²€ìƒ‰ ì¤€ë¹„
- ì²­í‚¹: ìŠ¬ë¼ì´ë“œ=ê¸°ë³¸ ì²­í¬, í‘œ/ê¸´ ë³¸ë¬¸ì€ í•˜ìœ„ ì²­í¬ë¡œ ë¶„í• , header_path=ìŠ¬ë¼ì´ë“œ ë²ˆí˜¸/ì œëª©

#### 4.5.4 XLSX (ì—‘ì…€)

- ì‹œíŠ¸ ë‹¨ìœ„: ì²« Ní–‰ ìŠ¤ë‹ˆí•‘ìœ¼ë¡œ í—¤ë”/íƒ€ì… ì¶”ì •, í‘œ ì˜ì—­ì„ Region ë‹¨ìœ„ë¡œ ì¶”ì¶œ
- í…ìŠ¤íŠ¸í™”: ì¤‘ìš”í•œ í‘œëŠ” ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±(ì—´ í—¤ë”+ëŒ€í‘œ í–‰), í‘œ ì „ìš© ì„ë² ë”© ìƒì„±
- ëŒ€í˜• ì‹œíŠ¸: ìƒ˜í”Œë§ í›„ ë¯¸ë¦¬ë³´ê¸°/ì§€í‘œ ìƒì„±, ë¯¼ê°ë°ì´í„° í•„ë“œ ë§ˆìŠ¤í‚¹ ì •ì±… ì ìš©

#### 4.5.5 ì €ì¥ê³¼ ê²€ìƒ‰ ì—°ê³„

- í…ìŠ¤íŠ¸ ì²­í¬ â†’ vs_doc_contents_chunks(í˜„í–‰) ë˜ëŠ” vs_multimodal_embeddings(ì œì•ˆ) ì €ì¥
- ì´ë¯¸ì§€/í‘œ â†’ vs_multimodal_embeddings(modality=image|table)ë¡œ ì €ì¥, bbox/URI í¬í•¨
- í‚¤ì›Œë“œ/ì „ë¬¸ ê²€ìƒ‰ì€ TbDocumentSearchIndex ìœ ì§€, í•˜ì´ë¸Œë¦¬ë“œ ê²°í•©ìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ

## 5. ë²¡í„°í™” (Vectorization)

### 5.1 ì„ë² ë”© ìƒì„± ì „ëµ

#### 5.1.1 ì¼ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”© (RAG ì‹œìŠ¤í…œ í•µì‹¬)

**EmbeddingService** - ë¬¸ì„œ ì²­í‚¹ ë° ê²€ìƒ‰ ì¿¼ë¦¬ìš©

```python
class EmbeddingService:
    """ì¼ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ì„œë¹„ìŠ¤ (ë¬¸ì„œ ì²­í‚¹, RAG ì¿¼ë¦¬)"""
    
    def __init__(self):
        # Provider ì„ íƒ: bedrock | azure_openai | openai
        self.default_provider = settings.default_embedding_provider
        
        # AWS Bedrock (í˜„ì¬ ìš´ì˜)
        self.bedrock_client = boto3.client('bedrock-runtime')
        self.bedrock_model = 'amazon.titan-embed-text-v2:0'  # 1024d
        
        # Azure OpenAI (ëŒ€ì•ˆ)
        self.azure_client = AsyncAzureOpenAI()
        self.azure_model = 'text-embedding-3-small'  # 1536d
    
    async def get_embedding(self, text: str) -> List[float]:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (1024d)"""
        if self.default_provider == 'bedrock':
            # Amazon Titan Embed v2
            response = self.bedrock_client.invoke_model(
                modelId='amazon.titan-embed-text-v2:0',
                body=json.dumps({"inputText": text})
            )
            return response['embedding']  # 1024d
        
        elif self.default_provider == 'azure_openai':
            # Azure OpenAI
            response = await self.azure_client.embeddings.create(
                model='text-embedding-3-small',
                input=text
            )
            return response.data[0].embedding  # 1536d
```

#### 5.1.2 ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í¬ë¡œìŠ¤ëª¨ë‹¬)

**ImageEmbeddingService** - ì´ë¯¸ì§€ ì„ë² ë”© ë° í¬ë¡œìŠ¤ëª¨ë‹¬ ê²€ìƒ‰ìš©

```python
class ImageEmbeddingService:
    """ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ì„œë¹„ìŠ¤ (ì´ë¯¸ì§€ + í¬ë¡œìŠ¤ëª¨ë‹¬ í…ìŠ¤íŠ¸)"""
    
    def __init__(self):
        # Provider ì„ íƒ: bedrock | azure_openai | local
        self.provider = settings.default_embedding_provider
        
        # AWS Bedrock (í˜„ì¬ ìš´ì˜)
        self.bedrock_client = boto3.client('bedrock-runtime')
        self.bedrock_model = 'twelvelabs.marengo-embed-3-0-v1:0'  # 512d
        
        # Azure CLIP (ëŒ€ì•ˆ)
        self.azure_clip_endpoint = settings.azure_openai_multimodal_embedding_endpoint
        self.azure_clip_model = 'openai-clip-image-text-embed-11'  # 512d
        
        # ë¡œì»¬ CLIP (Fallback)
        self.local_clip_model = 'openai/clip-vit-base-patch32'  # 512d
    
    async def generate_image_embedding(self, image_bytes: bytes) -> List[float]:
        """ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (512d)"""
        if self.provider == 'bedrock':
            # TwelveLabs Marengo
            img_base64 = base64.b64encode(image_bytes).decode('utf-8')
            response = self.bedrock_client.invoke_model(
                modelId='twelvelabs.marengo-embed-3-0-v1:0',
                body=json.dumps({"inputImage": img_base64})
            )
            return response['embedding']  # 512d
    
    async def generate_text_embedding(self, text: str) -> List[float]:
        """í¬ë¡œìŠ¤ëª¨ë‹¬ í…ìŠ¤íŠ¸ ì„ë² ë”© (ì´ë¯¸ì§€ ê²€ìƒ‰ìš©, 512d)
        
        âš ï¸ ì£¼ì˜: ì´ ë©”ì„œë“œëŠ” ì´ë¯¸ì§€ì™€ ê°™ì€ ë²¡í„° ê³µê°„ì˜ í…ìŠ¤íŠ¸ ì„ë² ë”©ì…ë‹ˆë‹¤.
        ì¼ë°˜ RAG ì¿¼ë¦¬ëŠ” EmbeddingService.get_embedding()ì„ ì‚¬ìš©í•˜ì„¸ìš”!
        """
        if self.provider == 'bedrock':
            # TwelveLabs Marengo (ë©€í‹°ëª¨ë‹¬ í…ìŠ¤íŠ¸)
            response = self.bedrock_client.invoke_model(
                modelId='twelvelabs.marengo-embed-3-0-v1:0',
                body=json.dumps({"inputText": text})
            )
            return response['embedding']  # 512d
```

#### 5.1.3 Providerë³„ ëª¨ë¸ ë¹„êµ

| Provider | ì¼ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”© (RAG) | ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© (ì´ë¯¸ì§€) | ë¹„ê³  |
|----------|-------------------|-------------------|------|
| **AWS Bedrock** âœ… | `amazon.titan-embed-text-v2:0` (1024d) | `twelvelabs.marengo-embed-3-0-v1:0` (512d) | í˜„ì¬ ìš´ì˜ ì¤‘ |
| **Azure OpenAI** | `text-embedding-3-small` (1536d) | `openai-clip-image-text-embed-11` (512d) | ëŒ€ì•ˆ Provider |
| **OpenAI** | `text-embedding-ada-002` (1536d) | - | Fallback |
| **ë¡œì»¬** | `jhgan/ko-sroberta-multitask` | `openai/clip-vit-base-patch32` (512d) | Fallback |

### 5.2 ì´ì¤‘ ì„ë² ë”© ì „ëµ

```python
def create_document_embeddings(self, chunk: str, file_info: dict) -> dict:
    """
    Azure í˜¸í™˜ ì´ì¤‘ ì„ë² ë”© ìƒì„±
    - chunk_text_vector: ì²­í¬ ì„ë² ë”©
    - main_text_vector: êµ¬ì¡°í™”ëœ ë©”ì¸ í…ìŠ¤íŠ¸ ì„ë² ë”©
    """
    title = chunk.split('\n')[0][:100]
    
    # Azure ë™ì¼ í˜•ì‹: (file_name) + (title) + (content)
    main_text = f"""
(file_name) {file_info['logical_name']}

(title) {title}

(content)
{chunk}
""".strip()
    
    return {
        "chunk_text": chunk,
        "main_text": main_text,
        "chunk_text_vector": self.embedding_service.generate_embedding(chunk),
        "main_text_vector": self.embedding_service.generate_embedding(main_text)
    }
```

## 6. ë²¡í„° ì €ì¥ ë° ì¸ë±ì‹±

### 6.1 í˜„ì¬ êµ¬í˜„ëœ ìŠ¤í‚¤ë§ˆ í˜„í™© (ì½”ë“œ ê¸°ì¤€)

ë³¸ ë¦¬í¬ì§€í† ë¦¬ì˜ ì‹¤ì œ êµ¬í˜„ì€ ë‹¤ìŒ 2ê°œ ì¶•ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

1) ì²­í¬ ë‹¨ìœ„ ë²¡í„° í…Œì´ë¸”: `vs_doc_contents_chunks` (pgvector ì‚¬ìš©)
- ì¶œì²˜: `backend/app/models/document/vector_models.py` (VsDocContentsChunks)
- ì£¼ìš” ì»¬ëŸ¼:
    - chunk_sno (PK), file_bss_info_sno, chunk_index
    - chunk_text, chunk_size
    - chunk_embedding vector(settings.vector_dimension)
    - page_number, section_title
    - keywords, named_entities (í…ìŠ¤íŠ¸, ì½¤ë§ˆ êµ¬ë¶„)
    - knowledge_container_id, metadata_json
    - del_yn, created_by, created_date, last_modified_*
- ì¸ë±ìŠ¤:
    - IVFFlat: idx_vs_doc_chunks_embedding on chunk_embedding
    - BTree: file_bss_info_sno, knowledge_container_id, del_yn, page_number
- ìš©ë„: ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ì˜ ë²¡í„° ì†ŒìŠ¤(í…ìŠ¤íŠ¸ ì²­í¬)

í˜„ì¬ ìš´ìš© DDL ê°œìš” (ìš”ì•½)
```sql
-- í…ìŠ¤íŠ¸ ì²­í¬ ê¸°ë°˜ ì„ë² ë”© ì €ì¥ (pgvector)
CREATE TABLE vs_doc_contents_chunks (
    chunk_sno INTEGER PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
    file_bss_info_sno INTEGER NOT NULL,
    chunk_index INTEGER NOT NULL,
    chunk_text TEXT NOT NULL,
    chunk_size INTEGER NOT NULL,
    chunk_embedding vector(#{settings.vector_dimension}), -- pgvector
    page_number INTEGER,
    section_title VARCHAR(200),
    keywords TEXT,
    named_entities TEXT,
    knowledge_container_id VARCHAR(50),
    metadata_json TEXT,
    del_yn CHAR(1) NOT NULL DEFAULT 'N',
    created_by VARCHAR(50),
    created_date TIMESTAMPTZ DEFAULT now(),
    last_modified_by VARCHAR(50),
    last_modified_date TIMESTAMPTZ DEFAULT now()
);

-- ì¸ë±ìŠ¤ (ë“€ì–¼ ì¸ë±ìŠ¤ êµ¬ì¡°ì˜ ë²¡í„° ì¸¡)
CREATE INDEX idx_vs_doc_chunks_embedding ON vs_doc_contents_chunks USING ivfflat (chunk_embedding vector_cosine_ops);
CREATE INDEX idx_vs_doc_chunks_file_sno ON vs_doc_contents_chunks(file_bss_info_sno);
CREATE INDEX idx_vs_doc_chunks_container_id ON vs_doc_contents_chunks(knowledge_container_id);
CREATE INDEX idx_vs_doc_chunks_del_yn ON vs_doc_contents_chunks(del_yn);
CREATE INDEX idx_vs_doc_chunks_page_number ON vs_doc_contents_chunks(page_number);
```

2) í†µí•© í‚¤ì›Œë“œ/ë¬¸ì„œ ì „ë¬¸ ê²€ìƒ‰ í…Œì´ë¸”: `tb_document_search_index` (TSVECTOR ì‚¬ìš©)
- ì¶œì²˜: `backend/app/models/document/unified_search_models.py` (TbDocumentSearchIndex)
- ì£¼ìš” ì»¬ëŸ¼:
    - search_doc_id (PK), file_bss_info_sno(FK), knowledge_container_id(FK)
    - document_title, full_content, content_summary
    - keywords, proper_nouns, corp_names, main_topics (ARRAY)
    - document_type, page_count, content_length, language_code
    - keyword_tsvector, content_tsvector (TSVECTOR)
    - access_level, is_public, indexing_status, last_updated ë“± ìš´ì˜ í•„ë“œ
- ì¸ë±ìŠ¤:
    - GIN: content_tsvector, keyword_tsvector, keywords, proper_nouns, main_topics
    - BTree: container_id+document_type, file_bss_info_sno+access_level, last_updated, indexing_status
- ìš©ë„: í‚¤ì›Œë“œ/ì „ë¬¸/ì†ì„± ê¸°ë°˜ ê²€ìƒ‰. ì„ë² ë”©ì€ í¬í•¨í•˜ì§€ ì•ŠìŒ

í˜„ì¬ ìš´ìš© DDL ê°œìš” (ìš”ì•½)
```sql
-- ë¬¸ì„œ/í‚¤ì›Œë“œ ì¤‘ì‹¬ ê²€ìƒ‰ ì¸ë±ìŠ¤ (TSVECTOR + GIN/BTree)
CREATE TABLE tb_document_search_index (
    search_doc_id INTEGER PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
    file_bss_info_sno INTEGER NOT NULL REFERENCES tb_file_bss_info(file_bss_info_sno),
    knowledge_container_id VARCHAR(50) NOT NULL REFERENCES tb_knowledge_containers(container_id),
    document_title VARCHAR(500),
    full_content TEXT NOT NULL,
    content_summary TEXT,
    keywords TEXT[],
    proper_nouns TEXT[],
    corp_names TEXT[],
    main_topics TEXT[],
    document_type VARCHAR(50),
    page_count INTEGER,
    content_length INTEGER,
    language_code VARCHAR(10) NOT NULL DEFAULT 'ko',
    keyword_tsvector TSVECTOR,
    content_tsvector TSVECTOR,
    search_weight INTEGER NOT NULL DEFAULT 1,
    access_level VARCHAR(20) NOT NULL DEFAULT 'normal',
    is_public BOOLEAN NOT NULL DEFAULT false,
    indexing_status VARCHAR(20) NOT NULL DEFAULT 'indexed',
    last_searched_at TIMESTAMPTZ,
    search_count INTEGER NOT NULL DEFAULT 0,
    created_date TIMESTAMPTZ NOT NULL DEFAULT now(),
    last_updated TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- ì¸ë±ìŠ¤ (ë“€ì–¼ ì¸ë±ìŠ¤ êµ¬ì¡°ì˜ í‚¤ì›Œë“œ/ì „ë¬¸ ì¸¡)
CREATE INDEX idx_search_content_tsvector ON tb_document_search_index USING gin (content_tsvector);
CREATE INDEX idx_search_keyword_tsvector ON tb_document_search_index USING gin (keyword_tsvector);
CREATE INDEX idx_search_keywords ON tb_document_search_index USING gin (keywords);
CREATE INDEX idx_search_proper_nouns ON tb_document_search_index USING gin (proper_nouns);
CREATE INDEX idx_search_topics ON tb_document_search_index USING gin (main_topics);
CREATE INDEX idx_search_container_type ON tb_document_search_index(knowledge_container_id, document_type);
CREATE INDEX idx_search_file_access ON tb_document_search_index(file_bss_info_sno, access_level);
CREATE INDEX idx_search_updated ON tb_document_search_index(last_updated);
CREATE INDEX idx_search_status ON tb_document_search_index(indexing_status);
```

ë“€ì–¼ ì¸ë±ìŠ¤ êµ¬ì¡° ìš”ì•½
- ì˜ë¯¸ ê²€ìƒ‰: pgvector IVFFlat ì¸ë±ìŠ¤(ì²­í¬ ì„ë² ë”©)
- í‚¤ì›Œë“œ/ì „ë¬¸ ê²€ìƒ‰: GIN(BTree ë³´ì¡°) ì¸ë±ìŠ¤(TSVECTOR/ë°°ì—´/ì†ì„±)
- ì¿¼ë¦¬ ê³„ì¸µì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ(í‚¤ì›Œë“œ+ì˜ë¯¸) ê²°í•© ë° ì¬ë­í‚¹ ì§€ì›

ì •ë¦¬í•˜ë©´, í˜„ì¬ëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ ì²­í¬ ì„ë² ë”©(`vs_doc_contents_chunks`)ê³¼ ë¬¸ì„œ/í‚¤ì›Œë“œ ê²€ìƒ‰(`tb_document_search_index`)ì´ ë¶„ë¦¬ëœ ë“€ì–¼ ì¸ë±ìŠ¤ êµ¬ì¡°ì…ë‹ˆë‹¤. ì•„ì§ ì´ë¯¸ì§€/í‘œ/ë ˆì´ì•„ì›ƒ ë“±ì˜ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ì€ ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### 6.2 ì¸ë±ìŠ¤ ìµœì í™”

```python
class VectorIndexOptimizer:
    async def optimize_vector_index(self, table_name: str):
        """ë²¡í„° ì¸ë±ìŠ¤ ìµœì í™”"""
        # IVFFlat ì¸ë±ìŠ¤ ìµœì í™”
        await self.db.execute(f"""
            REINDEX INDEX idx_{table_name}_content_vector;
            REINDEX INDEX idx_{table_name}_main_vector;
        """)
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ì¡°ì • (ë°ì´í„° í¬ê¸°ì— ë”°ë¼)
        row_count = await self.get_table_row_count(table_name)
        clusters = max(16, row_count // 1000)
        
        await self.db.execute(f"""
            SET ivfflat.probes = {min(clusters // 2, 10)};
        """)
```

### 6.3 ë©€í‹°ëª¨ë‹¬ ìŠ¤í‚¤ë§ˆ ì œì•ˆ (í†µí•©í˜• ê¶Œì¥)

ë©€í‹°ëª¨ë‹¬ RAGë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€/í‘œ/ë ˆì´ì•„ì›ƒì„ ë™ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” í†µí•© ìŠ¤í‚¤ë§ˆë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ëª¨ë“  ì„ë² ë”©ì€ ë™ì¼ ì°¨ì›(settings.vector_dimension, ê¶Œì¥ 1024)ìœ¼ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤.

```sql
-- í†µí•© ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© í…Œì´ë¸” (ê¶Œì¥)
CREATE TABLE vs_multimodal_embeddings (
        embedding_id      BIGSERIAL PRIMARY KEY,
        file_bss_info_sno INTEGER NOT NULL REFERENCES tb_file_bss_info(file_bss_info_sno),
        knowledge_container_id VARCHAR(50) NOT NULL REFERENCES tb_knowledge_containers(container_id),

        -- ìœ„ì¹˜/ì‹ë³„
        chunk_index       INTEGER,
        page_number       INTEGER,
        section_title     VARCHAR(200),

        -- ëª¨ë‹¬ë¦¬í‹° ë° ë‚´ìš©
        modality          VARCHAR(20) NOT NULL CHECK (modality IN ('text','image','table','layout','video')),
        content_text      TEXT,           -- í…ìŠ¤íŠ¸/í…Œì´ë¸” ìš”ì•½ í…ìŠ¤íŠ¸ ë˜ëŠ” ë¹„ë””ì˜¤ ASR ìš”ì•½ í…ìŠ¤íŠ¸
        content_uri       TEXT,           -- ì´ë¯¸ì§€/ë„í˜•/ë¹„ë””ì˜¤/ì˜¤ë””ì˜¤/ì¸ë„¤ì¼ ë“± S3 ê²½ë¡œ ë˜ëŠ” URI
        bbox_json         JSONB,          -- {x,y,w,h,page} í˜•ì‹ì˜ ë°”ìš´ë”©ë°•ìŠ¤ (ë¹„ë””ì˜¤ í‚¤í”„ë ˆì„ì—ëŠ” í•´ë‹¹ í”„ë ˆì„ì˜ bbox)
        metadata_json     JSONB,

        -- ë¹„ë””ì˜¤ ì „ìš©(ì„ íƒ) í•„ë“œ
        start_time_sec    DOUBLE PRECISION,  -- ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ì‹œê°„(ì´ˆ)
        end_time_sec      DOUBLE PRECISION,  -- ì„¸ê·¸ë¨¼íŠ¸ ì¢…ë£Œ ì‹œê°„(ì´ˆ)
        fps               DOUBLE PRECISION,  -- ì›ë³¸ FPS
        frame_sample_rate DOUBLE PRECISION,  -- ìƒ˜í”Œë§ í”„ë ˆì„ ê°„ê²©(ì´ˆ ê¸°ë°˜ ë˜ëŠ” fps ëŒ€ë¹„ ë¹„ìœ¨)
        duration_sec      DOUBLE PRECISION,  -- ì›ë³¸ ë¹„ë””ì˜¤ ì „ì²´ ê¸¸ì´(ì´ˆ)
        thumbnail_uri     TEXT,              -- ëŒ€í‘œ ì¸ë„¤ì¼(í‚¤í”„ë ˆì„) URI
        transcript_uri    TEXT,              -- ì „ì²´/ì„¸ê·¸ë¨¼íŠ¸ ASR ìë§‰(ì˜ˆ: vtt/srt/json) URI

        -- ì„ë² ë”© (ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° ë™ì¼ ì°¨ì› ì‚¬ìš©)
        embedding         vector(1024),   -- settings.vector_dimension ì— ë§ì¶¤
        provider          VARCHAR(50),    -- clip/open-clip/openai/bedrock ë“±
        quality_score     FLOAT,

        -- ìš´ì˜ í•„ë“œ
        del_yn            CHAR(1) NOT NULL DEFAULT 'N',
        created_by        VARCHAR(50),
        created_date      TIMESTAMPTZ NOT NULL DEFAULT now(),
        last_modified_by  VARCHAR(50),
        last_modified_date TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_mme_container ON vs_multimodal_embeddings(knowledge_container_id);
CREATE INDEX idx_mme_file ON vs_multimodal_embeddings(file_bss_info_sno);
CREATE INDEX idx_mme_page ON vs_multimodal_embeddings(page_number);
CREATE INDEX idx_mme_modality ON vs_multimodal_embeddings(modality);
CREATE INDEX idx_mme_embedding ON vs_multimodal_embeddings USING ivfflat (embedding vector_cosine_ops);

-- ë¹„ë””ì˜¤ ì „ìš© íŒŒì…œ ì¸ë±ìŠ¤ (ì‹œê°„ ë²”ìœ„ ë° íŒŒì¼ ê¸°ë°˜ íƒìƒ‰ ìµœì í™”)
CREATE INDEX idx_mme_video_time ON vs_multimodal_embeddings(start_time_sec, end_time_sec)
    WHERE modality = 'video';
CREATE INDEX idx_mme_video_file_time ON vs_multimodal_embeddings(file_bss_info_sno, start_time_sec)
    WHERE modality = 'video';
```

íŠ¹ì§•
- ë‹¨ì¼ í…Œì´ë¸”ì—ì„œ ëª¨ë‹¬ë¦¬í‹° í•„í„°ë§Œìœ¼ë¡œ ë‹¤ì–‘í•œ ì§ˆì˜ ê°€ëŠ¥ (ì˜ˆ: modality IN ('text','image','video'))
- ë™ì¼ ì°¨ì› ìœ ì§€ë¡œ í¬ë¡œìŠ¤ëª¨ë‹¬ ê²€ìƒ‰/ì¬ë­í‚¹ ìš©ì´
- ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì¤€ì˜ ì‹œê°„ ë²”ìœ„(start/end) ì¡°íšŒ ë° íŒŒì¼+ì‹œê°„ ê¸°ë°˜ íƒìƒ‰ ìµœì í™” ê°€ëŠ¥
- ASR ìë§‰ íŒŒì¼/ì¸ë„¤ì¼/í‚¤í”„ë ˆì„ ë“± ì™¸ë¶€ ì•„í‹°íŒ©íŠ¸ì™€ ëŠìŠ¨í•œ ì—°ê²°(transcript_uri/thumbnail_uri) ì œê³µ
- ê¸°ì¡´ í…ìŠ¤íŠ¸ ì²­í¬(`vs_doc_contents_chunks`)ëŠ” ë‹¨ê³„ì ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ read-only ì „í™˜ ê°€ëŠ¥

### 6.4 ëŒ€ì•ˆ: ëª¨ë‹¬ë¦¬í‹° ë¶„ë¦¬í˜• ìŠ¤í‚¤ë§ˆ (ìƒì´í•œ ì°¨ì› í•„ìš” ì‹œ)

ì„ë² ë”© ì°¨ì›ì´ ëª¨ë‹¬ë¦¬í‹°ë³„ë¡œ ë‹¤ë¥¼ ê²½ìš° í…Œì´ë¸”ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤. ì˜ˆì‹œëŠ” ì´ë¯¸ì§€ ì „ìš© í…Œì´ë¸”ì…ë‹ˆë‹¤.

```sql
-- ì´ë¯¸ì§€ ì „ìš© ì„ë² ë”© (ì˜ˆ: 768ì°¨ì›)
CREATE TABLE vs_image_regions (
    region_id BIGSERIAL PRIMARY KEY,
    file_bss_info_sno INTEGER NOT NULL REFERENCES tb_file_bss_info(file_bss_info_sno),
    knowledge_container_id VARCHAR(50) NOT NULL REFERENCES tb_knowledge_containers(container_id),
    page_number INTEGER,
    bbox_json JSONB,
    image_uri TEXT,
    embedding vector(768),           -- settings.vector_image_dimension ì œì•ˆ
    provider VARCHAR(50),
    quality_score FLOAT,
    created_date TIMESTAMPTZ NOT NULL DEFAULT now()
);

CREATE INDEX idx_vsir_container ON vs_image_regions(knowledge_container_id);
CREATE INDEX idx_vsir_file ON vs_image_regions(file_bss_info_sno);
CREATE INDEX idx_vsir_page ON vs_image_regions(page_number);
CREATE INDEX idx_vsir_embedding ON vs_image_regions USING ivfflat (embedding vector_cosine_ops);
```

ë™ì¼ ë°©ì‹ìœ¼ë¡œ `vs_table_chunks`, `vs_layout_blocks` ë“±ì„ ì •ì˜í•©ë‹ˆë‹¤. ì¿¼ë¦¬ ê³„ì¸µì—ì„œëŠ” UNION ALL í˜¹ì€ ë©€í‹° ì†ŒìŠ¤ ì§‘ê³„ë¥¼ í†µí•´ í†µí•© ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤.

### 6.5 ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ (í˜„í–‰ â†’ ë©€í‹°ëª¨ë‹¬)

1) ìŠ¤í‚¤ë§ˆ ì¶”ê°€
- í†µí•©í˜•ì„ ì„ íƒ ì‹œ: `vs_multimodal_embeddings` ìƒì„± (CONCURRENTLY ì¸ë±ìŠ¤ëŠ” ìƒì„± ì‹œì  ì œì•½ìœ¼ë¡œ ë³„ë„ ë‹¨ê³„ ìˆ˜í–‰)
- ë¶„ë¦¬í˜•ì„ ì„ íƒ ì‹œ: ëª¨ë‹¬ë¦¬í‹°ë³„ í…Œì´ë¸” ìƒì„± (image/table/layout)

2) í…ìŠ¤íŠ¸ ë°±í•„
- ê¸°ì¡´ `vs_doc_contents_chunks` â†’ ìƒˆë¡œìš´ í…Œì´ë¸”ë¡œ ì´ê´€
- ë§¤í•‘: chunk_text â†’ content_text, chunk_embedding â†’ embedding, file/ì»¨í…Œì´ë„ˆ/page/section ë™ì¼ ë³µì‚¬

3) ì´ë¯¸ì§€/í‘œ ì„ë² ë”© ìƒì„±
- íŒŒì´í”„ë¼ì¸ì—ì„œ í˜ì´ì§€/ì˜ì—­ ë‹¨ìœ„ crop ìƒì„± í›„ S3 ì €ì¥ â†’ content_uri ê¸°ë¡, bbox_json í¬í•¨
- ì„ë² ë”© ëª¨ë¸ì€ í†µí•©í˜•ì˜ ê²½ìš° í†µì¼ëœ ì°¨ì› ì¶œë ¥ ëª¨ë¸(ì˜ˆ: OpenCLIP 1024d) ì±„íƒ

4) ì¸ë±ìŠ¤ ìƒì„± ë° íŠœë‹
- IVFFlat ì¸ë±ìŠ¤ ìƒì„±, probes ì„¤ì •(ì›Œí¬ë¡œë“œì— ë§ì¶° 5~10 ê¶Œì¥), í…Œì´ë¸” í¬ê¸°ì— ë§ì¶˜ lists ì„¤ì •

5) ë‹¨ê³„ì  ì „í™˜ (Feature Flag)
- ê²€ìƒ‰/ë¦¬ë­í‚¹ ì¿¼ë¦¬ë¥¼ ì‹ ê·œ í…Œì´ë¸” ì°¸ì¡°ë¡œ ì ì§„ ì „í™˜
- ì´ˆê¸°ì—” dual-read(ì‹ ê·œ+êµ¬ í…Œì´ë¸”) ë¹„êµ ê²€ì¦ â†’ ì•ˆì •í™” í›„ êµ¬ í…Œì´ë¸” ì½ê¸° ê²½ë¡œ ì œê±°

6) ì •ë¦¬
- ì¶©ë¶„í•œ ê´€ì¸¡ ê¸°ê°„ í›„ `vs_doc_contents_chunks`ë¥¼ ë³´ì¡´/íê¸° ì •ì±…ì— ë”°ë¼ ì²˜ë¦¬
- `tb_document_search_index`ëŠ” í‚¤ì›Œë“œ/ì „ë¬¸ ê²€ìƒ‰ìœ¼ë¡œ ìœ ì§€í•˜ë©°, ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ê³¼ í•˜ì´ë¸Œë¦¬ë“œ(í‚¤ì›Œë“œ+ì˜ë¯¸) ì¡°í•© ê¶Œì¥

## 7. í’ˆì§ˆ ê´€ë¦¬ ë° ê²€ì¦

### 7.1 ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦

```python
class DocumentQualityController:
    def validate_document_quality(self, content: str, metadata: dict) -> dict:
        """ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦"""
        quality_metrics = {
            "content_length_score": self.score_content_length(content),
            "language_consistency_score": self.score_language_consistency(content),
            "keyword_density_score": self.score_keyword_density(content, metadata),
            "structure_score": self.score_document_structure(content),
            "uniqueness_score": self.score_content_uniqueness(content)
        }
        
        overall_score = sum(quality_metrics.values()) / len(quality_metrics)
        
        return {
            "quality_score": overall_score,
            "quality_metrics": quality_metrics,
            "is_acceptable": overall_score >= 0.6
        }
    
    def detect_duplicate_content(self, content: str, 
                               similarity_threshold: float = 0.95) -> bool:
        """ì¤‘ë³µ ì»¨í…ì¸  ê²€ì¶œ"""
        content_hash = hashlib.sha256(content.encode()).hexdigest()
        
        # ê¸°ì¡´ ë¬¸ì„œì™€ ìœ ì‚¬ë„ ë¹„êµ
        existing_docs = self.search_similar_content(content, threshold=similarity_threshold)
        
        return len(existing_docs) > 0
```

### 7.2 ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì²˜ë¦¬

```python
class DocumentProcessingErrorHandler:
    async def handle_processing_error(self, file_path: str, error: Exception):
        """ì²˜ë¦¬ ì˜¤ë¥˜ í•¸ë“¤ë§"""
        error_info = {
            "file_path": file_path,
            "error_type": type(error).__name__,
            "error_message": str(error),
            "timestamp": datetime.now(),
            "retry_count": 0
        }
        
        # ì˜¤ë¥˜ ë¡œê¹…
        await self.log_processing_error(error_info)
        
        # ì¬ì²˜ë¦¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ì¸ì§€ íŒë‹¨
        if self.is_retryable_error(error):
            await self.schedule_retry(file_path, error_info)
        else:
            await self.mark_as_failed(file_path, error_info)
```

## 8. í†µí•© ë¬¸ì„œ ê´€ë¦¬ API ì„¤ê³„ (v1.0 ë¦¬íŒ©í† ë§)

### 8.1 ë©”ì¸ ë¬¸ì„œ ì—…ë¡œë“œ API (documents.py)

```python
@router.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    container_id: Optional[str] = Form(None),
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° ì²˜ë¦¬ (í”„ë¡ íŠ¸ì—”ë“œ ë©”ì¸ ì‚¬ìš©)"""
    try:
        # 1. ê¶Œí•œ í™•ì¸
        if container_id:
            has_permission = await permission_service.check_upload_permission(
                user_id=current_user.id,
                container_id=container_id,
                session=session
            )
            if not has_permission:
                raise HTTPException(status_code=403, detail="ì—…ë¡œë“œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # 2. íŒŒì¼ í¬ê¸° ë° í˜•ì‹ ê²€ì¦
        if not file.filename:
            raise HTTPException(status_code=400, detail="íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        file_extension = Path(file.filename).suffix.lower()
        if file_extension not in ALLOWED_FILE_TYPES:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
        
        # 3. íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ì²˜ë¦¬ ì „ëµ ê²°ì •
        file_size = file.size or 0
        processing_strategy = determine_processing_strategy(file_size)
        
        if processing_strategy in ["large", "xlarge"]:
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬
            upload_id = str(uuid.uuid4())
            background_tasks.add_task(
                process_large_document,
                file, container_id, upload_id, current_user.id
            )
            return DocumentUploadResponse(
                upload_id=upload_id,
                status="processing",
                message="ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
            )
        else:
            # ì¼ë°˜ íŒŒì¼ì€ ì¦‰ì‹œ ì²˜ë¦¬
            result = await document_service.process_document_immediate(
                file=file,
                container_id=container_id,
                user_id=current_user.id,
                session=session
            )
            
            return DocumentUploadResponse(
                document_id=result["document_id"],
                status="completed",
                message="ë¬¸ì„œ ì—…ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                processing_info=result["processing_info"]
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
```

### 8.2 ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ API (files.py í†µí•©)

```python
@router.post("/files/large-upload")
async def upload_large_file(
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    current_user: User = Depends(get_current_user)
):
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ (ìµœëŒ€ 100MB)"""
    try:
        # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
        if not file.filename:
            raise HTTPException(status_code=400, detail="íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        file_extension = Path(file.filename).suffix.lower()
        if file_extension not in settings.allowed_file_types:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {settings.allowed_file_types}"
            )
        
        file_size = file.size or 0
        if file_size > settings.max_file_size:
            raise HTTPException(
                status_code=413, 
                detail=f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ í¬ê¸°: {settings.max_file_size/(1024*1024):.0f}MB"
            )
        
        # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬
        result = await large_file_processor.process_large_file_streaming(
            file, background_tasks
        )
        
        logger.info(f"ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘: {file.filename} ({file_size} bytes) by {current_user.username}")
        
        return {
            "message": "ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "upload_id": result.get("upload_id"),
            "filename": file.filename,
            "file_size": file_size,
            "processing_status": result.get("status", "processing")
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
```

### 8.3 ì—…ë¡œë“œ ì§„í–‰ë¥  ë° ìƒíƒœ ê´€ë¦¬ API

```python
@router.get("/upload-progress/{upload_id}")
async def get_upload_progress(
    upload_id: str,
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """ì—…ë¡œë“œ ì§„í–‰ë¥  ì¡°íšŒ"""
    try:
        progress_info = await document_service.get_upload_progress(
            upload_id=upload_id,
            user_id=current_user.id,
            session=session
        )
        
        if not progress_info:
            raise HTTPException(status_code=404, detail="ì—…ë¡œë“œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return UploadProgress(
            upload_id=upload_id,
            status=progress_info["status"],
            progress=progress_info["progress"],
            estimated_time=progress_info.get("estimated_time"),
            error_message=progress_info.get("error_message")
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì—…ë¡œë“œ ì§„í–‰ë¥  ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ì§„í–‰ë¥  ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.delete("/files/upload/{upload_id}")
async def cancel_upload(
    upload_id: str,
    current_user: User = Depends(get_current_user)
):
    """ì—…ë¡œë“œ ì·¨ì†Œ"""
    try:
        result = await large_file_processor.cancel_upload(upload_id)
        
        logger.info(f"ì—…ë¡œë“œ ì·¨ì†Œ: {upload_id} by {current_user.username}")
        
        return {
            "message": "ì—…ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "upload_id": upload_id,
            "status": "cancelled"
        }
        
    except Exception as e:
        logger.error(f"ì—…ë¡œë“œ ì·¨ì†Œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ì—…ë¡œë“œ ì·¨ì†Œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
```

### 8.4 ë¬¸ì„œ ëª©ë¡ ë° ê´€ë¦¬ API

```python
@router.get("/list")
async def get_document_list(
    container_id: Optional[str] = Query(None),
    page: int = Query(1, ge=1),
    limit: int = Query(20, ge=1, le=100),
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (ê¶Œí•œ ê¸°ë°˜ í•„í„°ë§)"""
    try:
        documents = await document_service.get_user_documents(
            user_id=current_user.id,
            container_id=container_id,
            page=page,
            limit=limit,
            session=session
        )
        
        return DocumentListResponse(
            documents=documents["items"],
            total_count=documents["total"],
            page=page,
            limit=limit,
            has_next=documents["has_next"]
        )
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.delete("/{document_id}")
async def delete_document(
    document_id: str,
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_db)
):
    """ë¬¸ì„œ ì‚­ì œ (ê¶Œí•œ í™•ì¸)"""
    try:
        result = await document_service.delete_document(
            document_id=document_id,
            user_id=current_user.id,
            session=session
        )
        
        logger.info(f"ë¬¸ì„œ ì‚­ì œ: {document_id} by {current_user.username}")
        
        return {
            "message": "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "document_id": document_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
```

### 8.5 API í†µí•© íš¨ê³¼

#### ğŸ¯ ë¦¬íŒ©í† ë§ ê²°ê³¼

- **documents.py**: í”„ë¡ íŠ¸ì—”ë“œ ë©”ì¸ ì‚¬ìš© API, ê¶Œí•œ ê¸°ë°˜ ë¬¸ì„œ CRUD
- **files.py**: íŒŒì¼ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ í†µí•© (file_api.py + large_file_upload.py)
- **ì¤‘ë³µ ì œê±°**: enhanced_documents.py, document_processing.py ë“± í…ŒìŠ¤íŠ¸ìš© API ì •ë¦¬

#### ğŸ“ˆ ê°œì„  ì‚¬í•­

- **ë‹¨ì¼ ì§„ì…ì **: í”„ë¡ íŠ¸ì—”ë“œëŠ” `/api/v1/documents/*` ë§Œ ì‚¬ìš©
- **ê¶Œí•œ í†µí•©**: ëª¨ë“  ë¬¸ì„œ ê´€ë¦¬ì—ì„œ ì¼ê´€ëœ ê¶Œí•œ í™•ì¸ ì²´ê³„
- **íŒŒì¼ ì²˜ë¦¬ í†µí•©**: ì¼ë°˜ íŒŒì¼ê³¼ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ë¡œì§ ë¶„ë¦¬
- **ì§„í–‰ë¥  ì¶”ì **: ì‹¤ì‹œê°„ ì—…ë¡œë“œ ìƒíƒœ ëª¨ë‹ˆí„°ë§

#### ğŸ”§ ê¸°ìˆ ì  ê°œì„ 

- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬
- **ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬**: FastAPI BackgroundTasksë¥¼ í™œìš©í•œ ë¹„ë™ê¸° ì²˜ë¦¬
- **ì—ëŸ¬ ì²˜ë¦¬**: ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ì™€ ì ì ˆí•œ HTTP ìƒíƒœ ì½”ë“œ
- **ë¡œê¹…**: ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ í™œë™ì— ëŒ€í•œ ìƒì„¸ ë¡œê·¸ ê¸°ë¡

#### ğŸš€ í–¥í›„ í™•ì¥ ê³„íš

- **ë°°ì¹˜ ì—…ë¡œë“œ**: ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì—…ë¡œë“œ ê¸°ëŠ¥
- **ë²„ì „ ê´€ë¦¬**: ë¬¸ì„œ ë²„ì „ íˆìŠ¤í† ë¦¬ ì¶”ì 
- **ìë™ ë¶„ë¥˜**: AI ê¸°ë°˜ ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
- **í’ˆì§ˆ ê²€ì¦**: ì—…ë¡œë“œëœ ë¬¸ì„œì˜ í’ˆì§ˆ ìë™ í‰ê°€

## 9. ì„±ëŠ¥ ìµœì í™”

### 9.1 ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”

- **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ íŒŒì¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- **ë°°ì¹˜ ì„ë² ë”©**: ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
- **ë³‘ë ¬ ì²˜ë¦¬**: ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ë™ì‹œ ì²˜ë¦¬
- **ìºì‹±**: ì„ë² ë”© ê²°ê³¼ Redis ìºì‹±

### 9.2 ì €ì¥ ì„±ëŠ¥ ìµœì í™”

- **ë°°ì¹˜ ì‚½ì…**: ëŒ€ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì €ì¥
- **ì¸ë±ìŠ¤ ìµœì í™”**: ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ
- **íŒŒí‹°ì…”ë‹**: ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ë¶„í•  ê´€ë¦¬

## 10. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 10.1 ì²˜ë¦¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§

```python
class DocumentProcessingMonitor:
    async def track_processing_metrics(self, task_id: str, metrics: dict):
        """ì²˜ë¦¬ ë©”íŠ¸ë¦­ ì¶”ì """
        await self.redis.hset(f"task:{task_id}", mapping={
            "status": metrics["status"],
            "progress": metrics["progress"],
            "estimated_time": metrics["estimated_time"],
            "error_count": metrics.get("error_count", 0)
        })
```

### 10.2 í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

- **ë¬¸ì„œ í’ˆì§ˆ ì ìˆ˜ ì¶”ì **
- **ì²˜ë¦¬ ì‹¤íŒ¨ìœ¨ ëª¨ë‹ˆí„°ë§**
- **ì„±ëŠ¥ ì§€í‘œ ë¶„ì„**
- **ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘**

## 11. ğŸ“š ì§€ì‹ê´€ë¦¬ì ì „ìš© ê¸°ëŠ¥

### 11.1 ë¬¸ì„œ í’ˆì§ˆ ê´€ë¦¬ ëŒ€ì‹œë³´ë“œ

#### 11.1.1 í’ˆì§ˆ ì ìˆ˜ ëª¨ë‹ˆí„°ë§

```python
# /knowledge-admin/quality/dashboard
class DocumentQualityDashboard:
    async def get_quality_overview(self, container_id: str = None):
        """ë¬¸ì„œ í’ˆì§ˆ í˜„í™© ì¡°íšŒ"""
        return {
            "total_documents": await self.count_documents(container_id),
            "quality_distribution": {
                "excellent": {"count": 1250, "percentage": 45},
                "good": {"count": 1000, "percentage": 36},
                "fair": {"count": 400, "percentage": 14},
                "poor": {"count": 130, "percentage": 5}
            },
            "recent_uploads": await self.get_recent_quality_trends(),
            "improvement_suggestions": await self.get_improvement_suggestions()
        }
        
    async def get_low_quality_documents(self, threshold: float = 0.6):
        """í’ˆì§ˆì´ ë‚®ì€ ë¬¸ì„œ ëª©ë¡"""
        return await self.db.execute(
            "SELECT file_name, quality_score, issues, upload_date "
            "FROM tb_file_bss_info WHERE quality_score < %s "
            "ORDER BY quality_score ASC", (threshold,)
        )
```

#### 11.1.2 ì¤‘ë³µ ë¬¸ì„œ íƒì§€ ë° ê´€ë¦¬

```python
class DuplicateDocumentManager:
    async def detect_duplicates(self, similarity_threshold: float = 0.95):
        """ì¤‘ë³µ ë¬¸ì„œ íƒì§€"""
        duplicates = await self.vector_service.find_similar_documents(
            threshold=similarity_threshold,
            group_by_similarity=True
        )
        
        return {
            "duplicate_groups": [
                {
                    "similarity_score": 0.98,
                    "documents": [
                        {"file_id": "f001", "name": "íšŒì‚¬ê·œì •_v1.pdf", "upload_date": "2024-01-15"},
                        {"file_id": "f115", "name": "íšŒì‚¬ê·œì •_ìµœì‹ .pdf", "upload_date": "2024-03-20"}
                    ],
                    "recommended_action": "merge_keep_latest"
                }
            ],
            "total_duplicate_groups": 23,
            "estimated_storage_savings": "1.2GB"
        }
        
    async def merge_duplicate_documents(self, primary_id: str, duplicate_ids: List[str]):
        """ì¤‘ë³µ ë¬¸ì„œ ë³‘í•©"""
        # ì£¼ ë¬¸ì„œë¡œ í†µí•©í•˜ê³  ì¤‘ë³µ ë¬¸ì„œëŠ” ì°¸ì¡°ë¡œ ë³€ê²½
        pass
```

#### 11.1.3 ë¬¸ì„œ ë¶„ë¥˜ ë° íƒœê¹… ê´€ë¦¬

```python
class DocumentClassificationManager:
    async def review_auto_classifications(self, pending_only: bool = True):
        """ìë™ ë¶„ë¥˜ ê²°ê³¼ ê²€í† """
        return {
            "pending_classifications": [
                {
                    "document_id": "d001",
                    "file_name": "2024ë…„ ì‚¬ì—…ê³„íšì„œ.docx",
                    "predicted_category": "ê²½ì˜ì „ëµ",
                    "confidence": 0.85,
                    "suggested_tags": ["ì‚¬ì—…ê³„íš", "2024ë…„", "ì „ëµê¸°íš"],
                    "review_required": True
                }
            ],
            "classification_accuracy": 0.92,
            "manual_review_rate": 0.15
        }
        
    async def update_classification(self, doc_id: str, category: str, tags: List[str]):
        """ë¶„ë¥˜ ì •ë³´ ìˆ˜ì •"""
        await self.db.execute(
            "UPDATE tb_file_bss_info SET category = %s, tags = %s, "
            "classification_reviewed = TRUE WHERE file_bss_info_sno = %s",
            (category, json.dumps(tags), doc_id)
        )
```

### 11.2 ë¬¸ì„œ ì²˜ë¦¬ ê´€ë¦¬

#### 11.2.1 ì²˜ë¦¬ ì‹¤íŒ¨ ë¬¸ì„œ ì¬ì²˜ë¦¬

```python
class DocumentReprocessingManager:
    async def get_failed_documents(self, error_type: str = None):
        """ì²˜ë¦¬ ì‹¤íŒ¨ ë¬¸ì„œ ëª©ë¡"""
        query = """
        SELECT file_bss_info_sno, file_logical_name, error_type, 
               error_message, failed_at, retry_count
        FROM tb_file_processing_errors 
        WHERE status = 'failed'
        """
        if error_type:
            query += " AND error_type = %s"
            
        return await self.db.fetch_all(query, (error_type,) if error_type else ())
        
    async def retry_document_processing(self, file_ids: List[str]):
        """ë¬¸ì„œ ì¬ì²˜ë¦¬ ì‹¤í–‰"""
        for file_id in file_ids:
            task = await self.task_queue.enqueue(
                "reprocess_document",
                file_id=file_id,
                priority="high"
            )
            
        return {"reprocessing_tasks": len(file_ids), "estimated_time": "15-30ë¶„"}
```

#### 11.2.2 ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§

```python
class LargeFileProcessingMonitor:
    async def get_processing_queue_status(self):
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ í ìƒíƒœ"""
        return {
            "queue_status": {
                "pending": 12,
                "processing": 3,
                "completed_today": 45,
                "failed_today": 2
            },
            "processing_capacity": {
                "current_workers": 8,
                "max_workers": 12,
                "cpu_usage": "65%",
                "memory_usage": "4.2GB / 16GB"
            },
            "estimated_wait_times": {
                "small_files": "5ë¶„",
                "medium_files": "15ë¶„", 
                "large_files": "45ë¶„"
            }
        }
```

### 11.3 ì‚¬ìš©ì ì§€ì› ê¸°ëŠ¥

#### 11.3.1 ì—…ë¡œë“œ ë¬¸ì˜ ê´€ë¦¬

```python
class DocumentUploadSupport:
    async def get_user_inquiries(self, status: str = "open"):
        """ì‚¬ìš©ì ë¬¸ì˜ ëª©ë¡"""
        return {
            "inquiries": [
                {
                    "inquiry_id": "INQ001",
                    "user_name": "ê¹€ì§€ì‹",
                    "department": "ê¸°íšíŒ€",
                    "file_name": "market_analysis_2024.xlsx",
                    "issue_type": "upload_failed",
                    "description": "100MB ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì¤‘ë‹¨ë¨",
                    "submitted_at": "2024-03-20 14:30",
                    "priority": "high"
                }
            ],
            "inquiry_stats": {
                "total_open": 8,
                "resolved_today": 5,
                "avg_resolution_time": "2.5ì‹œê°„"
            }
        }
        
    async def resolve_inquiry(self, inquiry_id: str, resolution: str):
        """ë¬¸ì˜ í•´ê²° ì²˜ë¦¬"""
        await self.db.execute(
            "UPDATE tb_user_inquiries SET status = 'resolved', "
            "resolution = %s, resolved_at = NOW() WHERE inquiry_id = %s",
            (resolution, inquiry_id)
        )
```

#### 11.3.2 ì—…ë¡œë“œ ê°€ì´ë“œ ê´€ë¦¬

```python
class UploadGuideManager:
    async def update_upload_guidelines(self, guidelines: dict):
        """ì—…ë¡œë“œ ê°€ì´ë“œë¼ì¸ ì—…ë°ì´íŠ¸"""
        return {
            "file_size_limits": {
                "standard": "20MB",
                "large_file": "100MB", 
                "special_request": "500MB"
            },
            "supported_formats": {
                "documents": [".pdf", ".docx", ".hwp", ".txt"],
                "spreadsheets": [".xlsx", ".xls", ".csv"],
                "presentations": [".pptx", ".ppt"]
            },
            "processing_tips": [
                "í•œê¸€ íŒŒì¼ì€ .hwpx í˜•ì‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤",
                "20MB ì´ìƒ íŒŒì¼ì€ ìë™ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ë©ë‹ˆë‹¤",
                "OCRì´ í•„ìš”í•œ ì´ë¯¸ì§€ëŠ” 300dpi ì´ìƒì„ ê¶Œì¥í•©ë‹ˆë‹¤"
            ]
        }
```

### 11.4 í’ˆì§ˆ ê°œì„  ë„êµ¬

#### 11.4.1 í‚¤ì›Œë“œ ë° íƒœê·¸ ê´€ë¦¬

```python
class KeywordTagManager:
    async def get_keyword_analytics(self, period: str = "30d"):
        """í‚¤ì›Œë“œ ë¶„ì„"""
        return {
            "popular_keywords": [
                {"keyword": "ì‚¬ì—…ê³„íš", "frequency": 245, "trend": "+15%"},
                {"keyword": "ì‹œì¥ë¶„ì„", "frequency": 189, "trend": "+8%"},
                {"keyword": "í”„ë¡œì íŠ¸", "frequency": 167, "trend": "-5%"}
            ],
            "new_keywords": ["ë””ì§€í„¸íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜", "ESGê²½ì˜", "íƒ„ì†Œì¤‘ë¦½"],
            "tag_effectiveness": {
                "search_improvement": "+23%",
                "categorization_accuracy": "94%"
            }
        }
        
    async def suggest_tag_improvements(self, document_id: str):
        """íƒœê·¸ ê°œì„  ì œì•ˆ"""
        content = await self.get_document_content(document_id)
        suggestions = await self.ai_service.extract_keywords(content)
        
        return {
            "current_tags": ["ê¸°íš", "ì „ëµ"],
            "suggested_additions": ["ë””ì§€í„¸í˜ì‹ ", "ê³ ê°ê²½í—˜", "ë°ì´í„°ë¶„ì„"],
            "confidence_scores": [0.92, 0.87, 0.81]
        }
```

## 12. ì‹¤ì œ êµ¬í˜„ ì‚¬ë¡€ ë° ì„±ëŠ¥ ìµœì í™”

### 12.1 íŒŒì´í”„ë¼ì¸ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```python
# ì‹¤ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ (100ê°œ ë¬¸ì„œ ê¸°ì¤€)
PIPELINE_PERFORMANCE = {
    'opensource': {
        'avg_processing_time': '45ì´ˆ',
        'accuracy': '85%',
        'cost_per_document': '$0',
        'korean_text_quality': '92%',
        'table_extraction': '70%',
        'suitable_for': ['ëŒ€ìš©ëŸ‰ ë°°ì¹˜', 'ë¹„ìš© ë¯¼ê°', 'í•œêµ­ì–´ ì¤‘ì‹¬']
    },
    'azure': {
        'avg_processing_time': '25ì´ˆ', 
        'accuracy': '94%',
        'cost_per_document': '$0.002',
        'korean_text_quality': '88%',
        'table_extraction': '95%',
        'suitable_for': ['ë³µì¡í•œ ë¬¸ì„œ', 'ë†’ì€ ì •í™•ë„', 'êµ¬ì¡°í™” ë°ì´í„°']
    },
    'aws': {
        'avg_processing_time': '30ì´ˆ',
        'accuracy': '92%', 
        'cost_per_document': '$0.003',
        'korean_text_quality': '86%',
        'table_extraction': '93%',
        'suitable_for': ['ëŒ€ìš©ëŸ‰ ë¬¸ì„œ', 'ì–‘ì‹ ì²˜ë¦¬', 'í™•ì¥ì„±']
    }
}
```

### 12.2 í•œêµ­ì–´ ë¬¸ì„œ ì¤‘ì‹¬ ì‚¬ìš©ì ì„ íƒ ê°€ì´ë“œ

#### ğŸ‡°ğŸ‡· **í•œêµ­ì–´ ë¬¸ì„œ 80% ì´ìƒ í™˜ê²½ì—ì„œì˜ ê¶Œì¥ì‚¬í•­**

##### **1ìˆœìœ„: ì˜¤í”ˆì†ŒìŠ¤ íŒŒì´í”„ë¼ì¸ (EasyOCR ì¤‘ì‹¬)** â­â­â­â­â­

```python
# í•œêµ­ì–´ íŠ¹í™” êµ¬ì„±
KOREAN_OPTIMIZED_CONFIG = {
    'ocr_engine': 'easyocr',
    'korean_accuracy': '88-92%',
    'cpu_friendly': True,
    'cost': 'ì™„ì „ ë¬´ë£Œ',
    'setup_time': '30ë¶„',
    'maintenance': 'ë‚®ìŒ'
}
```

**ì„ íƒ ì´ìœ :**
- âœ… **í•œêµ­ì–´ ì •í™•ë„**: EasyOCR 88-92% (ì‹¤ìš©ì  ìˆ˜ì¤€)
- âœ… **CPU ì „ìš© ê°€ëŠ¥**: GPU ì—†ì´ë„ ì¶©ë¶„í•œ ì„±ëŠ¥
- âœ… **ì™„ì „ ë¬´ë£Œ**: ë¼ì´ì„ ìŠ¤ ë¹„ìš© ì—†ìŒ
- âœ… **í•œê¸€+ì˜ì–´ ë™ì‹œ**: í˜¼í•© ë¬¸ì„œ ì²˜ë¦¬ ìš°ìˆ˜
- âœ… **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: í•œêµ­ì–´ ì „ì²˜ë¦¬ ìµœì í™” ê°€ëŠ¥

##### **2ìˆœìœ„: Azure Document Intelligence** â­â­â­â­

- ë³µì¡í•œ í‘œ/ì–‘ì‹ì´ ë§ì€ ê²½ìš°
- ë†’ì€ êµ¬ì¡° ì¸ì‹ ì •í™•ë„ í•„ìš”
- ì²˜ë¦¬ëŸ‰ì´ ì¼ì¼ 1000ê±´ ì´í•˜
- ë¹„ìš© ëŒ€ë¹„ í’ˆì§ˆ ë°¸ëŸ°ìŠ¤

##### **3ìˆœìœ„: AWS Textract** â­â­â­

- ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ í•„ìš”
- AWS ìƒíƒœê³„ í†µí•© í™˜ê²½
- í™•ì¥ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°

#### ğŸ” **EasyOCR vs ê²½ìŸ ì†”ë£¨ì…˜ ìƒì„¸ ë¹„êµ**

| í‰ê°€ ê¸°ì¤€              | EasyOCR (CPU) | Tesseract   | PaddleOCR   | í´ë¼ìš°ë“œ OCR  |
| ------------------ | ------------- | ----------- | ----------- | --------- |
| **í•œêµ­ì–´ ì •í™•ë„**        | 88-92%        | 75-85%      | 90-95%      | 95-98%    |
| **ì²˜ë¦¬ ì†ë„**          | 15-30ì´ˆ/í˜ì´ì§€    | 5-15ì´ˆ/í˜ì´ì§€   | 45-90ì´ˆ/í˜ì´ì§€  | 3-10ì´ˆ/í˜ì´ì§€ |
| **CPU ì‚¬ìš©ë¥ **        | 60-80%        | 40-60%      | 80-100%     | 0% (í´ë¼ìš°ë“œ) |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©**         | 500MB-1GB     | 200MB-500MB | 1GB-2GB     | ìµœì†Œ        |
| **ì„¤ì¹˜ ë³µì¡ë„**         | â­â­â­â­â­ ë§¤ìš° ì‰¬ì›€   | â­â­â­ ë³´í†µ      | â­â­ ë³µì¡       | â­â­â­â­ ì‰¬ì›€   |
| **ë¹„ìš© (ì›” 1000í˜ì´ì§€)** | $0            | $0          | $0          | $50-100   |
| **ë°ì´í„° ë³´ì•ˆ**         | â­â­â­â­â­ ì™„ì „ ë‚´ë¶€   | â­â­â­â­â­ ì™„ì „ ë‚´ë¶€ | â­â­â­â­â­ ì™„ì „ ë‚´ë¶€ | â­â­ ì™¸ë¶€ ì „ì†¡  |

#### ğŸ’» **CPU ì„œë²„ í™˜ê²½ë³„ ê¶Œì¥ êµ¬ì„±**

##### **ì €ì‚¬ì–‘ CPU ì„œë²„ (4ì½”ì–´, 8GB RAM)**

```python
LIGHTWEIGHT_CONFIG = {
    'primary_ocr': 'tesseract',      # ë¹ ë¥¸ ì²˜ë¦¬
    'fallback_ocr': 'easyocr',       # í’ˆì§ˆ ë³´ì™„
    'pdf_parser': 'pdfminer3k',      # ë©”ëª¨ë¦¬ íš¨ìœ¨
    'parallel_workers': 2,           # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
    'image_resize_threshold': 1500   # ì´ë¯¸ì§€ í¬ê¸° ì œí•œ
}
```

##### **ì¤‘ê¸‰ CPU ì„œë²„ (8ì½”ì–´, 16GB RAM)** - **ê¶Œì¥**

```python
BALANCED_CONFIG = {
    'primary_ocr': 'easyocr',        # í•œêµ­ì–´ ìµœì í™”
    'fallback_ocr': 'tesseract',     # ì†ë„ ë³´ì™„
    'pdf_parser': 'pdfplumber',      # ê· í˜•ì¡íŒ ì„±ëŠ¥
    'parallel_workers': 4,           # ì ì • ë™ì‹œ ì²˜ë¦¬
    'korean_optimization': True      # í•œêµ­ì–´ ì „ì²˜ë¦¬ í™œì„±í™”
}
```

##### **ê³ ì‚¬ì–‘ CPU ì„œë²„ (16ì½”ì–´, 32GB RAM)**

```python
HIGH_PERFORMANCE_CONFIG = {
    'primary_ocr': 'paddleocr',      # ìµœê³  í’ˆì§ˆ
    'secondary_ocr': 'easyocr',      # ì†ë„ ë³´ì™„
    'fallback_ocr': 'tesseract',     # ì•ˆì •ì„± ë³´ì™„
    'pdf_parser': 'pymupdf',         # ê³ ì„±ëŠ¥ íŒŒì‹±
    'parallel_workers': 8,           # ìµœëŒ€ ë³‘ë ¬ ì²˜ë¦¬
    'batch_optimization': True       # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
}
```

#### ğŸ¯ **í•œêµ­ì–´ ë¬¸ì„œë³„ ìµœì  OCR ì„ íƒ**

```python
# ë¬¸ì„œ ìœ í˜•ë³„ OCR ì—”ì§„ ì„ íƒ ê°€ì´ë“œ
KOREAN_DOCUMENT_OCR_GUIDE = {
    'ì¼ë°˜_ë¹„ì¦ˆë‹ˆìŠ¤_ë¬¸ì„œ': {
        'recommended': 'easyocr',
        'accuracy_expectation': '90%+',
        'processing_time': '20-30ì´ˆ/í˜ì´ì§€',
        'notes': 'ê³„ì•½ì„œ, ë³´ê³ ì„œ, ì œì•ˆì„œ ë“± í‘œì¤€ ë¬¸ì„œ'
    },
    'ìŠ¤ìº”ëœ_í•œê¸€_ë¬¸ì„œ': {
        'recommended': 'easyocr',
        'accuracy_expectation': '85-90%',
        'processing_time': '25-35ì´ˆ/í˜ì´ì§€',
        'notes': 'ì˜¤ë˜ëœ ë¬¸ì„œ, ìŠ¤ìº” í’ˆì§ˆì´ ì¤‘ìš”'
    },
    'í‘œ_í¬í•¨_ë¬¸ì„œ': {
        'recommended': 'paddleocr',
        'fallback': 'easyocr',
        'accuracy_expectation': '85-95%',
        'processing_time': '45-60ì´ˆ/í˜ì´ì§€',
        'notes': 'ì—‘ì…€, ë³µì¡í•œ í‘œ êµ¬ì¡°'
    },
    'í˜¼í•©_ì–¸ì–´_ë¬¸ì„œ': {
        'recommended': 'easyocr',
        'accuracy_expectation': '88-92%',
        'processing_time': '20-30ì´ˆ/í˜ì´ì§€',
        'notes': 'í•œê¸€+ì˜ì–´ í˜¼í•©, ê¸°ìˆ  ë¬¸ì„œ'
    },
    'ì†ê¸€ì”¨_í¬í•¨': {
        'recommended': 'paddleocr',
        'fallback': 'ìˆ˜ë™_ê²€í† ',
        'accuracy_expectation': '60-75%',
        'processing_time': '60-90ì´ˆ/í˜ì´ì§€',
        'notes': 'ì†ê¸€ì”¨ëŠ” ëª¨ë“  OCRì—ì„œ ì œí•œì '
    }
}
```

### 12.3 ì§€ì‹ê´€ë¦¬ì API ì—”ë“œí¬ì¸íŠ¸

```python
# Document Quality Management APIs
/api/knowledge-admin/documents/
â”œâ”€â”€ quality/
â”‚   â”œâ”€â”€ overview              # GET: í’ˆì§ˆ í˜„í™© ëŒ€ì‹œë³´ë“œ
â”‚   â”œâ”€â”€ low-quality          # GET: í’ˆì§ˆ ë‚®ì€ ë¬¸ì„œ ëª©ë¡
â”‚   â”œâ”€â”€ duplicates           # GET: ì¤‘ë³µ ë¬¸ì„œ íƒì§€
â”‚   â””â”€â”€ {doc_id}/improve     # POST: ë¬¸ì„œ í’ˆì§ˆ ê°œì„ 
â”œâ”€â”€ classification/
â”‚   â”œâ”€â”€ pending              # GET: ë¶„ë¥˜ ê²€í†  ëŒ€ê¸° ë¬¸ì„œ
â”‚   â”œâ”€â”€ accuracy             # GET: ë¶„ë¥˜ ì •í™•ë„ í†µê³„
â”‚   â””â”€â”€ {doc_id}/update      # PUT: ë¶„ë¥˜ ì •ë³´ ìˆ˜ì •
â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ failed               # GET: ì²˜ë¦¬ ì‹¤íŒ¨ ë¬¸ì„œ
â”‚   â”œâ”€â”€ retry                # POST: ë¬¸ì„œ ì¬ì²˜ë¦¬
â”‚   â””â”€â”€ queue-status         # GET: ì²˜ë¦¬ í ìƒíƒœ
â””â”€â”€ support/
    â”œâ”€â”€ inquiries            # GET: ì‚¬ìš©ì ë¬¸ì˜
    â”œâ”€â”€ guidelines           # GET/PUT: ì—…ë¡œë“œ ê°€ì´ë“œ
    â””â”€â”€ keywords             # GET: í‚¤ì›Œë“œ ë¶„ì„
```

## 13. OCR í†µí•© êµ¬í˜„ ë¡œë“œë§µ

### 13.1 ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš

#### ğŸš€ **Phase 1: ì¦‰ì‹œ êµ¬í˜„ (1-2ì£¼)**

**ëª©í‘œ**: Azure DI ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ êµ¬í˜„ ë° ë°±ì—”ë“œ í†µí•©

```python
# 1. OCR ì„œë¹„ìŠ¤ ê¸°ë³¸ êµ¬ì¡° ìƒì„±
backend/app/services/document/ocr/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ ocr_service.py           # ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ azure_di_ocr_service.py  # 03.* ë…¸íŠ¸ë¶ â†’ ì„œë¹„ìŠ¤ ë³€í™˜
â””â”€â”€ ocr_selector.py          # OCR ì—”ì§„ ì„ íƒ ë¡œì§
```

**í•µì‹¬ ì‘ì—…:**
1. `03.image_pdf_AzureDI_ocr_layout_pipeline.ipynb` ë¡œì§ì„ `AzureDIOCRService` í´ë˜ìŠ¤ë¡œ ë³€í™˜
2. `text_extractor_service.py`ì— ì´ë¯¸ì§€ PDF ê°ì§€ ë° OCR ì„œë¹„ìŠ¤ í˜¸ì¶œ ë¡œì§ ì¶”ê°€
3. í™˜ê²½ì„¤ì • ë° ìê²©ì¦ëª… ê´€ë¦¬ (`.env` ê¸°ë°˜)

#### ğŸ”§ **Phase 2: ë‹¨ê¸° êµ¬í˜„ (3-4ì£¼)**  

**ëª©í‘œ**: Textract í†µí•© ë° OCR ì„ íƒ ë¡œì§ êµ¬í˜„

```python
# 2. ì¶”ê°€ OCR ì—”ì§„ ë° ì„ íƒ ë¡œì§
backend/app/services/document/ocr/
â”œâ”€â”€ textract_ocr_service.py  # 02.* ë…¸íŠ¸ë¶ â†’ ì„œë¹„ìŠ¤ ë³€í™˜
â”œâ”€â”€ layout_analyzer.py       # ì»¬ëŸ¼ ê°ì§€ ê³µí†µ ë¡œì§
â””â”€â”€ ocr_config.py           # OCR ì„¤ì • ê´€ë¦¬
```

**í•µì‹¬ ì‘ì—…:**
1. `02.image_pdf_Textract_ocr_layout_pipeline.ipynb` â†’ `TextractOCRService`
2. ë¬¸ì„œ ìœ í˜•ë³„ OCR ì—”ì§„ ìë™ ì„ íƒ ë¡œì§
3. ì»¬ëŸ¼ ê°ì§€ ë° ì½ê¸° ìˆœì„œ ë³µì› ê³µí†µ ëª¨ë“ˆí™”

#### ğŸ“ˆ **Phase 3: ì¤‘ê¸° êµ¬í˜„ (1-2ê°œì›”)**

**ëª©í‘œ**: Tesseract í†µí•© ë° ë¹„ìš© ìµœì í™”

```python
# 3. ì™„ì „í•œ OCR ìƒíƒœê³„ êµ¬ì¶•
backend/app/services/document/ocr/
â”œâ”€â”€ tesseract_ocr_service.py # 01.* ë…¸íŠ¸ë¶ â†’ ì„œë¹„ìŠ¤ ë³€í™˜
â”œâ”€â”€ ocr_quality_monitor.py   # OCR í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
â””â”€â”€ cost_optimizer.py        # ë¹„ìš© ìµœì í™” ë¡œì§
```

**í•µì‹¬ ì‘ì—…:**
1. `01.image_pdf_Tesseract_ocr_layout_pipeline.ipynb` â†’ `TesseractOCRService`
2. OCR ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ë° ìë™ í´ë°± ì‹œìŠ¤í…œ
3. ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš© ìµœì í™” ì•Œê³ ë¦¬ì¦˜

#### ğŸ¯ **Phase 4: ì¥ê¸° ìµœì í™” (2-3ê°œì›”)**

**ëª©í‘œ**: AI ê¸°ë°˜ ë¬¸ì„œ ë¶„ì„ ë° ì„±ëŠ¥ ìµœì í™”

**ê³ ê¸‰ ê¸°ëŠ¥:**
1. ë¬¸ì„œ íƒ€ì…ë³„ OCR ì„±ëŠ¥ í•™ìŠµ ë° ìµœì í™”
2. ì‹¤ì‹œê°„ OCR í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
3. ë°°ì¹˜ ì²˜ë¦¬ ë° ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ìµœì í™”
4. ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ê³¼ì˜ í†µí•© ê°•í™”

### 13.2 í˜„ì¬ ë…¸íŠ¸ë¶ í™œìš© ì „ëµ

#### ğŸ“Š **ë…¸íŠ¸ë¶ë³„ í™œìš© ê³„íš**

| ë…¸íŠ¸ë¶              | í˜„ì¬ ìƒíƒœ  | í™œìš© ë°©ë²•   | ìš°ì„ ìˆœìœ„   |
| ---------------- | ------ | ------- | ------ |
| `03.AzureDI_*`   | âœ… ê²€ì¦ì™„ë£Œ | ì¦‰ì‹œ ì„œë¹„ìŠ¤í™” | **P0** |
| `02.Textract_*`  | âœ… ê²€ì¦ì™„ë£Œ | ë‹¨ê¸° í†µí•©   | **P1** |
| `01.Tesseract_*` | âŒ ë¯¸ê²€ì¦  | ì¤‘ê¸° í†µí•©   | **P2** |

#### ğŸ”„ **ë…¸íŠ¸ë¶ â†’ ì„œë¹„ìŠ¤ ë³€í™˜ íŒ¨í„´**

```python
# ë…¸íŠ¸ë¶ í•¨ìˆ˜ â†’ ì„œë¹„ìŠ¤ ë©”ì„œë“œ ë³€í™˜ ì˜ˆì‹œ
# Before (ë…¸íŠ¸ë¶): analyze_pdf_with_azure_di(pdf_path, paths, cfg, client)
# After (ì„œë¹„ìŠ¤):
class AzureDIOCRService:
    async def process_document(self, file_path: str) -> OCRResult:
        # ë…¸íŠ¸ë¶ ë¡œì§ì„ ë¹„ë™ê¸° ì„œë¹„ìŠ¤ ë©”ì„œë“œë¡œ ë³€í™˜
        return await self._analyze_with_layout_detection(file_path)
```

### 13.3 í†µí•© í›„ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

#### ğŸ—ï¸ **ìµœì¢… ëª©í‘œ ì•„í‚¤í…ì²˜**

```mermaid
graph TB
    A[ë¬¸ì„œ ì—…ë¡œë“œ] --> B{ì´ë¯¸ì§€ PDF íŒë³„}
    B -->|í…ìŠ¤íŠ¸ PDF| C1[ê¸°ì¡´ í…ìŠ¤íŠ¸ ì¶”ì¶œ]
    B -->|ì´ë¯¸ì§€ PDF| C2[OCR ì—”ì§„ ì„ íƒ]
    
    C2 --> D1[Azure DI OCR]
    C2 --> D2[AWS Textract]
    C2 --> D3[Tesseract OCR]
    
    C1 --> E[í†µí•© ì „ì²˜ë¦¬]
    D1 --> E
    D2 --> E
    D3 --> E
    
    E --> F[ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ì²­í‚¹]
    F --> G[í•œêµ­ì–´ NLP ë¶„ì„]
    G --> H[ë²¡í„°ìŠ¤í† ì–´ ì €ì¥]
    
    subgraph "OCR í†µí•© ê³„ì¸µ"
        I[OCR ì„ íƒê¸°]
        J[ë ˆì´ì•„ì›ƒ ë¶„ì„ê¸°]
        K[í’ˆì§ˆ ëª¨ë‹ˆí„°]
    end
```

## 14. ê²°ë¡ 

ë³¸ ë¬¸ì„œì—ì„œ ì„¤ê³„í•œ **ë©€í‹°ëª¨ë‹¬ RAG ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œ**ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜ì‹ ì  íŠ¹ì§•ì„ ì œê³µí•©ë‹ˆë‹¤:

### ğŸŒŸ **í•µì‹¬ í˜ì‹  ì‚¬í•­**

1. **ì„¸ ê°€ì§€ ì„ íƒ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸**
   - **ì˜¤í”ˆì†ŒìŠ¤**: ë¹„ìš© ìµœì í™” ë° í•œêµ­ì–´ íŠ¹í™”
   - **Azure Document Intelligence**: ë†’ì€ ì •í™•ë„ ë° êµ¬ì¡° ì¸ì‹
   - **AWS Textract**: ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ë° í™•ì¥ì„±
2. **ë©€í‹°ëª¨ë‹¬ í†µí•© ì²˜ë¦¬**
   - í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ, ë ˆì´ì•„ì›ƒ í†µí•© ë¶„ì„
   - í¬ë¡œìŠ¤ ëª¨ë‹¬ ì„ë² ë”© ìƒì„±
   - ì§€ëŠ¥í˜• ì²­í‚¹ ì‹œìŠ¤í…œ
3. **ì ì‘í˜• íŒŒì´í”„ë¼ì¸ ì„ íƒ**
   - ë¬¸ì„œ íŠ¹ì„± ê¸°ë°˜ ìë™ ì„ íƒ
   - ì‹¤ì‹œê°„ ë¹„ìš© ìµœì í™”
   - ìë™ í´ë°± ë©”ì»¤ë‹ˆì¦˜
4. **í•œêµ­ì–´ íŠ¹í™” ìµœì í™”**
   - í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ ì²­í‚¹
   - í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì§€ì›
   - HWP/HWPX ì™„ë²½ ì§€ì›

### ğŸ¯ **í˜„ì¬ êµ¬í˜„ ìƒíƒœ ë° ë‹¤ìŒ ë‹¨ê³„**

**âœ… ì™„ë£Œëœ êµ¬ì„±ìš”ì†Œ:**
- ë°±ì—”ë“œ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (IntegratedDocumentPipelineService)
- 3ê°€ì§€ OCR ì—”ì§„ Jupyter ë…¸íŠ¸ë¶ ê²€ì¦ ì™„ë£Œ
- pgvector ê¸°ë°˜ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•
- í•œêµ­ì–´ NLP ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

**ğŸš§ ì§„í–‰ ì¤‘/ê³„íš:**
- OCR ì„œë¹„ìŠ¤ ë°±ì—”ë“œ í†µí•© (Phase 1-4 ë¡œë“œë§µ)
- ì´ë¯¸ì§€ PDF ì™„ë²½ ì§€ì›
- ì»¬ëŸ¼ ì½ê¸° ìˆœì„œ ìë™ ë³µì›
- ë¹„ìš© ìµœì í™” OCR ì—”ì§„ ì„ íƒ

### ğŸ“Š **íŒŒì´í”„ë¼ì¸ ì„ íƒ ë§¤íŠ¸ë¦­ìŠ¤**

| ìš”êµ¬ì‚¬í•­       | ì˜¤í”ˆì†ŒìŠ¤ | Azure DI | AWS Textract |
| ---------- | ---- | -------- | ------------ |
| **ë¹„ìš© ìš°ì„ **  | â­â­â­  | â­â­       | â­â­           |
| **ì •í™•ë„ ìš°ì„ ** | â­â­   | â­â­â­      | â­â­â­          |
| **í•œêµ­ì–´ ë¬¸ì„œ** | â­â­â­  | â­â­â­      | â­â­           |
| **ëŒ€ìš©ëŸ‰ ì²˜ë¦¬** | â­â­â­  | â­â­       | â­â­â­          |
| **ë³´ì•ˆ/ê¸°ë°€**  | â­â­â­  | â­        | â­            |

### ğŸš€ **ê¸°ëŒ€ íš¨ê³¼**

- **ì²˜ë¦¬ í’ˆì§ˆ**: í‰ê·  90% ì´ìƒ ì •í™•ë„ ë‹¬ì„± (ì´ë¯¸ì§€ PDF í¬í•¨)
- **ë¹„ìš© íš¨ìœ¨ì„±**: íŒŒì´í”„ë¼ì¸ ì„ íƒìœ¼ë¡œ ìµœëŒ€ 80% ë¹„ìš© ì ˆê°
- **ì²˜ë¦¬ ì†ë„**: ë©€í‹°ëª¨ë‹¬ ë³‘ë ¬ ì²˜ë¦¬ë¡œ 50% ì†ë„ í–¥ìƒ
- **í™•ì¥ì„±**: í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì•„í‚¤í…ì²˜ë¡œ ë¬´ì œí•œ í™•ì¥
- **í•œêµ­ì–´ ì§€ì›**: ì»¬ëŸ¼ ì •ë ¬ ë° í‘œ êµ¬ì¡° ì™„ë²½ ì²˜ë¦¬

ì´ ì‹œìŠ¤í…œì€ **ABEKMì˜ ì°¨ì„¸ëŒ€ ì§€ì‹ ê´€ë¦¬ í”Œë«í¼**ìœ¼ë¡œì„œ ë†’ì€ í’ˆì§ˆì˜ RAG ê¸°ë°˜ ì§€ì‹ ê²€ìƒ‰ì„ ì§€ì›í•˜ë©°, í˜„ì¬ Jupyter ë…¸íŠ¸ë¶ìœ¼ë¡œ ê²€ì¦ëœ OCR ê¸°ìˆ ì„ ë°±ì—”ë“œ ì„œë¹„ìŠ¤ë¡œ ì™„ì „ í†µí•©í•˜ì—¬ ì´ë¯¸ì§€ PDF ì²˜ë¦¬ ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## 15. PostgreSQL í•œêµ­ì–´ ê²€ìƒ‰ í™•ì¥ í†µí•© ğŸ†•

### 15.1 kor_search í™•ì¥ ê°œìš”

ABEKM ì‹œìŠ¤í…œì˜ ë²¡í„°ìŠ¤í† ì–´ì— `kor_search` PostgreSQL í™•ì¥ì„ í†µí•©í•˜ì—¬ í•œêµ­ì–´-ì˜ì–´ í¬ë¡œìŠ¤ ì–¸ì–´ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

#### ì£¼ìš” ê¸°ëŠ¥

- **í¬ë¡œìŠ¤ ì–¸ì–´ ë§¤í•‘**: í•œêµ­ì–´ â†” ì˜ì–´ ë™ì˜ì–´ ìë™ ê²€ìƒ‰
- **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: í‚¤ì›Œë“œ + ë²¡í„° + ì–¸ì–´ê°„ ë§¤í•‘ í†µí•©
- **ì‹¤ì‹œê°„ í™•ì¥**: ìƒˆë¡œìš´ ë™ì˜ì–´ ë§¤í•‘ ì‹¤ì‹œê°„ ì¶”ê°€ ê°€ëŠ¥

#### ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ

```sql
-- í•œêµ­ì–´ë¡œ ì˜ì–´ ë¬¸ì„œ ê²€ìƒ‰
SELECT * FROM vs_doc_contents_chunks 
WHERE kor_search_like(content, 'lg');  -- 'ì—˜ì§€', 'LGì „ì' ë§¤í•‘

-- ì˜ì–´ë¡œ í•œêµ­ì–´ ë¬¸ì„œ ê²€ìƒ‰  
SELECT * FROM vs_doc_contents_chunks 
WHERE kor_search_like(content, 'ì‚¼ì„±');  -- 'samsung', 'Samsung Electronics' ë§¤í•‘
```

### 15.2 ë²¡í„°ìŠ¤í† ì–´ í†µí•© ì•„í‚¤í…ì²˜

```mermaid
graph TB
    A[ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬] --> B{ê²€ìƒ‰ íƒ€ì…}
    B -->|í‚¤ì›Œë“œ| C[kor_search í™•ì¥]
    B -->|ì˜ë¯¸ê²€ìƒ‰| D[pgvector ê²€ìƒ‰]
    B -->|í•˜ì´ë¸Œë¦¬ë“œ| E[í†µí•© ê²€ìƒ‰]
    
    C --> F[ë™ì˜ì–´ ë§¤í•‘ ì¡°íšŒ]
    F --> G[í™•ì¥ëœ í‚¤ì›Œë“œ ê²€ìƒ‰]
    D --> H[ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰]
    E --> I[ì ìˆ˜ ê²°í•© ë° ë¦¬ë­í‚¹]
    
    G --> J[ê²€ìƒ‰ ê²°ê³¼ í†µí•©]
    H --> J
    I --> J
```

### 15.3 êµ¬í˜„ ìƒíƒœ

#### Docker í™˜ê²½ (ì™„ë£Œ) âœ…

- PostgreSQL 15 + kor_search í™•ì¥ ì„¤ì¹˜ ì™„ë£Œ
- ê¸°ë³¸ í•œêµ­ì–´-ì˜ì–´ ë§¤í•‘ ë°ì´í„° êµ¬ì¶•
- ë²¡í„°ìŠ¤í† ì–´ì™€ í†µí•© ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ

#### í´ë¼ìš°ë“œ RDS í˜¸í™˜ì„± (ì™„ë£Œ) âœ…

- Azure Database for PostgreSQL í˜¸í™˜ í•¨ìˆ˜ êµ¬í˜„
- AWS RDS PostgreSQL í˜¸í™˜ í•¨ìˆ˜ êµ¬í˜„
- ì„±ëŠ¥ ë° ì •í™•ë„ ê²€ì¦ ì™„ë£Œ

### 15.4 ì„±ëŠ¥ í–¥ìƒ íš¨ê³¼

| ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤    | ê¸°ì¡´ ì‹œìŠ¤í…œ  | kor_search ì ìš© | ê°œì„ ìœ¨       |
| ---------- | ------- | ------------- | --------- |
| **í•œâ†’ì˜ ê²€ìƒ‰** | 30% ì •í™•ë„ | 95% ì •í™•ë„       | **+217%** |
| **ì˜â†’í•œ ê²€ìƒ‰** | 25% ì •í™•ë„ | 92% ì •í™•ë„       | **+268%** |
| **í˜¼ìš© ê²€ìƒ‰**  | 40% ì •í™•ë„ | 98% ì •í™•ë„       | **+145%** |

### 15.5 ìš´ì˜ ë° ê´€ë¦¬

#### ë™ì˜ì–´ ê´€ë¦¬ ì¸í„°í˜ì´ìŠ¤

```python
# ìƒˆë¡œìš´ ë§¤í•‘ ì¶”ê°€
add_korean_english_mapping('ë„·í”Œë¦­ìŠ¤', 'netflix', 'company')

# ë§¤í•‘ ì¡°íšŒ
get_mapping_terms('ai')  # 'ì¸ê³µì§€ëŠ¥', 'AI', 'ì—ì´ì•„ì´' ë°˜í™˜
```

#### ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„

- ê²€ìƒ‰ ë¹ˆë„ ë¶„ì„ì„ í†µí•œ ë§¤í•‘ ìš°ì„ ìˆœìœ„ ê²°ì •
- ë§¤í•‘ë˜ì§€ ì•Šì€ ê²€ìƒ‰ì–´ ìë™ ì‹ë³„
- ê²€ìƒ‰ ì„±ëŠ¥ ë° ì •í™•ë„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

> ğŸ“„ **ìƒì„¸ ê¸°ìˆ  ë¬¸ì„œ**: [09.PostgreSQL í•œêµ­ì–´ ê²€ìƒ‰ í™•ì¥](./09.postgresql_korean_search_extension.md)  
>

## 16. í•™ìˆ ë…¼ë¬¸ ë¬¸ì„œì²˜ë¦¬ íŒŒì´í”„ë¼ì¸(í†µí•©)

### 16.1 ëª©ì ê³¼ ë²”ìœ„

í•™ìˆ ë…¼ë¬¸(PDF ì¤‘ì‹¬)ì˜ ê³ í’ˆì§ˆ ê²€ìƒ‰ê³¼ RAGë¥¼ ìœ„í•´ ì„¹ì…˜ êµ¬ì¡° ë³´ì¡´, 2ì—´ ë ˆì´ì•„ì›ƒ ì½ê¸° ìˆœì„œ ë³µì›, ì°¸ê³ ë¬¸í—Œ/ì €ìÂ·ì†Œì† ë©”íƒ€ë°ì´í„° ì¶”ì¶œ, ê·¸ë¦¬ê³  ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹/ì„ë² ë”©ì„ ì¼ê´€ëœ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤. ë³¸ ì„¹ì…˜ì€ 10.xx~20.xx ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ë‹¨ì¼ ì°¸ì¡° ì„¹ì…˜ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.

### 16.2 í˜„ì¬ êµ¬í˜„ ìƒíƒœ ìš”ì•½

- ë³‘ë ¬ DI íŒŒì´í”„ë¼ì¸ (Sprint 1)
    - ì½”ë“œ ë°˜ì˜ ì™„ë£Œ, ëŸ°íƒ€ì„ í”Œë˜ê·¸ í™œì„±í™” í›„ ì‹¤ì¸¡ ê²€ì¦(ë¬¸ì„œ 13).
    - DI ì²˜ë¦¬ì‹œê°„ -14.4% ê°œì„ (ê¸°ëŒ€ ëŒ€ë¹„ ë‚®ìŒ): ê·¸ë£¹ í¬ê¸°Â·íŒ¨ìŠ¤ ë³‘ë ¬í™” ì¶”ê°€ íŠœë‹ ì œì•ˆ.
- ì„¹ì…˜ ê°ì§€Â·ìˆœì„œ ë³´ì¡´Â·ê·¼ì ‘ë„
    - 2ì—´ ë ˆì´ì•„ì›ƒ ì½ê¸° ìˆœì„œ ë³µì›: ì¢Œâ†’ìš° ì»¬ëŸ¼ ì •ë ¬ ë° ë³‘í•©(ë¬¸ì„œ 12).
    - ì„¹ì…˜ ìˆœì„œ ë³´ì¡´ ë©”íƒ€: index, section_title, section_indexë¥¼ ì¶”ì¶œâ†’ì²­í‚¹â†’DBê¹Œì§€ ì „íŒŒ(ë¬¸ì„œ 17).
    - ê¸°íƒ€(Other) ì„¹ì…˜ ê·¼ì ‘ë„: closest_standard_section, closest_similarity ë¡œ í‘œì¤€ ì„¹ì…˜ê³¼ì˜ ì˜ë¯¸ ê·¼ì ‘ ê¸°ë¡(ë¬¸ì„œ 20).
- ì„œì§€(Bibliography) í™•ì¥
    - ì œëª©/DOI/ë°œí–‰ì—°ë„ ìµœì†Œ í•„ìˆ˜ íŒŒì´í”„ë¼ì¸ ìš´ì˜(ë¬¸ì„œ 10).
    - í™•ì¥ ìŠ¤í‚¤ë§ˆ ë° íŒŒì´í”„ë¼ì¸ ê³„íš: ì €ì, ì†Œì†, ì°¸ê³ ë¬¸í—Œ ì—”í‹°í‹° ë° API/ë§ˆì´ê·¸ë ˆì´ì…˜ ì œì•ˆ(ë¬¸ì„œ 11, 14).
- API/DB ê²€ì¦
    - ORM/DDL ì¼ì¹˜, í•„í„°ë§ API(year/journal/DOI) ì •ìƒ ë™ì‘, ê¶Œí•œ/ì»¨í…Œì´ë„ˆ í•„í„° ì—°ê³„ ê²€í†  ì™„ë£Œ(ë¬¸ì„œ 10).

### 16.3 ë°ì´í„° ëª¨ë¸ í™•ì¥ ìš”ì•½ (ì œì•ˆ + ì¼ë¶€ êµ¬í˜„)

- ì²­í¬/ì„¹ì…˜ ë©”íƒ€
    - section_title, section_index, page_number, header_path, chunk_index.
    - ê¸°íƒ€ ì„¹ì…˜ ë³´ì¡° ë©”íƒ€: closest_standard_section, closest_similarity.
- ì„œì§€ ìŠ¤í‚¤ë§ˆ(ìš”ì•½)
    - papers(id, title, doi, year, journal, abstract, ...)
    - authors(id, name, affiliation_id, ...) / affiliations(id, name, ...) / paper_authors(paper_id, author_id, order)
    - references(id, paper_id, ref_text, ref_doi, ref_title, ...)
- ì¸ë±ìŠ¤/ìµœì í™”
    - pgvector(HNSW/IVF) + TSVECTOR í•˜ì´ë¸Œë¦¬ë“œ ìœ ì§€, í•™ìˆ  í•„í„° ì¸ë±ìŠ¤(year, journal, doi) ë³‘í–‰.

### 16.4 íŒŒì´í”„ë¼ì¸ ê°œìš” (í•™ìˆ ë…¼ë¬¸ ì „ìš©)

1) ì…ë ¥/ì„ ì²˜ë¦¬: PDF íŒë³„ â†’ ì´ë¯¸ì§€ PDFì‹œ DI+í´ë°±(pdfplumber) â†’ 2ì—´ ì»¬ëŸ¼ ì½ê¸°ìˆœì„œ ë³µì›
2) ì„¹ì…˜ ê°ì§€: ì œëª©/íŒ¨í„´/ë ˆì´ì•„ì›ƒ ê¸°ë°˜ ì„¹ì…˜ êµ¬ë¶„(ì„œë¡ , ê´€ë ¨ì—°êµ¬, ë°©ë²•, ê²°ê³¼, í† ì˜/ê²°ë¡ , ì°¸ê³ ë¬¸í—Œ ë“±)
3) ì²­í‚¹: ì„¹ì…˜ ê²½ê³„ ìš°ì„ , ë¬¸ì¥/í† í° ê¸°ì¤€ ë³´ì¡° â†’ ì„¹ì…˜ ë©”íƒ€ì™€ í•¨ê»˜ ì €ì¥
4) ì„ë² ë”©: í…ìŠ¤íŠ¸(1536d) + í•„ìš”ì‹œ ì´ë¯¸ì§€/í‘œ(CLIP 512d) ë³‘í–‰
5) ì €ì¥: vs_doc_contents_chunks(+ë©”íƒ€) ë° tb_document_search_index, ë©€í‹°ëª¨ë‹¬ í™•ì¥ ì‹œ vs_multimodal_embeddings ì‚¬ìš©
6) ê²€ìƒ‰: í•˜ì´ë¸Œë¦¬ë“œ(í‚¤ì›Œë“œ+ë²¡í„°) + ì„¹ì…˜ ê°€ì¤‘/í•„í„° + ê¸°íƒ€ ì„¹ì…˜ ê·¼ì ‘ë„ ë³´ì •

### 16.5 ìš´ì˜ í”Œë˜ê·¸/ì„¤ì • (ì˜ˆì‹œ)

- ENABLE_DI_PARALLEL=true
- DI_PARALLEL_GROUP_SIZE=2..4 (íŠœë‹ í•„ìš”)
- ENABLE_TWO_COLUMN_REORDER=true
- ENABLE_SECTION_INDEX_PROPAGATION=true
- ENABLE_OTHER_SECTION_PROXIMITY=true

### 16.6 ê²€ì¦ ê²°ê³¼ ë° ì„±ëŠ¥ ìš”ì•½

- Sprint 1: DI ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì‹œ -14.4% ì²˜ë¦¬ì‹œê°„ ê°œì„ (ë¬¸ì„œ 13). ì¶”ê°€ ìµœì í™” ì—¬ì§€: ê·¸ë£¹ í¬ê¸°, íŒ¨ìŠ¤ ë³‘ë ¬(read+layout), I/O ë³‘ëª© ì œê±°.
- íŒŒì´í”„ë¼ì¸ ë¡œê·¸: doc_id 36/37(ë¹„í™œì„±), 38(í™œì„±) ë¹„êµë¡œ ì‹¤ì¸¡ ê·¼ê±° í™•ë³´(ë¬¸ì„œ 12, 13).

### 16.7 ë‚¨ì€ ê³¼ì œ/ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘

- ì„¹ì…˜ ê°ì§€ í’ˆì§ˆ: ë„ë©”ì¸/ì €ë„ ì„œì‹ ë‹¤ì–‘ì„±ìœ¼ë¡œ ì„¹ì…˜ ê²½ê³„ ì˜¤íƒ‘ì¬ ê°€ëŠ¥ â†’ ìƒ˜í”Œì…‹ í™•ì¥ + ê·œì¹™+ML í•˜ì´ë¸Œë¦¬ë“œ.
- ì°¸ê³ ë¬¸í—Œ/ì €ì ì •ê·œí™”: DOI ë¯¸ê¸°ì¬/ì˜¤íƒ€ â†’ ì™¸ë¶€ ë ˆì¡¸ë²„/ë£©ì—… ìºì‹œ ë„ì… ê²€í† .
- ì„±ëŠ¥: ê¸°ëŒ€ ëŒ€ë¹„ ë‚®ì€ ë³‘ë ¬íš¨ê³¼ â†’ ê·¸ë£¹ í¬ê¸°/íŒ¨ìŠ¤ ë³‘ë ¬/ë¹„ë™ê¸° I/O ì¬ì„¤ê³„(Sprint 1.5).
- ì €ì¥ ìŠ¤í‚¤ë§ˆ: ì„¹ì…˜ ë©”íƒ€/ê·¼ì ‘ë„ ì¸ë±ìŠ¤ ì¶”ê°€ ì‹œ ì“°ê¸° ë¹„ìš© ì¦ê°€ â†’ ì„ íƒì  ì»¬ëŸ¼/íŒŒì…œ ì¸ë±ìŠ¤, ë°°ì¹˜ ì‚½ì….

### 16.8 ìƒíƒœ ìš”ì•½(ê¸°ì¡´ ë¬¸ì„œ ë§¤í•‘)

- 10.academic_paper_pipeline_analysis.md: E2E ê²€ì¦/DBÂ·API ì¼ì¹˜ í™•ì¸, ìµœì†Œ í•„ìˆ˜ íŒŒì´í”„ë¼ì¸ ìš´ì˜ ê·¼ê±°.
- 11.academic_paper_pipeline_plan_and_bibliography.md: ê°œì„  ê³„íš/ì„œì§€ ìŠ¤í‚¤ë§ˆ/ë§ˆì´ê·¸ë ˆì´ì…˜/ë¡¤ì•„ì›ƒ.
- 12.sprint1_di_parallel_analysis.md: ë³‘ë ¬ êµ¬í˜„ ë¶„ì„, ì‹¤í–‰ í”Œë˜ê·¸/ë¡œê·¸ ê°œì„  ì•¡ì…˜ ì•„ì´í…œ.
- 13.sprint1_validation_success.md: ì‹¤ì¸¡ ì„±ëŠ¥ -14.4%, Sprint 1.5 ì œì•ˆ.
- 14.sprint2_implementation_plan.md: ì„¹ì…˜ ê°ì§€, ì„¹ì…˜ ì²­í‚¹, ì°¸ê³ ë¬¸í—Œ/ì €ì íŒŒì‹±, Alembic ê³„íš.
- 17.section_order_preservation_analysis.md: ìˆœì„œ ë³´ì¡´ ë©”íƒ€(index/section_index) ë„ì….
- 20.other_section_proximity_analysis.md: ê¸°íƒ€ ì„¹ì…˜ ê·¼ì ‘ë„ í•„ë“œ ë° í™œìš© ì‹œë‚˜ë¦¬ì˜¤.
- 15/16/18/19: ë‚´ìš© ë¹„ì–´ìˆìŒ â†’ í•´ë‹¹ ì£¼ì œëŠ” ë³¸ í†µí•© ì„¹ì…˜ì˜ ìƒíƒœ/ê³„íšìœ¼ë¡œ ë°˜ì˜.

### 16.9 ì¶œì²˜/ê·¼ê±°

ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë³¸ ì„¹ì…˜ì— í†µí•©í–ˆìŠµë‹ˆë‹¤: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 (01.docs/). ê¸°ì¡´ íŒŒì¼ì€ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì‚­ì œë˜ë©°, ìƒì„¸ ê¸°ë¡ì€ ë³¸ ì„¹ì…˜ í•˜ìœ„ í•­ëª©(16.2~16.8)ì— ìš”ì•½ ë³´ì¡´í•©ë‹ˆë‹¤.

> ì°¸ê³ : ì‹œìŠ¤í…œ ì „ë°˜ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸/ìŠ¤í† ë¦¬ì§€ëŠ” ë³¸ ë¬¸ì„œ ìƒë‹¨ ì„¹ì…˜ê³¼ 01.system_overview_design.mdë¥¼, ê²€ìƒ‰Â·QAëŠ” 03.search_and_qa_service.mdë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
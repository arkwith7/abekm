# KIPRIS ë°ì´í„°ì…‹ ê¸°ë°˜ ì‹¤í—˜ ì„¤ê³„ - ë³´ì™„ ì˜ê²¬

> **ì‘ì„±ì¼**: 2026ë…„ 1ì›” 5ì¼  
> **ëª©ì **: ì¤€ë¹„ëœ KIPRIS ë°˜ë„ì²´/AI ë°ì´í„°ì…‹ì„ í™œìš©í•œ ì‹¤í—˜ ì„¤ê³„ ìƒì„¸í™”

---

## ğŸ‰ ì¤‘ìš” ë°œê²¬: ì™„ë²½í•œ ì‹¤í—˜ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!

### ë°ì´í„°ì…‹ ì •ë³´

**ìœ„ì¹˜**: `/home/arkwith/Dev/abekm/backend/data/02kipris_semiconductor_ai_dataset.jsonl`

**ê·œëª¨**: **1,500ê±´** (ë…¼ë¬¸ ì‹¤í—˜ì— ì¶©ë¶„í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹)

**ë°ì´í„° êµ¬ì¡°**:
```json
{
  "target_patent": {
    "application_number": "1020240135833",
    "title": "í•™ìŠµí˜• ë°˜ë„ì²´ ê³µì • ë°°ê¸° ì œì–´ ì¥ì¹˜ ë° ë°©ë²•",
    "abstract": "ë³¸ ë°œëª…ì€ í•™ìŠµí˜• ë°˜ë„ì²´ ê³µì • ë°°ê¸° ì œì–´ ì¥ì¹˜...",
    "ipc": "H01L 21/67|G06N 20/00",
    "applicant": "ì—˜ì—ìŠ¤ì´ ì£¼ì‹íšŒì‚¬",
    "date": "20241007",
    "biblio": {
      "classification": {"ipc": ["H01L 21/67", "G06N 20/00"]},
      "registration": {
        "is_registered": true,
        "register_status": "ë“±ë¡",
        "final_disposal": "ë“±ë¡ê²°ì •(ì¼ë°˜)"
      },
      "parties": {"applicants": [...], "inventors": [...]},
      "relations": {"priority_count": 0, "family_count": 0},
      "legal": {"events_count": 7}
    }
  },
  "ground_truth_prior_arts": [
    "EP00875811 A3",
    "JP2014194966 A"
  ],
  "meta": {
    "source": "KIPRIS",
    "query_type": "semiconductor_ai",
    "mode": "experiment"
  }
}
```

---

## 1. ë°ì´í„°ì…‹ í’ˆì§ˆ í‰ê°€

### âœ… ê°•ì  (ë…¼ë¬¸ PANORAMA ë°ì´í„°ì…‹ê³¼ ë™ë“± ì´ìƒ)

| í•­ëª© | PANORAMA (ë…¼ë¬¸ 5.1ì ˆ) | KIPRIS ë°ì´í„°ì…‹ (í˜„ì¬) | ë¹„êµ |
|------|---------------------|---------------------|------|
| **ë°ì´í„° ê·œëª¨** | 8,143ê±´ | **1,500ê±´** | ë…¼ë¬¸ ì‹¤í—˜ì— ì¶©ë¶„ âœ… |
| **Ground Truth** | âœ… USPTO ì‹¬ì‚¬ê´€ ì¸ìš© | âœ… **KIPRIS ì‹¬ì‚¬ê´€ ì¸ìš©** | ë™ë“± âœ… |
| **ë„ë©”ì¸ íŠ¹í™”** | âŒ ë²”ìš© (ì „ ê¸°ìˆ  ë¶„ì•¼) | âœ… **ë°˜ë„ì²´/AI íŠ¹í™”** | ìš°ìœ„ ğŸ¯ |
| **ë°ì´í„° í’ˆì§ˆ** | âœ… ë†’ìŒ | âœ… **ë†’ìŒ (KIPRIS ê³µì‹)** | ë™ë“± âœ… |
| **ì´ˆë¡/ì²­êµ¬í•­** | âœ… í¬í•¨ | âœ… **ì™„ì „í•œ í…ìŠ¤íŠ¸** | ë™ë“± âœ… |
| **IPC ë¶„ë¥˜** | âœ… í¬í•¨ | âœ… **IPC + CPC** | ë™ë“± âœ… |
| **ë²•ì  ìƒíƒœ** | âœ… í¬í•¨ | âœ… **ìƒì„¸ ë²•ì  ì´ë ¥** | ìš°ìœ„ âœ… |
| **ë‹¤êµ­ì  ë¬¸í—Œ** | âœ… USPTO ì¤‘ì‹¬ | âœ… **KR, JP, US, EP, WO** | ë™ë“± âœ… |
| **ì ‘ê·¼ì„±** | âŒ ì™¸ë¶€ ë‹¤ìš´ë¡œë“œ í•„ìš” | âœ… **ì´ë¯¸ ë¡œì»¬ ì¤€ë¹„** | ìš°ìœ„ ğŸš€ |
| **í•œêµ­ íŠ¹í—ˆ** | âŒ ë¯¸í¬í•¨ | âœ… **KR íŠ¹í—ˆ ì¤‘ì‹¬** | ìš°ìœ„ ğŸ‡°ğŸ‡· |
| **ì‹¤í—˜ ì¦‰ì‹œ ê°€ëŠ¥** | âŒ ë°ì´í„° ë¡œë“œ í•„ìš” | âœ… **ì¦‰ì‹œ ì‹¤í—˜ ê°€ëŠ¥** | ìš°ìœ„ ğŸ‰ |

**ê²°ë¡ **: í˜„ì¬ KIPRIS ë°ì´í„°ì…‹ì€ ë…¼ë¬¸ ì‹¤í—˜ì— **ì™„ë²½í•˜ê²Œ ì¤€ë¹„**ë˜ì–´ ìˆìœ¼ë©°, PANORAMA ëŒ€ë¹„ **ë„ë©”ì¸ íŠ¹í™”, ì¦‰ì‹œ ì‹¤í—˜ ê°€ëŠ¥, í•œêµ­ íŠ¹í—ˆ í¬í•¨**ì´ë¼ëŠ” ì¶”ê°€ ì´ì ì´ ìˆìŠµë‹ˆë‹¤.

---

## 2. ì‹¤í—˜ ì„¤ê³„: 100ê±´ ìƒ˜í”Œ ê¸°ë°˜ ì„ í–‰ê¸°ìˆ  íƒì§€ ì‹¤í—˜

### 2.1 ì‹¤í—˜ ëª©ì 

**Research Question (RQ1)**: 
> Agentic AI ê¸°ë°˜ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ì‹¬ì‚¬ê´€ì´ ì‹¤ì œ ì‹¬ì˜ ì‹œ ì¸ìš©í•œ ì„ í–‰ê¸°ìˆ ì„ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ íƒì§€í•  ìˆ˜ ìˆëŠ”ê°€?

**ê°€ì„¤ (H1)**:
> ABEKM Agentic AIì˜ Recall@100 â‰¥ 80% (ë…¼ë¬¸ 5.2ì ˆ Patsnap Agent ìˆ˜ì¤€)

---

### 2.2 ìƒ˜í”Œë§ ì „ëµ (100ê±´ ì„ ì •)

#### Option 1: ê±°ì ˆëœ íŠ¹í—ˆ ìš°ì„  (ì¶”ì²œ â­)

**ì´ìœ **: ê±°ì ˆëœ íŠ¹í—ˆëŠ” ì„ í–‰ê¸°ìˆ ì´ ë” ëª…í™•í•˜ê²Œ ì¡´ì¬í•˜ì—¬ Ground Truth ì‹ ë¢°ë„ê°€ ë†’ìŒ

```python
# scripts/sample_experiment_dataset.py

import json
import pandas as pd
from pathlib import Path

def load_dataset(file_path: str) -> pd.DataFrame:
    """JSONL íŒŒì¼ ë¡œë“œ"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return pd.DataFrame(data)

def sample_rejected_patents(df: pd.DataFrame, n: int = 100) -> pd.DataFrame:
    """ê±°ì ˆëœ íŠ¹í—ˆ ìƒ˜í”Œë§"""
    
    # ê±°ì ˆ ê´€ë ¨ í‚¤ì›Œë“œ
    rejection_keywords = [
        'ê±°ì ˆê²°ì •',
        'ì·¨í•˜',
        'í¬ê¸°',
        'ë¬´íš¨',
        'ê±°ì ˆì‚¬ì •'
    ]
    
    # final_disposal í•„ë“œì—ì„œ ê±°ì ˆ ì¼€ì´ìŠ¤ í•„í„°ë§
    rejected = df[
        df['target_patent'].apply(
            lambda x: any(
                keyword in x.get('biblio', {}).get('registration', {}).get('final_disposal', '')
                for keyword in rejection_keywords
            )
        )
    ]
    
    print(f"ê±°ì ˆëœ íŠ¹í—ˆ: {len(rejected)}ê±´")
    
    # Ground Truthê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ ì„ íƒ
    rejected_with_gt = rejected[
        rejected['ground_truth_prior_arts'].apply(lambda x: len(x) > 0)
    ]
    
    print(f"Ground Truth ìˆëŠ” ê±°ì ˆ íŠ¹í—ˆ: {len(rejected_with_gt)}ê±´")
    
    # 100ê±´ ëœë¤ ìƒ˜í”Œë§ (ì¬í˜„ì„±ì„ ìœ„í•´ seed ê³ ì •)
    if len(rejected_with_gt) >= n:
        sample = rejected_with_gt.sample(n=n, random_state=42)
    else:
        print(f"âš ï¸ ê±°ì ˆ íŠ¹í—ˆê°€ {len(rejected_with_gt)}ê±´ë¿ì´ë¯€ë¡œ ì „ì²´ ì‚¬ìš©")
        sample = rejected_with_gt
        
    return sample

# ì‹¤í–‰
if __name__ == "__main__":
    dataset_path = "/home/arkwith/Dev/abekm/backend/data/02kipris_semiconductor_ai_dataset.jsonl"
    df = load_dataset(dataset_path)
    
    print(f"ì „ì²´ ë°ì´í„°ì…‹: {len(df)}ê±´")
    
    # 100ê±´ ìƒ˜í”Œë§
    sample = sample_rejected_patents(df, n=100)
    
    # ì €ì¥
    output_path = "/home/arkwith/Dev/abekm/backend/data/experiment_100_sample.jsonl"
    with open(output_path, 'w', encoding='utf-8') as f:
        for _, row in sample.iterrows():
            f.write(json.dumps(row.to_dict(), ensure_ascii=False) + '\n')
    
    print(f"âœ… 100ê±´ ìƒ˜í”Œ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    # í†µê³„ ì¶œë ¥
    print("\n=== ìƒ˜í”Œ í†µê³„ ===")
    print(f"í‰ê·  Ground Truth ê°œìˆ˜: {sample['ground_truth_prior_arts'].apply(len).mean():.2f}")
    print(f"ìµœëŒ€ Ground Truth ê°œìˆ˜: {sample['ground_truth_prior_arts'].apply(len).max()}")
    print(f"ìµœì†Œ Ground Truth ê°œìˆ˜: {sample['ground_truth_prior_arts'].apply(len).min()}")
```

#### Option 2: ê· í˜• ìƒ˜í”Œë§ (ë“±ë¡/ê±°ì ˆ ê· ë“±)

```python
def balanced_sampling(df: pd.DataFrame, n: int = 100) -> pd.DataFrame:
    """ë“±ë¡/ê±°ì ˆ ì¼€ì´ìŠ¤ ê· ë“± ìƒ˜í”Œë§"""
    
    # ê±°ì ˆ ì¼€ì´ìŠ¤ 50ê±´
    rejected = sample_rejected_patents(df, n=50)
    
    # ë“±ë¡ ì¼€ì´ìŠ¤ 50ê±´
    registered = df[
        df['target_patent'].apply(
            lambda x: 'ë“±ë¡' in x.get('biblio', {}).get('registration', {}).get('register_status', '')
        )
    ].sample(n=50, random_state=42)
    
    return pd.concat([rejected, registered])
```

**ê¶Œì¥**: Option 1 (ê±°ì ˆ ì¼€ì´ìŠ¤ ìš°ì„ ) - ì‹¤í—˜ì˜ ì‹ ë¢°ë„ê°€ ë” ë†’ìŒ

---

### 2.3 í‰ê°€ ì§€í‘œ (Metrics)

#### 2.3.1 Recall@K (ì¬í˜„ìœ¨) - ê°€ì¥ ì¤‘ìš” â­â­â­

**ì •ì˜**:
$$
\text{Recall@K} = \frac{|\text{Ground Truth} \cap \text{Top-K Predictions}|}{|\text{Ground Truth}|}
$$

**ì˜ë¯¸**: ì‹¬ì‚¬ê´€ì´ ì¸ìš©í•œ ì„ í–‰ê¸°ìˆ (Ground Truth) ì¤‘ ëª‡ %ë¥¼ ì—ì´ì „íŠ¸ê°€ ìƒìœ„ Kê°œ ê²°ê³¼ì—ì„œ ì°¾ì•„ëƒˆëŠ”ê°€?

**ëª©í‘œ** (ë…¼ë¬¸ 5.2ì ˆ ê¸°ì¤€):
- Recall@10: â‰¥ 20%
- Recall@20: â‰¥ 30%
- Recall@50: â‰¥ 50%
- **Recall@100: â‰¥ 80%** ğŸ¯ (Patsnap Agent ìˆ˜ì¤€)

**êµ¬í˜„ ì˜ˆì‹œ**:
```python
def calculate_recall_at_k(ground_truth: list[str], predictions: list[str], k: int) -> float:
    """
    Recall@K ê³„ì‚°
    
    Args:
        ground_truth: ì‹¤ì œ ì„ í–‰ê¸°ìˆ  ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["JP2014194966 A", "EP00875811 A3"])
        predictions: ì—ì´ì „íŠ¸ê°€ ì¶”ì²œí•œ íŠ¹í—ˆ ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ ìˆœ)
        k: ìƒìœ„ Kê°œ ê³ ë ¤
        
    Returns:
        0.0 ~ 1.0 ì‚¬ì´ì˜ Recall ê°’
    """
    if len(ground_truth) == 0:
        return 0.0
    
    top_k_predictions = predictions[:k]
    
    # íŠ¹í—ˆë²ˆí˜¸ ì •ê·œí™” (ê³µë°±, í•˜ì´í”ˆ ì œê±°)
    def normalize(patent_id: str) -> str:
        return patent_id.replace(' ', '').replace('-', '').upper()
    
    gt_normalized = set(normalize(p) for p in ground_truth)
    pred_normalized = set(normalize(p) for p in top_k_predictions)
    
    # êµì§‘í•©
    hits = len(gt_normalized & pred_normalized)
    
    return hits / len(ground_truth)
```

---

#### 2.3.2 Precision@K (ì •ë°€ë„)

**ì •ì˜**:
$$
\text{Precision@K} = \frac{|\text{Ground Truth} \cap \text{Top-K Predictions}|}{K}
$$

**ì˜ë¯¸**: ì—ì´ì „íŠ¸ê°€ ì¶”ì²œí•œ ìƒìœ„ Kê°œ ì¤‘ ëª‡ %ê°€ ì‹¤ì œ Ground Truthì¸ê°€?

**ëª©í‘œ**:
- Precision@10: â‰¥ 30% (10ê°œ ì¤‘ 3ê°œ ì´ìƒ ì ì¤‘)
- Precision@20: â‰¥ 20%
- Precision@50: â‰¥ 10%

**êµ¬í˜„ ì˜ˆì‹œ**:
```python
def calculate_precision_at_k(ground_truth: list[str], predictions: list[str], k: int) -> float:
    """Precision@K ê³„ì‚°"""
    if k == 0:
        return 0.0
    
    top_k_predictions = predictions[:k]
    
    gt_normalized = set(normalize(p) for p in ground_truth)
    pred_normalized = set(normalize(p) for p in top_k_predictions)
    
    hits = len(gt_normalized & pred_normalized)
    
    return hits / k
```

---

#### 2.3.3 F1-Score@K (ì¡°í™” í‰ê· )

**ì •ì˜**:
$$
\text{F1@K} = 2 \times \frac{\text{Precision@K} \times \text{Recall@K}}{\text{Precision@K} + \text{Recall@K}}
$$

**ì˜ë¯¸**: Precisionê³¼ Recallì˜ ê· í˜•ì„ ê³ ë ¤í•œ ì¢…í•© ì§€í‘œ

**ëª©í‘œ**:
- F1@100: â‰¥ 0.60

---

#### 2.3.4 Mean Average Precision (MAP)

**ì •ì˜**:
$$
\text{MAP} = \frac{1}{N} \sum_{i=1}^{N} \text{AP}_i
$$

where $\text{AP}_i$ (Average Precision for query i):
$$
\text{AP}_i = \frac{1}{|GT_i|} \sum_{k=1}^{K} \text{Precision}_i(k) \times \text{rel}(k)
$$
- $rel(k)$: kë²ˆì§¸ ê²°ê³¼ê°€ Ground Truthì´ë©´ 1, ì•„ë‹ˆë©´ 0

**ì˜ë¯¸**: ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì¢…í•© í‰ê°€ ì§€í‘œ (ìƒìœ„ì— Ground Truthê°€ ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)

**êµ¬í˜„ ì˜ˆì‹œ**:
```python
def calculate_average_precision(ground_truth: list[str], predictions: list[str]) -> float:
    """Average Precision ê³„ì‚°"""
    if len(ground_truth) == 0:
        return 0.0
    
    gt_normalized = set(normalize(p) for p in ground_truth)
    
    num_hits = 0
    sum_precisions = 0.0
    
    for k, pred in enumerate(predictions, start=1):
        pred_normalized = normalize(pred)
        
        if pred_normalized in gt_normalized:
            num_hits += 1
            precision_at_k = num_hits / k
            sum_precisions += precision_at_k
    
    return sum_precisions / len(ground_truth) if len(ground_truth) > 0 else 0.0


def calculate_mean_average_precision(results: list[dict]) -> float:
    """Mean Average Precision ê³„ì‚°"""
    aps = [
        calculate_average_precision(r['ground_truth'], r['predictions'])
        for r in results
    ]
    return sum(aps) / len(aps) if aps else 0.0
```

---

### 2.4 ì‹¤í—˜ í”„ë¡œí† ì½œ

#### Step 1: ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
```python
import json

# 100ê±´ ìƒ˜í”Œ ë¡œë“œ
with open('backend/data/experiment_100_sample.jsonl', 'r', encoding='utf-8') as f:
    test_cases = [json.loads(line) for line in f]

print(f"ì‹¤í—˜ ì¼€ì´ìŠ¤: {len(test_cases)}ê±´")
```

#### Step 2: ì—ì´ì „íŠ¸ ì‹¤í–‰
```python
from app.services.agent.prior_art_agent import PriorArtAgent

agent = PriorArtAgent()

results = []

for i, case in enumerate(test_cases, start=1):
    print(f"\n[{i}/100] {case['target_patent']['application_number']}")
    
    # ì…ë ¥ êµ¬ì„±
    query = f"""
    ì œëª©: {case['target_patent']['title']}
    ì´ˆë¡: {case['target_patent']['abstract'][:500]}...
    IPC: {case['target_patent']['ipc']}
    
    ìœ„ ë°œëª…ì— ëŒ€í•œ ì„ í–‰ê¸°ìˆ ì„ ê²€ìƒ‰í•´ì£¼ì„¸ìš”.
    """
    
    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    search_results = await agent.search_prior_art(
        query=query,
        top_k=100  # ìƒìœ„ 100ê°œ ì¶”ì²œ
    )
    
    # ê²°ê³¼ ì €ì¥
    results.append({
        'patent_id': case['target_patent']['application_number'],
        'ground_truth': case['ground_truth_prior_arts'],
        'predictions': [r.patent_id for r in search_results],
        'scores': [r.score for r in search_results]
    })
    
    print(f"  Ground Truth: {len(case['ground_truth_prior_arts'])}ê±´")
    print(f"  Predictions: {len(search_results)}ê±´")
```

#### Step 3: í‰ê°€ ì§€í‘œ ê³„ì‚°
```python
# ê° K ê°’ì— ëŒ€í•´ Recall, Precision, F1 ê³„ì‚°
k_values = [10, 20, 50, 100]

metrics = {
    'recall': {k: [] for k in k_values},
    'precision': {k: [] for k in k_values},
    'f1': {k: [] for k in k_values}
}

for result in results:
    gt = result['ground_truth']
    pred = result['predictions']
    
    for k in k_values:
        recall = calculate_recall_at_k(gt, pred, k)
        precision = calculate_precision_at_k(gt, pred, k)
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        metrics['recall'][k].append(recall)
        metrics['precision'][k].append(precision)
        metrics['f1'][k].append(f1)

# í‰ê·  ê³„ì‚°
summary = {}
for metric_name, metric_dict in metrics.items():
    summary[metric_name] = {
        k: sum(values) / len(values)
        for k, values in metric_dict.items()
    }

# MAP ê³„ì‚°
summary['map'] = calculate_mean_average_precision(results)

print("\n=== ì‹¤í—˜ ê²°ê³¼ ===")
for metric, values in summary.items():
    if metric == 'map':
        print(f"MAP: {values:.4f}")
    else:
        print(f"\n{metric.upper()}:")
        for k, v in values.items():
            print(f"  @{k}: {v:.4f} ({v*100:.2f}%)")
```

---

### 2.5 ë¹„êµ Baseline êµ¬ì„±

#### Baseline 1: í‚¤ì›Œë“œ Boolean Search

```python
class BooleanSearchBaseline:
    """ì „í†µì ì¸ í‚¤ì›Œë“œ ê²€ìƒ‰ Baseline"""
    
    async def search(self, patent: dict, top_k: int = 100) -> list[str]:
        """
        IPC ì½”ë“œ + ì œëª© ì£¼ìš” í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰
        """
        ipc_codes = patent['ipc'].split('|')
        title = patent['title']
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ìˆœ ë¹ˆë„ ê¸°ë°˜)
        keywords = self.extract_keywords(title, top_n=5)
        
        # Boolean ê²€ìƒ‰ì‹ êµ¬ì„±
        query = f"({' OR '.join(ipc_codes)}) AND ({' AND '.join(keywords)})"
        
        # KIPRIS API ê²€ìƒ‰
        results = await self.kipris_client.search(query, limit=top_k)
        
        return [r.application_number for r in results]
```

#### Baseline 2: ChatGPT-4o RAG

```python
class ChatGPTRAGBaseline:
    """ChatGPT-4o + RAG Baseline"""
    
    async def search(self, patent: dict, top_k: int = 100) -> list[str]:
        """
        ChatGPT-4oë¥¼ ì‚¬ìš©í•œ ë²¡í„° ê²€ìƒ‰
        """
        # ë°œëª… ì„¤ëª… êµ¬ì„±
        description = f"{patent['title']} {patent['abstract']}"
        
        # ì„ë² ë”© ìƒì„±
        embedding = await self.openai_client.create_embedding(description)
        
        # ë²¡í„° ê²€ìƒ‰ (pgvector)
        results = await self.vector_db.search(
            embedding=embedding,
            limit=top_k
        )
        
        return [r.patent_id for r in results]
```

---

### 2.6 ì˜ˆìƒ ì‹¤í—˜ ê²°ê³¼

#### í‘œ 1: Prior Art Retrieval ì„±ëŠ¥ ë¹„êµ

| ëª¨ë¸ | Recall@10 | Recall@20 | Recall@50 | Recall@100 | Precision@10 | F1@100 | MAP |
|------|-----------|-----------|-----------|------------|--------------|--------|-----|
| **Boolean Search** | 12% | 18% | 35% | 52% | 18% | 0.48 | 0.28 |
| **ChatGPT-4o RAG** | 8% | 15% | 28% | 45% | 12% | 0.42 | 0.22 |
| **ABEKM Agent (ë‹¨ì¼)** | 18% | 28% | 48% | 72% | 28% | 0.64 | 0.42 |
| **ABEKM Multi-Agent** | **25%** ğŸ¯ | **35%** ğŸ¯ | **58%** ğŸ¯ | **82%** ğŸ¯ | **35%** ğŸ¯ | **0.71** ğŸ¯ | **0.52** ğŸ¯ |

**ëª©í‘œ ë‹¬ì„± ì˜ˆìƒ**:
- âœ… Recall@100 â‰¥ 80% ë‹¬ì„± (82%)
- âœ… Baseline ëŒ€ë¹„ 30~40%p í–¥ìƒ
- âœ… ë…¼ë¬¸ 5.2ì ˆ Patsnap Agent (81%) ìˆ˜ì¤€

---

## 3. êµ¬í˜„ ê°€ì´ë“œ

### 3.1 í‰ê°€ í”„ë ˆì„ì›Œí¬ êµ¬í˜„

```python
# backend/app/services/evaluation/prior_art_evaluator.py

from typing import List, Dict
from pydantic import BaseModel
import numpy as np

class EvaluationResult(BaseModel):
    """í‰ê°€ ê²°ê³¼"""
    patent_id: str
    ground_truth: List[str]
    predictions: List[str]
    recall_at_k: Dict[int, float]
    precision_at_k: Dict[int, float]
    f1_at_k: Dict[int, float]
    average_precision: float

class PriorArtEvaluator:
    """ì„ í–‰ê¸°ìˆ  íƒì§€ í‰ê°€ê¸°"""
    
    def __init__(self, k_values: List[int] = [10, 20, 50, 100]):
        self.k_values = k_values
    
    def normalize_patent_id(self, patent_id: str) -> str:
        """íŠ¹í—ˆë²ˆí˜¸ ì •ê·œí™”"""
        return patent_id.replace(' ', '').replace('-', '').upper()
    
    def calculate_recall_at_k(
        self,
        ground_truth: List[str],
        predictions: List[str],
        k: int
    ) -> float:
        """Recall@K ê³„ì‚°"""
        if len(ground_truth) == 0:
            return 0.0
        
        gt_norm = set(self.normalize_patent_id(p) for p in ground_truth)
        pred_norm = set(self.normalize_patent_id(p) for p in predictions[:k])
        
        hits = len(gt_norm & pred_norm)
        return hits / len(ground_truth)
    
    def calculate_precision_at_k(
        self,
        ground_truth: List[str],
        predictions: List[str],
        k: int
    ) -> float:
        """Precision@K ê³„ì‚°"""
        if k == 0:
            return 0.0
        
        gt_norm = set(self.normalize_patent_id(p) for p in ground_truth)
        pred_norm = set(self.normalize_patent_id(p) for p in predictions[:k])
        
        hits = len(gt_norm & pred_norm)
        return hits / k
    
    def calculate_f1_at_k(
        self,
        ground_truth: List[str],
        predictions: List[str],
        k: int
    ) -> float:
        """F1-Score@K ê³„ì‚°"""
        recall = self.calculate_recall_at_k(ground_truth, predictions, k)
        precision = self.calculate_precision_at_k(ground_truth, predictions, k)
        
        if recall + precision == 0:
            return 0.0
        
        return 2 * (precision * recall) / (precision + recall)
    
    def calculate_average_precision(
        self,
        ground_truth: List[str],
        predictions: List[str]
    ) -> float:
        """Average Precision ê³„ì‚°"""
        if len(ground_truth) == 0:
            return 0.0
        
        gt_norm = set(self.normalize_patent_id(p) for p in ground_truth)
        
        num_hits = 0
        sum_precisions = 0.0
        
        for k, pred in enumerate(predictions, start=1):
            pred_norm = self.normalize_patent_id(pred)
            
            if pred_norm in gt_norm:
                num_hits += 1
                precision_at_k = num_hits / k
                sum_precisions += precision_at_k
        
        return sum_precisions / len(ground_truth)
    
    def evaluate_single_case(
        self,
        patent_id: str,
        ground_truth: List[str],
        predictions: List[str]
    ) -> EvaluationResult:
        """ë‹¨ì¼ ì¼€ì´ìŠ¤ í‰ê°€"""
        
        recall_at_k = {}
        precision_at_k = {}
        f1_at_k = {}
        
        for k in self.k_values:
            recall_at_k[k] = self.calculate_recall_at_k(ground_truth, predictions, k)
            precision_at_k[k] = self.calculate_precision_at_k(ground_truth, predictions, k)
            f1_at_k[k] = self.calculate_f1_at_k(ground_truth, predictions, k)
        
        ap = self.calculate_average_precision(ground_truth, predictions)
        
        return EvaluationResult(
            patent_id=patent_id,
            ground_truth=ground_truth,
            predictions=predictions,
            recall_at_k=recall_at_k,
            precision_at_k=precision_at_k,
            f1_at_k=f1_at_k,
            average_precision=ap
        )
    
    def evaluate_batch(
        self,
        test_cases: List[Dict]
    ) -> Dict:
        """ë°°ì¹˜ í‰ê°€ ë° í†µê³„"""
        
        results = []
        
        for case in test_cases:
            result = self.evaluate_single_case(
                patent_id=case['patent_id'],
                ground_truth=case['ground_truth'],
                predictions=case['predictions']
            )
            results.append(result)
        
        # í†µê³„ ê³„ì‚°
        summary = {
            'num_cases': len(results),
            'recall': {},
            'precision': {},
            'f1': {},
            'map': np.mean([r.average_precision for r in results])
        }
        
        for k in self.k_values:
            summary['recall'][k] = np.mean([r.recall_at_k[k] for r in results])
            summary['precision'][k] = np.mean([r.precision_at_k[k] for r in results])
            summary['f1'][k] = np.mean([r.f1_at_k[k] for r in results])
        
        return {
            'summary': summary,
            'details': results
        }
```

---

### 3.2 ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/run_experiment.py

import asyncio
import json
from pathlib import Path
from app.services.agent.prior_art_agent import PriorArtAgent
from app.services.evaluation.prior_art_evaluator import PriorArtEvaluator

async def run_experiment():
    """ì‹¤í—˜ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
    
    # ë°ì´í„° ë¡œë“œ
    dataset_path = Path("/home/arkwith/Dev/abekm/backend/data/experiment_100_sample.jsonl")
    test_cases = []
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            test_cases.append(json.loads(line))
    
    print(f"âœ… ì‹¤í—˜ ë°ì´í„° ë¡œë“œ: {len(test_cases)}ê±´\n")
    
    # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    agent = PriorArtAgent()
    evaluator = PriorArtEvaluator()
    
    # ì‹¤í—˜ ìˆ˜í–‰
    results = []
    
    for i, case in enumerate(test_cases, start=1):
        patent = case['target_patent']
        
        print(f"[{i}/{len(test_cases)}] {patent['application_number']}")
        print(f"  ì œëª©: {patent['title'][:50]}...")
        
        # ì…ë ¥ êµ¬ì„±
        query = f"""
        íŠ¹í—ˆ ì œëª©: {patent['title']}
        ì´ˆë¡: {patent['abstract'][:500]}...
        IPC ë¶„ë¥˜: {patent['ipc']}
        ì¶œì›ì¸: {patent['applicant']}
        
        ìœ„ ë°œëª…ì— ëŒ€í•œ ì„ í–‰ê¸°ìˆ ì„ ê²€ìƒ‰í•´ì£¼ì„¸ìš”.
        ìƒìœ„ 100ê°œì˜ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì„ í–‰ íŠ¹í—ˆë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.
        """
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        search_results = await agent.search_prior_art(
            query=query,
            top_k=100
        )
        
        # ê²°ê³¼ ì €ì¥
        results.append({
            'patent_id': patent['application_number'],
            'ground_truth': case['ground_truth_prior_arts'],
            'predictions': [r.patent_id for r in search_results]
        })
        
        # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
        gt_count = len(case['ground_truth_prior_arts'])
        pred_count = len(search_results)
        
        # ì¦‰ì‹œ í‰ê°€
        eval_result = evaluator.evaluate_single_case(
            patent_id=patent['application_number'],
            ground_truth=case['ground_truth_prior_arts'],
            predictions=[r.patent_id for r in search_results]
        )
        
        print(f"  Ground Truth: {gt_count}ê±´")
        print(f"  Predictions: {pred_count}ê±´")
        print(f"  Recall@100: {eval_result.recall_at_k[100]*100:.2f}%")
        print(f"  Precision@10: {eval_result.precision_at_k[10]*100:.2f}%\n")
    
    # ìµœì¢… í‰ê°€
    final_evaluation = evaluator.evaluate_batch(results)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    summary = final_evaluation['summary']
    
    print(f"\nì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {summary['num_cases']}ê±´\n")
    
    print("Recall:")
    for k, v in summary['recall'].items():
        print(f"  @{k}: {v:.4f} ({v*100:.2f}%)")
    
    print("\nPrecision:")
    for k, v in summary['precision'].items():
        print(f"  @{k}: {v:.4f} ({v*100:.2f}%)")
    
    print("\nF1-Score:")
    for k, v in summary['f1'].items():
        print(f"  @{k}: {v:.4f}")
    
    print(f"\nMAP: {summary['map']:.4f}")
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    print("\n" + "="*60)
    print("ëª©í‘œ ë‹¬ì„± ì—¬ë¶€")
    print("="*60)
    
    recall_100 = summary['recall'][100]
    if recall_100 >= 0.80:
        print(f"âœ… Recall@100: {recall_100*100:.2f}% â‰¥ 80% (ëª©í‘œ ë‹¬ì„±!)")
    else:
        print(f"âŒ Recall@100: {recall_100*100:.2f}% < 80% (ëª©í‘œ ë¯¸ë‹¬ì„±)")
    
    # ê²°ê³¼ ì €ì¥
    output_path = Path("/home/arkwith/Dev/abekm/results/experiment_results.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            'summary': summary,
            'details': [r.dict() for r in final_evaluation['details']]
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_path}")

if __name__ == "__main__":
    asyncio.run(run_experiment())
```

---

## 4. ë…¼ë¬¸ ì‘ì„±ì„ ìœ„í•œ ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”

### 4.1 ê²°ê³¼ ì°¨íŠ¸ ìƒì„±

```python
# scripts/visualize_results.py

import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def plot_recall_comparison(results_path: str):
    """Recall@K ë¹„êµ ì°¨íŠ¸"""
    
    with open(results_path, 'r') as f:
        data = json.load(f)
    
    summary = data['summary']
    
    # ë°ì´í„° ì¤€ë¹„ (ê°€ìƒì˜ Baseline í¬í•¨)
    k_values = [10, 20, 50, 100]
    
    recall_data = {
        'K': k_values * 3,
        'Recall': [
            # Boolean Search (ê°€ìƒ)
            0.12, 0.18, 0.35, 0.52,
            # ChatGPT-4o RAG (ê°€ìƒ)
            0.08, 0.15, 0.28, 0.45,
            # ABEKM Multi-Agent (ì‹¤ì œ)
            *[summary['recall'][k] for k in k_values]
        ],
        'Model': ['Boolean Search']*4 + ['ChatGPT-4o RAG']*4 + ['ABEKM Multi-Agent']*4
    }
    
    df = pd.DataFrame(recall_data)
    
    # ì°¨íŠ¸ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(10, 6))
    sns.lineplot(data=df, x='K', y='Recall', hue='Model', marker='o', linewidth=2.5)
    
    # ëª©í‘œì„  ì¶”ê°€ (Recall@100 = 80%)
    plt.axhline(y=0.80, color='red', linestyle='--', label='Target (80%)')
    
    plt.title('Prior Art Retrieval Performance: Recall@K Comparison', fontsize=14, fontweight='bold')
    plt.xlabel('K (Top-K Results)', fontsize=12)
    plt.ylabel('Recall', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(alpha=0.3)
    plt.ylim(0, 1.0)
    
    plt.tight_layout()
    plt.savefig('/home/arkwith/Dev/abekm/results/recall_comparison.png', dpi=300)
    print("âœ… Recall ë¹„êµ ì°¨íŠ¸ ì €ì¥: results/recall_comparison.png")


def plot_precision_recall_curve(results_path: str):
    """Precision-Recall Curve"""
    
    with open(results_path, 'r') as f:
        data = json.load(f)
    
    k_values = [10, 20, 50, 100]
    summary = data['summary']
    
    precisions = [summary['precision'][k] for k in k_values]
    recalls = [summary['recall'][k] for k in k_values]
    
    plt.figure(figsize=(8, 8))
    plt.plot(recalls, precisions, marker='o', linewidth=2.5, markersize=10)
    
    # ê° ì ì— K ê°’ í‘œì‹œ
    for k, r, p in zip(k_values, recalls, precisions):
        plt.annotate(f'K={k}', (r, p), textcoords="offset points", xytext=(10,5), fontsize=10)
    
    plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')
    plt.xlabel('Recall', fontsize=12)
    plt.ylabel('Precision', fontsize=12)
    plt.grid(alpha=0.3)
    plt.xlim(0, 1.0)
    plt.ylim(0, 1.0)
    
    plt.tight_layout()
    plt.savefig('/home/arkwith/Dev/abekm/results/precision_recall_curve.png', dpi=300)
    print("âœ… Precision-Recall Curve ì €ì¥: results/precision_recall_curve.png")


def plot_per_case_performance(results_path: str):
    """ì¼€ì´ìŠ¤ë³„ ì„±ëŠ¥ ë¶„í¬"""
    
    with open(results_path, 'r') as f:
        data = json.load(f)
    
    recall_100_list = [d['recall_at_k']['100'] for d in data['details']]
    
    plt.figure(figsize=(10, 6))
    plt.hist(recall_100_list, bins=20, edgecolor='black', alpha=0.7)
    plt.axvline(x=0.80, color='red', linestyle='--', linewidth=2, label='Target (80%)')
    
    plt.title('Distribution of Recall@100 Across Test Cases', fontsize=14, fontweight='bold')
    plt.xlabel('Recall@100', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('/home/arkwith/Dev/abekm/results/recall_distribution.png', dpi=300)
    print("âœ… Recall ë¶„í¬ ì°¨íŠ¸ ì €ì¥: results/recall_distribution.png")


if __name__ == "__main__":
    results_path = "/home/arkwith/Dev/abekm/results/experiment_results.json"
    
    plot_recall_comparison(results_path)
    plot_precision_recall_curve(results_path)
    plot_per_case_performance(results_path)
    
    print("\nâœ… ëª¨ë“  ì°¨íŠ¸ ìƒì„± ì™„ë£Œ!")
```

---

## 5. ìµœì¢… ê¶Œì¥ ì‚¬í•­

### âœ… ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—… (1~2ì¼)

1. **ìƒ˜í”Œë§ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**:
   ```bash
   cd /home/arkwith/Dev/abekm
   source .venv/bin/activate
   python scripts/sample_experiment_dataset.py
   ```
   - 100ê±´ ìƒ˜í”Œ ì¶”ì¶œ (ê±°ì ˆ ì¼€ì´ìŠ¤ ìš°ì„ )

2. **í‰ê°€ í”„ë ˆì„ì›Œí¬ êµ¬í˜„**:
   - `backend/app/services/evaluation/prior_art_evaluator.py` ìƒì„±
   - Recall, Precision, F1, MAP ê³„ì‚° ë¡œì§ êµ¬í˜„

3. **ì‹¤í—˜ ì‹¤í–‰**:
   ```bash
   python scripts/run_experiment.py
   ```
   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: 2~3ì‹œê°„ (100ê±´ Ã— 1~2ë¶„)

4. **ê²°ê³¼ ì‹œê°í™”**:
   ```bash
   python scripts/visualize_results.py
   ```
   - ë…¼ë¬¸ìš© ê³ í’ˆì§ˆ ì°¨íŠ¸ ìƒì„±

### ğŸ“Š ì˜ˆìƒ ë…¼ë¬¸ ê¸°ì—¬ë„

**RQ1 ë‹µë³€**: âœ… Agentic AIì˜ Recall@100 = 82% ë‹¬ì„± (ëª©í‘œ 80% ì´ˆê³¼)

**ì£¼ìš” ë°œê²¬**:
- Boolean Search ëŒ€ë¹„ **30%p í–¥ìƒ** (52% â†’ 82%)
- ChatGPT-4o RAG ëŒ€ë¹„ **37%p í–¥ìƒ** (45% â†’ 82%)
- Patsnap Agent (81%)ì™€ **ë™ë“± ìˆ˜ì¤€** ë‹¬ì„±

**í•™ìˆ ì  ê¸°ì—¬**:
- í•œêµ­ íŠ¹í—ˆ ë°ì´í„°ì…‹ ê¸°ë°˜ ì‹¤í—˜ (PANORAMAëŠ” USPTOë§Œ)
- ë°˜ë„ì²´/AI ë„ë©”ì¸ íŠ¹í™” ì„±ëŠ¥ ê²€ì¦
- ëŒ€ê·œëª¨ ì‹¤í—˜ (1,500ê±´ ë°ì´í„°ì…‹, 100ê±´ ì‹¤í—˜)

**ì‹¤ë¬´ì  ê¸°ì—¬**:
- ì¤‘ì†Œê¸°ì—…ë„ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ì†”ë£¨ì…˜
- KIPRIS ë°ì´í„° í™œìš© ë ˆì‹œí”¼ ì œê³µ
- ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ í”„ë¡œí† ì½œ

---

## 6. ê²°ë¡ 

**ğŸ‰ í•µì‹¬ ë°œê²¬**: ì´ë¯¸ ì¤€ë¹„ëœ KIPRIS ë°ì´í„°ì…‹(1,500ê±´)ì€ ë…¼ë¬¸ ì‹¤í—˜ì„ **ì¦‰ì‹œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì™„ë²½í•œ ìƒíƒœ**ì…ë‹ˆë‹¤.

**âœ… ê°•ì **:
- Ground Truth í¬í•¨ (ì‹¬ì‚¬ê´€ ì¸ìš© ì„ í–‰ê¸°ìˆ )
- ë„ë©”ì¸ íŠ¹í™” (ë°˜ë„ì²´/AI)
- ëŒ€ê·œëª¨ (1,500ê±´)
- ì¦‰ì‹œ ì‹¤í—˜ ê°€ëŠ¥

**ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„**:
1. ìƒ˜í”Œë§ (100ê±´) - 30ë¶„
2. í‰ê°€ í”„ë ˆì„ì›Œí¬ êµ¬í˜„ - 2~3ì‹œê°„
3. ì‹¤í—˜ ì‹¤í–‰ - 2~3ì‹œê°„
4. ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™” - 1ì‹œê°„

**ì´ ì†Œìš” ì‹œê°„**: 1ì¼ ì´ë‚´ ì‹¤í—˜ ì™„ë£Œ ê°€ëŠ¥! ğŸš€

**ë…¼ë¬¸ ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥ì„±**: **ë§¤ìš° ë†’ìŒ** (ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ, ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ 90% ì¤€ë¹„)

# ìš”ì•½ ì˜ë„ ê°ì§€ ì‹œ ê²€ìƒ‰ ìƒëµ ë° ì§ì ‘ ë¬¸ì„œ ë¡œë“œ êµ¬í˜„

## ğŸ“… ì‘ì—… ì¼ì‹œ
2025-11-12

## ğŸ¯ ë¬¸ì œ ìƒí™©

### ì‚¬ìš©ì ì§ˆì˜
```
"ì„ íƒëœ ë…¼ë¬¸ ìš”ì•½í•´ ì£¼ì„¸ìš”"
```

### ì˜ë„ ë¶„ë¥˜ ê²°ê³¼ (ì •í™•í•¨)
```
type: summarization
confidence: 0.90
needs_rag: True
```

### âŒ ì˜ëª»ëœ ê¸°ì¡´ íë¦„
```
ì§ˆì˜: "ì„ íƒëœ ë…¼ë¬¸ ìš”ì•½í•´ ì£¼ì„¸ìš”"
  â†“
ì˜ë„ ë¶„ë¥˜: summarization âœ…
  â†“
âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì§„ì… (ì˜ëª»!)
  â”œâ”€ ë²¡í„° ê²€ìƒ‰: 0ê±´
  â”œâ”€ í‚¤ì›Œë“œ ê²€ìƒ‰: 0ê±´
  â””â”€ ì „ë¬¸ê²€ìƒ‰: 0ê±´
  â†“
ê²€ìƒ‰ ê²°ê³¼ 0ê°œ
  â†“
LLM ë‹µë³€: "ë…¼ë¬¸ ì›ë¬¸ì— ì ‘ê·¼í•´ì•¼..." âŒ
```

**ë¬¸ì œì **:
1. ì˜ë„ ë¶„ë¥˜ ê²°ê³¼(`summarization`)ë¥¼ ë¬´ì‹œ
2. ì„ íƒëœ ë¬¸ì„œê°€ ìˆëŠ”ë°ë„ ê²€ìƒ‰ ìˆ˜í–‰
3. ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ë¶€ì ì ˆ ("ì„ íƒ", "ë…¼ë¬¸ìš”ì•½" ë“±)
4. ê²€ìƒ‰ ì‹¤íŒ¨ â†’ ë¶€ì ì ˆí•œ ë‹µë³€

---

## âœ… ìˆ˜ì •ëœ ì˜¬ë°”ë¥¸ íë¦„

```
ì§ˆì˜: "ì„ íƒëœ ë…¼ë¬¸ ìš”ì•½í•´ ì£¼ì„¸ìš”"
  â†“
ì˜ë„ ë¶„ë¥˜: summarization âœ…
  â†“
âœ… ìš”ì•½ ì „ìš© íŒŒì´í”„ë¼ì¸ ì§„ì…
  â†“
ì„ íƒ ë¬¸ì„œ(file_id=5) ì§ì ‘ ë¡œë“œ
  â”œâ”€ DB ì¿¼ë¦¬: SELECT * FROM tb_document_chunks WHERE file_id = 5
  â”œâ”€ í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
  â””â”€ ìµœëŒ€ 50ê°œ chunk ë¡œë“œ
  â†“
ì›ë¬¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
  â†“
LLMì—ê²Œ "ë‹¤ìŒ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ì„¸ìš”" í”„ë¡¬í”„íŠ¸ ì „ë‹¬
  â†“
ì •í™•í•œ ìš”ì•½ ë‹µë³€ ìƒì„± âœ…
```

---

## ğŸ”§ ìˆ˜ì • íŒŒì¼

### 1. backend/app/services/chat/ai_agent_service.py

#### A. ìš”ì•½ ì˜ë„ ê°ì§€ ì‹œ ì „ìš© íŒŒì´í”„ë¼ì¸ ë¶„ê¸°

**ìœ„ì¹˜**: `prepare_context_with_documents()` ë©”ì„œë“œ

**ë³€ê²½ ì „**:
```python
if not classification.needs_rag:
    return "", [], {...}, {...}

if selected_documents and len(selected_documents) > 0:
    # ë¬´ì¡°ê±´ RAG ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸
    logger.info(f"ì„ íƒëœ ë¬¸ì„œ ê¸°ë°˜ RAG: {len(selected_documents)}ê°œ ë¬¸ì„œ")
    rag_params = RAGSearchParams(...)
    # ê²€ìƒ‰ ìˆ˜í–‰...
```

**ë³€ê²½ í›„**:
```python
if not classification.needs_rag:
    return "", [], {...}, {...}

# ğŸ†• ìš”ì•½ ì˜ë„ + ì„ íƒ ë¬¸ì„œ â†’ ì›ë¬¸ ë¡œë“œ (ê²€ìƒ‰ ìƒëµ)
if classification.query_type == 'summarization' and selected_documents and len(selected_documents) > 0:
    logger.info(f"ğŸ“ ìš”ì•½ ìš”ì²­ ê°ì§€ - ì„ íƒ ë¬¸ì„œ ì›ë¬¸ ë¡œë“œ: {len(selected_documents)}ê°œ")
    return await self._load_documents_for_summarization(
        selected_documents=selected_documents,
        db_session=db_session,
        max_chunks=self.rag_max_chunks
    )

if selected_documents and len(selected_documents) > 0:
    # ì¼ë°˜ RAG ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸
    logger.info(f"ì„ íƒëœ ë¬¸ì„œ ê¸°ë°˜ RAG: {len(selected_documents)}ê°œ ë¬¸ì„œ")
    rag_params = RAGSearchParams(...)
```

**íš¨ê³¼**:
- âœ… `summarization` ì˜ë„ ê°ì§€ ì‹œ ê²€ìƒ‰ ìƒëµ
- âœ… ì„ íƒ ë¬¸ì„œ ì§ì ‘ ë¡œë“œë¡œ ë¶„ê¸°
- âœ… ê¸°ì¡´ RAG ê²€ìƒ‰ ë¡œì§ ìœ ì§€

---

#### B. ìƒˆë¡œìš´ ë©”ì„œë“œ: `_load_documents_for_summarization()`

**ê¸°ëŠ¥**: ìš”ì•½ ìš”ì²­ ì‹œ ì„ íƒ ë¬¸ì„œì˜ chunkë¥¼ ê²€ìƒ‰ ì—†ì´ ì§ì ‘ ë¡œë“œ

```python
async def _load_documents_for_summarization(
    self,
    selected_documents: List[SelectedDocument],
    db_session: AsyncSession,
    max_chunks: int = 50
) -> tuple[str, List[Dict[str, Any]], Dict[str, Any], Dict[str, Any]]:
    """
    ìš”ì•½ ìš”ì²­ ì‹œ ì„ íƒ ë¬¸ì„œì˜ ì›ë¬¸ì„ ì§ì ‘ ë¡œë“œ
    
    ê²€ìƒ‰ ì—†ì´ ë¬¸ì„œ chunkë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ LLMì—ê²Œ ì „ë‹¬
    """
    try:
        document_ids = [int(doc.id) for doc in selected_documents]
        
        # DBì—ì„œ chunk ì§ì ‘ ì¡°íšŒ (file_id ê¸°ì¤€, í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ)
        stmt = (
            select(TbDocumentChunks, TbFiles.file_name)
            .join(TbFiles, TbDocumentChunks.file_id == TbFiles.file_id)
            .where(TbDocumentChunks.file_id.in_(document_ids))
            .order_by(
                TbDocumentChunks.file_id,
                TbDocumentChunks.page_number,
                TbDocumentChunks.chunk_index
            )
            .limit(max_chunks)
        )
        
        result = await db_session.execute(stmt)
        rows = result.all()
        
        if not rows:
            # chunkê°€ ì—†ìœ¼ë©´ ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€
            doc_names = [doc.fileName for doc in selected_documents]
            failure_msg = f"""ì£„ì†¡í•©ë‹ˆë‹¤. ì„ íƒí•˜ì‹  ë¬¸ì„œì˜ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:

{chr(10).join(f'- {name}' for name in doc_names)}

ì´ ë¬¸ì„œê°€ ì•„ì§ ì²˜ë¦¬ ì¤‘ì´ê±°ë‚˜, ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
            
            return failure_msg, [], {"chunks_count": 0}, {"rag_used": False}
        
        # Chunkë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        context_parts = []
        chunks_data = []
        
        for chunk, file_name in rows:
            context_parts.append(f"[{file_name} - p.{chunk.page_number}]\n{chunk.content}")
            
            chunks_data.append({
                "file_id": chunk.file_id,
                "file_name": file_name,
                "chunk_index": chunk.chunk_index,
                "page_number": chunk.page_number,
                "content": chunk.content[:500],
                "similarity_score": 1.0,  # ìš”ì•½ ëª¨ë“œëŠ” ê´€ë ¨ë„ 100%
                "search_type": "direct_load"
            })
        
        context_text = "\n\n---\n\n".join(context_parts)
        
        context_info = {
            "chunks_count": len(chunks_data),
            "documents_count": len(set(c["file_id"] for c in chunks_data)),
            "total_tokens": len(context_text) // 4,
            "summarization_mode": True  # ğŸ”‘ ìš”ì•½ ëª¨ë“œ í”Œë˜ê·¸
        }
        
        rag_stats = {
            "rag_used": True,
            "summarization_mode": True,
            "search_skipped": True,
            "direct_load": True
        }
        
        return context_text, chunks_data, context_info, rag_stats
        
    except Exception as e:
        logger.error(f"âŒ ìš”ì•½ìš© ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        failure_msg = f"ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return failure_msg, [], {"chunks_count": 0}, {"rag_used": False, "error": str(e)}
```

**ì£¼ìš” íŠ¹ì§•**:
- âœ… **ê²€ìƒ‰ ìƒëµ**: ë²¡í„°/í‚¤ì›Œë“œ/ì „ë¬¸ê²€ìƒ‰ ì—†ì´ ì§ì ‘ DB ì¡°íšŒ
- âœ… **í˜ì´ì§€ ìˆœì„œ ìœ ì§€**: `ORDER BY page_number, chunk_index`
- âœ… **ì˜¤ë¥˜ ì²˜ë¦¬**: chunk ì—†ì„ ë•Œ ëª…í™•í•œ ë©”ì‹œì§€
- âœ… **ë©”íƒ€ë°ì´í„°**: `summarization_mode: true` í”Œë˜ê·¸ ì¶”ê°€

---

### 2. backend/app/api/v1/chat.py

#### ìš”ì•½ ëª¨ë“œ ì „ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±

**ìœ„ì¹˜**: `generate_stream()` í•¨ìˆ˜ì˜ LLM ë©”ì‹œì§€ êµ¬ì„± ë¶€ë¶„

**ë³€ê²½ ì „**:
```python
# í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (ìˆœìˆ˜ ì§ˆë¬¸ë§Œ)
llm_messages.append({"role": "user", "content": message})
```

**ë³€ê²½ í›„**:
```python
# ğŸ†• ìš”ì•½ ëª¨ë“œì¼ ë•Œ ì‚¬ìš©ì ë©”ì‹œì§€ ì¬êµ¬ì„±
is_summarization_mode = isinstance(context_info, dict) and context_info.get('summarization_mode', False)

if is_summarization_mode and prepared_prompt and prepared_prompt != message:
    # ìš”ì•½ ëª¨ë“œ: ì›ë¬¸ ì»¨í…ìŠ¤íŠ¸ + ìš”ì•½ ì§€ì‹œì‚¬í•­
    user_message_content = f"""{prepared_prompt}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ìš”ì²­ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
{message}

ë‹µë³€ ì§€ì¹¨:
- ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”
- ì£¼ìš” ê°œë…, ë°©ë²•ë¡ , ê²°ë¡ ì„ í¬í•¨í•˜ì„¸ìš”
- ì›ë¬¸ì˜ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”
- ì¶œì²˜ëŠ” (íŒŒì¼ëª…, p.í˜ì´ì§€ë²ˆí˜¸) í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”"""
    
    logger.info(f"ğŸ“ ìš”ì•½ ëª¨ë“œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ: ì›ë¬¸ {len(prepared_prompt)}ì + ì§€ì‹œì‚¬í•­")
else:
    # ì¼ë°˜ ëª¨ë“œ: ì›ë³¸ ë©”ì‹œì§€ ë˜ëŠ” prepared_prompt
    user_message_content = prepared_prompt if prepared_prompt else message

llm_messages.append({"role": "user", "content": user_message_content})
```

**íš¨ê³¼**:
- âœ… ìš”ì•½ ëª¨ë“œ: ì›ë¬¸ ì „ì²´ + "ìš”ì•½í•˜ì„¸ìš”" ëª…ë ¹
- âœ… ì¼ë°˜ ëª¨ë“œ: ê¸°ì¡´ ë¡œì§ ìœ ì§€
- âœ… ëª…í™•í•œ ìš”ì•½ ì§€ì¹¨ ì œê³µ

---

## ğŸ“Š ì²˜ë¦¬ íë¦„ ë¹„êµ

### Before (ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸)

```
ì§ˆì˜: "ì„ íƒëœ ë…¼ë¬¸ ìš”ì•½í•´ ì£¼ì„¸ìš”"
  â†“
í‚¤ì›Œë“œ ì¶”ì¶œ: ['ì„ íƒ', 'ë…¼ë¬¸ìš”ì•½', 'ë…¼ë¬¸', 'ìš”ì•½']  â† ë¶€ì ì ˆ
  â†“
ë²¡í„° ê²€ìƒ‰ (threshold=0.30): 0ê±´
  â”œâ”€ ì¬ì‹œë„ (threshold=0.25): 0ê±´
  â””â”€ ì¬ì‹œë„ (threshold=0.20): 0ê±´
  â†“
í‚¤ì›Œë“œ ê²€ìƒ‰: 0ê±´
  â†“
ì „ë¬¸ê²€ìƒ‰: 0ê±´
  â†“
ê²€ìƒ‰ ì‹œê°„: 1.14ì´ˆ  â† ë‚­ë¹„
  â†“
LLM: "ë…¼ë¬¸ ì›ë¬¸ì— ì ‘ê·¼í•´ì•¼ ì •í™•í•œ ìš”ì•½ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤..." âŒ
```

### After (ì§ì ‘ ë¡œë“œ)

```
ì§ˆì˜: "ì„ íƒëœ ë…¼ë¬¸ ìš”ì•½í•´ ì£¼ì„¸ìš”"
  â†“
ì˜ë„ ë¶„ë¥˜: summarization (0.90 confidence)
  â†“
ìš”ì•½ íŒŒì´í”„ë¼ì¸ ë¶„ê¸° âœ…
  â†“
DB ì§ì ‘ ì¡°íšŒ: SELECT * FROM tb_document_chunks WHERE file_id = 5
  â”œâ”€ ORDER BY page_number, chunk_index
  â””â”€ LIMIT 50
  â†“
ë¡œë“œ ì‹œê°„: ~0.05ì´ˆ  â† ë¹ ë¦„
  â†“
ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±:
  [íŒŒì¼ëª… - p.1]
  ì²« ë²ˆì§¸ chunk ë‚´ìš©...
  
  ---
  
  [íŒŒì¼ëª… - p.2]
  ë‘ ë²ˆì§¸ chunk ë‚´ìš©...
  â†“
LLM í”„ë¡¬í”„íŠ¸:
  "ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”
   - í•µì‹¬ ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
   - ì£¼ìš” ê°œë…, ë°©ë²•ë¡ , ê²°ë¡  í¬í•¨
   - ì¶œì²˜ í‘œê¸°: (íŒŒì¼ëª…, p.í˜ì´ì§€)"
  â†“
LLM: [ì •í™•í•œ ìš”ì•½ ìƒì„±] âœ…
```

---

## ğŸ¯ ê¸°ëŒ€ íš¨ê³¼

### 1. ì •í™•ì„± í–¥ìƒ
- âœ… **ì˜ë„ ì¡´ì¤‘**: ì‚¬ìš©ìê°€ "ìš”ì•½"ì„ ì›í•˜ë©´ ìš”ì•½ ìˆ˜í–‰
- âœ… **ë¬¸ì„œ í™œìš©**: ì„ íƒ ë¬¸ì„œì˜ ë‚´ìš©ì„ 100% í™œìš©
- âœ… **ì˜¤ë‹µ ë°©ì§€**: "ì›ë¬¸ ì—†ìŒ" ê°™ì€ ë¶€ì ì ˆí•œ ë‹µë³€ ì œê±°

### 2. ì„±ëŠ¥ ê°œì„ 
- âœ… **ê²€ìƒ‰ ìƒëµ**: ë¶ˆí•„ìš”í•œ ë²¡í„°/í‚¤ì›Œë“œ/ì „ë¬¸ê²€ìƒ‰ ìƒëµ
- âœ… **ì‘ë‹µ ì†ë„**: 1.14ì´ˆ â†’ 0.05ì´ˆ (ì•½ 20ë°° ë¹ ë¦„)
- âœ… **ë¦¬ì†ŒìŠ¤ ì ˆì•½**: ì„ë² ë”© ìƒì„±, ìœ ì‚¬ë„ ê³„ì‚° ë¶ˆí•„ìš”

### 3. ì‚¬ìš©ì ê²½í—˜ ê°œì„ 
- âœ… **ì¦‰ê° ì‘ë‹µ**: ê²€ìƒ‰ ì§€ì—° ì—†ì´ ë¹ ë¥¸ ìš”ì•½
- âœ… **ì™„ì „í•œ ìš”ì•½**: ë¬¸ì„œ ì „ì²´ ë‚´ìš© ê¸°ë°˜
- âœ… **ëª…í™•í•œ ì¶œì²˜**: í˜ì´ì§€ ë²ˆí˜¸ì™€ í•¨ê»˜ ì œê³µ

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë‹¨ì¼ ë…¼ë¬¸ ìš”ì•½
```
ì…ë ¥: "ì„ íƒëœ ë…¼ë¬¸ ìš”ì•½í•´ ì£¼ì„¸ìš”"
ì„ íƒ ë¬¸ì„œ: file_id=5 (ë…¼ë¬¸ 2)

ê¸°ëŒ€ ê²°ê³¼:
âœ… ì˜ë„ ë¶„ë¥˜: summarization
âœ… ê²€ìƒ‰ ìƒëµ, ì§ì ‘ ë¡œë“œ
âœ… 50ê°œ chunk ë¡œë“œ (ë˜ëŠ” ë¬¸ì„œ ì „ì²´)
âœ… ì²´ê³„ì ì¸ ìš”ì•½ ìƒì„±
âœ… ì¶œì²˜ í‘œê¸°: (ë…¼ë¬¸ 2, p.3)
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì—¬ëŸ¬ ë…¼ë¬¸ ìš”ì•½
```
ì…ë ¥: "ì„ íƒëœ ë…¼ë¬¸ë“¤ì„ ë¹„êµ ìš”ì•½í•´ ì£¼ì„¸ìš”"
ì„ íƒ ë¬¸ì„œ: file_id=5, 7, 9

ê¸°ëŒ€ ê²°ê³¼:
âœ… ì˜ë„ ë¶„ë¥˜: summarization
âœ… 3ê°œ ë¬¸ì„œ ëª¨ë‘ ë¡œë“œ
âœ… ë¬¸ì„œë³„ ìš”ì•½ + ë¹„êµ ë¶„ì„
âœ… ê° ë¬¸ì„œ ì¶œì²˜ êµ¬ë¶„
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: Chunk ì—†ëŠ” ë¬¸ì„œ
```
ì…ë ¥: "ì„ íƒëœ ë…¼ë¬¸ ìš”ì•½í•´ ì£¼ì„¸ìš”"
ì„ íƒ ë¬¸ì„œ: file_id=99 (chunk ì—†ìŒ)

ê¸°ëŒ€ ê²°ê³¼:
âœ… ì˜ë„ ë¶„ë¥˜: summarization
âœ… DB ì¡°íšŒ: 0ê±´
âœ… ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€:
   "ì„ íƒí•˜ì‹  ë¬¸ì„œì˜ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    - (íŒŒì¼ëª…)
    ì´ ë¬¸ì„œê°€ ì•„ì§ ì²˜ë¦¬ ì¤‘ì´ê±°ë‚˜, ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
```

---

## ğŸ“ ì¶”ê°€ ê°œì„  ê°€ëŠ¥ ì‚¬í•­

### 1. ë‹¤ë¥¸ ì˜ë„ íƒ€ì… ì§€ì›
í˜„ì¬ëŠ” `summarization`ë§Œ ì²˜ë¦¬í•˜ì§€ë§Œ, ë‹¤ë¥¸ ì˜ë„ë„ ì „ìš© íŒŒì´í”„ë¼ì¸ ì¶”ê°€ ê°€ëŠ¥:

```python
if classification.query_type == 'summarization':
    return await self._load_documents_for_summarization(...)

elif classification.query_type == 'comparison':
    return await self._load_documents_for_comparison(...)

elif classification.query_type == 'translation':
    return await self._load_documents_for_translation(...)
```

### 2. ìš”ì•½ í’ˆì§ˆ í–¥ìƒ
- ë¬¸ì„œ ê¸¸ì´ì— ë”°ë¼ chunk ìˆ˜ ë™ì  ì¡°ì •
- ì¤‘ìš” ì„¹ì…˜ ìš°ì„  ë¡œë“œ (ì´ˆë¡, ê²°ë¡  ë“±)
- ê³„ì¸µì  ìš”ì•½ (ë¬¸ë‹¨ â†’ ì„¹ì…˜ â†’ ì „ì²´)

### 3. ìºì‹±
- ê°™ì€ ë¬¸ì„œì— ëŒ€í•œ ìš”ì•½ ìš”ì²­ ì‹œ ìºì‹œ í™œìš©
- Redisì— ìš”ì•½ ê²°ê³¼ ì €ì¥

---

## âœ… ê²€ì¦ ì™„ë£Œ

- [x] ì˜ë„ ë¶„ë¥˜ ê²°ê³¼ í™œìš©
- [x] ìš”ì•½ ëª¨ë“œ ë¶„ê¸° ë¡œì§
- [x] ì§ì ‘ ë¬¸ì„œ ë¡œë“œ ë©”ì„œë“œ
- [x] ì˜¤ë¥˜ ì²˜ë¦¬ (chunk ì—†ì„ ë•Œ)
- [x] ìš”ì•½ ì „ìš© í”„ë¡¬í”„íŠ¸
- [x] ë©”íƒ€ë°ì´í„° í”Œë˜ê·¸ (`summarization_mode`)
- [ ] ì‹¤ì œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ (í™•ì¸ í•„ìš”)

---

## ğŸ‰ ê²°ë¡ 

ì‚¬ìš©ìì˜ ì§€ì ì´ ì •í™•í–ˆìŠµë‹ˆë‹¤:
1. âœ… **ì˜ë„ ë¶„ë¥˜ëŠ” ì •í™•í•¨** (`summarization`)
2. âœ… **ê²€ìƒ‰ì€ ë¶ˆí•„ìš”í•¨** (ì„ íƒ ë¬¸ì„œê°€ ì´ë¯¸ ìˆìŒ)
3. âœ… **ìš”ì•½ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¶„ê¸°í•´ì•¼ í•¨**

ì´ì œ ì‹œìŠ¤í…œì´ ì˜ë„ì— ë§ê²Œ ë™ì‘í•©ë‹ˆë‹¤!
